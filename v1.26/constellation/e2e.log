I0811 14:02:20.587259      21 e2e.go:126] Starting e2e run "e9629551-98de-4270-b96a-6c3e0c707863" on Ginkgo node 1
Aug 11 14:02:20.602: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1691762540 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 11 14:02:20.700: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:02:20.701: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 11 14:02:20.716: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 11 14:02:20.756: INFO: 65 / 65 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 11 14:02:20.756: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Aug 11 14:02:20.756: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-gce-pd-node' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'gcp-guest-agent' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'join-service' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'key-service' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'verification-service' (0 seconds elapsed)
Aug 11 14:02:20.762: INFO: e2e test version: v1.26.6
Aug 11 14:02:20.762: INFO: kube-apiserver version: v1.26.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 11 14:02:20.763: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:02:20.766: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.067 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 11 14:02:20.700: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:02:20.701: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 11 14:02:20.716: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 11 14:02:20.756: INFO: 65 / 65 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 11 14:02:20.756: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
    Aug 11 14:02:20.756: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-gce-pd-node' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'gcp-guest-agent' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'join-service' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'key-service' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'verification-service' (0 seconds elapsed)
    Aug 11 14:02:20.762: INFO: e2e test version: v1.26.6
    Aug 11 14:02:20.762: INFO: kube-apiserver version: v1.26.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 11 14:02:20.763: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:02:20.766: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:02:20.793
Aug 11 14:02:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:02:20.794
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:20.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:20.811
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 08/11/23 14:02:20.827
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:02:20.833
Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:20.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:20.842: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:21.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:21.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:22.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:22.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:02:23.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:23.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:23.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:23.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:23.852: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:24.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:24.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:02:25.850: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/11/23 14:02:25.853
Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:25.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:02:25.873: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:26.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:02:26.881: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:02:27.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:02:27.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/11/23 14:02:27.881
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:02:27.886
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9659, will wait for the garbage collector to delete the pods 08/11/23 14:02:27.886
Aug 11 14:02:27.945: INFO: Deleting DaemonSet.extensions daemon-set took: 6.121136ms
Aug 11 14:02:28.046: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.029649ms
Aug 11 14:02:35.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:02:35.249: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:02:35.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5713"},"items":null}

Aug 11 14:02:35.257: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5713"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:02:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9659" for this suite. 08/11/23 14:02:35.269
------------------------------
• [SLOW TEST] [14.482 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:02:20.793
    Aug 11 14:02:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:02:20.794
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:20.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:20.811
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 08/11/23 14:02:20.827
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:02:20.833
    Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:20.839: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:20.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:20.842: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:21.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:21.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:21.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:22.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:22.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:22.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:02:23.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:23.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:23.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:23.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:23.852: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:24.846: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:24.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:24.849: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.847: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:02:25.850: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/11/23 14:02:25.853
    Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.868: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:25.873: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:02:25.873: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:26.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:26.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:02:26.881: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:27.878: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:02:27.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:02:27.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/11/23 14:02:27.881
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:02:27.886
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9659, will wait for the garbage collector to delete the pods 08/11/23 14:02:27.886
    Aug 11 14:02:27.945: INFO: Deleting DaemonSet.extensions daemon-set took: 6.121136ms
    Aug 11 14:02:28.046: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.029649ms
    Aug 11 14:02:35.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:02:35.249: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:02:35.254: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5713"},"items":null}

    Aug 11 14:02:35.257: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5713"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:02:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9659" for this suite. 08/11/23 14:02:35.269
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:02:35.275
Aug 11 14:02:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename lease-test 08/11/23 14:02:35.276
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:35.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:35.291
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 11 14:02:35.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-8329" for this suite. 08/11/23 14:02:35.345
------------------------------
• [0.078 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:02:35.275
    Aug 11 14:02:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename lease-test 08/11/23 14:02:35.276
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:35.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:35.291
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:02:35.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-8329" for this suite. 08/11/23 14:02:35.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:02:35.354
Aug 11 14:02:35.354: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:02:35.354
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:35.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:35.372
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:02:35.374
Aug 11 14:02:35.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 11 14:02:35.455: INFO: stderr: ""
Aug 11 14:02:35.455: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/11/23 14:02:35.455
Aug 11 14:02:35.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 11 14:02:36.236: INFO: stderr: ""
Aug 11 14:02:36.236: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:02:36.236
Aug 11 14:02:36.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 delete pods e2e-test-httpd-pod'
Aug 11 14:02:39.091: INFO: stderr: ""
Aug 11 14:02:39.091: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:02:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3503" for this suite. 08/11/23 14:02:39.097
------------------------------
• [3.749 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:02:35.354
    Aug 11 14:02:35.354: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:02:35.354
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:35.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:35.372
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:02:35.374
    Aug 11 14:02:35.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 11 14:02:35.455: INFO: stderr: ""
    Aug 11 14:02:35.455: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/11/23 14:02:35.455
    Aug 11 14:02:35.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 11 14:02:36.236: INFO: stderr: ""
    Aug 11 14:02:36.236: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:02:36.236
    Aug 11 14:02:36.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3503 delete pods e2e-test-httpd-pod'
    Aug 11 14:02:39.091: INFO: stderr: ""
    Aug 11 14:02:39.091: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:02:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3503" for this suite. 08/11/23 14:02:39.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:02:39.103
Aug 11 14:02:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:02:39.104
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:39.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:39.121
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6064 08/11/23 14:02:39.123
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-6064 08/11/23 14:02:39.127
Aug 11 14:02:39.140: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:02:49.652: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/11/23 14:02:49.658
STEP: updating a scale subresource 08/11/23 14:02:49.66
STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:02:50.204
STEP: Patch a scale subresource 08/11/23 14:02:50.576
STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:02:50.587
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:02:50.592: INFO: Deleting all statefulset in ns statefulset-6064
Aug 11 14:02:50.602: INFO: Scaling statefulset ss to 0
Aug 11 14:03:00.622: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:03:00.625: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:03:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6064" for this suite. 08/11/23 14:03:00.647
------------------------------
• [SLOW TEST] [21.549 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:02:39.103
    Aug 11 14:02:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:02:39.104
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:39.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:39.121
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6064 08/11/23 14:02:39.123
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-6064 08/11/23 14:02:39.127
    Aug 11 14:02:39.140: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:02:49.652: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/11/23 14:02:49.658
    STEP: updating a scale subresource 08/11/23 14:02:49.66
    STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:02:50.204
    STEP: Patch a scale subresource 08/11/23 14:02:50.576
    STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:02:50.587
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:02:50.592: INFO: Deleting all statefulset in ns statefulset-6064
    Aug 11 14:02:50.602: INFO: Scaling statefulset ss to 0
    Aug 11 14:03:00.622: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:03:00.625: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:03:00.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6064" for this suite. 08/11/23 14:03:00.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:03:00.652
Aug 11 14:03:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename cronjob 08/11/23 14:03:00.653
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:00.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:00.668
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/11/23 14:03:00.671
STEP: Ensuring a job is scheduled 08/11/23 14:03:00.676
STEP: Ensuring exactly one is scheduled 08/11/23 14:04:00.681
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:04:00.683
STEP: Ensuring no more jobs are scheduled 08/11/23 14:04:00.686
STEP: Removing cronjob 08/11/23 14:09:00.692
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:00.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-842" for this suite. 08/11/23 14:09:00.702
------------------------------
• [SLOW TEST] [360.056 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:03:00.652
    Aug 11 14:03:00.653: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:03:00.653
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:00.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:00.668
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/11/23 14:03:00.671
    STEP: Ensuring a job is scheduled 08/11/23 14:03:00.676
    STEP: Ensuring exactly one is scheduled 08/11/23 14:04:00.681
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:04:00.683
    STEP: Ensuring no more jobs are scheduled 08/11/23 14:04:00.686
    STEP: Removing cronjob 08/11/23 14:09:00.692
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:00.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-842" for this suite. 08/11/23 14:09:00.702
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:00.71
Aug 11 14:09:00.710: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:09:00.711
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:00.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:00.733
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/11/23 14:09:00.735
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:00.749
STEP: Creating a pod in the namespace 08/11/23 14:09:00.751
STEP: Waiting for the pod to have running status 08/11/23 14:09:00.758
Aug 11 14:09:00.758: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4735" to be "running"
Aug 11 14:09:00.763: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201645ms
Aug 11 14:09:02.768: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009701477s
Aug 11 14:09:02.768: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/11/23 14:09:02.768
STEP: Waiting for the namespace to be removed. 08/11/23 14:09:02.777
STEP: Recreating the namespace 08/11/23 14:09:13.781
STEP: Verifying there are no pods in the namespace 08/11/23 14:09:13.8
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2712" for this suite. 08/11/23 14:09:13.806
STEP: Destroying namespace "nsdeletetest-4735" for this suite. 08/11/23 14:09:13.812
Aug 11 14:09:13.815: INFO: Namespace nsdeletetest-4735 was already deleted
STEP: Destroying namespace "nsdeletetest-1377" for this suite. 08/11/23 14:09:13.815
------------------------------
• [SLOW TEST] [13.111 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:00.71
    Aug 11 14:09:00.710: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:09:00.711
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:00.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:00.733
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/11/23 14:09:00.735
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:00.749
    STEP: Creating a pod in the namespace 08/11/23 14:09:00.751
    STEP: Waiting for the pod to have running status 08/11/23 14:09:00.758
    Aug 11 14:09:00.758: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4735" to be "running"
    Aug 11 14:09:00.763: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201645ms
    Aug 11 14:09:02.768: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009701477s
    Aug 11 14:09:02.768: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/11/23 14:09:02.768
    STEP: Waiting for the namespace to be removed. 08/11/23 14:09:02.777
    STEP: Recreating the namespace 08/11/23 14:09:13.781
    STEP: Verifying there are no pods in the namespace 08/11/23 14:09:13.8
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:13.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2712" for this suite. 08/11/23 14:09:13.806
    STEP: Destroying namespace "nsdeletetest-4735" for this suite. 08/11/23 14:09:13.812
    Aug 11 14:09:13.815: INFO: Namespace nsdeletetest-4735 was already deleted
    STEP: Destroying namespace "nsdeletetest-1377" for this suite. 08/11/23 14:09:13.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:13.822
Aug 11 14:09:13.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:09:13.823
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:13.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:13.842
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/11/23 14:09:13.844
STEP: Creating a ResourceQuota 08/11/23 14:09:18.847
STEP: Ensuring resource quota status is calculated 08/11/23 14:09:18.852
STEP: Creating a Service 08/11/23 14:09:20.857
STEP: Creating a NodePort Service 08/11/23 14:09:20.877
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/11/23 14:09:20.905
STEP: Ensuring resource quota status captures service creation 08/11/23 14:09:20.938
STEP: Deleting Services 08/11/23 14:09:22.942
STEP: Ensuring resource quota status released usage 08/11/23 14:09:22.987
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:24.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8356" for this suite. 08/11/23 14:09:24.995
------------------------------
• [SLOW TEST] [11.180 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:13.822
    Aug 11 14:09:13.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:09:13.823
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:13.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:13.842
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/11/23 14:09:13.844
    STEP: Creating a ResourceQuota 08/11/23 14:09:18.847
    STEP: Ensuring resource quota status is calculated 08/11/23 14:09:18.852
    STEP: Creating a Service 08/11/23 14:09:20.857
    STEP: Creating a NodePort Service 08/11/23 14:09:20.877
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/11/23 14:09:20.905
    STEP: Ensuring resource quota status captures service creation 08/11/23 14:09:20.938
    STEP: Deleting Services 08/11/23 14:09:22.942
    STEP: Ensuring resource quota status released usage 08/11/23 14:09:22.987
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:24.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8356" for this suite. 08/11/23 14:09:24.995
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:25.002
Aug 11 14:09:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 14:09:25.003
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:25.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:25.018
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4478.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/11/23 14:09:25.021
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4478.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/11/23 14:09:25.021
STEP: creating a pod to probe /etc/hosts 08/11/23 14:09:25.021
STEP: submitting the pod to kubernetes 08/11/23 14:09:25.021
Aug 11 14:09:25.030: INFO: Waiting up to 15m0s for pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459" in namespace "dns-4478" to be "running"
Aug 11 14:09:25.033: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911603ms
Aug 11 14:09:27.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007678853s
Aug 11 14:09:29.039: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00965694s
Aug 11 14:09:31.036: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006405663s
Aug 11 14:09:33.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007447699s
Aug 11 14:09:35.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Running", Reason="", readiness=true. Elapsed: 10.007131186s
Aug 11 14:09:35.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459" satisfied condition "running"
STEP: retrieving the pod 08/11/23 14:09:35.037
STEP: looking for the results for each expected name from probers 08/11/23 14:09:35.04
Aug 11 14:09:35.073: INFO: DNS probes using dns-4478/dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459 succeeded

STEP: deleting the pod 08/11/23 14:09:35.073
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:35.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4478" for this suite. 08/11/23 14:09:35.092
------------------------------
• [SLOW TEST] [10.095 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:25.002
    Aug 11 14:09:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 14:09:25.003
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:25.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:25.018
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4478.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/11/23 14:09:25.021
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4478.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4478.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/11/23 14:09:25.021
    STEP: creating a pod to probe /etc/hosts 08/11/23 14:09:25.021
    STEP: submitting the pod to kubernetes 08/11/23 14:09:25.021
    Aug 11 14:09:25.030: INFO: Waiting up to 15m0s for pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459" in namespace "dns-4478" to be "running"
    Aug 11 14:09:25.033: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.911603ms
    Aug 11 14:09:27.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007678853s
    Aug 11 14:09:29.039: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00965694s
    Aug 11 14:09:31.036: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006405663s
    Aug 11 14:09:33.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007447699s
    Aug 11 14:09:35.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459": Phase="Running", Reason="", readiness=true. Elapsed: 10.007131186s
    Aug 11 14:09:35.037: INFO: Pod "dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 14:09:35.037
    STEP: looking for the results for each expected name from probers 08/11/23 14:09:35.04
    Aug 11 14:09:35.073: INFO: DNS probes using dns-4478/dns-test-56b93e9f-13db-4fba-85fb-d7b9df96f459 succeeded

    STEP: deleting the pod 08/11/23 14:09:35.073
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:35.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4478" for this suite. 08/11/23 14:09:35.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:35.099
Aug 11 14:09:35.099: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:09:35.1
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:35.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:35.116
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/11/23 14:09:35.118
STEP: wait for the container to reach Succeeded 08/11/23 14:09:35.126
STEP: get the container status 08/11/23 14:09:42.158
STEP: the container should be terminated 08/11/23 14:09:42.16
STEP: the termination message should be set 08/11/23 14:09:42.16
Aug 11 14:09:42.160: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/11/23 14:09:42.16
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:42.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7440" for this suite. 08/11/23 14:09:42.179
------------------------------
• [SLOW TEST] [7.086 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:35.099
    Aug 11 14:09:35.099: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:09:35.1
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:35.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:35.116
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/11/23 14:09:35.118
    STEP: wait for the container to reach Succeeded 08/11/23 14:09:35.126
    STEP: get the container status 08/11/23 14:09:42.158
    STEP: the container should be terminated 08/11/23 14:09:42.16
    STEP: the termination message should be set 08/11/23 14:09:42.16
    Aug 11 14:09:42.160: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/11/23 14:09:42.16
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:42.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7440" for this suite. 08/11/23 14:09:42.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:42.185
Aug 11 14:09:42.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:09:42.186
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:42.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:42.206
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/11/23 14:09:42.208
Aug 11 14:09:42.209: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:09:44.188: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:09:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4690" for this suite. 08/11/23 14:09:51.293
------------------------------
• [SLOW TEST] [9.114 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:42.185
    Aug 11 14:09:42.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:09:42.186
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:42.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:42.206
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/11/23 14:09:42.208
    Aug 11 14:09:42.209: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:09:44.188: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:09:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4690" for this suite. 08/11/23 14:09:51.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:09:51.301
Aug 11 14:09:51.302: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:09:51.302
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:51.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:51.319
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-1895 08/11/23 14:09:51.321
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[] 08/11/23 14:09:51.338
Aug 11 14:09:51.342: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 11 14:09:52.352: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1895 08/11/23 14:09:52.352
Aug 11 14:09:52.361: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1895" to be "running and ready"
Aug 11 14:09:52.366: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916955ms
Aug 11 14:09:52.366: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:09:54.370: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009366344s
Aug 11 14:09:54.370: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 14:09:54.370: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod1:[100]] 08/11/23 14:09:54.373
Aug 11 14:09:54.381: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1895 08/11/23 14:09:54.381
Aug 11 14:09:54.386: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1895" to be "running and ready"
Aug 11 14:09:54.390: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217894ms
Aug 11 14:09:54.390: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:09:56.394: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008417713s
Aug 11 14:09:56.394: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:09:58.394: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008781069s
Aug 11 14:09:58.394: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 14:09:58.394: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod1:[100] pod2:[101]] 08/11/23 14:09:58.397
Aug 11 14:09:58.407: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/11/23 14:09:58.407
Aug 11 14:09:58.407: INFO: Creating new exec pod
Aug 11 14:09:58.414: INFO: Waiting up to 5m0s for pod "execpodlhvdl" in namespace "services-1895" to be "running"
Aug 11 14:09:58.416: INFO: Pod "execpodlhvdl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750672ms
Aug 11 14:10:00.421: INFO: Pod "execpodlhvdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007436502s
Aug 11 14:10:00.421: INFO: Pod "execpodlhvdl" satisfied condition "running"
Aug 11 14:10:01.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 11 14:10:01.551: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:01.551: INFO: stdout: ""
Aug 11 14:10:01.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 10.102.72.245 80'
Aug 11 14:10:01.681: INFO: stderr: "+ nc -v -z -w 2 10.102.72.245 80\nConnection to 10.102.72.245 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:01.681: INFO: stdout: ""
Aug 11 14:10:01.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 11 14:10:01.807: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 11 14:10:01.807: INFO: stdout: ""
Aug 11 14:10:01.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 10.102.72.245 81'
Aug 11 14:10:01.934: INFO: stderr: "+ nc -v -z -w 2 10.102.72.245 81\nConnection to 10.102.72.245 81 port [tcp/*] succeeded!\n"
Aug 11 14:10:01.934: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-1895 08/11/23 14:10:01.934
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod2:[101]] 08/11/23 14:10:01.953
Aug 11 14:10:01.963: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1895 08/11/23 14:10:01.963
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[] 08/11/23 14:10:01.978
Aug 11 14:10:01.992: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:02.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1895" for this suite. 08/11/23 14:10:02.024
------------------------------
• [SLOW TEST] [10.729 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:09:51.301
    Aug 11 14:09:51.302: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:09:51.302
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:51.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:51.319
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-1895 08/11/23 14:09:51.321
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[] 08/11/23 14:09:51.338
    Aug 11 14:09:51.342: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 11 14:09:52.352: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1895 08/11/23 14:09:52.352
    Aug 11 14:09:52.361: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1895" to be "running and ready"
    Aug 11 14:09:52.366: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916955ms
    Aug 11 14:09:52.366: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:09:54.370: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009366344s
    Aug 11 14:09:54.370: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 14:09:54.370: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod1:[100]] 08/11/23 14:09:54.373
    Aug 11 14:09:54.381: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-1895 08/11/23 14:09:54.381
    Aug 11 14:09:54.386: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1895" to be "running and ready"
    Aug 11 14:09:54.390: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.217894ms
    Aug 11 14:09:54.390: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:09:56.394: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008417713s
    Aug 11 14:09:56.394: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:09:58.394: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008781069s
    Aug 11 14:09:58.394: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 14:09:58.394: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod1:[100] pod2:[101]] 08/11/23 14:09:58.397
    Aug 11 14:09:58.407: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/11/23 14:09:58.407
    Aug 11 14:09:58.407: INFO: Creating new exec pod
    Aug 11 14:09:58.414: INFO: Waiting up to 5m0s for pod "execpodlhvdl" in namespace "services-1895" to be "running"
    Aug 11 14:09:58.416: INFO: Pod "execpodlhvdl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750672ms
    Aug 11 14:10:00.421: INFO: Pod "execpodlhvdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007436502s
    Aug 11 14:10:00.421: INFO: Pod "execpodlhvdl" satisfied condition "running"
    Aug 11 14:10:01.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 11 14:10:01.551: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:01.551: INFO: stdout: ""
    Aug 11 14:10:01.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 10.102.72.245 80'
    Aug 11 14:10:01.681: INFO: stderr: "+ nc -v -z -w 2 10.102.72.245 80\nConnection to 10.102.72.245 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:01.681: INFO: stdout: ""
    Aug 11 14:10:01.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 11 14:10:01.807: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 11 14:10:01.807: INFO: stdout: ""
    Aug 11 14:10:01.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-1895 exec execpodlhvdl -- /bin/sh -x -c nc -v -z -w 2 10.102.72.245 81'
    Aug 11 14:10:01.934: INFO: stderr: "+ nc -v -z -w 2 10.102.72.245 81\nConnection to 10.102.72.245 81 port [tcp/*] succeeded!\n"
    Aug 11 14:10:01.934: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-1895 08/11/23 14:10:01.934
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[pod2:[101]] 08/11/23 14:10:01.953
    Aug 11 14:10:01.963: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-1895 08/11/23 14:10:01.963
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1895 to expose endpoints map[] 08/11/23 14:10:01.978
    Aug 11 14:10:01.992: INFO: successfully validated that service multi-endpoint-test in namespace services-1895 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:02.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1895" for this suite. 08/11/23 14:10:02.024
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:02.031
Aug 11 14:10:02.031: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:10:02.032
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:02.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:02.051
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8071 08/11/23 14:10:02.053
STEP: creating service affinity-nodeport-transition in namespace services-8071 08/11/23 14:10:02.054
STEP: creating replication controller affinity-nodeport-transition in namespace services-8071 08/11/23 14:10:02.073
I0811 14:10:02.081366      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8071, replica count: 3
I0811 14:10:05.133541      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:10:05.143: INFO: Creating new exec pod
Aug 11 14:10:05.151: INFO: Waiting up to 5m0s for pod "execpod-affinityctp9k" in namespace "services-8071" to be "running"
Aug 11 14:10:05.154: INFO: Pod "execpod-affinityctp9k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478053ms
Aug 11 14:10:07.159: INFO: Pod "execpod-affinityctp9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.007538903s
Aug 11 14:10:07.159: INFO: Pod "execpod-affinityctp9k" satisfied condition "running"
Aug 11 14:10:08.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 11 14:10:08.294: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:08.294: INFO: stdout: ""
Aug 11 14:10:08.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 10.111.147.81 80'
Aug 11 14:10:08.417: INFO: stderr: "+ nc -v -z -w 2 10.111.147.81 80\nConnection to 10.111.147.81 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:08.417: INFO: stdout: ""
Aug 11 14:10:08.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 31713'
Aug 11 14:10:08.532: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 31713\nConnection to 192.168.178.2 31713 port [tcp/*] succeeded!\n"
Aug 11 14:10:08.532: INFO: stdout: ""
Aug 11 14:10:08.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 31713'
Aug 11 14:10:08.652: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 31713\nConnection to 192.168.178.3 31713 port [tcp/*] succeeded!\n"
Aug 11 14:10:08.653: INFO: stdout: ""
Aug 11 14:10:08.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31713/ ; done'
Aug 11 14:10:08.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n"
Aug 11 14:10:08.848: INFO: stdout: "\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm"
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
Aug 11 14:10:08.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31713/ ; done'
Aug 11 14:10:09.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n"
Aug 11 14:10:09.048: INFO: stdout: "\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r"
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
Aug 11 14:10:09.048: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8071, will wait for the garbage collector to delete the pods 08/11/23 14:10:09.065
Aug 11 14:10:09.125: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.107886ms
Aug 11 14:10:09.225: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.499489ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:11.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8071" for this suite. 08/11/23 14:10:11.06
------------------------------
• [SLOW TEST] [9.035 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:02.031
    Aug 11 14:10:02.031: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:10:02.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:02.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:02.051
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8071 08/11/23 14:10:02.053
    STEP: creating service affinity-nodeport-transition in namespace services-8071 08/11/23 14:10:02.054
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8071 08/11/23 14:10:02.073
    I0811 14:10:02.081366      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8071, replica count: 3
    I0811 14:10:05.133541      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:10:05.143: INFO: Creating new exec pod
    Aug 11 14:10:05.151: INFO: Waiting up to 5m0s for pod "execpod-affinityctp9k" in namespace "services-8071" to be "running"
    Aug 11 14:10:05.154: INFO: Pod "execpod-affinityctp9k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478053ms
    Aug 11 14:10:07.159: INFO: Pod "execpod-affinityctp9k": Phase="Running", Reason="", readiness=true. Elapsed: 2.007538903s
    Aug 11 14:10:07.159: INFO: Pod "execpod-affinityctp9k" satisfied condition "running"
    Aug 11 14:10:08.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 11 14:10:08.294: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:08.294: INFO: stdout: ""
    Aug 11 14:10:08.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 10.111.147.81 80'
    Aug 11 14:10:08.417: INFO: stderr: "+ nc -v -z -w 2 10.111.147.81 80\nConnection to 10.111.147.81 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:08.417: INFO: stdout: ""
    Aug 11 14:10:08.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 31713'
    Aug 11 14:10:08.532: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 31713\nConnection to 192.168.178.2 31713 port [tcp/*] succeeded!\n"
    Aug 11 14:10:08.532: INFO: stdout: ""
    Aug 11 14:10:08.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 31713'
    Aug 11 14:10:08.652: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 31713\nConnection to 192.168.178.3 31713 port [tcp/*] succeeded!\n"
    Aug 11 14:10:08.653: INFO: stdout: ""
    Aug 11 14:10:08.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31713/ ; done'
    Aug 11 14:10:08.847: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n"
    Aug 11 14:10:08.848: INFO: stdout: "\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-52wrm\naffinity-nodeport-transition-6fvx2\naffinity-nodeport-transition-52wrm"
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-6fvx2
    Aug 11 14:10:08.848: INFO: Received response from host: affinity-nodeport-transition-52wrm
    Aug 11 14:10:08.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8071 exec execpod-affinityctp9k -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31713/ ; done'
    Aug 11 14:10:09.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31713/\n"
    Aug 11 14:10:09.048: INFO: stdout: "\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r\naffinity-nodeport-transition-gkk6r"
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Received response from host: affinity-nodeport-transition-gkk6r
    Aug 11 14:10:09.048: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8071, will wait for the garbage collector to delete the pods 08/11/23 14:10:09.065
    Aug 11 14:10:09.125: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.107886ms
    Aug 11 14:10:09.225: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.499489ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:11.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8071" for this suite. 08/11/23 14:10:11.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:11.067
Aug 11 14:10:11.067: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:10:11.067
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:11.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:11.084
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-584ea75f-a584-47da-b494-ebc581212bdc 08/11/23 14:10:11.086
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:11.09
Aug 11 14:10:11.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961" in namespace "projected-8118" to be "Succeeded or Failed"
Aug 11 14:10:11.100: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683143ms
Aug 11 14:10:13.104: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616369s
Aug 11 14:10:15.106: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008082628s
STEP: Saw pod success 08/11/23 14:10:15.106
Aug 11 14:10:15.106: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961" satisfied condition "Succeeded or Failed"
Aug 11 14:10:15.108: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:10:15.13
Aug 11 14:10:15.141: INFO: Waiting for pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 to disappear
Aug 11 14:10:15.143: INFO: Pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:15.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8118" for this suite. 08/11/23 14:10:15.147
------------------------------
• [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:11.067
    Aug 11 14:10:11.067: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:10:11.067
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:11.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:11.084
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-584ea75f-a584-47da-b494-ebc581212bdc 08/11/23 14:10:11.086
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:11.09
    Aug 11 14:10:11.097: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961" in namespace "projected-8118" to be "Succeeded or Failed"
    Aug 11 14:10:11.100: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683143ms
    Aug 11 14:10:13.104: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006616369s
    Aug 11 14:10:15.106: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008082628s
    STEP: Saw pod success 08/11/23 14:10:15.106
    Aug 11 14:10:15.106: INFO: Pod "pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:15.108: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:10:15.13
    Aug 11 14:10:15.141: INFO: Waiting for pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 to disappear
    Aug 11 14:10:15.143: INFO: Pod pod-projected-configmaps-36e0cac0-63e0-46fc-9799-51dd82756961 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:15.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8118" for this suite. 08/11/23 14:10:15.147
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:15.153
Aug 11 14:10:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:10:15.154
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:15.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:15.171
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/11/23 14:10:15.174
STEP: setting up watch 08/11/23 14:10:15.174
STEP: submitting the pod to kubernetes 08/11/23 14:10:15.277
STEP: verifying the pod is in kubernetes 08/11/23 14:10:15.287
STEP: verifying pod creation was observed 08/11/23 14:10:15.29
Aug 11 14:10:15.290: INFO: Waiting up to 5m0s for pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4" in namespace "pods-4759" to be "running"
Aug 11 14:10:15.295: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083545ms
Aug 11 14:10:17.299: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008996363s
Aug 11 14:10:17.299: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 14:10:17.301
STEP: verifying pod deletion was observed 08/11/23 14:10:17.311
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:19.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4759" for this suite. 08/11/23 14:10:19.999
------------------------------
• [4.853 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:15.153
    Aug 11 14:10:15.154: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:10:15.154
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:15.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:15.171
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/11/23 14:10:15.174
    STEP: setting up watch 08/11/23 14:10:15.174
    STEP: submitting the pod to kubernetes 08/11/23 14:10:15.277
    STEP: verifying the pod is in kubernetes 08/11/23 14:10:15.287
    STEP: verifying pod creation was observed 08/11/23 14:10:15.29
    Aug 11 14:10:15.290: INFO: Waiting up to 5m0s for pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4" in namespace "pods-4759" to be "running"
    Aug 11 14:10:15.295: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083545ms
    Aug 11 14:10:17.299: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008996363s
    Aug 11 14:10:17.299: INFO: Pod "pod-submit-remove-db1550d3-8fdb-41b4-b293-ff09b6888ee4" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 14:10:17.301
    STEP: verifying pod deletion was observed 08/11/23 14:10:17.311
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:19.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4759" for this suite. 08/11/23 14:10:19.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:20.007
Aug 11 14:10:20.007: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:10:20.008
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:20.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:20.024
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2697" for this suite. 08/11/23 14:10:20.032
------------------------------
• [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:20.007
    Aug 11 14:10:20.007: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:10:20.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:20.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:20.024
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2697" for this suite. 08/11/23 14:10:20.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:20.038
Aug 11 14:10:20.039: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:10:20.039
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:20.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:20.056
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/11/23 14:10:20.061
STEP: waiting for Deployment to be created 08/11/23 14:10:20.066
STEP: waiting for all Replicas to be Ready 08/11/23 14:10:20.067
Aug 11 14:10:20.068: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.068: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.077: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.077: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.089: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.089: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.111: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.111: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 14:10:20.812: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 11 14:10:20.813: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 11 14:10:21.001: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/11/23 14:10:21.001
W0811 14:10:21.012385      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 14:10:21.014: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/11/23 14:10:21.014
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.026: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.026: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.040: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.040: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.055: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.055: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:21.061: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:21.061: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:22.009: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:22.009: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:22.023: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
STEP: listing Deployments 08/11/23 14:10:22.023
Aug 11 14:10:22.026: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/11/23 14:10:22.026
Aug 11 14:10:22.043: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/11/23 14:10:22.043
Aug 11 14:10:22.049: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:22.049: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:22.062: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:22.079: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:22.087: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:23.028: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:23.050: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:23.059: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 14:10:23.869: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/11/23 14:10:23.888
STEP: fetching the DeploymentStatus 08/11/23 14:10:23.894
Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 3
STEP: deleting the Deployment 08/11/23 14:10:23.899
Aug 11 14:10:23.908: INFO: observed event type MODIFIED
Aug 11 14:10:23.908: INFO: observed event type MODIFIED
Aug 11 14:10:23.908: INFO: observed event type MODIFIED
Aug 11 14:10:23.908: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
Aug 11 14:10:23.909: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:10:23.911: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 11 14:10:23.916: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8120  fab81303-5b6d-44c6-b2c3-bda47d965d75 9214 2 2023-08-11 14:10:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617567 0xc005617568}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056175f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 11 14:10:23.919: INFO: pod: "test-deployment-7b7876f9d6-lp29w":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-lp29w test-deployment-7b7876f9d6- deployment-8120  8f901941-0415-4ba3-a33e-4d4c66362ab8 9213 0 2023-08-11 14:10:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 fab81303-5b6d-44c6-b2c3-bda47d965d75 0xc005466d97 0xc005466d98}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fab81303-5b6d-44c6-b2c3-bda47d965d75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8zwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8zwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.78,StartTime:2023-08-11 14:10:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50a859c9347959440811ac15b6ce492c163a8e76d74d71d73a11de97cdf8bbb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 11 14:10:23.920: INFO: pod: "test-deployment-7b7876f9d6-qqzbv":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-qqzbv test-deployment-7b7876f9d6- deployment-8120  17d1d8d9-9af5-4e3d-b38d-c408e7288ce9 9177 0 2023-08-11 14:10:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 fab81303-5b6d-44c6-b2c3-bda47d965d75 0xc005466f77 0xc005466f78}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fab81303-5b6d-44c6-b2c3-bda47d965d75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzmc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzmc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.178,StartTime:2023-08-11 14:10:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5317109c7611eb57a862ded24be729cd6cbdead9ed1053a883b53bba6079185d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 11 14:10:23.920: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8120  c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 9221 4 2023-08-11 14:10:21 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617657 0xc005617658}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056176e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 11 14:10:23.923: INFO: pod: "test-deployment-7df74c55ff-l9xnk":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-l9xnk test-deployment-7df74c55ff- deployment-8120  d31bb5cc-51be-48d1-bb65-2e2e9fcbf60b 9217 0 2023-08-11 14:10:21 +0000 UTC 2023-08-11 14:10:24 +0000 UTC 0xc005617a90 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 0xc005617ac7 0xc005617ac8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qj98b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qj98b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.123,StartTime:2023-08-11 14:10:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://997ee7641581b5e4708f39239c6fbbc9dcd7b600e449b077ad05e290abb53314,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 11 14:10:23.923: INFO: pod: "test-deployment-7df74c55ff-mxz64":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-mxz64 test-deployment-7df74c55ff- deployment-8120  abd3d653-326e-41a1-9a2f-25a512ef74b3 9209 0 2023-08-11 14:10:22 +0000 UTC 2023-08-11 14:10:24 +0000 UTC 0xc005617c80 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 0xc005617cb7 0xc005617cb8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vtr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vtr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.192,StartTime:2023-08-11 14:10:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://685d2e1bf99a74a24718017180def6c58aff7fea08bef04818c38057d91be12c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 11 14:10:23.923: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8120  5ca0f3ef-73f2-4412-9429-f693058c7996 9130 3 2023-08-11 14:10:20 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617747 0xc005617748}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056177d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 11 14:10:23.928: INFO: pod: "test-deployment-f4dbc4647-8gw4s":
&Pod{ObjectMeta:{test-deployment-f4dbc4647-8gw4s test-deployment-f4dbc4647- deployment-8120  35a99bca-31b2-42a4-b0e9-f3376c37891e 9097 0 2023-08-11 14:10:20 +0000 UTC 2023-08-11 14:10:22 +0000 UTC 0xc001557c70 map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-f4dbc4647 5ca0f3ef-73f2-4412-9429-f693058c7996 0xc001557ca7 0xc001557ca8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ca0f3ef-73f2-4412-9429-f693058c7996\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sp8zk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sp8zk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.145,StartTime:2023-08-11 14:10:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://03ea2f757e8769c0325e0d4a86ab72da703720e0946cbcb069eeeff4ae7b77aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:23.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8120" for this suite. 08/11/23 14:10:23.931
------------------------------
• [3.903 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:20.038
    Aug 11 14:10:20.039: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:10:20.039
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:20.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:20.056
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/11/23 14:10:20.061
    STEP: waiting for Deployment to be created 08/11/23 14:10:20.066
    STEP: waiting for all Replicas to be Ready 08/11/23 14:10:20.067
    Aug 11 14:10:20.068: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.068: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.077: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.077: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.089: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.089: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.111: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.111: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.812: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 11 14:10:20.813: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 11 14:10:21.001: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/11/23 14:10:21.001
    W0811 14:10:21.012385      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 14:10:21.014: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/11/23 14:10:21.014
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 0
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.015: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.026: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.026: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.040: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.040: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.055: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.055: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:21.061: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:21.061: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:22.009: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:22.009: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:22.023: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    STEP: listing Deployments 08/11/23 14:10:22.023
    Aug 11 14:10:22.026: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/11/23 14:10:22.026
    Aug 11 14:10:22.043: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/11/23 14:10:22.043
    Aug 11 14:10:22.049: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:22.049: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:22.062: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:22.079: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:22.087: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:23.028: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:23.050: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:23.059: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 14:10:23.869: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/11/23 14:10:23.888
    STEP: fetching the DeploymentStatus 08/11/23 14:10:23.894
    Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:23.898: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 1
    Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 2
    Aug 11 14:10:23.899: INFO: observed Deployment test-deployment in namespace deployment-8120 with ReadyReplicas 3
    STEP: deleting the Deployment 08/11/23 14:10:23.899
    Aug 11 14:10:23.908: INFO: observed event type MODIFIED
    Aug 11 14:10:23.908: INFO: observed event type MODIFIED
    Aug 11 14:10:23.908: INFO: observed event type MODIFIED
    Aug 11 14:10:23.908: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    Aug 11 14:10:23.909: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:10:23.911: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 11 14:10:23.916: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8120  fab81303-5b6d-44c6-b2c3-bda47d965d75 9214 2 2023-08-11 14:10:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617567 0xc005617568}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056175f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 11 14:10:23.919: INFO: pod: "test-deployment-7b7876f9d6-lp29w":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-lp29w test-deployment-7b7876f9d6- deployment-8120  8f901941-0415-4ba3-a33e-4d4c66362ab8 9213 0 2023-08-11 14:10:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 fab81303-5b6d-44c6-b2c3-bda47d965d75 0xc005466d97 0xc005466d98}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fab81303-5b6d-44c6-b2c3-bda47d965d75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.78\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8zwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8zwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.78,StartTime:2023-08-11 14:10:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50a859c9347959440811ac15b6ce492c163a8e76d74d71d73a11de97cdf8bbb0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 11 14:10:23.920: INFO: pod: "test-deployment-7b7876f9d6-qqzbv":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-qqzbv test-deployment-7b7876f9d6- deployment-8120  17d1d8d9-9af5-4e3d-b38d-c408e7288ce9 9177 0 2023-08-11 14:10:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 fab81303-5b6d-44c6-b2c3-bda47d965d75 0xc005466f77 0xc005466f78}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fab81303-5b6d-44c6-b2c3-bda47d965d75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mzmc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mzmc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.178,StartTime:2023-08-11 14:10:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5317109c7611eb57a862ded24be729cd6cbdead9ed1053a883b53bba6079185d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 11 14:10:23.920: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8120  c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 9221 4 2023-08-11 14:10:21 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617657 0xc005617658}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056176e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 11 14:10:23.923: INFO: pod: "test-deployment-7df74c55ff-l9xnk":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-l9xnk test-deployment-7df74c55ff- deployment-8120  d31bb5cc-51be-48d1-bb65-2e2e9fcbf60b 9217 0 2023-08-11 14:10:21 +0000 UTC 2023-08-11 14:10:24 +0000 UTC 0xc005617a90 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 0xc005617ac7 0xc005617ac8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qj98b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qj98b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.123,StartTime:2023-08-11 14:10:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://997ee7641581b5e4708f39239c6fbbc9dcd7b600e449b077ad05e290abb53314,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 11 14:10:23.923: INFO: pod: "test-deployment-7df74c55ff-mxz64":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-mxz64 test-deployment-7df74c55ff- deployment-8120  abd3d653-326e-41a1-9a2f-25a512ef74b3 9209 0 2023-08-11 14:10:22 +0000 UTC 2023-08-11 14:10:24 +0000 UTC 0xc005617c80 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee 0xc005617cb7 0xc005617cb8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9bb3a1b-5b7f-4a67-aa53-153e4cb93aee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vtr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vtr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.192,StartTime:2023-08-11 14:10:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://685d2e1bf99a74a24718017180def6c58aff7fea08bef04818c38057d91be12c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 11 14:10:23.923: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8120  5ca0f3ef-73f2-4412-9429-f693058c7996 9130 3 2023-08-11 14:10:20 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cca33f37-d408-450f-8d4a-f25d2c8909a2 0xc005617747 0xc005617748}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:10:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cca33f37-d408-450f-8d4a-f25d2c8909a2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:10:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0056177d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 11 14:10:23.928: INFO: pod: "test-deployment-f4dbc4647-8gw4s":
    &Pod{ObjectMeta:{test-deployment-f4dbc4647-8gw4s test-deployment-f4dbc4647- deployment-8120  35a99bca-31b2-42a4-b0e9-f3376c37891e 9097 0 2023-08-11 14:10:20 +0000 UTC 2023-08-11 14:10:22 +0000 UTC 0xc001557c70 map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-f4dbc4647 5ca0f3ef-73f2-4412-9429-f693058c7996 0xc001557ca7 0xc001557ca8}] [] [{kube-controller-manager Update v1 2023-08-11 14:10:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ca0f3ef-73f2-4412-9429-f693058c7996\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:10:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sp8zk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sp8zk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:10:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.145,StartTime:2023-08-11 14:10:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:10:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://03ea2f757e8769c0325e0d4a86ab72da703720e0946cbcb069eeeff4ae7b77aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:23.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8120" for this suite. 08/11/23 14:10:23.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:23.942
Aug 11 14:10:23.942: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:10:23.943
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:23.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:23.959
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 14:10:23.961
Aug 11 14:10:23.970: INFO: Waiting up to 5m0s for pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c" in namespace "emptydir-5847" to be "Succeeded or Failed"
Aug 11 14:10:23.973: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535873ms
Aug 11 14:10:25.976: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887291s
Aug 11 14:10:27.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006751668s
Aug 11 14:10:29.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006589322s
STEP: Saw pod success 08/11/23 14:10:29.977
Aug 11 14:10:29.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c" satisfied condition "Succeeded or Failed"
Aug 11 14:10:29.980: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c container test-container: <nil>
STEP: delete the pod 08/11/23 14:10:29.99
Aug 11 14:10:30.002: INFO: Waiting for pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c to disappear
Aug 11 14:10:30.004: INFO: Pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5847" for this suite. 08/11/23 14:10:30.009
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:23.942
    Aug 11 14:10:23.942: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:10:23.943
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:23.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:23.959
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 14:10:23.961
    Aug 11 14:10:23.970: INFO: Waiting up to 5m0s for pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c" in namespace "emptydir-5847" to be "Succeeded or Failed"
    Aug 11 14:10:23.973: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535873ms
    Aug 11 14:10:25.976: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005887291s
    Aug 11 14:10:27.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006751668s
    Aug 11 14:10:29.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006589322s
    STEP: Saw pod success 08/11/23 14:10:29.977
    Aug 11 14:10:29.977: INFO: Pod "pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:29.980: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c container test-container: <nil>
    STEP: delete the pod 08/11/23 14:10:29.99
    Aug 11 14:10:30.002: INFO: Waiting for pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c to disappear
    Aug 11 14:10:30.004: INFO: Pod pod-67b3b63e-d9f7-4a73-9273-5fed56cd191c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:30.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5847" for this suite. 08/11/23 14:10:30.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:30.016
Aug 11 14:10:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:10:30.017
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:30.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:30.04
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/11/23 14:10:30.042
STEP: wait for the container to reach Succeeded 08/11/23 14:10:30.049
STEP: get the container status 08/11/23 14:10:33.063
STEP: the container should be terminated 08/11/23 14:10:33.066
STEP: the termination message should be set 08/11/23 14:10:33.066
Aug 11 14:10:33.066: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/11/23 14:10:33.066
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:33.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9108" for this suite. 08/11/23 14:10:33.081
------------------------------
• [3.073 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:30.016
    Aug 11 14:10:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:10:30.017
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:30.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:30.04
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/11/23 14:10:30.042
    STEP: wait for the container to reach Succeeded 08/11/23 14:10:30.049
    STEP: get the container status 08/11/23 14:10:33.063
    STEP: the container should be terminated 08/11/23 14:10:33.066
    STEP: the termination message should be set 08/11/23 14:10:33.066
    Aug 11 14:10:33.066: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/11/23 14:10:33.066
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:33.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9108" for this suite. 08/11/23 14:10:33.081
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:33.09
Aug 11 14:10:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:10:33.09
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:33.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:33.106
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/11/23 14:10:33.109
Aug 11 14:10:33.116: INFO: Waiting up to 5m0s for pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4" in namespace "var-expansion-5783" to be "Succeeded or Failed"
Aug 11 14:10:33.120: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278344ms
Aug 11 14:10:35.125: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009241365s
Aug 11 14:10:37.126: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00948556s
STEP: Saw pod success 08/11/23 14:10:37.126
Aug 11 14:10:37.126: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4" satisfied condition "Succeeded or Failed"
Aug 11 14:10:37.129: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:10:37.139
Aug 11 14:10:37.153: INFO: Waiting for pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 to disappear
Aug 11 14:10:37.155: INFO: Pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:37.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5783" for this suite. 08/11/23 14:10:37.159
------------------------------
• [4.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:33.09
    Aug 11 14:10:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:10:33.09
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:33.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:33.106
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/11/23 14:10:33.109
    Aug 11 14:10:33.116: INFO: Waiting up to 5m0s for pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4" in namespace "var-expansion-5783" to be "Succeeded or Failed"
    Aug 11 14:10:33.120: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278344ms
    Aug 11 14:10:35.125: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009241365s
    Aug 11 14:10:37.126: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00948556s
    STEP: Saw pod success 08/11/23 14:10:37.126
    Aug 11 14:10:37.126: INFO: Pod "var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:37.129: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:10:37.139
    Aug 11 14:10:37.153: INFO: Waiting for pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 to disappear
    Aug 11 14:10:37.155: INFO: Pod var-expansion-f8632808-7fbe-4122-ba5f-59b8606cbcf4 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:37.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5783" for this suite. 08/11/23 14:10:37.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:37.167
Aug 11 14:10:37.167: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:10:37.168
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:37.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:37.183
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 11 14:10:37.197: INFO: Waiting up to 5m0s for pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9" in namespace "svcaccounts-6885" to be "running"
Aug 11 14:10:37.201: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197105ms
Aug 11 14:10:39.205: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008450423s
Aug 11 14:10:39.205: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9" satisfied condition "running"
STEP: reading a file in the container 08/11/23 14:10:39.206
Aug 11 14:10:39.206: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/11/23 14:10:39.334
Aug 11 14:10:39.334: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/11/23 14:10:39.451
Aug 11 14:10:39.451: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 11 14:10:39.582: INFO: Got root ca configmap in namespace "svcaccounts-6885"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:39.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6885" for this suite. 08/11/23 14:10:39.587
------------------------------
• [2.428 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:37.167
    Aug 11 14:10:37.167: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:10:37.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:37.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:37.183
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 11 14:10:37.197: INFO: Waiting up to 5m0s for pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9" in namespace "svcaccounts-6885" to be "running"
    Aug 11 14:10:37.201: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197105ms
    Aug 11 14:10:39.205: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008450423s
    Aug 11 14:10:39.205: INFO: Pod "pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9" satisfied condition "running"
    STEP: reading a file in the container 08/11/23 14:10:39.206
    Aug 11 14:10:39.206: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/11/23 14:10:39.334
    Aug 11 14:10:39.334: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/11/23 14:10:39.451
    Aug 11 14:10:39.451: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6885 pod-service-account-e3487662-cf12-4d1f-a4a8-e29a677cafe9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 11 14:10:39.582: INFO: Got root ca configmap in namespace "svcaccounts-6885"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:39.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6885" for this suite. 08/11/23 14:10:39.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:39.595
Aug 11 14:10:39.595: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:10:39.596
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:39.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:39.612
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/11/23 14:10:39.614
STEP: submitting the pod to kubernetes 08/11/23 14:10:39.614
Aug 11 14:10:39.622: INFO: Waiting up to 5m0s for pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" in namespace "pods-4681" to be "running and ready"
Aug 11 14:10:39.625: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604404ms
Aug 11 14:10:39.625: INFO: The phase of Pod pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:10:41.630: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007952744s
Aug 11 14:10:41.630: INFO: The phase of Pod pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f is Running (Ready = true)
Aug 11 14:10:41.630: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/11/23 14:10:41.633
STEP: updating the pod 08/11/23 14:10:41.635
Aug 11 14:10:42.148: INFO: Successfully updated pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f"
Aug 11 14:10:42.148: INFO: Waiting up to 5m0s for pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" in namespace "pods-4681" to be "running"
Aug 11 14:10:42.151: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Running", Reason="", readiness=true. Elapsed: 3.510023ms
Aug 11 14:10:42.151: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/11/23 14:10:42.151
Aug 11 14:10:42.158: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4681" for this suite. 08/11/23 14:10:42.162
------------------------------
• [2.574 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:39.595
    Aug 11 14:10:39.595: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:10:39.596
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:39.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:39.612
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/11/23 14:10:39.614
    STEP: submitting the pod to kubernetes 08/11/23 14:10:39.614
    Aug 11 14:10:39.622: INFO: Waiting up to 5m0s for pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" in namespace "pods-4681" to be "running and ready"
    Aug 11 14:10:39.625: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604404ms
    Aug 11 14:10:39.625: INFO: The phase of Pod pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:10:41.630: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007952744s
    Aug 11 14:10:41.630: INFO: The phase of Pod pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f is Running (Ready = true)
    Aug 11 14:10:41.630: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/11/23 14:10:41.633
    STEP: updating the pod 08/11/23 14:10:41.635
    Aug 11 14:10:42.148: INFO: Successfully updated pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f"
    Aug 11 14:10:42.148: INFO: Waiting up to 5m0s for pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" in namespace "pods-4681" to be "running"
    Aug 11 14:10:42.151: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f": Phase="Running", Reason="", readiness=true. Elapsed: 3.510023ms
    Aug 11 14:10:42.151: INFO: Pod "pod-update-8e2c6737-a77b-4793-a67d-c6e64f7ac73f" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/11/23 14:10:42.151
    Aug 11 14:10:42.158: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4681" for this suite. 08/11/23 14:10:42.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:42.17
Aug 11 14:10:42.170: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:10:42.171
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:42.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:42.186
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-13ad636c-5a46-44db-ac82-7e299ae06cda 08/11/23 14:10:42.188
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:42.192
Aug 11 14:10:42.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775" in namespace "projected-237" to be "Succeeded or Failed"
Aug 11 14:10:42.208: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985413ms
Aug 11 14:10:44.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632984s
Aug 11 14:10:46.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008121459s
STEP: Saw pod success 08/11/23 14:10:46.212
Aug 11 14:10:46.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775" satisfied condition "Succeeded or Failed"
Aug 11 14:10:46.214: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/11/23 14:10:46.223
Aug 11 14:10:46.235: INFO: Waiting for pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 to disappear
Aug 11 14:10:46.238: INFO: Pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-237" for this suite. 08/11/23 14:10:46.241
------------------------------
• [4.077 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:42.17
    Aug 11 14:10:42.170: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:10:42.171
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:42.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:42.186
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-13ad636c-5a46-44db-ac82-7e299ae06cda 08/11/23 14:10:42.188
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:42.192
    Aug 11 14:10:42.204: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775" in namespace "projected-237" to be "Succeeded or Failed"
    Aug 11 14:10:42.208: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985413ms
    Aug 11 14:10:44.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008632984s
    Aug 11 14:10:46.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008121459s
    STEP: Saw pod success 08/11/23 14:10:46.212
    Aug 11 14:10:46.212: INFO: Pod "pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:46.214: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:10:46.223
    Aug 11 14:10:46.235: INFO: Waiting for pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 to disappear
    Aug 11 14:10:46.238: INFO: Pod pod-projected-configmaps-b2a1b345-83c8-4410-bd7a-460ff104c775 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-237" for this suite. 08/11/23 14:10:46.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:46.248
Aug 11 14:10:46.248: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:10:46.249
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.266
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:10:46.268
Aug 11 14:10:46.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0" in namespace "projected-1004" to be "Succeeded or Failed"
Aug 11 14:10:46.278: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362133ms
Aug 11 14:10:48.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006806033s
Aug 11 14:10:50.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006254818s
STEP: Saw pod success 08/11/23 14:10:50.282
Aug 11 14:10:50.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0" satisfied condition "Succeeded or Failed"
Aug 11 14:10:50.285: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 container client-container: <nil>
STEP: delete the pod 08/11/23 14:10:50.292
Aug 11 14:10:50.306: INFO: Waiting for pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 to disappear
Aug 11 14:10:50.309: INFO: Pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:10:50.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1004" for this suite. 08/11/23 14:10:50.313
------------------------------
• [4.071 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:46.248
    Aug 11 14:10:46.248: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:10:46.249
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.266
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:10:46.268
    Aug 11 14:10:46.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0" in namespace "projected-1004" to be "Succeeded or Failed"
    Aug 11 14:10:46.278: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362133ms
    Aug 11 14:10:48.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006806033s
    Aug 11 14:10:50.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006254818s
    STEP: Saw pod success 08/11/23 14:10:50.282
    Aug 11 14:10:50.282: INFO: Pod "downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:50.285: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:10:50.292
    Aug 11 14:10:50.306: INFO: Waiting for pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 to disappear
    Aug 11 14:10:50.309: INFO: Pod downwardapi-volume-31537379-66ed-40c4-a0e1-32a15eb8fac0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:10:50.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1004" for this suite. 08/11/23 14:10:50.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:10:50.32
Aug 11 14:10:50.320: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename subpath 08/11/23 14:10:50.32
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:50.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:50.338
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:10:50.34
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-58mw 08/11/23 14:10:50.35
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:10:50.35
Aug 11 14:10:50.358: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-58mw" in namespace "subpath-1938" to be "Succeeded or Failed"
Aug 11 14:10:50.360: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556012ms
Aug 11 14:10:52.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 2.006964342s
Aug 11 14:10:54.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 4.007616959s
Aug 11 14:10:56.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 6.007343524s
Aug 11 14:10:58.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 8.00778811s
Aug 11 14:11:00.364: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 10.006546594s
Aug 11 14:11:02.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 12.007931581s
Aug 11 14:11:04.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 14.007102996s
Aug 11 14:11:06.364: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 16.006441732s
Aug 11 14:11:08.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 18.008343928s
Aug 11 14:11:10.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 20.007301584s
Aug 11 14:11:12.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=false. Elapsed: 22.007323709s
Aug 11 14:11:14.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007606326s
STEP: Saw pod success 08/11/23 14:11:14.365
Aug 11 14:11:14.366: INFO: Pod "pod-subpath-test-configmap-58mw" satisfied condition "Succeeded or Failed"
Aug 11 14:11:14.368: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-configmap-58mw container test-container-subpath-configmap-58mw: <nil>
STEP: delete the pod 08/11/23 14:11:14.378
Aug 11 14:11:14.388: INFO: Waiting for pod pod-subpath-test-configmap-58mw to disappear
Aug 11 14:11:14.390: INFO: Pod pod-subpath-test-configmap-58mw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-58mw 08/11/23 14:11:14.39
Aug 11 14:11:14.391: INFO: Deleting pod "pod-subpath-test-configmap-58mw" in namespace "subpath-1938"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:14.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1938" for this suite. 08/11/23 14:11:14.399
------------------------------
• [SLOW TEST] [24.085 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:10:50.32
    Aug 11 14:10:50.320: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename subpath 08/11/23 14:10:50.32
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:50.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:50.338
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:10:50.34
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-58mw 08/11/23 14:10:50.35
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:10:50.35
    Aug 11 14:10:50.358: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-58mw" in namespace "subpath-1938" to be "Succeeded or Failed"
    Aug 11 14:10:50.360: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.556012ms
    Aug 11 14:10:52.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 2.006964342s
    Aug 11 14:10:54.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 4.007616959s
    Aug 11 14:10:56.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 6.007343524s
    Aug 11 14:10:58.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 8.00778811s
    Aug 11 14:11:00.364: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 10.006546594s
    Aug 11 14:11:02.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 12.007931581s
    Aug 11 14:11:04.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 14.007102996s
    Aug 11 14:11:06.364: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 16.006441732s
    Aug 11 14:11:08.366: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 18.008343928s
    Aug 11 14:11:10.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=true. Elapsed: 20.007301584s
    Aug 11 14:11:12.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Running", Reason="", readiness=false. Elapsed: 22.007323709s
    Aug 11 14:11:14.365: INFO: Pod "pod-subpath-test-configmap-58mw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007606326s
    STEP: Saw pod success 08/11/23 14:11:14.365
    Aug 11 14:11:14.366: INFO: Pod "pod-subpath-test-configmap-58mw" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:14.368: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-configmap-58mw container test-container-subpath-configmap-58mw: <nil>
    STEP: delete the pod 08/11/23 14:11:14.378
    Aug 11 14:11:14.388: INFO: Waiting for pod pod-subpath-test-configmap-58mw to disappear
    Aug 11 14:11:14.390: INFO: Pod pod-subpath-test-configmap-58mw no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-58mw 08/11/23 14:11:14.39
    Aug 11 14:11:14.391: INFO: Deleting pod "pod-subpath-test-configmap-58mw" in namespace "subpath-1938"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:14.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1938" for this suite. 08/11/23 14:11:14.399
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:14.405
Aug 11 14:11:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:11:14.406
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:14.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:14.424
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:11:14.426
Aug 11 14:11:14.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a" in namespace "downward-api-7638" to be "Succeeded or Failed"
Aug 11 14:11:14.437: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521552ms
Aug 11 14:11:16.441: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006765391s
Aug 11 14:11:18.442: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007853567s
STEP: Saw pod success 08/11/23 14:11:18.442
Aug 11 14:11:18.442: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a" satisfied condition "Succeeded or Failed"
Aug 11 14:11:18.445: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a container client-container: <nil>
STEP: delete the pod 08/11/23 14:11:18.454
Aug 11 14:11:18.464: INFO: Waiting for pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a to disappear
Aug 11 14:11:18.467: INFO: Pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:18.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7638" for this suite. 08/11/23 14:11:18.472
------------------------------
• [4.073 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:14.405
    Aug 11 14:11:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:11:14.406
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:14.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:14.424
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:11:14.426
    Aug 11 14:11:14.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a" in namespace "downward-api-7638" to be "Succeeded or Failed"
    Aug 11 14:11:14.437: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521552ms
    Aug 11 14:11:16.441: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006765391s
    Aug 11 14:11:18.442: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007853567s
    STEP: Saw pod success 08/11/23 14:11:18.442
    Aug 11 14:11:18.442: INFO: Pod "downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:18.445: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a container client-container: <nil>
    STEP: delete the pod 08/11/23 14:11:18.454
    Aug 11 14:11:18.464: INFO: Waiting for pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a to disappear
    Aug 11 14:11:18.467: INFO: Pod downwardapi-volume-e0cc8f2b-e81c-4969-a108-eb122a57978a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:18.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7638" for this suite. 08/11/23 14:11:18.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:18.478
Aug 11 14:11:18.478: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:11:18.479
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:18.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:18.495
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/11/23 14:11:18.497
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:18.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9496" for this suite. 08/11/23 14:11:18.503
------------------------------
• [0.030 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:18.478
    Aug 11 14:11:18.478: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:11:18.479
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:18.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:18.495
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/11/23 14:11:18.497
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:18.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9496" for this suite. 08/11/23 14:11:18.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:18.509
Aug 11 14:11:18.509: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context 08/11/23 14:11:18.51
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:18.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:18.525
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:11:18.527
Aug 11 14:11:18.534: INFO: Waiting up to 5m0s for pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1" in namespace "security-context-3599" to be "Succeeded or Failed"
Aug 11 14:11:18.536: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342122ms
Aug 11 14:11:20.541: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007045041s
Aug 11 14:11:22.545: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011494063s
STEP: Saw pod success 08/11/23 14:11:22.545
Aug 11 14:11:22.545: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1" satisfied condition "Succeeded or Failed"
Aug 11 14:11:22.548: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 container test-container: <nil>
STEP: delete the pod 08/11/23 14:11:22.556
Aug 11 14:11:22.567: INFO: Waiting for pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 to disappear
Aug 11 14:11:22.569: INFO: Pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:22.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3599" for this suite. 08/11/23 14:11:22.574
------------------------------
• [4.070 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:18.509
    Aug 11 14:11:18.509: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context 08/11/23 14:11:18.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:18.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:18.525
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:11:18.527
    Aug 11 14:11:18.534: INFO: Waiting up to 5m0s for pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1" in namespace "security-context-3599" to be "Succeeded or Failed"
    Aug 11 14:11:18.536: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342122ms
    Aug 11 14:11:20.541: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007045041s
    Aug 11 14:11:22.545: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011494063s
    STEP: Saw pod success 08/11/23 14:11:22.545
    Aug 11 14:11:22.545: INFO: Pod "security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:22.548: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:11:22.556
    Aug 11 14:11:22.567: INFO: Waiting for pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 to disappear
    Aug 11 14:11:22.569: INFO: Pod security-context-5cf56c37-aac7-4b9a-a291-84b423a819e1 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:22.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3599" for this suite. 08/11/23 14:11:22.574
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:22.579
Aug 11 14:11:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 14:11:22.58
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:22.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:22.598
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/11/23 14:11:22.6
STEP: Ensuring job reaches completions 08/11/23 14:11:22.606
STEP: Ensuring pods with index for job exist 08/11/23 14:11:34.611
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:34.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4490" for this suite. 08/11/23 14:11:34.618
------------------------------
• [SLOW TEST] [12.045 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:22.579
    Aug 11 14:11:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 14:11:22.58
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:22.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:22.598
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/11/23 14:11:22.6
    STEP: Ensuring job reaches completions 08/11/23 14:11:22.606
    STEP: Ensuring pods with index for job exist 08/11/23 14:11:34.611
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:34.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4490" for this suite. 08/11/23 14:11:34.618
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:34.625
Aug 11 14:11:34.625: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename containers 08/11/23 14:11:34.626
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:34.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:34.643
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/11/23 14:11:34.645
Aug 11 14:11:34.652: INFO: Waiting up to 5m0s for pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856" in namespace "containers-5039" to be "Succeeded or Failed"
Aug 11 14:11:34.657: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137265ms
Aug 11 14:11:36.661: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008807644s
Aug 11 14:11:38.663: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010445801s
STEP: Saw pod success 08/11/23 14:11:38.663
Aug 11 14:11:38.663: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856" satisfied condition "Succeeded or Failed"
Aug 11 14:11:38.666: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:11:38.674
Aug 11 14:11:38.684: INFO: Waiting for pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 to disappear
Aug 11 14:11:38.687: INFO: Pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:38.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5039" for this suite. 08/11/23 14:11:38.69
------------------------------
• [4.072 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:34.625
    Aug 11 14:11:34.625: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename containers 08/11/23 14:11:34.626
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:34.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:34.643
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/11/23 14:11:34.645
    Aug 11 14:11:34.652: INFO: Waiting up to 5m0s for pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856" in namespace "containers-5039" to be "Succeeded or Failed"
    Aug 11 14:11:34.657: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Pending", Reason="", readiness=false. Elapsed: 5.137265ms
    Aug 11 14:11:36.661: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008807644s
    Aug 11 14:11:38.663: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010445801s
    STEP: Saw pod success 08/11/23 14:11:38.663
    Aug 11 14:11:38.663: INFO: Pod "client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:38.666: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:11:38.674
    Aug 11 14:11:38.684: INFO: Waiting for pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 to disappear
    Aug 11 14:11:38.687: INFO: Pod client-containers-18e45a0e-8b33-4b74-942f-6cfd3f789856 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:38.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5039" for this suite. 08/11/23 14:11:38.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:38.697
Aug 11 14:11:38.698: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:11:38.698
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:38.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:38.714
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-42cdc2df-52e9-4125-ab84-e4a39be7a77b 08/11/23 14:11:38.716
STEP: Creating a pod to test consume configMaps 08/11/23 14:11:38.72
Aug 11 14:11:38.728: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61" in namespace "projected-7467" to be "Succeeded or Failed"
Aug 11 14:11:38.730: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251452ms
Aug 11 14:11:40.735: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006743351s
Aug 11 14:11:42.736: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008016758s
STEP: Saw pod success 08/11/23 14:11:42.736
Aug 11 14:11:42.736: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61" satisfied condition "Succeeded or Failed"
Aug 11 14:11:42.739: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:11:42.749
Aug 11 14:11:42.761: INFO: Waiting for pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 to disappear
Aug 11 14:11:42.763: INFO: Pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:42.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7467" for this suite. 08/11/23 14:11:42.767
------------------------------
• [4.079 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:38.697
    Aug 11 14:11:38.698: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:11:38.698
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:38.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:38.714
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-42cdc2df-52e9-4125-ab84-e4a39be7a77b 08/11/23 14:11:38.716
    STEP: Creating a pod to test consume configMaps 08/11/23 14:11:38.72
    Aug 11 14:11:38.728: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61" in namespace "projected-7467" to be "Succeeded or Failed"
    Aug 11 14:11:38.730: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251452ms
    Aug 11 14:11:40.735: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006743351s
    Aug 11 14:11:42.736: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008016758s
    STEP: Saw pod success 08/11/23 14:11:42.736
    Aug 11 14:11:42.736: INFO: Pod "pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:42.739: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:11:42.749
    Aug 11 14:11:42.761: INFO: Waiting for pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 to disappear
    Aug 11 14:11:42.763: INFO: Pod pod-projected-configmaps-dd38b1e0-a222-4ef2-90ee-b171b5a73b61 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:42.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7467" for this suite. 08/11/23 14:11:42.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:42.777
Aug 11 14:11:42.777: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename containers 08/11/23 14:11:42.778
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:42.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:42.8
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/11/23 14:11:42.802
Aug 11 14:11:42.810: INFO: Waiting up to 5m0s for pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6" in namespace "containers-3918" to be "Succeeded or Failed"
Aug 11 14:11:42.814: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414474ms
Aug 11 14:11:44.818: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008617504s
Aug 11 14:11:46.819: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00962089s
STEP: Saw pod success 08/11/23 14:11:46.819
Aug 11 14:11:46.819: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6" satisfied condition "Succeeded or Failed"
Aug 11 14:11:46.822: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:11:46.834
Aug 11 14:11:46.847: INFO: Waiting for pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 to disappear
Aug 11 14:11:46.849: INFO: Pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:46.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3918" for this suite. 08/11/23 14:11:46.853
------------------------------
• [4.082 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:42.777
    Aug 11 14:11:42.777: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename containers 08/11/23 14:11:42.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:42.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:42.8
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/11/23 14:11:42.802
    Aug 11 14:11:42.810: INFO: Waiting up to 5m0s for pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6" in namespace "containers-3918" to be "Succeeded or Failed"
    Aug 11 14:11:42.814: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414474ms
    Aug 11 14:11:44.818: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008617504s
    Aug 11 14:11:46.819: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00962089s
    STEP: Saw pod success 08/11/23 14:11:46.819
    Aug 11 14:11:46.819: INFO: Pod "client-containers-a379d959-90c9-4be0-9680-7616ff03bed6" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:46.822: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:11:46.834
    Aug 11 14:11:46.847: INFO: Waiting for pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 to disappear
    Aug 11 14:11:46.849: INFO: Pod client-containers-a379d959-90c9-4be0-9680-7616ff03bed6 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:46.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3918" for this suite. 08/11/23 14:11:46.853
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:46.859
Aug 11 14:11:46.860: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename events 08/11/23 14:11:46.86
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:46.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:46.876
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/11/23 14:11:46.879
STEP: get a list of Events with a label in the current namespace 08/11/23 14:11:46.896
STEP: delete a list of events 08/11/23 14:11:46.899
Aug 11 14:11:46.899: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/11/23 14:11:46.921
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:46.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7126" for this suite. 08/11/23 14:11:46.927
------------------------------
• [0.075 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:46.859
    Aug 11 14:11:46.860: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename events 08/11/23 14:11:46.86
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:46.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:46.876
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/11/23 14:11:46.879
    STEP: get a list of Events with a label in the current namespace 08/11/23 14:11:46.896
    STEP: delete a list of events 08/11/23 14:11:46.899
    Aug 11 14:11:46.899: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/11/23 14:11:46.921
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:46.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7126" for this suite. 08/11/23 14:11:46.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:46.936
Aug 11 14:11:46.936: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename events 08/11/23 14:11:46.936
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:46.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:46.952
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/11/23 14:11:46.954
STEP: listing events in all namespaces 08/11/23 14:11:46.958
STEP: listing events in test namespace 08/11/23 14:11:46.969
STEP: listing events with field selection filtering on source 08/11/23 14:11:46.972
STEP: listing events with field selection filtering on reportingController 08/11/23 14:11:46.974
STEP: getting the test event 08/11/23 14:11:46.976
STEP: patching the test event 08/11/23 14:11:46.979
STEP: getting the test event 08/11/23 14:11:46.987
STEP: updating the test event 08/11/23 14:11:46.99
STEP: getting the test event 08/11/23 14:11:46.997
STEP: deleting the test event 08/11/23 14:11:47
STEP: listing events in all namespaces 08/11/23 14:11:47.007
STEP: listing events in test namespace 08/11/23 14:11:47.017
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:11:47.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9230" for this suite. 08/11/23 14:11:47.022
------------------------------
• [0.092 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:46.936
    Aug 11 14:11:46.936: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename events 08/11/23 14:11:46.936
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:46.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:46.952
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/11/23 14:11:46.954
    STEP: listing events in all namespaces 08/11/23 14:11:46.958
    STEP: listing events in test namespace 08/11/23 14:11:46.969
    STEP: listing events with field selection filtering on source 08/11/23 14:11:46.972
    STEP: listing events with field selection filtering on reportingController 08/11/23 14:11:46.974
    STEP: getting the test event 08/11/23 14:11:46.976
    STEP: patching the test event 08/11/23 14:11:46.979
    STEP: getting the test event 08/11/23 14:11:46.987
    STEP: updating the test event 08/11/23 14:11:46.99
    STEP: getting the test event 08/11/23 14:11:46.997
    STEP: deleting the test event 08/11/23 14:11:47
    STEP: listing events in all namespaces 08/11/23 14:11:47.007
    STEP: listing events in test namespace 08/11/23 14:11:47.017
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:11:47.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9230" for this suite. 08/11/23 14:11:47.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:11:47.028
Aug 11 14:11:47.028: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:11:47.029
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:47.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:47.047
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a in namespace container-probe-7796 08/11/23 14:11:47.049
Aug 11 14:11:47.055: INFO: Waiting up to 5m0s for pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a" in namespace "container-probe-7796" to be "not pending"
Aug 11 14:11:47.058: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519782ms
Aug 11 14:11:49.062: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006654152s
Aug 11 14:11:49.062: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a" satisfied condition "not pending"
Aug 11 14:11:49.062: INFO: Started pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a in namespace container-probe-7796
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:11:49.062
Aug 11 14:11:49.065: INFO: Initial restart count of pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a is 0
Aug 11 14:12:39.173: INFO: Restart count of pod container-probe-7796/busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a is now 1 (50.108403617s elapsed)
STEP: deleting the pod 08/11/23 14:12:39.173
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:12:39.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7796" for this suite. 08/11/23 14:12:39.19
------------------------------
• [SLOW TEST] [52.170 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:11:47.028
    Aug 11 14:11:47.028: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:11:47.029
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:47.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:47.047
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a in namespace container-probe-7796 08/11/23 14:11:47.049
    Aug 11 14:11:47.055: INFO: Waiting up to 5m0s for pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a" in namespace "container-probe-7796" to be "not pending"
    Aug 11 14:11:47.058: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519782ms
    Aug 11 14:11:49.062: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006654152s
    Aug 11 14:11:49.062: INFO: Pod "busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a" satisfied condition "not pending"
    Aug 11 14:11:49.062: INFO: Started pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a in namespace container-probe-7796
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:11:49.062
    Aug 11 14:11:49.065: INFO: Initial restart count of pod busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a is 0
    Aug 11 14:12:39.173: INFO: Restart count of pod container-probe-7796/busybox-e2ffeb77-2f04-44da-8c23-4629be01a79a is now 1 (50.108403617s elapsed)
    STEP: deleting the pod 08/11/23 14:12:39.173
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:12:39.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7796" for this suite. 08/11/23 14:12:39.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:12:39.199
Aug 11 14:12:39.199: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:12:39.2
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:39.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:39.216
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/11/23 14:12:39.218
STEP: When the matched label of one of its pods change 08/11/23 14:12:39.223
Aug 11 14:12:39.226: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 11 14:12:44.233: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/11/23 14:12:44.243
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:12:45.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9383" for this suite. 08/11/23 14:12:45.252
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:12:39.199
    Aug 11 14:12:39.199: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:12:39.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:39.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:39.216
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/11/23 14:12:39.218
    STEP: When the matched label of one of its pods change 08/11/23 14:12:39.223
    Aug 11 14:12:39.226: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 11 14:12:44.233: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/11/23 14:12:44.243
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:12:45.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9383" for this suite. 08/11/23 14:12:45.252
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:12:45.26
Aug 11 14:12:45.260: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename cronjob 08/11/23 14:12:45.261
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:45.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:45.279
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/11/23 14:12:45.281
STEP: Ensuring a job is scheduled 08/11/23 14:12:45.286
STEP: Ensuring exactly one is scheduled 08/11/23 14:13:01.29
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:13:01.294
STEP: Ensuring the job is replaced with a new one 08/11/23 14:13:01.297
STEP: Removing cronjob 08/11/23 14:14:01.301
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 11 14:14:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5198" for this suite. 08/11/23 14:14:01.311
------------------------------
• [SLOW TEST] [76.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:12:45.26
    Aug 11 14:12:45.260: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:12:45.261
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:45.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:45.279
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/11/23 14:12:45.281
    STEP: Ensuring a job is scheduled 08/11/23 14:12:45.286
    STEP: Ensuring exactly one is scheduled 08/11/23 14:13:01.29
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:13:01.294
    STEP: Ensuring the job is replaced with a new one 08/11/23 14:13:01.297
    STEP: Removing cronjob 08/11/23 14:14:01.301
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:14:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5198" for this suite. 08/11/23 14:14:01.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:14:01.323
Aug 11 14:14:01.324: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename taint-multiple-pods 08/11/23 14:14:01.324
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:14:01.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:14:01.346
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 11 14:14:01.348: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:15:01.380: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 11 14:15:01.383: INFO: Starting informer...
STEP: Starting pods... 08/11/23 14:15:01.383
Aug 11 14:15:01.600: INFO: Pod1 is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
Aug 11 14:15:01.809: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-88" to be "running"
Aug 11 14:15:01.812: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078464ms
Aug 11 14:15:03.816: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006815815s
Aug 11 14:15:03.816: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 11 14:15:03.816: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-88" to be "running"
Aug 11 14:15:03.819: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.853153ms
Aug 11 14:15:03.819: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 11 14:15:03.819: INFO: Pod2 is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
STEP: Trying to apply a taint on the Node 08/11/23 14:15:03.819
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:15:03.83
STEP: Waiting for Pod1 and Pod2 to be deleted 08/11/23 14:15:03.833
Aug 11 14:15:09.672: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 11 14:15:29.689: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:15:29.704
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-88" for this suite. 08/11/23 14:15:29.711
------------------------------
• [SLOW TEST] [88.406 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:14:01.323
    Aug 11 14:14:01.324: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename taint-multiple-pods 08/11/23 14:14:01.324
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:14:01.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:14:01.346
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 11 14:14:01.348: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:15:01.380: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 11 14:15:01.383: INFO: Starting informer...
    STEP: Starting pods... 08/11/23 14:15:01.383
    Aug 11 14:15:01.600: INFO: Pod1 is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
    Aug 11 14:15:01.809: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-88" to be "running"
    Aug 11 14:15:01.812: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078464ms
    Aug 11 14:15:03.816: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006815815s
    Aug 11 14:15:03.816: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 11 14:15:03.816: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-88" to be "running"
    Aug 11 14:15:03.819: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.853153ms
    Aug 11 14:15:03.819: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 11 14:15:03.819: INFO: Pod2 is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
    STEP: Trying to apply a taint on the Node 08/11/23 14:15:03.819
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:15:03.83
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/11/23 14:15:03.833
    Aug 11 14:15:09.672: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 11 14:15:29.689: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:15:29.704
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-88" for this suite. 08/11/23 14:15:29.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:29.732
Aug 11 14:15:29.732: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:15:29.733
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:29.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:29.758
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 11 14:15:29.763: INFO: Got root ca configmap in namespace "svcaccounts-560"
Aug 11 14:15:29.768: INFO: Deleted root ca configmap in namespace "svcaccounts-560"
STEP: waiting for a new root ca configmap created 08/11/23 14:15:30.269
Aug 11 14:15:30.272: INFO: Recreated root ca configmap in namespace "svcaccounts-560"
Aug 11 14:15:30.278: INFO: Updated root ca configmap in namespace "svcaccounts-560"
STEP: waiting for the root ca configmap reconciled 08/11/23 14:15:30.779
Aug 11 14:15:30.782: INFO: Reconciled root ca configmap in namespace "svcaccounts-560"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:30.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-560" for this suite. 08/11/23 14:15:30.786
------------------------------
• [1.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:29.732
    Aug 11 14:15:29.732: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:15:29.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:29.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:29.758
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 11 14:15:29.763: INFO: Got root ca configmap in namespace "svcaccounts-560"
    Aug 11 14:15:29.768: INFO: Deleted root ca configmap in namespace "svcaccounts-560"
    STEP: waiting for a new root ca configmap created 08/11/23 14:15:30.269
    Aug 11 14:15:30.272: INFO: Recreated root ca configmap in namespace "svcaccounts-560"
    Aug 11 14:15:30.278: INFO: Updated root ca configmap in namespace "svcaccounts-560"
    STEP: waiting for the root ca configmap reconciled 08/11/23 14:15:30.779
    Aug 11 14:15:30.782: INFO: Reconciled root ca configmap in namespace "svcaccounts-560"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:30.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-560" for this suite. 08/11/23 14:15:30.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:30.793
Aug 11 14:15:30.793: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 14:15:30.794
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:30.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:30.81
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/11/23 14:15:30.812
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/11/23 14:15:30.817
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/11/23 14:15:30.817
STEP: creating a pod to probe DNS 08/11/23 14:15:30.817
STEP: submitting the pod to kubernetes 08/11/23 14:15:30.817
Aug 11 14:15:30.833: INFO: Waiting up to 15m0s for pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614" in namespace "dns-6733" to be "running"
Aug 11 14:15:30.836: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546942ms
Aug 11 14:15:32.841: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614": Phase="Running", Reason="", readiness=true. Elapsed: 2.007542354s
Aug 11 14:15:32.841: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614" satisfied condition "running"
STEP: retrieving the pod 08/11/23 14:15:32.841
STEP: looking for the results for each expected name from probers 08/11/23 14:15:32.844
Aug 11 14:15:32.876: INFO: DNS probes using dns-6733/dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614 succeeded

STEP: deleting the pod 08/11/23 14:15:32.876
STEP: deleting the test headless service 08/11/23 14:15:32.893
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:32.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6733" for this suite. 08/11/23 14:15:32.916
------------------------------
• [2.131 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:30.793
    Aug 11 14:15:30.793: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 14:15:30.794
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:30.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:30.81
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/11/23 14:15:30.812
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/11/23 14:15:30.817
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6733.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/11/23 14:15:30.817
    STEP: creating a pod to probe DNS 08/11/23 14:15:30.817
    STEP: submitting the pod to kubernetes 08/11/23 14:15:30.817
    Aug 11 14:15:30.833: INFO: Waiting up to 15m0s for pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614" in namespace "dns-6733" to be "running"
    Aug 11 14:15:30.836: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546942ms
    Aug 11 14:15:32.841: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614": Phase="Running", Reason="", readiness=true. Elapsed: 2.007542354s
    Aug 11 14:15:32.841: INFO: Pod "dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 14:15:32.841
    STEP: looking for the results for each expected name from probers 08/11/23 14:15:32.844
    Aug 11 14:15:32.876: INFO: DNS probes using dns-6733/dns-test-16ad3c7d-8e4a-4de3-b173-72afafd71614 succeeded

    STEP: deleting the pod 08/11/23 14:15:32.876
    STEP: deleting the test headless service 08/11/23 14:15:32.893
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:32.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6733" for this suite. 08/11/23 14:15:32.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:32.925
Aug 11 14:15:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:15:32.926
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:32.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:32.944
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:15:32.946
Aug 11 14:15:32.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a" in namespace "downward-api-9959" to be "Succeeded or Failed"
Aug 11 14:15:32.959: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.840234ms
Aug 11 14:15:34.963: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Running", Reason="", readiness=false. Elapsed: 2.009375085s
Aug 11 14:15:36.962: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008518218s
STEP: Saw pod success 08/11/23 14:15:36.962
Aug 11 14:15:36.962: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a" satisfied condition "Succeeded or Failed"
Aug 11 14:15:36.965: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a container client-container: <nil>
STEP: delete the pod 08/11/23 14:15:36.987
Aug 11 14:15:36.997: INFO: Waiting for pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a to disappear
Aug 11 14:15:37.004: INFO: Pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:37.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9959" for this suite. 08/11/23 14:15:37.007
------------------------------
• [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:32.925
    Aug 11 14:15:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:15:32.926
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:32.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:32.944
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:15:32.946
    Aug 11 14:15:32.954: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a" in namespace "downward-api-9959" to be "Succeeded or Failed"
    Aug 11 14:15:32.959: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.840234ms
    Aug 11 14:15:34.963: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Running", Reason="", readiness=false. Elapsed: 2.009375085s
    Aug 11 14:15:36.962: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008518218s
    STEP: Saw pod success 08/11/23 14:15:36.962
    Aug 11 14:15:36.962: INFO: Pod "downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a" satisfied condition "Succeeded or Failed"
    Aug 11 14:15:36.965: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a container client-container: <nil>
    STEP: delete the pod 08/11/23 14:15:36.987
    Aug 11 14:15:36.997: INFO: Waiting for pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a to disappear
    Aug 11 14:15:37.004: INFO: Pod downwardapi-volume-92e416a8-bfb7-4c75-8f16-331394c1c70a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:37.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9959" for this suite. 08/11/23 14:15:37.007
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:37.014
Aug 11 14:15:37.014: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:15:37.015
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:37.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:37.03
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Aug 11 14:15:37.046: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/11/23 14:15:37.052
Aug 11 14:15:37.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:37.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/11/23 14:15:37.056
Aug 11 14:15:37.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:37.078: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:15:38.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:38.081: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:15:39.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:15:39.082: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/11/23 14:15:39.085
Aug 11 14:15:39.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:15:39.101: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 11 14:15:40.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:40.105: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/11/23 14:15:40.105
Aug 11 14:15:40.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:40.116: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:15:41.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:41.120: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:15:42.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:42.120: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
Aug 11 14:15:43.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:15:43.121: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:15:43.127
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2838, will wait for the garbage collector to delete the pods 08/11/23 14:15:43.127
Aug 11 14:15:43.186: INFO: Deleting DaemonSet.extensions daemon-set took: 6.266327ms
Aug 11 14:15:43.286: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732826ms
Aug 11 14:15:45.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:15:45.790: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:15:45.793: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12169"},"items":null}

Aug 11 14:15:45.796: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12169"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:45.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2838" for this suite. 08/11/23 14:15:45.817
------------------------------
• [SLOW TEST] [8.811 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:37.014
    Aug 11 14:15:37.014: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:15:37.015
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:37.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:37.03
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Aug 11 14:15:37.046: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/11/23 14:15:37.052
    Aug 11 14:15:37.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:37.056: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/11/23 14:15:37.056
    Aug 11 14:15:37.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:37.078: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:15:38.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:38.081: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:15:39.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:15:39.082: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/11/23 14:15:39.085
    Aug 11 14:15:39.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:15:39.101: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 11 14:15:40.105: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:40.105: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/11/23 14:15:40.105
    Aug 11 14:15:40.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:40.116: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:15:41.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:41.120: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:15:42.120: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:42.120: INFO: Node constell-1cf5d931-worker-6381a7ba-nd80 is running 0 daemon pod, expected 1
    Aug 11 14:15:43.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:15:43.121: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:15:43.127
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2838, will wait for the garbage collector to delete the pods 08/11/23 14:15:43.127
    Aug 11 14:15:43.186: INFO: Deleting DaemonSet.extensions daemon-set took: 6.266327ms
    Aug 11 14:15:43.286: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732826ms
    Aug 11 14:15:45.790: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:15:45.790: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:15:45.793: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12169"},"items":null}

    Aug 11 14:15:45.796: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12169"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:45.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2838" for this suite. 08/11/23 14:15:45.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:45.826
Aug 11 14:15:45.826: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:15:45.827
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:45.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:45.849
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/11/23 14:15:45.852
STEP: Getting a ResourceQuota 08/11/23 14:15:45.856
STEP: Listing all ResourceQuotas with LabelSelector 08/11/23 14:15:45.861
STEP: Patching the ResourceQuota 08/11/23 14:15:45.864
STEP: Deleting a Collection of ResourceQuotas 08/11/23 14:15:45.871
STEP: Verifying the deleted ResourceQuota 08/11/23 14:15:45.879
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:45.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6748" for this suite. 08/11/23 14:15:45.885
------------------------------
• [0.064 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:45.826
    Aug 11 14:15:45.826: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:15:45.827
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:45.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:45.849
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/11/23 14:15:45.852
    STEP: Getting a ResourceQuota 08/11/23 14:15:45.856
    STEP: Listing all ResourceQuotas with LabelSelector 08/11/23 14:15:45.861
    STEP: Patching the ResourceQuota 08/11/23 14:15:45.864
    STEP: Deleting a Collection of ResourceQuotas 08/11/23 14:15:45.871
    STEP: Verifying the deleted ResourceQuota 08/11/23 14:15:45.879
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:45.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6748" for this suite. 08/11/23 14:15:45.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:45.891
Aug 11 14:15:45.891: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:15:45.892
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:45.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:45.907
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:15:45.911
Aug 11 14:15:45.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d" in namespace "projected-1860" to be "Succeeded or Failed"
Aug 11 14:15:45.921: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484902ms
Aug 11 14:15:47.925: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006927772s
Aug 11 14:15:49.925: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275189s
STEP: Saw pod success 08/11/23 14:15:49.925
Aug 11 14:15:49.926: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d" satisfied condition "Succeeded or Failed"
Aug 11 14:15:49.928: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d container client-container: <nil>
STEP: delete the pod 08/11/23 14:15:49.937
Aug 11 14:15:49.952: INFO: Waiting for pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d to disappear
Aug 11 14:15:49.955: INFO: Pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:15:49.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1860" for this suite. 08/11/23 14:15:49.959
------------------------------
• [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:45.891
    Aug 11 14:15:45.891: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:15:45.892
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:45.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:45.907
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:15:45.911
    Aug 11 14:15:45.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d" in namespace "projected-1860" to be "Succeeded or Failed"
    Aug 11 14:15:45.921: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484902ms
    Aug 11 14:15:47.925: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006927772s
    Aug 11 14:15:49.925: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275189s
    STEP: Saw pod success 08/11/23 14:15:49.925
    Aug 11 14:15:49.926: INFO: Pod "downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d" satisfied condition "Succeeded or Failed"
    Aug 11 14:15:49.928: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d container client-container: <nil>
    STEP: delete the pod 08/11/23 14:15:49.937
    Aug 11 14:15:49.952: INFO: Waiting for pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d to disappear
    Aug 11 14:15:49.955: INFO: Pod downwardapi-volume-c1890e94-f3be-41fe-af11-ae807ca6a51d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:15:49.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1860" for this suite. 08/11/23 14:15:49.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:15:49.966
Aug 11 14:15:49.966: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:15:49.966
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:49.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:49.985
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:15:49.999
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:15:50.242
STEP: Deploying the webhook pod 08/11/23 14:15:50.251
STEP: Wait for the deployment to be ready 08/11/23 14:15:50.263
Aug 11 14:15:50.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:15:52.278
STEP: Verifying the service has paired with the endpoint 08/11/23 14:15:52.297
Aug 11 14:15:53.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 14:15:53.301
STEP: create a pod that should be denied by the webhook 08/11/23 14:15:53.327
STEP: create a pod that causes the webhook to hang 08/11/23 14:15:53.353
STEP: create a configmap that should be denied by the webhook 08/11/23 14:16:03.363
STEP: create a configmap that should be admitted by the webhook 08/11/23 14:16:03.415
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 14:16:03.431
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 14:16:03.44
STEP: create a namespace that bypass the webhook 08/11/23 14:16:03.447
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/11/23 14:16:03.453
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:16:03.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3163" for this suite. 08/11/23 14:16:03.533
STEP: Destroying namespace "webhook-3163-markers" for this suite. 08/11/23 14:16:03.545
------------------------------
• [SLOW TEST] [13.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:15:49.966
    Aug 11 14:15:49.966: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:15:49.966
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:15:49.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:15:49.985
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:15:49.999
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:15:50.242
    STEP: Deploying the webhook pod 08/11/23 14:15:50.251
    STEP: Wait for the deployment to be ready 08/11/23 14:15:50.263
    Aug 11 14:15:50.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:15:52.278
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:15:52.297
    Aug 11 14:15:53.297: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 14:15:53.301
    STEP: create a pod that should be denied by the webhook 08/11/23 14:15:53.327
    STEP: create a pod that causes the webhook to hang 08/11/23 14:15:53.353
    STEP: create a configmap that should be denied by the webhook 08/11/23 14:16:03.363
    STEP: create a configmap that should be admitted by the webhook 08/11/23 14:16:03.415
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 14:16:03.431
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 14:16:03.44
    STEP: create a namespace that bypass the webhook 08/11/23 14:16:03.447
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/11/23 14:16:03.453
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:16:03.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3163" for this suite. 08/11/23 14:16:03.533
    STEP: Destroying namespace "webhook-3163-markers" for this suite. 08/11/23 14:16:03.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:16:03.554
Aug 11 14:16:03.554: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:16:03.555
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:03.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:03.572
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/11/23 14:16:03.582
STEP: watching for Pod to be ready 08/11/23 14:16:03.59
Aug 11 14:16:03.591: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 11 14:16:03.599: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
Aug 11 14:16:03.609: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
Aug 11 14:16:04.787: INFO: Found Pod pod-test in namespace pods-3321 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/11/23 14:16:04.79
STEP: getting the Pod and ensuring that it's patched 08/11/23 14:16:04.8
STEP: replacing the Pod's status Ready condition to False 08/11/23 14:16:04.804
STEP: check the Pod again to ensure its Ready conditions are False 08/11/23 14:16:04.814
STEP: deleting the Pod via a Collection with a LabelSelector 08/11/23 14:16:04.814
STEP: watching for the Pod to be deleted 08/11/23 14:16:04.823
Aug 11 14:16:04.825: INFO: observed event type MODIFIED
Aug 11 14:16:06.790: INFO: observed event type MODIFIED
Aug 11 14:16:07.793: INFO: observed event type MODIFIED
Aug 11 14:16:07.805: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:16:07.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3321" for this suite. 08/11/23 14:16:07.816
------------------------------
• [4.268 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:16:03.554
    Aug 11 14:16:03.554: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:16:03.555
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:03.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:03.572
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/11/23 14:16:03.582
    STEP: watching for Pod to be ready 08/11/23 14:16:03.59
    Aug 11 14:16:03.591: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 11 14:16:03.599: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
    Aug 11 14:16:03.609: INFO: observed Pod pod-test in namespace pods-3321 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
    Aug 11 14:16:04.787: INFO: Found Pod pod-test in namespace pods-3321 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:16:03 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/11/23 14:16:04.79
    STEP: getting the Pod and ensuring that it's patched 08/11/23 14:16:04.8
    STEP: replacing the Pod's status Ready condition to False 08/11/23 14:16:04.804
    STEP: check the Pod again to ensure its Ready conditions are False 08/11/23 14:16:04.814
    STEP: deleting the Pod via a Collection with a LabelSelector 08/11/23 14:16:04.814
    STEP: watching for the Pod to be deleted 08/11/23 14:16:04.823
    Aug 11 14:16:04.825: INFO: observed event type MODIFIED
    Aug 11 14:16:06.790: INFO: observed event type MODIFIED
    Aug 11 14:16:07.793: INFO: observed event type MODIFIED
    Aug 11 14:16:07.805: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:16:07.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3321" for this suite. 08/11/23 14:16:07.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:16:07.822
Aug 11 14:16:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:16:07.823
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:07.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:07.841
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/11/23 14:16:07.843
STEP: Counting existing ResourceQuota 08/11/23 14:16:12.847
STEP: Creating a ResourceQuota 08/11/23 14:16:17.851
STEP: Ensuring resource quota status is calculated 08/11/23 14:16:17.857
STEP: Creating a Secret 08/11/23 14:16:19.862
STEP: Ensuring resource quota status captures secret creation 08/11/23 14:16:19.874
STEP: Deleting a secret 08/11/23 14:16:21.878
STEP: Ensuring resource quota status released usage 08/11/23 14:16:21.888
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:16:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6303" for this suite. 08/11/23 14:16:23.897
------------------------------
• [SLOW TEST] [16.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:16:07.822
    Aug 11 14:16:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:16:07.823
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:07.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:07.841
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/11/23 14:16:07.843
    STEP: Counting existing ResourceQuota 08/11/23 14:16:12.847
    STEP: Creating a ResourceQuota 08/11/23 14:16:17.851
    STEP: Ensuring resource quota status is calculated 08/11/23 14:16:17.857
    STEP: Creating a Secret 08/11/23 14:16:19.862
    STEP: Ensuring resource quota status captures secret creation 08/11/23 14:16:19.874
    STEP: Deleting a secret 08/11/23 14:16:21.878
    STEP: Ensuring resource quota status released usage 08/11/23 14:16:21.888
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:16:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6303" for this suite. 08/11/23 14:16:23.897
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:16:23.906
Aug 11 14:16:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:16:23.907
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:23.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:23.927
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-147e6a84-5e26-41b1-9635-cdb5e315249e 08/11/23 14:16:23.933
STEP: Creating secret with name s-test-opt-upd-429c4b93-a554-4ce9-b229-676f0970ef08 08/11/23 14:16:23.937
STEP: Creating the pod 08/11/23 14:16:23.942
Aug 11 14:16:23.951: INFO: Waiting up to 5m0s for pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b" in namespace "secrets-166" to be "running and ready"
Aug 11 14:16:23.955: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756885ms
Aug 11 14:16:23.955: INFO: The phase of Pod pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:16:25.960: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009012436s
Aug 11 14:16:25.960: INFO: The phase of Pod pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b is Running (Ready = true)
Aug 11 14:16:25.960: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-147e6a84-5e26-41b1-9635-cdb5e315249e 08/11/23 14:16:25.986
STEP: Updating secret s-test-opt-upd-429c4b93-a554-4ce9-b229-676f0970ef08 08/11/23 14:16:25.994
STEP: Creating secret with name s-test-opt-create-61e86e35-7ca0-4ec9-8a96-bb6fc0edbdcf 08/11/23 14:16:25.999
STEP: waiting to observe update in volume 08/11/23 14:16:26.003
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:16:30.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-166" for this suite. 08/11/23 14:16:30.053
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:16:23.906
    Aug 11 14:16:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:16:23.907
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:23.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:23.927
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-147e6a84-5e26-41b1-9635-cdb5e315249e 08/11/23 14:16:23.933
    STEP: Creating secret with name s-test-opt-upd-429c4b93-a554-4ce9-b229-676f0970ef08 08/11/23 14:16:23.937
    STEP: Creating the pod 08/11/23 14:16:23.942
    Aug 11 14:16:23.951: INFO: Waiting up to 5m0s for pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b" in namespace "secrets-166" to be "running and ready"
    Aug 11 14:16:23.955: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.756885ms
    Aug 11 14:16:23.955: INFO: The phase of Pod pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:16:25.960: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009012436s
    Aug 11 14:16:25.960: INFO: The phase of Pod pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b is Running (Ready = true)
    Aug 11 14:16:25.960: INFO: Pod "pod-secrets-1ea8eb91-22a5-4011-8d61-f97968546a5b" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-147e6a84-5e26-41b1-9635-cdb5e315249e 08/11/23 14:16:25.986
    STEP: Updating secret s-test-opt-upd-429c4b93-a554-4ce9-b229-676f0970ef08 08/11/23 14:16:25.994
    STEP: Creating secret with name s-test-opt-create-61e86e35-7ca0-4ec9-8a96-bb6fc0edbdcf 08/11/23 14:16:25.999
    STEP: waiting to observe update in volume 08/11/23 14:16:26.003
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:16:30.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-166" for this suite. 08/11/23 14:16:30.053
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:16:30.059
Aug 11 14:16:30.060: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 14:16:30.06
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:30.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:30.077
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/11/23 14:16:30.079
STEP: Ensuring active pods == parallelism 08/11/23 14:16:30.084
STEP: delete a job 08/11/23 14:16:34.088
STEP: deleting Job.batch foo in namespace job-8128, will wait for the garbage collector to delete the pods 08/11/23 14:16:34.088
Aug 11 14:16:34.147: INFO: Deleting Job.batch foo took: 5.570055ms
Aug 11 14:16:34.248: INFO: Terminating Job.batch foo pods took: 101.103558ms
STEP: Ensuring job was deleted 08/11/23 14:17:07.649
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:07.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8128" for this suite. 08/11/23 14:17:07.656
------------------------------
• [SLOW TEST] [37.606 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:16:30.059
    Aug 11 14:16:30.060: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 14:16:30.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:30.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:30.077
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/11/23 14:16:30.079
    STEP: Ensuring active pods == parallelism 08/11/23 14:16:30.084
    STEP: delete a job 08/11/23 14:16:34.088
    STEP: deleting Job.batch foo in namespace job-8128, will wait for the garbage collector to delete the pods 08/11/23 14:16:34.088
    Aug 11 14:16:34.147: INFO: Deleting Job.batch foo took: 5.570055ms
    Aug 11 14:16:34.248: INFO: Terminating Job.batch foo pods took: 101.103558ms
    STEP: Ensuring job was deleted 08/11/23 14:17:07.649
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:07.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8128" for this suite. 08/11/23 14:17:07.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:07.667
Aug 11 14:17:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:17:07.668
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:07.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:07.686
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 11 14:17:07.688: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/11/23 14:17:08.702
STEP: Checking rc "condition-test" has the desired failure condition set 08/11/23 14:17:08.708
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/11/23 14:17:09.714
Aug 11 14:17:09.722: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/11/23 14:17:09.722
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:10.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8277" for this suite. 08/11/23 14:17:10.734
------------------------------
• [3.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:07.667
    Aug 11 14:17:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:17:07.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:07.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:07.686
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 11 14:17:07.688: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/11/23 14:17:08.702
    STEP: Checking rc "condition-test" has the desired failure condition set 08/11/23 14:17:08.708
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/11/23 14:17:09.714
    Aug 11 14:17:09.722: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/11/23 14:17:09.722
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:10.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8277" for this suite. 08/11/23 14:17:10.734
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:10.74
Aug 11 14:17:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 14:17:10.741
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:10.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:10.759
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/11/23 14:17:10.765
STEP: create the rc2 08/11/23 14:17:10.769
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/11/23 14:17:15.785
STEP: delete the rc simpletest-rc-to-be-deleted 08/11/23 14:17:16.137
STEP: wait for the rc to be deleted 08/11/23 14:17:16.143
Aug 11 14:17:21.164: INFO: 81 pods remaining
Aug 11 14:17:21.164: INFO: 69 pods has nil DeletionTimestamp
Aug 11 14:17:21.164: INFO: 
Aug 11 14:17:26.154: INFO: 59 pods remaining
Aug 11 14:17:26.154: INFO: 50 pods has nil DeletionTimestamp
Aug 11 14:17:26.154: INFO: 
STEP: Gathering metrics 08/11/23 14:17:31.153
Aug 11 14:17:31.188: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 14:17:31.191: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.166384ms
Aug 11 14:17:31.191: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 14:17:31.191: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 14:17:31.276: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 11 14:17:31.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-246j5" in namespace "gc-8919"
Aug 11 14:17:31.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gt8f" in namespace "gc-8919"
Aug 11 14:17:31.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rk9x" in namespace "gc-8919"
Aug 11 14:17:31.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-4742x" in namespace "gc-8919"
Aug 11 14:17:31.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-494wt" in namespace "gc-8919"
Aug 11 14:17:31.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j5d6" in namespace "gc-8919"
Aug 11 14:17:31.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-687nf" in namespace "gc-8919"
Aug 11 14:17:31.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c7f7" in namespace "gc-8919"
Aug 11 14:17:31.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dbjl" in namespace "gc-8919"
Aug 11 14:17:31.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jbvn" in namespace "gc-8919"
Aug 11 14:17:31.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-6slk5" in namespace "gc-8919"
Aug 11 14:17:31.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-74ndq" in namespace "gc-8919"
Aug 11 14:17:31.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-758f5" in namespace "gc-8919"
Aug 11 14:17:31.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-78fnd" in namespace "gc-8919"
Aug 11 14:17:31.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-7clws" in namespace "gc-8919"
Aug 11 14:17:31.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dbrt" in namespace "gc-8919"
Aug 11 14:17:31.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7cm" in namespace "gc-8919"
Aug 11 14:17:31.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zhvn" in namespace "gc-8919"
Aug 11 14:17:31.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-97f6d" in namespace "gc-8919"
Aug 11 14:17:31.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rcsp" in namespace "gc-8919"
Aug 11 14:17:31.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2dd6" in namespace "gc-8919"
Aug 11 14:17:31.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl7pz" in namespace "gc-8919"
Aug 11 14:17:31.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7z27" in namespace "gc-8919"
Aug 11 14:17:31.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-crkgm" in namespace "gc-8919"
Aug 11 14:17:31.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwc7z" in namespace "gc-8919"
Aug 11 14:17:31.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7kvj" in namespace "gc-8919"
Aug 11 14:17:31.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbhxs" in namespace "gc-8919"
Aug 11 14:17:31.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq254" in namespace "gc-8919"
Aug 11 14:17:31.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-f27r2" in namespace "gc-8919"
Aug 11 14:17:31.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4zgk" in namespace "gc-8919"
Aug 11 14:17:31.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbdh8" in namespace "gc-8919"
Aug 11 14:17:31.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhq8p" in namespace "gc-8919"
Aug 11 14:17:31.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj8kk" in namespace "gc-8919"
Aug 11 14:17:31.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqw8k" in namespace "gc-8919"
Aug 11 14:17:31.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwkjp" in namespace "gc-8919"
Aug 11 14:17:31.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdsbj" in namespace "gc-8919"
Aug 11 14:17:31.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfrvf" in namespace "gc-8919"
Aug 11 14:17:31.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnq9k" in namespace "gc-8919"
Aug 11 14:17:31.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq8h7" in namespace "gc-8919"
Aug 11 14:17:31.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbgtv" in namespace "gc-8919"
Aug 11 14:17:31.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd8xq" in namespace "gc-8919"
Aug 11 14:17:31.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmjxt" in namespace "gc-8919"
Aug 11 14:17:31.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqbfh" in namespace "gc-8919"
Aug 11 14:17:31.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwbsc" in namespace "gc-8919"
Aug 11 14:17:31.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6rrk" in namespace "gc-8919"
Aug 11 14:17:31.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-jrn9h" in namespace "gc-8919"
Aug 11 14:17:31.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvcpt" in namespace "gc-8919"
Aug 11 14:17:31.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2jrp" in namespace "gc-8919"
Aug 11 14:17:31.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-lddcg" in namespace "gc-8919"
Aug 11 14:17:31.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-lnqmz" in namespace "gc-8919"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:32.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8919" for this suite. 08/11/23 14:17:32.009
------------------------------
• [SLOW TEST] [21.274 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:10.74
    Aug 11 14:17:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 14:17:10.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:10.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:10.759
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/11/23 14:17:10.765
    STEP: create the rc2 08/11/23 14:17:10.769
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/11/23 14:17:15.785
    STEP: delete the rc simpletest-rc-to-be-deleted 08/11/23 14:17:16.137
    STEP: wait for the rc to be deleted 08/11/23 14:17:16.143
    Aug 11 14:17:21.164: INFO: 81 pods remaining
    Aug 11 14:17:21.164: INFO: 69 pods has nil DeletionTimestamp
    Aug 11 14:17:21.164: INFO: 
    Aug 11 14:17:26.154: INFO: 59 pods remaining
    Aug 11 14:17:26.154: INFO: 50 pods has nil DeletionTimestamp
    Aug 11 14:17:26.154: INFO: 
    STEP: Gathering metrics 08/11/23 14:17:31.153
    Aug 11 14:17:31.188: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 14:17:31.191: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.166384ms
    Aug 11 14:17:31.191: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 14:17:31.191: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 14:17:31.276: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 11 14:17:31.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-246j5" in namespace "gc-8919"
    Aug 11 14:17:31.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gt8f" in namespace "gc-8919"
    Aug 11 14:17:31.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rk9x" in namespace "gc-8919"
    Aug 11 14:17:31.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-4742x" in namespace "gc-8919"
    Aug 11 14:17:31.355: INFO: Deleting pod "simpletest-rc-to-be-deleted-494wt" in namespace "gc-8919"
    Aug 11 14:17:31.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j5d6" in namespace "gc-8919"
    Aug 11 14:17:31.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-687nf" in namespace "gc-8919"
    Aug 11 14:17:31.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c7f7" in namespace "gc-8919"
    Aug 11 14:17:31.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dbjl" in namespace "gc-8919"
    Aug 11 14:17:31.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jbvn" in namespace "gc-8919"
    Aug 11 14:17:31.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-6slk5" in namespace "gc-8919"
    Aug 11 14:17:31.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-74ndq" in namespace "gc-8919"
    Aug 11 14:17:31.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-758f5" in namespace "gc-8919"
    Aug 11 14:17:31.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-78fnd" in namespace "gc-8919"
    Aug 11 14:17:31.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-7clws" in namespace "gc-8919"
    Aug 11 14:17:31.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dbrt" in namespace "gc-8919"
    Aug 11 14:17:31.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x7cm" in namespace "gc-8919"
    Aug 11 14:17:31.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zhvn" in namespace "gc-8919"
    Aug 11 14:17:31.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-97f6d" in namespace "gc-8919"
    Aug 11 14:17:31.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rcsp" in namespace "gc-8919"
    Aug 11 14:17:31.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2dd6" in namespace "gc-8919"
    Aug 11 14:17:31.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-bl7pz" in namespace "gc-8919"
    Aug 11 14:17:31.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7z27" in namespace "gc-8919"
    Aug 11 14:17:31.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-crkgm" in namespace "gc-8919"
    Aug 11 14:17:31.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwc7z" in namespace "gc-8919"
    Aug 11 14:17:31.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7kvj" in namespace "gc-8919"
    Aug 11 14:17:31.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbhxs" in namespace "gc-8919"
    Aug 11 14:17:31.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq254" in namespace "gc-8919"
    Aug 11 14:17:31.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-f27r2" in namespace "gc-8919"
    Aug 11 14:17:31.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4zgk" in namespace "gc-8919"
    Aug 11 14:17:31.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbdh8" in namespace "gc-8919"
    Aug 11 14:17:31.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhq8p" in namespace "gc-8919"
    Aug 11 14:17:31.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj8kk" in namespace "gc-8919"
    Aug 11 14:17:31.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqw8k" in namespace "gc-8919"
    Aug 11 14:17:31.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwkjp" in namespace "gc-8919"
    Aug 11 14:17:31.798: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdsbj" in namespace "gc-8919"
    Aug 11 14:17:31.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfrvf" in namespace "gc-8919"
    Aug 11 14:17:31.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnq9k" in namespace "gc-8919"
    Aug 11 14:17:31.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq8h7" in namespace "gc-8919"
    Aug 11 14:17:31.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbgtv" in namespace "gc-8919"
    Aug 11 14:17:31.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd8xq" in namespace "gc-8919"
    Aug 11 14:17:31.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmjxt" in namespace "gc-8919"
    Aug 11 14:17:31.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqbfh" in namespace "gc-8919"
    Aug 11 14:17:31.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwbsc" in namespace "gc-8919"
    Aug 11 14:17:31.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6rrk" in namespace "gc-8919"
    Aug 11 14:17:31.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-jrn9h" in namespace "gc-8919"
    Aug 11 14:17:31.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvcpt" in namespace "gc-8919"
    Aug 11 14:17:31.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2jrp" in namespace "gc-8919"
    Aug 11 14:17:31.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-lddcg" in namespace "gc-8919"
    Aug 11 14:17:31.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-lnqmz" in namespace "gc-8919"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:32.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8919" for this suite. 08/11/23 14:17:32.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:32.017
Aug 11 14:17:32.018: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 14:17:32.018
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:32.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:32.036
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/11/23 14:17:32.038
STEP: Ensure pods equal to parallelism count is attached to the job 08/11/23 14:17:32.044
STEP: patching /status 08/11/23 14:17:40.049
STEP: updating /status 08/11/23 14:17:40.056
STEP: get /status 08/11/23 14:17:40.063
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:40.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-463" for this suite. 08/11/23 14:17:40.071
------------------------------
• [SLOW TEST] [8.061 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:32.017
    Aug 11 14:17:32.018: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 14:17:32.018
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:32.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:32.036
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/11/23 14:17:32.038
    STEP: Ensure pods equal to parallelism count is attached to the job 08/11/23 14:17:32.044
    STEP: patching /status 08/11/23 14:17:40.049
    STEP: updating /status 08/11/23 14:17:40.056
    STEP: get /status 08/11/23 14:17:40.063
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:40.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-463" for this suite. 08/11/23 14:17:40.071
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:40.079
Aug 11 14:17:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:17:40.08
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:40.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:40.097
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-e8d9190b-a224-45cf-ad29-a766b35c750a 08/11/23 14:17:40.099
STEP: Creating a pod to test consume configMaps 08/11/23 14:17:40.104
Aug 11 14:17:40.111: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e" in namespace "projected-6777" to be "Succeeded or Failed"
Aug 11 14:17:40.114: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559902ms
Aug 11 14:17:42.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007090633s
Aug 11 14:17:44.124: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012374354s
Aug 11 14:17:46.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006739216s
STEP: Saw pod success 08/11/23 14:17:46.118
Aug 11 14:17:46.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e" satisfied condition "Succeeded or Failed"
Aug 11 14:17:46.121: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:17:46.128
Aug 11 14:17:46.141: INFO: Waiting for pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e to disappear
Aug 11 14:17:46.143: INFO: Pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6777" for this suite. 08/11/23 14:17:46.146
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:40.079
    Aug 11 14:17:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:17:40.08
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:40.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:40.097
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-e8d9190b-a224-45cf-ad29-a766b35c750a 08/11/23 14:17:40.099
    STEP: Creating a pod to test consume configMaps 08/11/23 14:17:40.104
    Aug 11 14:17:40.111: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e" in namespace "projected-6777" to be "Succeeded or Failed"
    Aug 11 14:17:40.114: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.559902ms
    Aug 11 14:17:42.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007090633s
    Aug 11 14:17:44.124: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012374354s
    Aug 11 14:17:46.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006739216s
    STEP: Saw pod success 08/11/23 14:17:46.118
    Aug 11 14:17:46.118: INFO: Pod "pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e" satisfied condition "Succeeded or Failed"
    Aug 11 14:17:46.121: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:17:46.128
    Aug 11 14:17:46.141: INFO: Waiting for pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e to disappear
    Aug 11 14:17:46.143: INFO: Pod pod-projected-configmaps-efd086fa-21a9-47fb-9a50-e7214c78508e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6777" for this suite. 08/11/23 14:17:46.146
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:46.152
Aug 11 14:17:46.152: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename watch 08/11/23 14:17:46.153
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:46.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:46.17
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/11/23 14:17:46.173
STEP: modifying the configmap once 08/11/23 14:17:46.177
STEP: modifying the configmap a second time 08/11/23 14:17:46.185
STEP: deleting the configmap 08/11/23 14:17:46.192
STEP: creating a watch on configmaps from the resource version returned by the first update 08/11/23 14:17:46.198
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/11/23 14:17:46.199
Aug 11 14:17:46.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7632  ebf6be8f-e022-4546-acce-2f2ca1bc7a5c 14901 0 2023-08-11 14:17:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:17:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:17:46.199: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7632  ebf6be8f-e022-4546-acce-2f2ca1bc7a5c 14902 0 2023-08-11 14:17:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:17:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7632" for this suite. 08/11/23 14:17:46.203
------------------------------
• [0.057 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:46.152
    Aug 11 14:17:46.152: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename watch 08/11/23 14:17:46.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:46.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:46.17
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/11/23 14:17:46.173
    STEP: modifying the configmap once 08/11/23 14:17:46.177
    STEP: modifying the configmap a second time 08/11/23 14:17:46.185
    STEP: deleting the configmap 08/11/23 14:17:46.192
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/11/23 14:17:46.198
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/11/23 14:17:46.199
    Aug 11 14:17:46.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7632  ebf6be8f-e022-4546-acce-2f2ca1bc7a5c 14901 0 2023-08-11 14:17:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:17:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:17:46.199: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7632  ebf6be8f-e022-4546-acce-2f2ca1bc7a5c 14902 0 2023-08-11 14:17:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:17:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:46.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7632" for this suite. 08/11/23 14:17:46.203
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:46.209
Aug 11 14:17:46.209: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 14:17:46.21
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:46.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:46.226
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/11/23 14:17:46.229
STEP: Ensuring job reaches completions 08/11/23 14:17:46.234
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:58.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6990" for this suite. 08/11/23 14:17:58.243
------------------------------
• [SLOW TEST] [12.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:46.209
    Aug 11 14:17:46.209: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 14:17:46.21
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:46.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:46.226
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/11/23 14:17:46.229
    STEP: Ensuring job reaches completions 08/11/23 14:17:46.234
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:58.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6990" for this suite. 08/11/23 14:17:58.243
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:58.249
Aug 11 14:17:58.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename csiinlinevolumes 08/11/23 14:17:58.25
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.267
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/11/23 14:17:58.269
STEP: getting 08/11/23 14:17:58.286
STEP: listing 08/11/23 14:17:58.29
STEP: deleting 08/11/23 14:17:58.293
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:58.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2068" for this suite. 08/11/23 14:17:58.313
------------------------------
• [0.069 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:58.249
    Aug 11 14:17:58.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename csiinlinevolumes 08/11/23 14:17:58.25
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.267
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/11/23 14:17:58.269
    STEP: getting 08/11/23 14:17:58.286
    STEP: listing 08/11/23 14:17:58.29
    STEP: deleting 08/11/23 14:17:58.293
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:58.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2068" for this suite. 08/11/23 14:17:58.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:58.32
Aug 11 14:17:58.320: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename cronjob 08/11/23 14:17:58.32
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.339
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/11/23 14:17:58.341
STEP: creating 08/11/23 14:17:58.341
STEP: getting 08/11/23 14:17:58.346
STEP: listing 08/11/23 14:17:58.349
STEP: watching 08/11/23 14:17:58.352
Aug 11 14:17:58.352: INFO: starting watch
STEP: cluster-wide listing 08/11/23 14:17:58.353
STEP: cluster-wide watching 08/11/23 14:17:58.356
Aug 11 14:17:58.356: INFO: starting watch
STEP: patching 08/11/23 14:17:58.357
STEP: updating 08/11/23 14:17:58.365
Aug 11 14:17:58.373: INFO: waiting for watch events with expected annotations
Aug 11 14:17:58.373: INFO: saw patched and updated annotations
STEP: patching /status 08/11/23 14:17:58.373
STEP: updating /status 08/11/23 14:17:58.38
STEP: get /status 08/11/23 14:17:58.386
STEP: deleting 08/11/23 14:17:58.389
STEP: deleting a collection 08/11/23 14:17:58.402
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 11 14:17:58.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8115" for this suite. 08/11/23 14:17:58.417
------------------------------
• [0.102 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:58.32
    Aug 11 14:17:58.320: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:17:58.32
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.339
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/11/23 14:17:58.341
    STEP: creating 08/11/23 14:17:58.341
    STEP: getting 08/11/23 14:17:58.346
    STEP: listing 08/11/23 14:17:58.349
    STEP: watching 08/11/23 14:17:58.352
    Aug 11 14:17:58.352: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 14:17:58.353
    STEP: cluster-wide watching 08/11/23 14:17:58.356
    Aug 11 14:17:58.356: INFO: starting watch
    STEP: patching 08/11/23 14:17:58.357
    STEP: updating 08/11/23 14:17:58.365
    Aug 11 14:17:58.373: INFO: waiting for watch events with expected annotations
    Aug 11 14:17:58.373: INFO: saw patched and updated annotations
    STEP: patching /status 08/11/23 14:17:58.373
    STEP: updating /status 08/11/23 14:17:58.38
    STEP: get /status 08/11/23 14:17:58.386
    STEP: deleting 08/11/23 14:17:58.389
    STEP: deleting a collection 08/11/23 14:17:58.402
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:17:58.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8115" for this suite. 08/11/23 14:17:58.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:17:58.425
Aug 11 14:17:58.425: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:17:58.425
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.441
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/11/23 14:17:58.443
Aug 11 14:17:58.451: INFO: Waiting up to 5m0s for pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579" in namespace "emptydir-6850" to be "Succeeded or Failed"
Aug 11 14:17:58.454: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343393ms
Aug 11 14:18:00.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006822054s
Aug 11 14:18:02.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006623249s
STEP: Saw pod success 08/11/23 14:18:02.458
Aug 11 14:18:02.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579" satisfied condition "Succeeded or Failed"
Aug 11 14:18:02.461: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 container test-container: <nil>
STEP: delete the pod 08/11/23 14:18:02.468
Aug 11 14:18:02.482: INFO: Waiting for pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 to disappear
Aug 11 14:18:02.485: INFO: Pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:02.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6850" for this suite. 08/11/23 14:18:02.488
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:17:58.425
    Aug 11 14:17:58.425: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:17:58.425
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:58.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:58.441
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/11/23 14:17:58.443
    Aug 11 14:17:58.451: INFO: Waiting up to 5m0s for pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579" in namespace "emptydir-6850" to be "Succeeded or Failed"
    Aug 11 14:17:58.454: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343393ms
    Aug 11 14:18:00.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006822054s
    Aug 11 14:18:02.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006623249s
    STEP: Saw pod success 08/11/23 14:18:02.458
    Aug 11 14:18:02.458: INFO: Pod "pod-f3bb49ba-d139-48b0-bf56-950699f93579" satisfied condition "Succeeded or Failed"
    Aug 11 14:18:02.461: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:18:02.468
    Aug 11 14:18:02.482: INFO: Waiting for pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 to disappear
    Aug 11 14:18:02.485: INFO: Pod pod-f3bb49ba-d139-48b0-bf56-950699f93579 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:02.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6850" for this suite. 08/11/23 14:18:02.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:02.496
Aug 11 14:18:02.496: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename subpath 08/11/23 14:18:02.497
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:02.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:02.514
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:18:02.516
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-h42g 08/11/23 14:18:02.524
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:18:02.525
Aug 11 14:18:02.532: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h42g" in namespace "subpath-8645" to be "Succeeded or Failed"
Aug 11 14:18:02.536: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518074ms
Aug 11 14:18:04.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939295s
Aug 11 14:18:06.539: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 4.007044079s
Aug 11 14:18:08.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 6.007350935s
Aug 11 14:18:10.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 8.008497414s
Aug 11 14:18:12.542: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 10.009989641s
Aug 11 14:18:14.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 12.008296655s
Aug 11 14:18:16.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 14.007834521s
Aug 11 14:18:18.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 16.008538908s
Aug 11 14:18:20.542: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 18.009396085s
Aug 11 14:18:22.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 20.008456621s
Aug 11 14:18:24.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=false. Elapsed: 22.009016107s
Aug 11 14:18:26.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008167281s
STEP: Saw pod success 08/11/23 14:18:26.541
Aug 11 14:18:26.541: INFO: Pod "pod-subpath-test-configmap-h42g" satisfied condition "Succeeded or Failed"
Aug 11 14:18:26.544: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-configmap-h42g container test-container-subpath-configmap-h42g: <nil>
STEP: delete the pod 08/11/23 14:18:26.555
Aug 11 14:18:26.568: INFO: Waiting for pod pod-subpath-test-configmap-h42g to disappear
Aug 11 14:18:26.570: INFO: Pod pod-subpath-test-configmap-h42g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h42g 08/11/23 14:18:26.571
Aug 11 14:18:26.571: INFO: Deleting pod "pod-subpath-test-configmap-h42g" in namespace "subpath-8645"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:26.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8645" for this suite. 08/11/23 14:18:26.577
------------------------------
• [SLOW TEST] [24.086 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:02.496
    Aug 11 14:18:02.496: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename subpath 08/11/23 14:18:02.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:02.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:02.514
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:18:02.516
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-h42g 08/11/23 14:18:02.524
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:18:02.525
    Aug 11 14:18:02.532: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h42g" in namespace "subpath-8645" to be "Succeeded or Failed"
    Aug 11 14:18:02.536: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518074ms
    Aug 11 14:18:04.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939295s
    Aug 11 14:18:06.539: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 4.007044079s
    Aug 11 14:18:08.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 6.007350935s
    Aug 11 14:18:10.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 8.008497414s
    Aug 11 14:18:12.542: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 10.009989641s
    Aug 11 14:18:14.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 12.008296655s
    Aug 11 14:18:16.540: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 14.007834521s
    Aug 11 14:18:18.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 16.008538908s
    Aug 11 14:18:20.542: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 18.009396085s
    Aug 11 14:18:22.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=true. Elapsed: 20.008456621s
    Aug 11 14:18:24.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Running", Reason="", readiness=false. Elapsed: 22.009016107s
    Aug 11 14:18:26.541: INFO: Pod "pod-subpath-test-configmap-h42g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008167281s
    STEP: Saw pod success 08/11/23 14:18:26.541
    Aug 11 14:18:26.541: INFO: Pod "pod-subpath-test-configmap-h42g" satisfied condition "Succeeded or Failed"
    Aug 11 14:18:26.544: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-configmap-h42g container test-container-subpath-configmap-h42g: <nil>
    STEP: delete the pod 08/11/23 14:18:26.555
    Aug 11 14:18:26.568: INFO: Waiting for pod pod-subpath-test-configmap-h42g to disappear
    Aug 11 14:18:26.570: INFO: Pod pod-subpath-test-configmap-h42g no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-h42g 08/11/23 14:18:26.571
    Aug 11 14:18:26.571: INFO: Deleting pod "pod-subpath-test-configmap-h42g" in namespace "subpath-8645"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:26.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8645" for this suite. 08/11/23 14:18:26.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:26.583
Aug 11 14:18:26.583: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename limitrange 08/11/23 14:18:26.584
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:26.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:26.601
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/11/23 14:18:26.603
STEP: Setting up watch 08/11/23 14:18:26.603
STEP: Submitting a LimitRange 08/11/23 14:18:26.706
STEP: Verifying LimitRange creation was observed 08/11/23 14:18:26.712
STEP: Fetching the LimitRange to ensure it has proper values 08/11/23 14:18:26.712
Aug 11 14:18:26.715: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 11 14:18:26.715: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/11/23 14:18:26.715
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/11/23 14:18:26.721
Aug 11 14:18:26.727: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 11 14:18:26.727: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/11/23 14:18:26.727
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/11/23 14:18:26.733
Aug 11 14:18:26.737: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 11 14:18:26.737: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/11/23 14:18:26.737
STEP: Failing to create a Pod with more than max resources 08/11/23 14:18:26.738
STEP: Updating a LimitRange 08/11/23 14:18:26.74
STEP: Verifying LimitRange updating is effective 08/11/23 14:18:26.745
STEP: Creating a Pod with less than former min resources 08/11/23 14:18:28.75
STEP: Failing to create a Pod with more than max resources 08/11/23 14:18:28.756
STEP: Deleting a LimitRange 08/11/23 14:18:28.757
STEP: Verifying the LimitRange was deleted 08/11/23 14:18:28.767
Aug 11 14:18:33.772: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/11/23 14:18:33.772
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:33.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5843" for this suite. 08/11/23 14:18:33.784
------------------------------
• [SLOW TEST] [7.210 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:26.583
    Aug 11 14:18:26.583: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename limitrange 08/11/23 14:18:26.584
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:26.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:26.601
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/11/23 14:18:26.603
    STEP: Setting up watch 08/11/23 14:18:26.603
    STEP: Submitting a LimitRange 08/11/23 14:18:26.706
    STEP: Verifying LimitRange creation was observed 08/11/23 14:18:26.712
    STEP: Fetching the LimitRange to ensure it has proper values 08/11/23 14:18:26.712
    Aug 11 14:18:26.715: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 11 14:18:26.715: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/11/23 14:18:26.715
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/11/23 14:18:26.721
    Aug 11 14:18:26.727: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 11 14:18:26.727: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/11/23 14:18:26.727
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/11/23 14:18:26.733
    Aug 11 14:18:26.737: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 11 14:18:26.737: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/11/23 14:18:26.737
    STEP: Failing to create a Pod with more than max resources 08/11/23 14:18:26.738
    STEP: Updating a LimitRange 08/11/23 14:18:26.74
    STEP: Verifying LimitRange updating is effective 08/11/23 14:18:26.745
    STEP: Creating a Pod with less than former min resources 08/11/23 14:18:28.75
    STEP: Failing to create a Pod with more than max resources 08/11/23 14:18:28.756
    STEP: Deleting a LimitRange 08/11/23 14:18:28.757
    STEP: Verifying the LimitRange was deleted 08/11/23 14:18:28.767
    Aug 11 14:18:33.772: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/11/23 14:18:33.772
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:33.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5843" for this suite. 08/11/23 14:18:33.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:33.794
Aug 11 14:18:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:18:33.795
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:33.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:33.811
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 11 14:18:33.813: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:36.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8266" for this suite. 08/11/23 14:18:36.912
------------------------------
• [3.125 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:33.794
    Aug 11 14:18:33.794: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:18:33.795
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:33.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:33.811
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 11 14:18:33.813: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:36.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8266" for this suite. 08/11/23 14:18:36.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:36.92
Aug 11 14:18:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 14:18:36.92
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:36.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:36.938
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/11/23 14:18:36.94
STEP: delete the rc 08/11/23 14:18:41.952
STEP: wait for all pods to be garbage collected 08/11/23 14:18:41.958
STEP: Gathering metrics 08/11/23 14:18:46.965
Aug 11 14:18:46.986: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 14:18:46.989: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.662472ms
Aug 11 14:18:46.989: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 14:18:46.989: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 14:18:47.059: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:47.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3957" for this suite. 08/11/23 14:18:47.063
------------------------------
• [SLOW TEST] [10.150 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:36.92
    Aug 11 14:18:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 14:18:36.92
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:36.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:36.938
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/11/23 14:18:36.94
    STEP: delete the rc 08/11/23 14:18:41.952
    STEP: wait for all pods to be garbage collected 08/11/23 14:18:41.958
    STEP: Gathering metrics 08/11/23 14:18:46.965
    Aug 11 14:18:46.986: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 14:18:46.989: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.662472ms
    Aug 11 14:18:46.989: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 14:18:46.989: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 14:18:47.059: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:47.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3957" for this suite. 08/11/23 14:18:47.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:47.071
Aug 11 14:18:47.071: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption 08/11/23 14:18:47.071
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:47.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:47.087
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/11/23 14:18:47.094
STEP: Updating PodDisruptionBudget status 08/11/23 14:18:49.102
STEP: Waiting for all pods to be running 08/11/23 14:18:49.11
Aug 11 14:18:49.116: INFO: running pods: 0 < 1
STEP: locating a running pod 08/11/23 14:18:51.12
STEP: Waiting for the pdb to be processed 08/11/23 14:18:51.132
STEP: Patching PodDisruptionBudget status 08/11/23 14:18:51.143
STEP: Waiting for the pdb to be processed 08/11/23 14:18:51.15
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:51.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3834" for this suite. 08/11/23 14:18:51.158
------------------------------
• [4.095 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:47.071
    Aug 11 14:18:47.071: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption 08/11/23 14:18:47.071
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:47.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:47.087
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/11/23 14:18:47.094
    STEP: Updating PodDisruptionBudget status 08/11/23 14:18:49.102
    STEP: Waiting for all pods to be running 08/11/23 14:18:49.11
    Aug 11 14:18:49.116: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/11/23 14:18:51.12
    STEP: Waiting for the pdb to be processed 08/11/23 14:18:51.132
    STEP: Patching PodDisruptionBudget status 08/11/23 14:18:51.143
    STEP: Waiting for the pdb to be processed 08/11/23 14:18:51.15
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:51.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3834" for this suite. 08/11/23 14:18:51.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:51.166
Aug 11 14:18:51.166: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:18:51.167
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:51.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:51.184
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-01e7a21d-1061-4c59-86b8-4f5fffa6f8ed 08/11/23 14:18:51.186
STEP: Creating a pod to test consume secrets 08/11/23 14:18:51.191
Aug 11 14:18:51.197: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f" in namespace "projected-4879" to be "Succeeded or Failed"
Aug 11 14:18:51.200: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272582ms
Aug 11 14:18:53.204: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636191s
Aug 11 14:18:55.204: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007133529s
STEP: Saw pod success 08/11/23 14:18:55.204
Aug 11 14:18:55.205: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f" satisfied condition "Succeeded or Failed"
Aug 11 14:18:55.207: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:18:55.216
Aug 11 14:18:55.230: INFO: Waiting for pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f to disappear
Aug 11 14:18:55.232: INFO: Pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:18:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4879" for this suite. 08/11/23 14:18:55.235
------------------------------
• [4.076 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:51.166
    Aug 11 14:18:51.166: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:18:51.167
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:51.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:51.184
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-01e7a21d-1061-4c59-86b8-4f5fffa6f8ed 08/11/23 14:18:51.186
    STEP: Creating a pod to test consume secrets 08/11/23 14:18:51.191
    Aug 11 14:18:51.197: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f" in namespace "projected-4879" to be "Succeeded or Failed"
    Aug 11 14:18:51.200: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272582ms
    Aug 11 14:18:53.204: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636191s
    Aug 11 14:18:55.204: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007133529s
    STEP: Saw pod success 08/11/23 14:18:55.204
    Aug 11 14:18:55.205: INFO: Pod "pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f" satisfied condition "Succeeded or Failed"
    Aug 11 14:18:55.207: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:18:55.216
    Aug 11 14:18:55.230: INFO: Waiting for pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f to disappear
    Aug 11 14:18:55.232: INFO: Pod pod-projected-secrets-508bc971-d3e8-4bbb-815d-930e90399d4f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:18:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4879" for this suite. 08/11/23 14:18:55.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:18:55.244
Aug 11 14:18:55.244: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:18:55.244
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:55.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:55.261
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/11/23 14:18:55.27
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/11/23 14:19:12.343
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/11/23 14:19:12.346
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/11/23 14:19:12.352
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/11/23 14:19:12.352
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/11/23 14:19:12.375
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/11/23 14:19:15.39
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/11/23 14:19:17.402
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/11/23 14:19:17.408
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/11/23 14:19:17.408
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/11/23 14:19:17.426
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/11/23 14:19:18.432
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/11/23 14:19:20.443
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/11/23 14:19:20.448
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/11/23 14:19:20.448
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 11 14:19:20.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1060" for this suite. 08/11/23 14:19:20.476
------------------------------
• [SLOW TEST] [25.239 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:18:55.244
    Aug 11 14:18:55.244: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:18:55.244
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:18:55.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:18:55.261
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/11/23 14:18:55.27
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/11/23 14:19:12.343
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/11/23 14:19:12.346
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/11/23 14:19:12.352
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/11/23 14:19:12.352
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/11/23 14:19:12.375
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/11/23 14:19:15.39
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/11/23 14:19:17.402
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/11/23 14:19:17.408
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/11/23 14:19:17.408
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/11/23 14:19:17.426
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/11/23 14:19:18.432
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/11/23 14:19:20.443
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/11/23 14:19:20.448
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/11/23 14:19:20.448
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:19:20.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1060" for this suite. 08/11/23 14:19:20.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:19:20.483
Aug 11 14:19:20.484: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 14:19:20.484
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:20.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:20.502
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/11/23 14:19:20.504
STEP: Ensuring active pods == parallelism 08/11/23 14:19:20.509
STEP: Orphaning one of the Job's Pods 08/11/23 14:19:24.514
Aug 11 14:19:25.028: INFO: Successfully updated pod "adopt-release-q2bb5"
STEP: Checking that the Job readopts the Pod 08/11/23 14:19:25.028
Aug 11 14:19:25.029: INFO: Waiting up to 15m0s for pod "adopt-release-q2bb5" in namespace "job-2501" to be "adopted"
Aug 11 14:19:25.031: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.599253ms
Aug 11 14:19:27.035: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006222061s
Aug 11 14:19:27.035: INFO: Pod "adopt-release-q2bb5" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/11/23 14:19:27.035
Aug 11 14:19:27.547: INFO: Successfully updated pod "adopt-release-q2bb5"
STEP: Checking that the Job releases the Pod 08/11/23 14:19:27.547
Aug 11 14:19:27.547: INFO: Waiting up to 15m0s for pod "adopt-release-q2bb5" in namespace "job-2501" to be "released"
Aug 11 14:19:27.549: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.469353ms
Aug 11 14:19:29.554: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006864132s
Aug 11 14:19:29.554: INFO: Pod "adopt-release-q2bb5" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 14:19:29.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2501" for this suite. 08/11/23 14:19:29.558
------------------------------
• [SLOW TEST] [9.081 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:19:20.483
    Aug 11 14:19:20.484: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 14:19:20.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:20.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:20.502
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/11/23 14:19:20.504
    STEP: Ensuring active pods == parallelism 08/11/23 14:19:20.509
    STEP: Orphaning one of the Job's Pods 08/11/23 14:19:24.514
    Aug 11 14:19:25.028: INFO: Successfully updated pod "adopt-release-q2bb5"
    STEP: Checking that the Job readopts the Pod 08/11/23 14:19:25.028
    Aug 11 14:19:25.029: INFO: Waiting up to 15m0s for pod "adopt-release-q2bb5" in namespace "job-2501" to be "adopted"
    Aug 11 14:19:25.031: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.599253ms
    Aug 11 14:19:27.035: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006222061s
    Aug 11 14:19:27.035: INFO: Pod "adopt-release-q2bb5" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/11/23 14:19:27.035
    Aug 11 14:19:27.547: INFO: Successfully updated pod "adopt-release-q2bb5"
    STEP: Checking that the Job releases the Pod 08/11/23 14:19:27.547
    Aug 11 14:19:27.547: INFO: Waiting up to 15m0s for pod "adopt-release-q2bb5" in namespace "job-2501" to be "released"
    Aug 11 14:19:27.549: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.469353ms
    Aug 11 14:19:29.554: INFO: Pod "adopt-release-q2bb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006864132s
    Aug 11 14:19:29.554: INFO: Pod "adopt-release-q2bb5" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:19:29.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2501" for this suite. 08/11/23 14:19:29.558
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:19:29.565
Aug 11 14:19:29.565: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:19:29.566
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:29.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:29.585
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/11/23 14:19:29.587
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:19:29.592
STEP: Creating a ResourceQuota with not terminating scope 08/11/23 14:19:31.596
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:19:31.602
STEP: Creating a long running pod 08/11/23 14:19:33.606
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/11/23 14:19:33.62
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/11/23 14:19:35.624
STEP: Deleting the pod 08/11/23 14:19:37.629
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:19:37.643
STEP: Creating a terminating pod 08/11/23 14:19:39.646
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/11/23 14:19:39.657
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/11/23 14:19:41.661
STEP: Deleting the pod 08/11/23 14:19:43.665
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:19:43.681
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:19:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6391" for this suite. 08/11/23 14:19:45.688
------------------------------
• [SLOW TEST] [16.131 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:19:29.565
    Aug 11 14:19:29.565: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:19:29.566
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:29.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:29.585
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/11/23 14:19:29.587
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:19:29.592
    STEP: Creating a ResourceQuota with not terminating scope 08/11/23 14:19:31.596
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:19:31.602
    STEP: Creating a long running pod 08/11/23 14:19:33.606
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/11/23 14:19:33.62
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/11/23 14:19:35.624
    STEP: Deleting the pod 08/11/23 14:19:37.629
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:19:37.643
    STEP: Creating a terminating pod 08/11/23 14:19:39.646
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/11/23 14:19:39.657
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/11/23 14:19:41.661
    STEP: Deleting the pod 08/11/23 14:19:43.665
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:19:43.681
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:19:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6391" for this suite. 08/11/23 14:19:45.688
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:19:45.696
Aug 11 14:19:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename events 08/11/23 14:19:45.697
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.713
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/11/23 14:19:45.715
Aug 11 14:19:45.724: INFO: created test-event-1
Aug 11 14:19:45.728: INFO: created test-event-2
Aug 11 14:19:45.732: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/11/23 14:19:45.732
STEP: delete collection of events 08/11/23 14:19:45.735
Aug 11 14:19:45.735: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/11/23 14:19:45.754
Aug 11 14:19:45.754: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 11 14:19:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4122" for this suite. 08/11/23 14:19:45.76
------------------------------
• [0.070 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:19:45.696
    Aug 11 14:19:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename events 08/11/23 14:19:45.697
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.713
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/11/23 14:19:45.715
    Aug 11 14:19:45.724: INFO: created test-event-1
    Aug 11 14:19:45.728: INFO: created test-event-2
    Aug 11 14:19:45.732: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/11/23 14:19:45.732
    STEP: delete collection of events 08/11/23 14:19:45.735
    Aug 11 14:19:45.735: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/11/23 14:19:45.754
    Aug 11 14:19:45.754: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:19:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4122" for this suite. 08/11/23 14:19:45.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:19:45.768
Aug 11 14:19:45.768: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:19:45.768
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.783
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:19:45.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1662" for this suite. 08/11/23 14:19:45.814
------------------------------
• [0.063 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:19:45.768
    Aug 11 14:19:45.768: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:19:45.768
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.783
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:19:45.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1662" for this suite. 08/11/23 14:19:45.814
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:19:45.831
Aug 11 14:19:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:19:45.832
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.863
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3170 08/11/23 14:19:45.865
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/11/23 14:19:45.87
Aug 11 14:19:45.883: INFO: Found 0 stateful pods, waiting for 3
Aug 11 14:19:55.889: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:19:55.889: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:19:55.889: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/11/23 14:19:55.898
Aug 11 14:19:55.920: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/11/23 14:19:55.92
STEP: Not applying an update when the partition is greater than the number of replicas 08/11/23 14:20:05.933
STEP: Performing a canary update 08/11/23 14:20:05.933
Aug 11 14:20:05.952: INFO: Updating stateful set ss2
Aug 11 14:20:05.960: INFO: Waiting for Pod statefulset-3170/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/11/23 14:20:15.969
Aug 11 14:20:16.015: INFO: Found 2 stateful pods, waiting for 3
Aug 11 14:20:26.020: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:20:26.020: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:20:26.020: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/11/23 14:20:26.025
Aug 11 14:20:26.044: INFO: Updating stateful set ss2
Aug 11 14:20:26.052: INFO: Waiting for Pod statefulset-3170/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 11 14:20:36.081: INFO: Updating stateful set ss2
Aug 11 14:20:36.089: INFO: Waiting for StatefulSet statefulset-3170/ss2 to complete update
Aug 11 14:20:36.089: INFO: Waiting for Pod statefulset-3170/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:20:46.097: INFO: Deleting all statefulset in ns statefulset-3170
Aug 11 14:20:46.100: INFO: Scaling statefulset ss2 to 0
Aug 11 14:20:56.117: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:20:56.120: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:20:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3170" for this suite. 08/11/23 14:20:56.136
------------------------------
• [SLOW TEST] [70.315 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:19:45.831
    Aug 11 14:19:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:19:45.832
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:19:45.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:19:45.863
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3170 08/11/23 14:19:45.865
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/11/23 14:19:45.87
    Aug 11 14:19:45.883: INFO: Found 0 stateful pods, waiting for 3
    Aug 11 14:19:55.889: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:19:55.889: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:19:55.889: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/11/23 14:19:55.898
    Aug 11 14:19:55.920: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/11/23 14:19:55.92
    STEP: Not applying an update when the partition is greater than the number of replicas 08/11/23 14:20:05.933
    STEP: Performing a canary update 08/11/23 14:20:05.933
    Aug 11 14:20:05.952: INFO: Updating stateful set ss2
    Aug 11 14:20:05.960: INFO: Waiting for Pod statefulset-3170/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/11/23 14:20:15.969
    Aug 11 14:20:16.015: INFO: Found 2 stateful pods, waiting for 3
    Aug 11 14:20:26.020: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:20:26.020: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:20:26.020: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/11/23 14:20:26.025
    Aug 11 14:20:26.044: INFO: Updating stateful set ss2
    Aug 11 14:20:26.052: INFO: Waiting for Pod statefulset-3170/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 11 14:20:36.081: INFO: Updating stateful set ss2
    Aug 11 14:20:36.089: INFO: Waiting for StatefulSet statefulset-3170/ss2 to complete update
    Aug 11 14:20:36.089: INFO: Waiting for Pod statefulset-3170/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:20:46.097: INFO: Deleting all statefulset in ns statefulset-3170
    Aug 11 14:20:46.100: INFO: Scaling statefulset ss2 to 0
    Aug 11 14:20:56.117: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:20:56.120: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:20:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3170" for this suite. 08/11/23 14:20:56.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:20:56.148
Aug 11 14:20:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:20:56.149
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:20:56.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:20:56.166
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 11 14:20:56.181: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:21:56.221: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/11/23 14:21:56.224
Aug 11 14:21:56.245: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 11 14:21:56.256: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 11 14:21:56.280: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 11 14:21:56.289: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/11/23 14:21:56.289
Aug 11 14:21:56.289: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6595" to be "running"
Aug 11 14:21:56.296: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146316ms
Aug 11 14:21:58.301: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011911797s
Aug 11 14:21:58.301: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 11 14:21:58.301: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
Aug 11 14:21:58.304: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.916323ms
Aug 11 14:21:58.304: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 14:21:58.304: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
Aug 11 14:21:58.307: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.725643ms
Aug 11 14:21:58.307: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 14:21:58.307: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
Aug 11 14:21:58.310: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.866763ms
Aug 11 14:21:58.310: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/11/23 14:21:58.31
Aug 11 14:21:58.317: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6595" to be "running"
Aug 11 14:21:58.319: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852823ms
Aug 11 14:22:00.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006940373s
Aug 11 14:22:02.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007132989s
Aug 11 14:22:04.324: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007883736s
Aug 11 14:22:04.324: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6595" for this suite. 08/11/23 14:22:04.374
------------------------------
• [SLOW TEST] [68.232 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:20:56.148
    Aug 11 14:20:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:20:56.149
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:20:56.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:20:56.166
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 11 14:20:56.181: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:21:56.221: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/11/23 14:21:56.224
    Aug 11 14:21:56.245: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 11 14:21:56.256: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 11 14:21:56.280: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 11 14:21:56.289: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/11/23 14:21:56.289
    Aug 11 14:21:56.289: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6595" to be "running"
    Aug 11 14:21:56.296: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146316ms
    Aug 11 14:21:58.301: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.011911797s
    Aug 11 14:21:58.301: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 11 14:21:58.301: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
    Aug 11 14:21:58.304: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.916323ms
    Aug 11 14:21:58.304: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 14:21:58.304: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
    Aug 11 14:21:58.307: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.725643ms
    Aug 11 14:21:58.307: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 14:21:58.307: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6595" to be "running"
    Aug 11 14:21:58.310: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.866763ms
    Aug 11 14:21:58.310: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/11/23 14:21:58.31
    Aug 11 14:21:58.317: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6595" to be "running"
    Aug 11 14:21:58.319: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.852823ms
    Aug 11 14:22:00.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006940373s
    Aug 11 14:22:02.324: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007132989s
    Aug 11 14:22:04.324: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007883736s
    Aug 11 14:22:04.324: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6595" for this suite. 08/11/23 14:22:04.374
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:04.381
Aug 11 14:22:04.381: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:22:04.382
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:04.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:04.397
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-5d2dbbd4-1eee-466a-8733-be9564cafc06 08/11/23 14:22:04.399
STEP: Creating a pod to test consume configMaps 08/11/23 14:22:04.405
Aug 11 14:22:04.413: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303" in namespace "projected-7370" to be "Succeeded or Failed"
Aug 11 14:22:04.416: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876273ms
Aug 11 14:22:06.420: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007022451s
Aug 11 14:22:08.421: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00798312s
STEP: Saw pod success 08/11/23 14:22:08.421
Aug 11 14:22:08.421: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303" satisfied condition "Succeeded or Failed"
Aug 11 14:22:08.423: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:22:08.442
Aug 11 14:22:08.452: INFO: Waiting for pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 to disappear
Aug 11 14:22:08.454: INFO: Pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:08.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7370" for this suite. 08/11/23 14:22:08.458
------------------------------
• [4.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:04.381
    Aug 11 14:22:04.381: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:22:04.382
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:04.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:04.397
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-5d2dbbd4-1eee-466a-8733-be9564cafc06 08/11/23 14:22:04.399
    STEP: Creating a pod to test consume configMaps 08/11/23 14:22:04.405
    Aug 11 14:22:04.413: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303" in namespace "projected-7370" to be "Succeeded or Failed"
    Aug 11 14:22:04.416: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.876273ms
    Aug 11 14:22:06.420: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007022451s
    Aug 11 14:22:08.421: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00798312s
    STEP: Saw pod success 08/11/23 14:22:08.421
    Aug 11 14:22:08.421: INFO: Pod "pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303" satisfied condition "Succeeded or Failed"
    Aug 11 14:22:08.423: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:22:08.442
    Aug 11 14:22:08.452: INFO: Waiting for pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 to disappear
    Aug 11 14:22:08.454: INFO: Pod pod-projected-configmaps-4b859ec6-655a-48f4-861f-50cbd4ced303 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:08.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7370" for this suite. 08/11/23 14:22:08.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:08.464
Aug 11 14:22:08.464: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sysctl 08/11/23 14:22:08.464
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:08.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:08.481
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/11/23 14:22:08.483
STEP: Watching for error events or started pod 08/11/23 14:22:08.489
STEP: Waiting for pod completion 08/11/23 14:22:10.494
Aug 11 14:22:10.494: INFO: Waiting up to 3m0s for pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a" in namespace "sysctl-5953" to be "completed"
Aug 11 14:22:10.497: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652422ms
Aug 11 14:22:12.501: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007189182s
Aug 11 14:22:12.501: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/11/23 14:22:12.504
STEP: Getting logs from the pod 08/11/23 14:22:12.504
STEP: Checking that the sysctl is actually updated 08/11/23 14:22:12.512
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5953" for this suite. 08/11/23 14:22:12.516
------------------------------
• [4.058 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:08.464
    Aug 11 14:22:08.464: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sysctl 08/11/23 14:22:08.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:08.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:08.481
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/11/23 14:22:08.483
    STEP: Watching for error events or started pod 08/11/23 14:22:08.489
    STEP: Waiting for pod completion 08/11/23 14:22:10.494
    Aug 11 14:22:10.494: INFO: Waiting up to 3m0s for pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a" in namespace "sysctl-5953" to be "completed"
    Aug 11 14:22:10.497: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652422ms
    Aug 11 14:22:12.501: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007189182s
    Aug 11 14:22:12.501: INFO: Pod "sysctl-eec1b48d-b7af-47fd-9e02-e11ea837c36a" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/11/23 14:22:12.504
    STEP: Getting logs from the pod 08/11/23 14:22:12.504
    STEP: Checking that the sysctl is actually updated 08/11/23 14:22:12.512
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5953" for this suite. 08/11/23 14:22:12.516
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:12.522
Aug 11 14:22:12.522: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:22:12.523
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.54
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/11/23 14:22:12.542
STEP: submitting the pod to kubernetes 08/11/23 14:22:12.542
STEP: verifying QOS class is set on the pod 08/11/23 14:22:12.549
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:12.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-294" for this suite. 08/11/23 14:22:12.556
------------------------------
• [0.040 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:12.522
    Aug 11 14:22:12.522: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:22:12.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.54
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/11/23 14:22:12.542
    STEP: submitting the pod to kubernetes 08/11/23 14:22:12.542
    STEP: verifying QOS class is set on the pod 08/11/23 14:22:12.549
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:12.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-294" for this suite. 08/11/23 14:22:12.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:12.563
Aug 11 14:22:12.563: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:22:12.564
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.579
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:22:12.593
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:22:12.73
STEP: Deploying the webhook pod 08/11/23 14:22:12.739
STEP: Wait for the deployment to be ready 08/11/23 14:22:12.75
Aug 11 14:22:12.756: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:22:14.766
STEP: Verifying the service has paired with the endpoint 08/11/23 14:22:14.782
Aug 11 14:22:15.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/11/23 14:22:15.787
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/11/23 14:22:15.788
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:22:15.788
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:22:15.788
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:22:15.789
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:22:15.789
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:22:15.79
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:15.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4299" for this suite. 08/11/23 14:22:15.841
STEP: Destroying namespace "webhook-4299-markers" for this suite. 08/11/23 14:22:15.85
------------------------------
• [3.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:12.563
    Aug 11 14:22:12.563: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:22:12.564
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.579
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:22:12.593
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:22:12.73
    STEP: Deploying the webhook pod 08/11/23 14:22:12.739
    STEP: Wait for the deployment to be ready 08/11/23 14:22:12.75
    Aug 11 14:22:12.756: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:22:14.766
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:22:14.782
    Aug 11 14:22:15.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/11/23 14:22:15.787
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/11/23 14:22:15.788
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:22:15.788
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:22:15.788
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:22:15.789
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:22:15.789
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:22:15.79
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:15.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4299" for this suite. 08/11/23 14:22:15.841
    STEP: Destroying namespace "webhook-4299-markers" for this suite. 08/11/23 14:22:15.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:15.859
Aug 11 14:22:15.859: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:22:15.859
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:15.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:15.878
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:22:15.892
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:22:16.107
STEP: Deploying the webhook pod 08/11/23 14:22:16.112
STEP: Wait for the deployment to be ready 08/11/23 14:22:16.124
Aug 11 14:22:16.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:22:18.143
STEP: Verifying the service has paired with the endpoint 08/11/23 14:22:18.157
Aug 11 14:22:19.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/11/23 14:22:19.22
STEP: Creating a configMap that should be mutated 08/11/23 14:22:19.243
STEP: Deleting the collection of validation webhooks 08/11/23 14:22:19.3
STEP: Creating a configMap that should not be mutated 08/11/23 14:22:19.354
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9652" for this suite. 08/11/23 14:22:19.407
STEP: Destroying namespace "webhook-9652-markers" for this suite. 08/11/23 14:22:19.416
------------------------------
• [3.566 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:15.859
    Aug 11 14:22:15.859: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:22:15.859
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:15.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:15.878
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:22:15.892
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:22:16.107
    STEP: Deploying the webhook pod 08/11/23 14:22:16.112
    STEP: Wait for the deployment to be ready 08/11/23 14:22:16.124
    Aug 11 14:22:16.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:22:18.143
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:22:18.157
    Aug 11 14:22:19.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/11/23 14:22:19.22
    STEP: Creating a configMap that should be mutated 08/11/23 14:22:19.243
    STEP: Deleting the collection of validation webhooks 08/11/23 14:22:19.3
    STEP: Creating a configMap that should not be mutated 08/11/23 14:22:19.354
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:19.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9652" for this suite. 08/11/23 14:22:19.407
    STEP: Destroying namespace "webhook-9652-markers" for this suite. 08/11/23 14:22:19.416
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:19.424
Aug 11 14:22:19.425: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:22:19.425
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:19.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:19.443
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 11 14:22:19.457: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6142 to be scheduled
Aug 11 14:22:19.462: INFO: 1 pods are not scheduled: [runtimeclass-6142/test-runtimeclass-runtimeclass-6142-preconfigured-handler-w5hd4(9bbd31bd-08eb-422c-990f-987b7d1de7c1)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 11 14:22:21.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6142" for this suite. 08/11/23 14:22:21.476
------------------------------
• [2.058 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:19.424
    Aug 11 14:22:19.425: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:22:19.425
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:19.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:19.443
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 11 14:22:19.457: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6142 to be scheduled
    Aug 11 14:22:19.462: INFO: 1 pods are not scheduled: [runtimeclass-6142/test-runtimeclass-runtimeclass-6142-preconfigured-handler-w5hd4(9bbd31bd-08eb-422c-990f-987b7d1de7c1)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:22:21.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6142" for this suite. 08/11/23 14:22:21.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:22:21.483
Aug 11 14:22:21.484: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:22:21.484
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:21.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:21.501
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9973 08/11/23 14:22:21.503
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/11/23 14:22:21.508
STEP: Creating stateful set ss in namespace statefulset-9973 08/11/23 14:22:21.515
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9973 08/11/23 14:22:21.521
Aug 11 14:22:21.523: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:22:31.529: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/11/23 14:22:31.529
Aug 11 14:22:31.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:22:31.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:22:31.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:22:31.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:22:31.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 11 14:22:41.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:22:41.678: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:22:41.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999987s
Aug 11 14:22:42.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99718698s
Aug 11 14:22:43.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993275997s
Aug 11 14:22:44.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989600135s
Aug 11 14:22:45.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985481233s
Aug 11 14:22:46.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98108427s
Aug 11 14:22:47.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977096818s
Aug 11 14:22:48.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973263776s
Aug 11 14:22:49.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969130135s
Aug 11 14:22:50.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.095632ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9973 08/11/23 14:22:51.727
Aug 11 14:22:51.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:22:51.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:22:51.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:22:51.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:22:51.870: INFO: Found 1 stateful pods, waiting for 3
Aug 11 14:23:01.875: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:23:01.875: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:23:01.875: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/11/23 14:23:01.875
STEP: Scale down will halt with unhealthy stateful pod 08/11/23 14:23:01.875
Aug 11 14:23:01.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:02.004: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:02.004: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:02.004: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:02.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:02.140: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:02.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:02.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:02.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:02.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:02.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:02.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:02.266: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:23:02.269: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 11 14:23:12.277: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:12.277: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:12.277: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:12.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
Aug 11 14:23:13.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997022899s
Aug 11 14:23:14.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992849466s
Aug 11 14:23:15.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988947574s
Aug 11 14:23:16.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984448801s
Aug 11 14:23:17.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980676649s
Aug 11 14:23:18.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976640407s
Aug 11 14:23:19.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972660954s
Aug 11 14:23:20.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967220791s
Aug 11 14:23:21.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.711129ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9973 08/11/23 14:23:22.328
Aug 11 14:23:22.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:23:22.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:23:22.454: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:23:22.454: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:23:22.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:23:22.578: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:23:22.578: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:23:22.578: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:23:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:23:22.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:23:22.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:23:22.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:23:22.713: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/11/23 14:23:32.729
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:23:32.729: INFO: Deleting all statefulset in ns statefulset-9973
Aug 11 14:23:32.732: INFO: Scaling statefulset ss to 0
Aug 11 14:23:32.741: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:23:32.743: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:23:32.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9973" for this suite. 08/11/23 14:23:32.761
------------------------------
• [SLOW TEST] [71.285 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:22:21.483
    Aug 11 14:22:21.484: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:22:21.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:21.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:21.501
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9973 08/11/23 14:22:21.503
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/11/23 14:22:21.508
    STEP: Creating stateful set ss in namespace statefulset-9973 08/11/23 14:22:21.515
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9973 08/11/23 14:22:21.521
    Aug 11 14:22:21.523: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:22:31.529: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/11/23 14:22:31.529
    Aug 11 14:22:31.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:22:31.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:22:31.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:22:31.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:22:31.673: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 11 14:22:41.678: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:22:41.678: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:22:41.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999987s
    Aug 11 14:22:42.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99718698s
    Aug 11 14:22:43.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993275997s
    Aug 11 14:22:44.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989600135s
    Aug 11 14:22:45.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985481233s
    Aug 11 14:22:46.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98108427s
    Aug 11 14:22:47.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977096818s
    Aug 11 14:22:48.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973263776s
    Aug 11 14:22:49.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.969130135s
    Aug 11 14:22:50.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 965.095632ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9973 08/11/23 14:22:51.727
    Aug 11 14:22:51.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:22:51.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:22:51.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:22:51.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:22:51.870: INFO: Found 1 stateful pods, waiting for 3
    Aug 11 14:23:01.875: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:23:01.875: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:23:01.875: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/11/23 14:23:01.875
    STEP: Scale down will halt with unhealthy stateful pod 08/11/23 14:23:01.875
    Aug 11 14:23:01.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:02.004: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:02.004: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:02.004: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:02.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:02.140: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:02.140: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:02.140: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:02.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:02.266: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:02.266: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:02.266: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:02.266: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:23:02.269: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 11 14:23:12.277: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:12.277: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:12.277: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:12.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999998s
    Aug 11 14:23:13.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997022899s
    Aug 11 14:23:14.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992849466s
    Aug 11 14:23:15.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988947574s
    Aug 11 14:23:16.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984448801s
    Aug 11 14:23:17.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980676649s
    Aug 11 14:23:18.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976640407s
    Aug 11 14:23:19.318: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972660954s
    Aug 11 14:23:20.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967220791s
    Aug 11 14:23:21.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.711129ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9973 08/11/23 14:23:22.328
    Aug 11 14:23:22.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:23:22.454: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:23:22.454: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:23:22.454: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:23:22.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:23:22.578: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:23:22.578: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:23:22.578: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:23:22.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-9973 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:23:22.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:23:22.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:23:22.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:23:22.713: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/11/23 14:23:32.729
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:23:32.729: INFO: Deleting all statefulset in ns statefulset-9973
    Aug 11 14:23:32.732: INFO: Scaling statefulset ss to 0
    Aug 11 14:23:32.741: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:23:32.743: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:23:32.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9973" for this suite. 08/11/23 14:23:32.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:23:32.77
Aug 11 14:23:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:23:32.771
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:32.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:32.787
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 14:23:32.789
Aug 11 14:23:32.797: INFO: Waiting up to 5m0s for pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8" in namespace "emptydir-9010" to be "Succeeded or Failed"
Aug 11 14:23:32.800: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463013ms
Aug 11 14:23:34.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006231772s
Aug 11 14:23:36.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005823647s
STEP: Saw pod success 08/11/23 14:23:36.803
Aug 11 14:23:36.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8" satisfied condition "Succeeded or Failed"
Aug 11 14:23:36.806: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 container test-container: <nil>
STEP: delete the pod 08/11/23 14:23:36.814
Aug 11 14:23:36.823: INFO: Waiting for pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 to disappear
Aug 11 14:23:36.825: INFO: Pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:23:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9010" for this suite. 08/11/23 14:23:36.829
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:23:32.77
    Aug 11 14:23:32.770: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:23:32.771
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:32.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:32.787
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 14:23:32.789
    Aug 11 14:23:32.797: INFO: Waiting up to 5m0s for pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8" in namespace "emptydir-9010" to be "Succeeded or Failed"
    Aug 11 14:23:32.800: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463013ms
    Aug 11 14:23:34.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006231772s
    Aug 11 14:23:36.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005823647s
    STEP: Saw pod success 08/11/23 14:23:36.803
    Aug 11 14:23:36.803: INFO: Pod "pod-cff93961-1ead-498d-ae16-e232ba29b2f8" satisfied condition "Succeeded or Failed"
    Aug 11 14:23:36.806: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:23:36.814
    Aug 11 14:23:36.823: INFO: Waiting for pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 to disappear
    Aug 11 14:23:36.825: INFO: Pod pod-cff93961-1ead-498d-ae16-e232ba29b2f8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:23:36.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9010" for this suite. 08/11/23 14:23:36.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:23:36.835
Aug 11 14:23:36.835: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:23:36.836
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:36.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:36.851
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:23:36.853
Aug 11 14:23:36.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf" in namespace "projected-3574" to be "Succeeded or Failed"
Aug 11 14:23:36.864: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535744ms
Aug 11 14:23:38.869: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007881095s
Aug 11 14:23:40.868: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007548291s
STEP: Saw pod success 08/11/23 14:23:40.868
Aug 11 14:23:40.868: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf" satisfied condition "Succeeded or Failed"
Aug 11 14:23:40.871: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf container client-container: <nil>
STEP: delete the pod 08/11/23 14:23:40.878
Aug 11 14:23:40.894: INFO: Waiting for pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf to disappear
Aug 11 14:23:40.896: INFO: Pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:23:40.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3574" for this suite. 08/11/23 14:23:40.9
------------------------------
• [4.070 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:23:36.835
    Aug 11 14:23:36.835: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:23:36.836
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:36.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:36.851
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:23:36.853
    Aug 11 14:23:36.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf" in namespace "projected-3574" to be "Succeeded or Failed"
    Aug 11 14:23:36.864: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535744ms
    Aug 11 14:23:38.869: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007881095s
    Aug 11 14:23:40.868: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007548291s
    STEP: Saw pod success 08/11/23 14:23:40.868
    Aug 11 14:23:40.868: INFO: Pod "downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf" satisfied condition "Succeeded or Failed"
    Aug 11 14:23:40.871: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf container client-container: <nil>
    STEP: delete the pod 08/11/23 14:23:40.878
    Aug 11 14:23:40.894: INFO: Waiting for pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf to disappear
    Aug 11 14:23:40.896: INFO: Pod downwardapi-volume-f19e05cf-eb55-4d45-9edf-d6e2ce45eccf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:23:40.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3574" for this suite. 08/11/23 14:23:40.9
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:23:40.906
Aug 11 14:23:40.906: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:23:40.907
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:40.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:40.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 11 14:23:40.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1130 version'
Aug 11 14:23:40.969: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 11 14:23:40.969: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:08Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:23:40.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1130" for this suite. 08/11/23 14:23:40.973
------------------------------
• [0.072 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:23:40.906
    Aug 11 14:23:40.906: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:23:40.907
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:40.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:40.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 11 14:23:40.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1130 version'
    Aug 11 14:23:40.969: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 11 14:23:40.969: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:08Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:23:40.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1130" for this suite. 08/11/23 14:23:40.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:23:40.979
Aug 11 14:23:40.979: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:23:40.98
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:40.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:40.995
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/11/23 14:23:40.997
Aug 11 14:23:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/11/23 14:23:47.769
Aug 11 14:23:47.769: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:23:49.779: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:23:57.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7360" for this suite. 08/11/23 14:23:57.03
------------------------------
• [SLOW TEST] [16.057 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:23:40.979
    Aug 11 14:23:40.979: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:23:40.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:40.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:40.995
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/11/23 14:23:40.997
    Aug 11 14:23:40.998: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/11/23 14:23:47.769
    Aug 11 14:23:47.769: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:23:49.779: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:23:57.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7360" for this suite. 08/11/23 14:23:57.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:23:57.036
Aug 11 14:23:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:23:57.037
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:57.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:57.052
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:23:57.066
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:23:57.414
STEP: Deploying the webhook pod 08/11/23 14:23:57.421
STEP: Wait for the deployment to be ready 08/11/23 14:23:57.432
Aug 11 14:23:57.437: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/11/23 14:23:59.446
STEP: Verifying the service has paired with the endpoint 08/11/23 14:23:59.459
Aug 11 14:24:00.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:24:00.463
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:24:00.488
STEP: Creating a dummy validating-webhook-configuration object 08/11/23 14:24:00.511
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/11/23 14:24:00.52
STEP: Creating a dummy mutating-webhook-configuration object 08/11/23 14:24:00.525
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/11/23 14:24:00.535
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8727" for this suite. 08/11/23 14:24:00.595
STEP: Destroying namespace "webhook-8727-markers" for this suite. 08/11/23 14:24:00.603
------------------------------
• [3.573 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:23:57.036
    Aug 11 14:23:57.036: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:23:57.037
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:57.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:57.052
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:23:57.066
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:23:57.414
    STEP: Deploying the webhook pod 08/11/23 14:23:57.421
    STEP: Wait for the deployment to be ready 08/11/23 14:23:57.432
    Aug 11 14:23:57.437: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/11/23 14:23:59.446
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:23:59.459
    Aug 11 14:24:00.460: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:24:00.463
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:24:00.488
    STEP: Creating a dummy validating-webhook-configuration object 08/11/23 14:24:00.511
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/11/23 14:24:00.52
    STEP: Creating a dummy mutating-webhook-configuration object 08/11/23 14:24:00.525
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/11/23 14:24:00.535
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:00.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8727" for this suite. 08/11/23 14:24:00.595
    STEP: Destroying namespace "webhook-8727-markers" for this suite. 08/11/23 14:24:00.603
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:00.609
Aug 11 14:24:00.609: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:24:00.61
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.625
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/11/23 14:24:00.628
Aug 11 14:24:00.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7331 cluster-info'
Aug 11 14:24:00.677: INFO: stderr: ""
Aug 11 14:24:00.678: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7331" for this suite. 08/11/23 14:24:00.681
------------------------------
• [0.078 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:00.609
    Aug 11 14:24:00.609: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:24:00.61
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.625
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/11/23 14:24:00.628
    Aug 11 14:24:00.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7331 cluster-info'
    Aug 11 14:24:00.677: INFO: stderr: ""
    Aug 11 14:24:00.678: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7331" for this suite. 08/11/23 14:24:00.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:00.688
Aug 11 14:24:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename limitrange 08/11/23 14:24:00.688
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.701
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-l7hn7" in namespace "limitrange-7634" 08/11/23 14:24:00.703
STEP: Creating another limitRange in another namespace 08/11/23 14:24:00.708
Aug 11 14:24:00.717: INFO: Namespace "e2e-limitrange-l7hn7-3556" created
Aug 11 14:24:00.717: INFO: Creating LimitRange "e2e-limitrange-l7hn7" in namespace "e2e-limitrange-l7hn7-3556"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-l7hn7" 08/11/23 14:24:00.721
Aug 11 14:24:00.723: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-l7hn7" in "limitrange-7634" namespace 08/11/23 14:24:00.723
Aug 11 14:24:00.729: INFO: LimitRange "e2e-limitrange-l7hn7" has been patched
STEP: Delete LimitRange "e2e-limitrange-l7hn7" by Collection with labelSelector: "e2e-limitrange-l7hn7=patched" 08/11/23 14:24:00.729
STEP: Confirm that the limitRange "e2e-limitrange-l7hn7" has been deleted 08/11/23 14:24:00.735
Aug 11 14:24:00.735: INFO: Requesting list of LimitRange to confirm quantity
Aug 11 14:24:00.737: INFO: Found 0 LimitRange with label "e2e-limitrange-l7hn7=patched"
Aug 11 14:24:00.737: INFO: LimitRange "e2e-limitrange-l7hn7" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-l7hn7" 08/11/23 14:24:00.737
Aug 11 14:24:00.739: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7634" for this suite. 08/11/23 14:24:00.743
STEP: Destroying namespace "e2e-limitrange-l7hn7-3556" for this suite. 08/11/23 14:24:00.749
------------------------------
• [0.066 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:00.688
    Aug 11 14:24:00.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename limitrange 08/11/23 14:24:00.688
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.701
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-l7hn7" in namespace "limitrange-7634" 08/11/23 14:24:00.703
    STEP: Creating another limitRange in another namespace 08/11/23 14:24:00.708
    Aug 11 14:24:00.717: INFO: Namespace "e2e-limitrange-l7hn7-3556" created
    Aug 11 14:24:00.717: INFO: Creating LimitRange "e2e-limitrange-l7hn7" in namespace "e2e-limitrange-l7hn7-3556"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-l7hn7" 08/11/23 14:24:00.721
    Aug 11 14:24:00.723: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-l7hn7" in "limitrange-7634" namespace 08/11/23 14:24:00.723
    Aug 11 14:24:00.729: INFO: LimitRange "e2e-limitrange-l7hn7" has been patched
    STEP: Delete LimitRange "e2e-limitrange-l7hn7" by Collection with labelSelector: "e2e-limitrange-l7hn7=patched" 08/11/23 14:24:00.729
    STEP: Confirm that the limitRange "e2e-limitrange-l7hn7" has been deleted 08/11/23 14:24:00.735
    Aug 11 14:24:00.735: INFO: Requesting list of LimitRange to confirm quantity
    Aug 11 14:24:00.737: INFO: Found 0 LimitRange with label "e2e-limitrange-l7hn7=patched"
    Aug 11 14:24:00.737: INFO: LimitRange "e2e-limitrange-l7hn7" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-l7hn7" 08/11/23 14:24:00.737
    Aug 11 14:24:00.739: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7634" for this suite. 08/11/23 14:24:00.743
    STEP: Destroying namespace "e2e-limitrange-l7hn7-3556" for this suite. 08/11/23 14:24:00.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:00.754
Aug 11 14:24:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:24:00.755
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.768
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8411 08/11/23 14:24:00.77
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 14:24:00.782
STEP: creating service externalsvc in namespace services-8411 08/11/23 14:24:00.782
STEP: creating replication controller externalsvc in namespace services-8411 08/11/23 14:24:00.797
I0811 14:24:00.804806      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8411, replica count: 2
I0811 14:24:03.856396      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/11/23 14:24:03.859
Aug 11 14:24:03.873: INFO: Creating new exec pod
Aug 11 14:24:03.881: INFO: Waiting up to 5m0s for pod "execpod5wdxr" in namespace "services-8411" to be "running"
Aug 11 14:24:03.883: INFO: Pod "execpod5wdxr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638852ms
Aug 11 14:24:05.887: INFO: Pod "execpod5wdxr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006108651s
Aug 11 14:24:05.887: INFO: Pod "execpod5wdxr" satisfied condition "running"
Aug 11 14:24:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8411 exec execpod5wdxr -- /bin/sh -x -c nslookup clusterip-service.services-8411.svc.cluster.local'
Aug 11 14:24:06.042: INFO: stderr: "+ nslookup clusterip-service.services-8411.svc.cluster.local\n"
Aug 11 14:24:06.042: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8411.svc.cluster.local\tcanonical name = externalsvc.services-8411.svc.cluster.local.\nName:\texternalsvc.services-8411.svc.cluster.local\nAddress: 10.111.124.254\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8411, will wait for the garbage collector to delete the pods 08/11/23 14:24:06.042
Aug 11 14:24:06.101: INFO: Deleting ReplicationController externalsvc took: 6.199626ms
Aug 11 14:24:06.202: INFO: Terminating ReplicationController externalsvc pods took: 100.671899ms
Aug 11 14:24:07.926: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:07.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8411" for this suite. 08/11/23 14:24:07.939
------------------------------
• [SLOW TEST] [7.190 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:00.754
    Aug 11 14:24:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:24:00.755
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:00.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:00.768
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8411 08/11/23 14:24:00.77
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 14:24:00.782
    STEP: creating service externalsvc in namespace services-8411 08/11/23 14:24:00.782
    STEP: creating replication controller externalsvc in namespace services-8411 08/11/23 14:24:00.797
    I0811 14:24:00.804806      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8411, replica count: 2
    I0811 14:24:03.856396      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/11/23 14:24:03.859
    Aug 11 14:24:03.873: INFO: Creating new exec pod
    Aug 11 14:24:03.881: INFO: Waiting up to 5m0s for pod "execpod5wdxr" in namespace "services-8411" to be "running"
    Aug 11 14:24:03.883: INFO: Pod "execpod5wdxr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638852ms
    Aug 11 14:24:05.887: INFO: Pod "execpod5wdxr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006108651s
    Aug 11 14:24:05.887: INFO: Pod "execpod5wdxr" satisfied condition "running"
    Aug 11 14:24:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-8411 exec execpod5wdxr -- /bin/sh -x -c nslookup clusterip-service.services-8411.svc.cluster.local'
    Aug 11 14:24:06.042: INFO: stderr: "+ nslookup clusterip-service.services-8411.svc.cluster.local\n"
    Aug 11 14:24:06.042: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-8411.svc.cluster.local\tcanonical name = externalsvc.services-8411.svc.cluster.local.\nName:\texternalsvc.services-8411.svc.cluster.local\nAddress: 10.111.124.254\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8411, will wait for the garbage collector to delete the pods 08/11/23 14:24:06.042
    Aug 11 14:24:06.101: INFO: Deleting ReplicationController externalsvc took: 6.199626ms
    Aug 11 14:24:06.202: INFO: Terminating ReplicationController externalsvc pods took: 100.671899ms
    Aug 11 14:24:07.926: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:07.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8411" for this suite. 08/11/23 14:24:07.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:07.946
Aug 11 14:24:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:24:07.946
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:07.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:07.96
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1097 08/11/23 14:24:07.962
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 11 14:24:07.976: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:24:17.980: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/11/23 14:24:17.985
W0811 14:24:17.993961      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 14:24:18.002: INFO: Found 1 stateful pods, waiting for 2
Aug 11 14:24:28.006: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:24:28.006: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/11/23 14:24:28.01
STEP: Delete all of the StatefulSets 08/11/23 14:24:28.012
STEP: Verify that StatefulSets have been deleted 08/11/23 14:24:28.018
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:24:28.020: INFO: Deleting all statefulset in ns statefulset-1097
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:28.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1097" for this suite. 08/11/23 14:24:28.041
------------------------------
• [SLOW TEST] [20.101 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:07.946
    Aug 11 14:24:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:24:07.946
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:07.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:07.96
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1097 08/11/23 14:24:07.962
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 11 14:24:07.976: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:24:17.980: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/11/23 14:24:17.985
    W0811 14:24:17.993961      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 14:24:18.002: INFO: Found 1 stateful pods, waiting for 2
    Aug 11 14:24:28.006: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:24:28.006: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/11/23 14:24:28.01
    STEP: Delete all of the StatefulSets 08/11/23 14:24:28.012
    STEP: Verify that StatefulSets have been deleted 08/11/23 14:24:28.018
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:24:28.020: INFO: Deleting all statefulset in ns statefulset-1097
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:28.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1097" for this suite. 08/11/23 14:24:28.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:28.047
Aug 11 14:24:28.047: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:24:28.048
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.06
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9090-delete-me 08/11/23 14:24:28.066
STEP: Waiting for the RuntimeClass to disappear 08/11/23 14:24:28.072
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:28.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9090" for this suite. 08/11/23 14:24:28.085
------------------------------
• [0.043 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:28.047
    Aug 11 14:24:28.047: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:24:28.048
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.06
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9090-delete-me 08/11/23 14:24:28.066
    STEP: Waiting for the RuntimeClass to disappear 08/11/23 14:24:28.072
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:28.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9090" for this suite. 08/11/23 14:24:28.085
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:28.091
Aug 11 14:24:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:24:28.092
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.103
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/11/23 14:24:28.105
STEP: fetching the ConfigMap 08/11/23 14:24:28.109
STEP: patching the ConfigMap 08/11/23 14:24:28.112
STEP: listing all ConfigMaps in all namespaces with a label selector 08/11/23 14:24:28.116
STEP: deleting the ConfigMap by collection with a label selector 08/11/23 14:24:28.119
STEP: listing all ConfigMaps in test namespace 08/11/23 14:24:28.124
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9073" for this suite. 08/11/23 14:24:28.129
------------------------------
• [0.043 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:28.091
    Aug 11 14:24:28.091: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:24:28.092
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.103
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/11/23 14:24:28.105
    STEP: fetching the ConfigMap 08/11/23 14:24:28.109
    STEP: patching the ConfigMap 08/11/23 14:24:28.112
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/11/23 14:24:28.116
    STEP: deleting the ConfigMap by collection with a label selector 08/11/23 14:24:28.119
    STEP: listing all ConfigMaps in test namespace 08/11/23 14:24:28.124
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9073" for this suite. 08/11/23 14:24:28.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:28.136
Aug 11 14:24:28.136: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:24:28.137
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.15
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-503 08/11/23 14:24:28.153
STEP: changing the ExternalName service to type=ClusterIP 08/11/23 14:24:28.157
STEP: creating replication controller externalname-service in namespace services-503 08/11/23 14:24:28.174
I0811 14:24:28.186253      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-503, replica count: 2
I0811 14:24:31.237021      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:24:31.237: INFO: Creating new exec pod
Aug 11 14:24:31.241: INFO: Waiting up to 5m0s for pod "execpodp27x9" in namespace "services-503" to be "running"
Aug 11 14:24:31.245: INFO: Pod "execpodp27x9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657594ms
Aug 11 14:24:33.248: INFO: Pod "execpodp27x9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003462s
Aug 11 14:24:33.248: INFO: Pod "execpodp27x9" satisfied condition "running"
Aug 11 14:24:34.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-503 exec execpodp27x9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 11 14:24:34.369: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 14:24:34.369: INFO: stdout: ""
Aug 11 14:24:34.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-503 exec execpodp27x9 -- /bin/sh -x -c nc -v -z -w 2 10.101.189.29 80'
Aug 11 14:24:34.488: INFO: stderr: "+ nc -v -z -w 2 10.101.189.29 80\nConnection to 10.101.189.29 80 port [tcp/http] succeeded!\n"
Aug 11 14:24:34.488: INFO: stdout: ""
Aug 11 14:24:34.488: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:34.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-503" for this suite. 08/11/23 14:24:34.51
------------------------------
• [SLOW TEST] [6.380 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:28.136
    Aug 11 14:24:28.136: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:24:28.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:28.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:28.15
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-503 08/11/23 14:24:28.153
    STEP: changing the ExternalName service to type=ClusterIP 08/11/23 14:24:28.157
    STEP: creating replication controller externalname-service in namespace services-503 08/11/23 14:24:28.174
    I0811 14:24:28.186253      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-503, replica count: 2
    I0811 14:24:31.237021      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:24:31.237: INFO: Creating new exec pod
    Aug 11 14:24:31.241: INFO: Waiting up to 5m0s for pod "execpodp27x9" in namespace "services-503" to be "running"
    Aug 11 14:24:31.245: INFO: Pod "execpodp27x9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657594ms
    Aug 11 14:24:33.248: INFO: Pod "execpodp27x9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007003462s
    Aug 11 14:24:33.248: INFO: Pod "execpodp27x9" satisfied condition "running"
    Aug 11 14:24:34.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-503 exec execpodp27x9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 11 14:24:34.369: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 14:24:34.369: INFO: stdout: ""
    Aug 11 14:24:34.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-503 exec execpodp27x9 -- /bin/sh -x -c nc -v -z -w 2 10.101.189.29 80'
    Aug 11 14:24:34.488: INFO: stderr: "+ nc -v -z -w 2 10.101.189.29 80\nConnection to 10.101.189.29 80 port [tcp/http] succeeded!\n"
    Aug 11 14:24:34.488: INFO: stdout: ""
    Aug 11 14:24:34.488: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:34.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-503" for this suite. 08/11/23 14:24:34.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:34.516
Aug 11 14:24:34.516: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename csiinlinevolumes 08/11/23 14:24:34.517
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:34.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:34.531
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/11/23 14:24:34.533
STEP: getting 08/11/23 14:24:34.548
STEP: listing in namespace 08/11/23 14:24:34.551
STEP: patching 08/11/23 14:24:34.554
STEP: deleting 08/11/23 14:24:34.568
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5436" for this suite. 08/11/23 14:24:34.58
------------------------------
• [0.069 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:34.516
    Aug 11 14:24:34.516: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename csiinlinevolumes 08/11/23 14:24:34.517
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:34.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:34.531
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/11/23 14:24:34.533
    STEP: getting 08/11/23 14:24:34.548
    STEP: listing in namespace 08/11/23 14:24:34.551
    STEP: patching 08/11/23 14:24:34.554
    STEP: deleting 08/11/23 14:24:34.568
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:34.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5436" for this suite. 08/11/23 14:24:34.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:34.586
Aug 11 14:24:34.586: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename init-container 08/11/23 14:24:34.587
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:34.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:34.6
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/11/23 14:24:34.602
Aug 11 14:24:34.602: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8575" for this suite. 08/11/23 14:24:37.364
------------------------------
• [2.783 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:34.586
    Aug 11 14:24:34.586: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename init-container 08/11/23 14:24:34.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:34.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:34.6
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/11/23 14:24:34.602
    Aug 11 14:24:34.602: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:37.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8575" for this suite. 08/11/23 14:24:37.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:37.371
Aug 11 14:24:37.371: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:24:37.372
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:37.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:37.385
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/11/23 14:24:37.387
Aug 11 14:24:37.388: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:24:39.317: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:46.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6460" for this suite. 08/11/23 14:24:46.709
------------------------------
• [SLOW TEST] [9.346 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:37.371
    Aug 11 14:24:37.371: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:24:37.372
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:37.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:37.385
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/11/23 14:24:37.387
    Aug 11 14:24:37.388: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:24:39.317: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:46.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6460" for this suite. 08/11/23 14:24:46.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:46.719
Aug 11 14:24:46.719: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:24:46.719
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:46.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:46.739
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 11 14:24:46.755: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8458 to be scheduled
Aug 11 14:24:46.757: INFO: 1 pods are not scheduled: [runtimeclass-8458/test-runtimeclass-runtimeclass-8458-preconfigured-handler-ktb9d(4e9080a0-db42-43fc-94ef-26da196b15d8)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:48.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8458" for this suite. 08/11/23 14:24:48.775
------------------------------
• [2.063 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:46.719
    Aug 11 14:24:46.719: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:24:46.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:46.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:46.739
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 11 14:24:46.755: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8458 to be scheduled
    Aug 11 14:24:46.757: INFO: 1 pods are not scheduled: [runtimeclass-8458/test-runtimeclass-runtimeclass-8458-preconfigured-handler-ktb9d(4e9080a0-db42-43fc-94ef-26da196b15d8)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:48.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8458" for this suite. 08/11/23 14:24:48.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:48.782
Aug 11 14:24:48.783: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:24:48.783
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:48.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:48.799
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-40060703-8369-46f3-abdb-61c4ebdc5078 08/11/23 14:24:48.805
STEP: Creating the pod 08/11/23 14:24:48.81
Aug 11 14:24:48.823: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d" in namespace "projected-194" to be "running and ready"
Aug 11 14:24:48.826: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666303ms
Aug 11 14:24:48.826: INFO: The phase of Pod pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:24:50.830: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006542283s
Aug 11 14:24:50.830: INFO: The phase of Pod pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d is Running (Ready = true)
Aug 11 14:24:50.830: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-40060703-8369-46f3-abdb-61c4ebdc5078 08/11/23 14:24:50.852
STEP: waiting to observe update in volume 08/11/23 14:24:50.857
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-194" for this suite. 08/11/23 14:24:54.909
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:48.782
    Aug 11 14:24:48.783: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:24:48.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:48.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:48.799
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-40060703-8369-46f3-abdb-61c4ebdc5078 08/11/23 14:24:48.805
    STEP: Creating the pod 08/11/23 14:24:48.81
    Aug 11 14:24:48.823: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d" in namespace "projected-194" to be "running and ready"
    Aug 11 14:24:48.826: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.666303ms
    Aug 11 14:24:48.826: INFO: The phase of Pod pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:24:50.830: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006542283s
    Aug 11 14:24:50.830: INFO: The phase of Pod pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d is Running (Ready = true)
    Aug 11 14:24:50.830: INFO: Pod "pod-projected-configmaps-e3eef5a2-4f4f-4e43-8d99-8d3ac7f9365d" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-40060703-8369-46f3-abdb-61c4ebdc5078 08/11/23 14:24:50.852
    STEP: waiting to observe update in volume 08/11/23 14:24:50.857
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-194" for this suite. 08/11/23 14:24:54.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:54.92
Aug 11 14:24:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename podtemplate 08/11/23 14:24:54.921
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:54.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:54.938
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/11/23 14:24:54.94
STEP: Replace a pod template 08/11/23 14:24:54.945
Aug 11 14:24:54.953: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 11 14:24:54.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-15" for this suite. 08/11/23 14:24:54.957
------------------------------
• [0.045 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:54.92
    Aug 11 14:24:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename podtemplate 08/11/23 14:24:54.921
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:54.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:54.938
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/11/23 14:24:54.94
    STEP: Replace a pod template 08/11/23 14:24:54.945
    Aug 11 14:24:54.953: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:24:54.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-15" for this suite. 08/11/23 14:24:54.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:24:54.966
Aug 11 14:24:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 14:24:54.967
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:54.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:54.982
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/11/23 14:24:54.989
STEP: delete the rc 08/11/23 14:24:59.999
STEP: wait for the rc to be deleted 08/11/23 14:25:00.01
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/11/23 14:25:05.015
STEP: Gathering metrics 08/11/23 14:25:35.03
Aug 11 14:25:35.064: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 14:25:35.068: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.215693ms
Aug 11 14:25:35.068: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 14:25:35.068: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 14:25:35.140: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 11 14:25:35.140: INFO: Deleting pod "simpletest.rc-296qn" in namespace "gc-3915"
Aug 11 14:25:35.150: INFO: Deleting pod "simpletest.rc-42xfn" in namespace "gc-3915"
Aug 11 14:25:35.164: INFO: Deleting pod "simpletest.rc-46jrw" in namespace "gc-3915"
Aug 11 14:25:35.183: INFO: Deleting pod "simpletest.rc-4dgrl" in namespace "gc-3915"
Aug 11 14:25:35.197: INFO: Deleting pod "simpletest.rc-4kdfj" in namespace "gc-3915"
Aug 11 14:25:35.219: INFO: Deleting pod "simpletest.rc-4kz7t" in namespace "gc-3915"
Aug 11 14:25:35.238: INFO: Deleting pod "simpletest.rc-4nlt5" in namespace "gc-3915"
Aug 11 14:25:35.251: INFO: Deleting pod "simpletest.rc-4v26q" in namespace "gc-3915"
Aug 11 14:25:35.267: INFO: Deleting pod "simpletest.rc-4z8dm" in namespace "gc-3915"
Aug 11 14:25:35.280: INFO: Deleting pod "simpletest.rc-5npjf" in namespace "gc-3915"
Aug 11 14:25:35.300: INFO: Deleting pod "simpletest.rc-6k56r" in namespace "gc-3915"
Aug 11 14:25:35.319: INFO: Deleting pod "simpletest.rc-6szss" in namespace "gc-3915"
Aug 11 14:25:35.338: INFO: Deleting pod "simpletest.rc-777f4" in namespace "gc-3915"
Aug 11 14:25:35.355: INFO: Deleting pod "simpletest.rc-7rh56" in namespace "gc-3915"
Aug 11 14:25:35.386: INFO: Deleting pod "simpletest.rc-8ff8q" in namespace "gc-3915"
Aug 11 14:25:35.410: INFO: Deleting pod "simpletest.rc-8xcql" in namespace "gc-3915"
Aug 11 14:25:35.432: INFO: Deleting pod "simpletest.rc-92t7p" in namespace "gc-3915"
Aug 11 14:25:35.457: INFO: Deleting pod "simpletest.rc-9gzbk" in namespace "gc-3915"
Aug 11 14:25:35.472: INFO: Deleting pod "simpletest.rc-9hq25" in namespace "gc-3915"
Aug 11 14:25:35.490: INFO: Deleting pod "simpletest.rc-9jx5p" in namespace "gc-3915"
Aug 11 14:25:35.506: INFO: Deleting pod "simpletest.rc-9klq2" in namespace "gc-3915"
Aug 11 14:25:35.524: INFO: Deleting pod "simpletest.rc-9krdf" in namespace "gc-3915"
Aug 11 14:25:35.540: INFO: Deleting pod "simpletest.rc-9lgwl" in namespace "gc-3915"
Aug 11 14:25:35.551: INFO: Deleting pod "simpletest.rc-9nqkt" in namespace "gc-3915"
Aug 11 14:25:35.561: INFO: Deleting pod "simpletest.rc-9xccw" in namespace "gc-3915"
Aug 11 14:25:35.574: INFO: Deleting pod "simpletest.rc-b8hqr" in namespace "gc-3915"
Aug 11 14:25:35.590: INFO: Deleting pod "simpletest.rc-bbcfz" in namespace "gc-3915"
Aug 11 14:25:35.600: INFO: Deleting pod "simpletest.rc-bjlzm" in namespace "gc-3915"
Aug 11 14:25:35.614: INFO: Deleting pod "simpletest.rc-bqfsh" in namespace "gc-3915"
Aug 11 14:25:35.625: INFO: Deleting pod "simpletest.rc-br4hp" in namespace "gc-3915"
Aug 11 14:25:35.635: INFO: Deleting pod "simpletest.rc-br5l8" in namespace "gc-3915"
Aug 11 14:25:35.653: INFO: Deleting pod "simpletest.rc-chp7t" in namespace "gc-3915"
Aug 11 14:25:35.664: INFO: Deleting pod "simpletest.rc-cw2nn" in namespace "gc-3915"
Aug 11 14:25:35.678: INFO: Deleting pod "simpletest.rc-dhdtx" in namespace "gc-3915"
Aug 11 14:25:35.690: INFO: Deleting pod "simpletest.rc-dmnkf" in namespace "gc-3915"
Aug 11 14:25:35.704: INFO: Deleting pod "simpletest.rc-dnxqk" in namespace "gc-3915"
Aug 11 14:25:35.718: INFO: Deleting pod "simpletest.rc-dqwwj" in namespace "gc-3915"
Aug 11 14:25:35.732: INFO: Deleting pod "simpletest.rc-ds7w5" in namespace "gc-3915"
Aug 11 14:25:35.745: INFO: Deleting pod "simpletest.rc-fcmth" in namespace "gc-3915"
Aug 11 14:25:35.761: INFO: Deleting pod "simpletest.rc-fhxkw" in namespace "gc-3915"
Aug 11 14:25:35.775: INFO: Deleting pod "simpletest.rc-frjwq" in namespace "gc-3915"
Aug 11 14:25:35.792: INFO: Deleting pod "simpletest.rc-fznk5" in namespace "gc-3915"
Aug 11 14:25:35.805: INFO: Deleting pod "simpletest.rc-gwfzq" in namespace "gc-3915"
Aug 11 14:25:35.816: INFO: Deleting pod "simpletest.rc-h8w7b" in namespace "gc-3915"
Aug 11 14:25:35.830: INFO: Deleting pod "simpletest.rc-hm66q" in namespace "gc-3915"
Aug 11 14:25:35.843: INFO: Deleting pod "simpletest.rc-hwlhv" in namespace "gc-3915"
Aug 11 14:25:35.862: INFO: Deleting pod "simpletest.rc-hxxdl" in namespace "gc-3915"
Aug 11 14:25:35.874: INFO: Deleting pod "simpletest.rc-jzdts" in namespace "gc-3915"
Aug 11 14:25:35.887: INFO: Deleting pod "simpletest.rc-k5bql" in namespace "gc-3915"
Aug 11 14:25:35.900: INFO: Deleting pod "simpletest.rc-khjsq" in namespace "gc-3915"
Aug 11 14:25:35.910: INFO: Deleting pod "simpletest.rc-knfjq" in namespace "gc-3915"
Aug 11 14:25:35.923: INFO: Deleting pod "simpletest.rc-ksxnp" in namespace "gc-3915"
Aug 11 14:25:35.933: INFO: Deleting pod "simpletest.rc-l2j55" in namespace "gc-3915"
Aug 11 14:25:35.946: INFO: Deleting pod "simpletest.rc-lbzlg" in namespace "gc-3915"
Aug 11 14:25:35.958: INFO: Deleting pod "simpletest.rc-llcnq" in namespace "gc-3915"
Aug 11 14:25:35.969: INFO: Deleting pod "simpletest.rc-n6c8z" in namespace "gc-3915"
Aug 11 14:25:35.978: INFO: Deleting pod "simpletest.rc-n8s4s" in namespace "gc-3915"
Aug 11 14:25:35.994: INFO: Deleting pod "simpletest.rc-nfdfl" in namespace "gc-3915"
Aug 11 14:25:36.004: INFO: Deleting pod "simpletest.rc-nfwbt" in namespace "gc-3915"
Aug 11 14:25:36.016: INFO: Deleting pod "simpletest.rc-nxcpk" in namespace "gc-3915"
Aug 11 14:25:36.026: INFO: Deleting pod "simpletest.rc-p56mm" in namespace "gc-3915"
Aug 11 14:25:36.040: INFO: Deleting pod "simpletest.rc-q7f9k" in namespace "gc-3915"
Aug 11 14:25:36.054: INFO: Deleting pod "simpletest.rc-qcjt4" in namespace "gc-3915"
Aug 11 14:25:36.077: INFO: Deleting pod "simpletest.rc-qmsnw" in namespace "gc-3915"
Aug 11 14:25:36.093: INFO: Deleting pod "simpletest.rc-qsgdr" in namespace "gc-3915"
Aug 11 14:25:36.105: INFO: Deleting pod "simpletest.rc-qx5tl" in namespace "gc-3915"
Aug 11 14:25:36.120: INFO: Deleting pod "simpletest.rc-r82tg" in namespace "gc-3915"
Aug 11 14:25:36.132: INFO: Deleting pod "simpletest.rc-r9ft6" in namespace "gc-3915"
Aug 11 14:25:36.143: INFO: Deleting pod "simpletest.rc-rknx5" in namespace "gc-3915"
Aug 11 14:25:36.184: INFO: Deleting pod "simpletest.rc-rnxzm" in namespace "gc-3915"
Aug 11 14:25:36.235: INFO: Deleting pod "simpletest.rc-rvph4" in namespace "gc-3915"
Aug 11 14:25:36.281: INFO: Deleting pod "simpletest.rc-rwhlk" in namespace "gc-3915"
Aug 11 14:25:36.335: INFO: Deleting pod "simpletest.rc-rx44d" in namespace "gc-3915"
Aug 11 14:25:36.384: INFO: Deleting pod "simpletest.rc-rxwwc" in namespace "gc-3915"
Aug 11 14:25:36.434: INFO: Deleting pod "simpletest.rc-sdcp7" in namespace "gc-3915"
Aug 11 14:25:36.482: INFO: Deleting pod "simpletest.rc-sjjt8" in namespace "gc-3915"
Aug 11 14:25:36.531: INFO: Deleting pod "simpletest.rc-sn6hc" in namespace "gc-3915"
Aug 11 14:25:36.584: INFO: Deleting pod "simpletest.rc-sshzf" in namespace "gc-3915"
Aug 11 14:25:36.635: INFO: Deleting pod "simpletest.rc-swqxr" in namespace "gc-3915"
Aug 11 14:25:36.683: INFO: Deleting pod "simpletest.rc-t4cn8" in namespace "gc-3915"
Aug 11 14:25:36.733: INFO: Deleting pod "simpletest.rc-tlg22" in namespace "gc-3915"
Aug 11 14:25:36.799: INFO: Deleting pod "simpletest.rc-tv748" in namespace "gc-3915"
Aug 11 14:25:36.834: INFO: Deleting pod "simpletest.rc-vgnpj" in namespace "gc-3915"
Aug 11 14:25:36.881: INFO: Deleting pod "simpletest.rc-vlfmc" in namespace "gc-3915"
Aug 11 14:25:36.932: INFO: Deleting pod "simpletest.rc-vll6w" in namespace "gc-3915"
Aug 11 14:25:36.979: INFO: Deleting pod "simpletest.rc-vt4mx" in namespace "gc-3915"
Aug 11 14:25:37.030: INFO: Deleting pod "simpletest.rc-wmdt9" in namespace "gc-3915"
Aug 11 14:25:37.085: INFO: Deleting pod "simpletest.rc-x4pgg" in namespace "gc-3915"
Aug 11 14:25:37.134: INFO: Deleting pod "simpletest.rc-xfnvf" in namespace "gc-3915"
Aug 11 14:25:37.186: INFO: Deleting pod "simpletest.rc-xfpvm" in namespace "gc-3915"
Aug 11 14:25:37.233: INFO: Deleting pod "simpletest.rc-xhm6p" in namespace "gc-3915"
Aug 11 14:25:37.283: INFO: Deleting pod "simpletest.rc-xkrfc" in namespace "gc-3915"
Aug 11 14:25:37.331: INFO: Deleting pod "simpletest.rc-xm5x6" in namespace "gc-3915"
Aug 11 14:25:37.384: INFO: Deleting pod "simpletest.rc-xt9lm" in namespace "gc-3915"
Aug 11 14:25:37.436: INFO: Deleting pod "simpletest.rc-z28sj" in namespace "gc-3915"
Aug 11 14:25:37.482: INFO: Deleting pod "simpletest.rc-z2xlr" in namespace "gc-3915"
Aug 11 14:25:37.534: INFO: Deleting pod "simpletest.rc-zbqnq" in namespace "gc-3915"
Aug 11 14:25:37.587: INFO: Deleting pod "simpletest.rc-zn7f2" in namespace "gc-3915"
Aug 11 14:25:37.632: INFO: Deleting pod "simpletest.rc-zrvs6" in namespace "gc-3915"
Aug 11 14:25:37.681: INFO: Deleting pod "simpletest.rc-zvb65" in namespace "gc-3915"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 14:25:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3915" for this suite. 08/11/23 14:25:37.775
------------------------------
• [SLOW TEST] [42.861 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:24:54.966
    Aug 11 14:24:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 14:24:54.967
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:24:54.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:24:54.982
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/11/23 14:24:54.989
    STEP: delete the rc 08/11/23 14:24:59.999
    STEP: wait for the rc to be deleted 08/11/23 14:25:00.01
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/11/23 14:25:05.015
    STEP: Gathering metrics 08/11/23 14:25:35.03
    Aug 11 14:25:35.064: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 14:25:35.068: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.215693ms
    Aug 11 14:25:35.068: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 14:25:35.068: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 14:25:35.140: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 11 14:25:35.140: INFO: Deleting pod "simpletest.rc-296qn" in namespace "gc-3915"
    Aug 11 14:25:35.150: INFO: Deleting pod "simpletest.rc-42xfn" in namespace "gc-3915"
    Aug 11 14:25:35.164: INFO: Deleting pod "simpletest.rc-46jrw" in namespace "gc-3915"
    Aug 11 14:25:35.183: INFO: Deleting pod "simpletest.rc-4dgrl" in namespace "gc-3915"
    Aug 11 14:25:35.197: INFO: Deleting pod "simpletest.rc-4kdfj" in namespace "gc-3915"
    Aug 11 14:25:35.219: INFO: Deleting pod "simpletest.rc-4kz7t" in namespace "gc-3915"
    Aug 11 14:25:35.238: INFO: Deleting pod "simpletest.rc-4nlt5" in namespace "gc-3915"
    Aug 11 14:25:35.251: INFO: Deleting pod "simpletest.rc-4v26q" in namespace "gc-3915"
    Aug 11 14:25:35.267: INFO: Deleting pod "simpletest.rc-4z8dm" in namespace "gc-3915"
    Aug 11 14:25:35.280: INFO: Deleting pod "simpletest.rc-5npjf" in namespace "gc-3915"
    Aug 11 14:25:35.300: INFO: Deleting pod "simpletest.rc-6k56r" in namespace "gc-3915"
    Aug 11 14:25:35.319: INFO: Deleting pod "simpletest.rc-6szss" in namespace "gc-3915"
    Aug 11 14:25:35.338: INFO: Deleting pod "simpletest.rc-777f4" in namespace "gc-3915"
    Aug 11 14:25:35.355: INFO: Deleting pod "simpletest.rc-7rh56" in namespace "gc-3915"
    Aug 11 14:25:35.386: INFO: Deleting pod "simpletest.rc-8ff8q" in namespace "gc-3915"
    Aug 11 14:25:35.410: INFO: Deleting pod "simpletest.rc-8xcql" in namespace "gc-3915"
    Aug 11 14:25:35.432: INFO: Deleting pod "simpletest.rc-92t7p" in namespace "gc-3915"
    Aug 11 14:25:35.457: INFO: Deleting pod "simpletest.rc-9gzbk" in namespace "gc-3915"
    Aug 11 14:25:35.472: INFO: Deleting pod "simpletest.rc-9hq25" in namespace "gc-3915"
    Aug 11 14:25:35.490: INFO: Deleting pod "simpletest.rc-9jx5p" in namespace "gc-3915"
    Aug 11 14:25:35.506: INFO: Deleting pod "simpletest.rc-9klq2" in namespace "gc-3915"
    Aug 11 14:25:35.524: INFO: Deleting pod "simpletest.rc-9krdf" in namespace "gc-3915"
    Aug 11 14:25:35.540: INFO: Deleting pod "simpletest.rc-9lgwl" in namespace "gc-3915"
    Aug 11 14:25:35.551: INFO: Deleting pod "simpletest.rc-9nqkt" in namespace "gc-3915"
    Aug 11 14:25:35.561: INFO: Deleting pod "simpletest.rc-9xccw" in namespace "gc-3915"
    Aug 11 14:25:35.574: INFO: Deleting pod "simpletest.rc-b8hqr" in namespace "gc-3915"
    Aug 11 14:25:35.590: INFO: Deleting pod "simpletest.rc-bbcfz" in namespace "gc-3915"
    Aug 11 14:25:35.600: INFO: Deleting pod "simpletest.rc-bjlzm" in namespace "gc-3915"
    Aug 11 14:25:35.614: INFO: Deleting pod "simpletest.rc-bqfsh" in namespace "gc-3915"
    Aug 11 14:25:35.625: INFO: Deleting pod "simpletest.rc-br4hp" in namespace "gc-3915"
    Aug 11 14:25:35.635: INFO: Deleting pod "simpletest.rc-br5l8" in namespace "gc-3915"
    Aug 11 14:25:35.653: INFO: Deleting pod "simpletest.rc-chp7t" in namespace "gc-3915"
    Aug 11 14:25:35.664: INFO: Deleting pod "simpletest.rc-cw2nn" in namespace "gc-3915"
    Aug 11 14:25:35.678: INFO: Deleting pod "simpletest.rc-dhdtx" in namespace "gc-3915"
    Aug 11 14:25:35.690: INFO: Deleting pod "simpletest.rc-dmnkf" in namespace "gc-3915"
    Aug 11 14:25:35.704: INFO: Deleting pod "simpletest.rc-dnxqk" in namespace "gc-3915"
    Aug 11 14:25:35.718: INFO: Deleting pod "simpletest.rc-dqwwj" in namespace "gc-3915"
    Aug 11 14:25:35.732: INFO: Deleting pod "simpletest.rc-ds7w5" in namespace "gc-3915"
    Aug 11 14:25:35.745: INFO: Deleting pod "simpletest.rc-fcmth" in namespace "gc-3915"
    Aug 11 14:25:35.761: INFO: Deleting pod "simpletest.rc-fhxkw" in namespace "gc-3915"
    Aug 11 14:25:35.775: INFO: Deleting pod "simpletest.rc-frjwq" in namespace "gc-3915"
    Aug 11 14:25:35.792: INFO: Deleting pod "simpletest.rc-fznk5" in namespace "gc-3915"
    Aug 11 14:25:35.805: INFO: Deleting pod "simpletest.rc-gwfzq" in namespace "gc-3915"
    Aug 11 14:25:35.816: INFO: Deleting pod "simpletest.rc-h8w7b" in namespace "gc-3915"
    Aug 11 14:25:35.830: INFO: Deleting pod "simpletest.rc-hm66q" in namespace "gc-3915"
    Aug 11 14:25:35.843: INFO: Deleting pod "simpletest.rc-hwlhv" in namespace "gc-3915"
    Aug 11 14:25:35.862: INFO: Deleting pod "simpletest.rc-hxxdl" in namespace "gc-3915"
    Aug 11 14:25:35.874: INFO: Deleting pod "simpletest.rc-jzdts" in namespace "gc-3915"
    Aug 11 14:25:35.887: INFO: Deleting pod "simpletest.rc-k5bql" in namespace "gc-3915"
    Aug 11 14:25:35.900: INFO: Deleting pod "simpletest.rc-khjsq" in namespace "gc-3915"
    Aug 11 14:25:35.910: INFO: Deleting pod "simpletest.rc-knfjq" in namespace "gc-3915"
    Aug 11 14:25:35.923: INFO: Deleting pod "simpletest.rc-ksxnp" in namespace "gc-3915"
    Aug 11 14:25:35.933: INFO: Deleting pod "simpletest.rc-l2j55" in namespace "gc-3915"
    Aug 11 14:25:35.946: INFO: Deleting pod "simpletest.rc-lbzlg" in namespace "gc-3915"
    Aug 11 14:25:35.958: INFO: Deleting pod "simpletest.rc-llcnq" in namespace "gc-3915"
    Aug 11 14:25:35.969: INFO: Deleting pod "simpletest.rc-n6c8z" in namespace "gc-3915"
    Aug 11 14:25:35.978: INFO: Deleting pod "simpletest.rc-n8s4s" in namespace "gc-3915"
    Aug 11 14:25:35.994: INFO: Deleting pod "simpletest.rc-nfdfl" in namespace "gc-3915"
    Aug 11 14:25:36.004: INFO: Deleting pod "simpletest.rc-nfwbt" in namespace "gc-3915"
    Aug 11 14:25:36.016: INFO: Deleting pod "simpletest.rc-nxcpk" in namespace "gc-3915"
    Aug 11 14:25:36.026: INFO: Deleting pod "simpletest.rc-p56mm" in namespace "gc-3915"
    Aug 11 14:25:36.040: INFO: Deleting pod "simpletest.rc-q7f9k" in namespace "gc-3915"
    Aug 11 14:25:36.054: INFO: Deleting pod "simpletest.rc-qcjt4" in namespace "gc-3915"
    Aug 11 14:25:36.077: INFO: Deleting pod "simpletest.rc-qmsnw" in namespace "gc-3915"
    Aug 11 14:25:36.093: INFO: Deleting pod "simpletest.rc-qsgdr" in namespace "gc-3915"
    Aug 11 14:25:36.105: INFO: Deleting pod "simpletest.rc-qx5tl" in namespace "gc-3915"
    Aug 11 14:25:36.120: INFO: Deleting pod "simpletest.rc-r82tg" in namespace "gc-3915"
    Aug 11 14:25:36.132: INFO: Deleting pod "simpletest.rc-r9ft6" in namespace "gc-3915"
    Aug 11 14:25:36.143: INFO: Deleting pod "simpletest.rc-rknx5" in namespace "gc-3915"
    Aug 11 14:25:36.184: INFO: Deleting pod "simpletest.rc-rnxzm" in namespace "gc-3915"
    Aug 11 14:25:36.235: INFO: Deleting pod "simpletest.rc-rvph4" in namespace "gc-3915"
    Aug 11 14:25:36.281: INFO: Deleting pod "simpletest.rc-rwhlk" in namespace "gc-3915"
    Aug 11 14:25:36.335: INFO: Deleting pod "simpletest.rc-rx44d" in namespace "gc-3915"
    Aug 11 14:25:36.384: INFO: Deleting pod "simpletest.rc-rxwwc" in namespace "gc-3915"
    Aug 11 14:25:36.434: INFO: Deleting pod "simpletest.rc-sdcp7" in namespace "gc-3915"
    Aug 11 14:25:36.482: INFO: Deleting pod "simpletest.rc-sjjt8" in namespace "gc-3915"
    Aug 11 14:25:36.531: INFO: Deleting pod "simpletest.rc-sn6hc" in namespace "gc-3915"
    Aug 11 14:25:36.584: INFO: Deleting pod "simpletest.rc-sshzf" in namespace "gc-3915"
    Aug 11 14:25:36.635: INFO: Deleting pod "simpletest.rc-swqxr" in namespace "gc-3915"
    Aug 11 14:25:36.683: INFO: Deleting pod "simpletest.rc-t4cn8" in namespace "gc-3915"
    Aug 11 14:25:36.733: INFO: Deleting pod "simpletest.rc-tlg22" in namespace "gc-3915"
    Aug 11 14:25:36.799: INFO: Deleting pod "simpletest.rc-tv748" in namespace "gc-3915"
    Aug 11 14:25:36.834: INFO: Deleting pod "simpletest.rc-vgnpj" in namespace "gc-3915"
    Aug 11 14:25:36.881: INFO: Deleting pod "simpletest.rc-vlfmc" in namespace "gc-3915"
    Aug 11 14:25:36.932: INFO: Deleting pod "simpletest.rc-vll6w" in namespace "gc-3915"
    Aug 11 14:25:36.979: INFO: Deleting pod "simpletest.rc-vt4mx" in namespace "gc-3915"
    Aug 11 14:25:37.030: INFO: Deleting pod "simpletest.rc-wmdt9" in namespace "gc-3915"
    Aug 11 14:25:37.085: INFO: Deleting pod "simpletest.rc-x4pgg" in namespace "gc-3915"
    Aug 11 14:25:37.134: INFO: Deleting pod "simpletest.rc-xfnvf" in namespace "gc-3915"
    Aug 11 14:25:37.186: INFO: Deleting pod "simpletest.rc-xfpvm" in namespace "gc-3915"
    Aug 11 14:25:37.233: INFO: Deleting pod "simpletest.rc-xhm6p" in namespace "gc-3915"
    Aug 11 14:25:37.283: INFO: Deleting pod "simpletest.rc-xkrfc" in namespace "gc-3915"
    Aug 11 14:25:37.331: INFO: Deleting pod "simpletest.rc-xm5x6" in namespace "gc-3915"
    Aug 11 14:25:37.384: INFO: Deleting pod "simpletest.rc-xt9lm" in namespace "gc-3915"
    Aug 11 14:25:37.436: INFO: Deleting pod "simpletest.rc-z28sj" in namespace "gc-3915"
    Aug 11 14:25:37.482: INFO: Deleting pod "simpletest.rc-z2xlr" in namespace "gc-3915"
    Aug 11 14:25:37.534: INFO: Deleting pod "simpletest.rc-zbqnq" in namespace "gc-3915"
    Aug 11 14:25:37.587: INFO: Deleting pod "simpletest.rc-zn7f2" in namespace "gc-3915"
    Aug 11 14:25:37.632: INFO: Deleting pod "simpletest.rc-zrvs6" in namespace "gc-3915"
    Aug 11 14:25:37.681: INFO: Deleting pod "simpletest.rc-zvb65" in namespace "gc-3915"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:25:37.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3915" for this suite. 08/11/23 14:25:37.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:25:37.835
Aug 11 14:25:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:25:37.836
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:37.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:37.856
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/11/23 14:25:47.933
STEP: referencing matching pods with named port 08/11/23 14:25:52.942
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/11/23 14:25:57.949
STEP: recreating EndpointSlices after they've been deleted 08/11/23 14:26:02.958
Aug 11 14:26:02.976: INFO: EndpointSlice for Service endpointslice-4968/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:12.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4968" for this suite. 08/11/23 14:26:12.99
------------------------------
• [SLOW TEST] [35.163 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:25:37.835
    Aug 11 14:25:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:25:37.836
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:37.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:37.856
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/11/23 14:25:47.933
    STEP: referencing matching pods with named port 08/11/23 14:25:52.942
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/11/23 14:25:57.949
    STEP: recreating EndpointSlices after they've been deleted 08/11/23 14:26:02.958
    Aug 11 14:26:02.976: INFO: EndpointSlice for Service endpointslice-4968/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:12.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4968" for this suite. 08/11/23 14:26:12.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:13.001
Aug 11 14:26:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:26:13.002
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:13.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:13.019
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:26:13.032
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:26:13.319
STEP: Deploying the webhook pod 08/11/23 14:26:13.329
STEP: Wait for the deployment to be ready 08/11/23 14:26:13.342
Aug 11 14:26:13.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:26:15.359
STEP: Verifying the service has paired with the endpoint 08/11/23 14:26:15.374
Aug 11 14:26:16.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/11/23 14:26:16.439
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:26:16.483
STEP: Deleting the collection of validation webhooks 08/11/23 14:26:16.517
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:26:16.57
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6832" for this suite. 08/11/23 14:26:16.656
STEP: Destroying namespace "webhook-6832-markers" for this suite. 08/11/23 14:26:16.664
------------------------------
• [3.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:13.001
    Aug 11 14:26:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:26:13.002
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:13.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:13.019
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:26:13.032
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:26:13.319
    STEP: Deploying the webhook pod 08/11/23 14:26:13.329
    STEP: Wait for the deployment to be ready 08/11/23 14:26:13.342
    Aug 11 14:26:13.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:26:15.359
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:26:15.374
    Aug 11 14:26:16.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/11/23 14:26:16.439
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:26:16.483
    STEP: Deleting the collection of validation webhooks 08/11/23 14:26:16.517
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:26:16.57
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6832" for this suite. 08/11/23 14:26:16.656
    STEP: Destroying namespace "webhook-6832-markers" for this suite. 08/11/23 14:26:16.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:16.672
Aug 11 14:26:16.672: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:26:16.673
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.719
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5944" 08/11/23 14:26:16.721
Aug 11 14:26:16.739: INFO: Namespace "namespaces-5944" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e9629551-98de-4270-b96a-6c3e0c707863", "kubernetes.io/metadata.name":"namespaces-5944", "namespaces-5944":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:16.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5944" for this suite. 08/11/23 14:26:16.749
------------------------------
• [0.084 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:16.672
    Aug 11 14:26:16.672: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:26:16.673
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.719
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5944" 08/11/23 14:26:16.721
    Aug 11 14:26:16.739: INFO: Namespace "namespaces-5944" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e9629551-98de-4270-b96a-6c3e0c707863", "kubernetes.io/metadata.name":"namespaces-5944", "namespaces-5944":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:16.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5944" for this suite. 08/11/23 14:26:16.749
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:16.757
Aug 11 14:26:16.757: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:26:16.757
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.775
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-3cbfb14b-cf9c-44b7-b2b3-05d8ec2e172d 08/11/23 14:26:16.777
STEP: Creating a pod to test consume configMaps 08/11/23 14:26:16.784
Aug 11 14:26:16.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3" in namespace "projected-2828" to be "Succeeded or Failed"
Aug 11 14:26:16.797: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032394ms
Aug 11 14:26:18.802: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008789935s
Aug 11 14:26:20.801: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008073357s
Aug 11 14:26:22.803: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009669918s
STEP: Saw pod success 08/11/23 14:26:22.803
Aug 11 14:26:22.803: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3" satisfied condition "Succeeded or Failed"
Aug 11 14:26:22.806: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:26:22.816
Aug 11 14:26:22.831: INFO: Waiting for pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 to disappear
Aug 11 14:26:22.833: INFO: Pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:22.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2828" for this suite. 08/11/23 14:26:22.837
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:16.757
    Aug 11 14:26:16.757: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:26:16.757
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.775
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-3cbfb14b-cf9c-44b7-b2b3-05d8ec2e172d 08/11/23 14:26:16.777
    STEP: Creating a pod to test consume configMaps 08/11/23 14:26:16.784
    Aug 11 14:26:16.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3" in namespace "projected-2828" to be "Succeeded or Failed"
    Aug 11 14:26:16.797: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032394ms
    Aug 11 14:26:18.802: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008789935s
    Aug 11 14:26:20.801: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008073357s
    Aug 11 14:26:22.803: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009669918s
    STEP: Saw pod success 08/11/23 14:26:22.803
    Aug 11 14:26:22.803: INFO: Pod "pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3" satisfied condition "Succeeded or Failed"
    Aug 11 14:26:22.806: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:26:22.816
    Aug 11 14:26:22.831: INFO: Waiting for pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 to disappear
    Aug 11 14:26:22.833: INFO: Pod pod-projected-configmaps-aa6e79bb-dd01-483e-8fee-e65136c154d3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:22.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2828" for this suite. 08/11/23 14:26:22.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:22.845
Aug 11 14:26:22.845: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:26:22.845
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:22.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:22.863
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:26:22.865
Aug 11 14:26:22.875: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8" in namespace "projected-7971" to be "Succeeded or Failed"
Aug 11 14:26:22.878: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922054ms
Aug 11 14:26:24.882: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811883s
Aug 11 14:26:26.883: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00876685s
STEP: Saw pod success 08/11/23 14:26:26.883
Aug 11 14:26:26.883: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8" satisfied condition "Succeeded or Failed"
Aug 11 14:26:26.887: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 container client-container: <nil>
STEP: delete the pod 08/11/23 14:26:26.896
Aug 11 14:26:26.912: INFO: Waiting for pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 to disappear
Aug 11 14:26:26.915: INFO: Pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7971" for this suite. 08/11/23 14:26:26.918
------------------------------
• [4.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:22.845
    Aug 11 14:26:22.845: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:26:22.845
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:22.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:22.863
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:26:22.865
    Aug 11 14:26:22.875: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8" in namespace "projected-7971" to be "Succeeded or Failed"
    Aug 11 14:26:22.878: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922054ms
    Aug 11 14:26:24.882: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007811883s
    Aug 11 14:26:26.883: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00876685s
    STEP: Saw pod success 08/11/23 14:26:26.883
    Aug 11 14:26:26.883: INFO: Pod "downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8" satisfied condition "Succeeded or Failed"
    Aug 11 14:26:26.887: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:26:26.896
    Aug 11 14:26:26.912: INFO: Waiting for pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 to disappear
    Aug 11 14:26:26.915: INFO: Pod downwardapi-volume-89647a1d-7ff8-4b5a-8f3e-5b7b35b04ae8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7971" for this suite. 08/11/23 14:26:26.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:26.927
Aug 11 14:26:26.927: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 14:26:26.927
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:26.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:26.945
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 11 14:26:26.964: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:26:26.964
Aug 11 14:26:26.964: INFO: Waiting up to 5m0s for pod "test-rs-p5m4b" in namespace "replicaset-3372" to be "running"
Aug 11 14:26:26.970: INFO: Pod "test-rs-p5m4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512496ms
Aug 11 14:26:28.975: INFO: Pod "test-rs-p5m4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599786s
Aug 11 14:26:28.975: INFO: Pod "test-rs-p5m4b" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  08/11/23 14:26:28.975
Aug 11 14:26:28.983: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/11/23 14:26:28.983
W0811 14:26:28.995078      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 14:26:28.996: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 14:26:29.003: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 14:26:29.016: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 14:26:29.022: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 14:26:30.139: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 2, AvailableReplicas 2
Aug 11 14:26:30.641: INFO: observed Replicaset test-rs in namespace replicaset-3372 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3372" for this suite. 08/11/23 14:26:30.646
------------------------------
• [3.726 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:26.927
    Aug 11 14:26:26.927: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:26:26.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:26.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:26.945
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 11 14:26:26.964: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:26:26.964
    Aug 11 14:26:26.964: INFO: Waiting up to 5m0s for pod "test-rs-p5m4b" in namespace "replicaset-3372" to be "running"
    Aug 11 14:26:26.970: INFO: Pod "test-rs-p5m4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.512496ms
    Aug 11 14:26:28.975: INFO: Pod "test-rs-p5m4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599786s
    Aug 11 14:26:28.975: INFO: Pod "test-rs-p5m4b" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  08/11/23 14:26:28.975
    Aug 11 14:26:28.983: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/11/23 14:26:28.983
    W0811 14:26:28.995078      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 14:26:28.996: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 14:26:29.003: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 14:26:29.016: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 14:26:29.022: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 14:26:30.139: INFO: observed ReplicaSet test-rs in namespace replicaset-3372 with ReadyReplicas 2, AvailableReplicas 2
    Aug 11 14:26:30.641: INFO: observed Replicaset test-rs in namespace replicaset-3372 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3372" for this suite. 08/11/23 14:26:30.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:30.653
Aug 11 14:26:30.653: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:26:30.654
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:30.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:30.674
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:26:30.687
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:26:31.038
STEP: Deploying the webhook pod 08/11/23 14:26:31.044
STEP: Wait for the deployment to be ready 08/11/23 14:26:31.056
Aug 11 14:26:31.063: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:26:33.074
STEP: Verifying the service has paired with the endpoint 08/11/23 14:26:33.088
Aug 11 14:26:34.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/11/23 14:26:34.092
STEP: create a configmap that should be updated by the webhook 08/11/23 14:26:34.119
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:34.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5212" for this suite. 08/11/23 14:26:34.207
STEP: Destroying namespace "webhook-5212-markers" for this suite. 08/11/23 14:26:34.218
------------------------------
• [3.572 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:30.653
    Aug 11 14:26:30.653: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:26:30.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:30.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:30.674
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:26:30.687
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:26:31.038
    STEP: Deploying the webhook pod 08/11/23 14:26:31.044
    STEP: Wait for the deployment to be ready 08/11/23 14:26:31.056
    Aug 11 14:26:31.063: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:26:33.074
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:26:33.088
    Aug 11 14:26:34.089: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/11/23 14:26:34.092
    STEP: create a configmap that should be updated by the webhook 08/11/23 14:26:34.119
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:34.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5212" for this suite. 08/11/23 14:26:34.207
    STEP: Destroying namespace "webhook-5212-markers" for this suite. 08/11/23 14:26:34.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:34.226
Aug 11 14:26:34.226: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:26:34.226
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:34.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:34.249
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:26:34.265
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:26:34.27
Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:34.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:26:34.280: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:35.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:26:35.289: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:36.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:26:36.289: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 08/11/23 14:26:36.291
STEP: DeleteCollection of the DaemonSets 08/11/23 14:26:36.294
STEP: Verify that ReplicaSets have been deleted 08/11/23 14:26:36.303
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Aug 11 14:26:36.314: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22577"},"items":null}

Aug 11 14:26:36.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22578"},"items":[{"metadata":{"name":"daemon-set-972zt","generateName":"daemon-set-","namespace":"daemonsets-1212","uid":"ad6c4cb5-d35f-4755-951b-345eb1bf16d4","resourceVersion":"22577","creationTimestamp":"2023-08-11T14:26:34Z","deletionTimestamp":"2023-08-11T14:27:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eba5d745-8309-4e35-ae8d-b66dd76a3b62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eba5d745-8309-4e35-ae8d-b66dd76a3b62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g8fhg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g8fhg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-1cf5d931-worker-6381a7ba-nd80","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-1cf5d931-worker-6381a7ba-nd80"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"}],"hostIP":"192.168.178.3","podIP":"10.10.1.9","podIPs":[{"ip":"10.10.1.9"}],"startTime":"2023-08-11T14:26:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:26:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e51881631ade73415ed787c265915b8d126e85ca70b59f7288457741911bc94c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rrjjd","generateName":"daemon-set-","namespace":"daemonsets-1212","uid":"e825bf12-3bab-4a08-a507-dba6fc15dcd5","resourceVersion":"22578","creationTimestamp":"2023-08-11T14:26:34Z","deletionTimestamp":"2023-08-11T14:27:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eba5d745-8309-4e35-ae8d-b66dd76a3b62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eba5d745-8309-4e35-ae8d-b66dd76a3b62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9mgbn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9mgbn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-1cf5d931-worker-6381a7ba-mt98","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-1cf5d931-worker-6381a7ba-mt98"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"}],"hostIP":"192.168.178.2","podIP":"10.10.0.102","podIPs":[{"ip":"10.10.0.102"}],"startTime":"2023-08-11T14:26:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:26:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4269cae4eab19fd3c93531a5c7229dd43e1501d32d2c4350a08821962b7d47b8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:36.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1212" for this suite. 08/11/23 14:26:36.333
------------------------------
• [2.115 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:34.226
    Aug 11 14:26:34.226: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:26:34.226
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:34.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:34.249
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:26:34.265
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:26:34.27
    Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:34.277: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:34.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:26:34.280: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:35.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:35.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:26:35.289: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:36.286: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:36.289: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:26:36.289: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 08/11/23 14:26:36.291
    STEP: DeleteCollection of the DaemonSets 08/11/23 14:26:36.294
    STEP: Verify that ReplicaSets have been deleted 08/11/23 14:26:36.303
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Aug 11 14:26:36.314: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22577"},"items":null}

    Aug 11 14:26:36.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22578"},"items":[{"metadata":{"name":"daemon-set-972zt","generateName":"daemon-set-","namespace":"daemonsets-1212","uid":"ad6c4cb5-d35f-4755-951b-345eb1bf16d4","resourceVersion":"22577","creationTimestamp":"2023-08-11T14:26:34Z","deletionTimestamp":"2023-08-11T14:27:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eba5d745-8309-4e35-ae8d-b66dd76a3b62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eba5d745-8309-4e35-ae8d-b66dd76a3b62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g8fhg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g8fhg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-1cf5d931-worker-6381a7ba-nd80","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-1cf5d931-worker-6381a7ba-nd80"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"}],"hostIP":"192.168.178.3","podIP":"10.10.1.9","podIPs":[{"ip":"10.10.1.9"}],"startTime":"2023-08-11T14:26:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:26:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e51881631ade73415ed787c265915b8d126e85ca70b59f7288457741911bc94c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rrjjd","generateName":"daemon-set-","namespace":"daemonsets-1212","uid":"e825bf12-3bab-4a08-a507-dba6fc15dcd5","resourceVersion":"22578","creationTimestamp":"2023-08-11T14:26:34Z","deletionTimestamp":"2023-08-11T14:27:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"eba5d745-8309-4e35-ae8d-b66dd76a3b62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eba5d745-8309-4e35-ae8d-b66dd76a3b62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:26:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9mgbn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9mgbn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-1cf5d931-worker-6381a7ba-mt98","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-1cf5d931-worker-6381a7ba-mt98"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:35Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:26:34Z"}],"hostIP":"192.168.178.2","podIP":"10.10.0.102","podIPs":[{"ip":"10.10.0.102"}],"startTime":"2023-08-11T14:26:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:26:34Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4269cae4eab19fd3c93531a5c7229dd43e1501d32d2c4350a08821962b7d47b8","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:36.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1212" for this suite. 08/11/23 14:26:36.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:36.341
Aug 11 14:26:36.341: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename containers 08/11/23 14:26:36.342
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:36.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:36.359
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 11 14:26:36.369: INFO: Waiting up to 5m0s for pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c" in namespace "containers-3265" to be "running"
Aug 11 14:26:36.373: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650064ms
Aug 11 14:26:38.376: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007638373s
Aug 11 14:26:38.377: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3265" for this suite. 08/11/23 14:26:38.39
------------------------------
• [2.057 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:36.341
    Aug 11 14:26:36.341: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename containers 08/11/23 14:26:36.342
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:36.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:36.359
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 11 14:26:36.369: INFO: Waiting up to 5m0s for pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c" in namespace "containers-3265" to be "running"
    Aug 11 14:26:36.373: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.650064ms
    Aug 11 14:26:38.376: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007638373s
    Aug 11 14:26:38.377: INFO: Pod "client-containers-baff3ac7-9fda-490b-8e5a-df28b15b1b4c" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3265" for this suite. 08/11/23 14:26:38.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:38.4
Aug 11 14:26:38.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:26:38.401
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:38.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:38.419
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/11/23 14:26:38.421
STEP: patching the Namespace 08/11/23 14:26:38.436
STEP: get the Namespace and ensuring it has the label 08/11/23 14:26:38.441
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:38.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4287" for this suite. 08/11/23 14:26:38.446
STEP: Destroying namespace "nspatchtest-b3d2ca0b-4adc-4cc9-b55c-2061c1b6b286-3988" for this suite. 08/11/23 14:26:38.454
------------------------------
• [0.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:38.4
    Aug 11 14:26:38.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:26:38.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:38.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:38.419
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/11/23 14:26:38.421
    STEP: patching the Namespace 08/11/23 14:26:38.436
    STEP: get the Namespace and ensuring it has the label 08/11/23 14:26:38.441
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:38.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4287" for this suite. 08/11/23 14:26:38.446
    STEP: Destroying namespace "nspatchtest-b3d2ca0b-4adc-4cc9-b55c-2061c1b6b286-3988" for this suite. 08/11/23 14:26:38.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:38.461
Aug 11 14:26:38.462: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:26:38.462
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:38.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:38.478
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Aug 11 14:26:38.495: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:26:38.503
Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:38.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:26:38.514: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:39.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:26:39.521: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:40.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:26:40.522: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 08/11/23 14:26:40.534
STEP: Check that daemon pods images are updated. 08/11/23 14:26:40.544
Aug 11 14:26:40.548: INFO: Wrong image for pod: daemon-set-6xdlp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 11 14:26:40.548: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 11 14:26:40.553: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:40.554: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:40.554: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:41.558: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:42.559: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:43.559: INFO: Pod daemon-set-52h6q is not available
Aug 11 14:26:43.559: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.559: INFO: Pod daemon-set-qw6vw is not available
Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 08/11/23 14:26:45.563
Aug 11 14:26:45.566: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.566: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.567: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:45.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:26:45.576: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:26:46.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:26:46.584: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:26:46.599
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9925, will wait for the garbage collector to delete the pods 08/11/23 14:26:46.599
Aug 11 14:26:46.660: INFO: Deleting DaemonSet.extensions daemon-set took: 7.026517ms
Aug 11 14:26:46.761: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.027389ms
Aug 11 14:26:48.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:26:48.764: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:26:48.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22841"},"items":null}

Aug 11 14:26:48.770: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22841"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:26:48.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9925" for this suite. 08/11/23 14:26:48.783
------------------------------
• [SLOW TEST] [10.328 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:38.461
    Aug 11 14:26:38.462: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:26:38.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:38.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:38.478
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Aug 11 14:26:38.495: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:26:38.503
    Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:38.511: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:38.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:26:38.514: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:39.518: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:39.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:26:39.521: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:40.519: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:40.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:26:40.522: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 08/11/23 14:26:40.534
    STEP: Check that daemon pods images are updated. 08/11/23 14:26:40.544
    Aug 11 14:26:40.548: INFO: Wrong image for pod: daemon-set-6xdlp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 11 14:26:40.548: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 11 14:26:40.553: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:40.554: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:40.554: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:41.558: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:41.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:42.559: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:42.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:43.559: INFO: Pod daemon-set-52h6q is not available
    Aug 11 14:26:43.559: INFO: Wrong image for pod: daemon-set-qtzgw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:43.564: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:44.562: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.559: INFO: Pod daemon-set-qw6vw is not available
    Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.563: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 08/11/23 14:26:45.563
    Aug 11 14:26:45.566: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.566: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.567: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:45.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:26:45.576: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:46.581: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:26:46.584: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:26:46.584: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:26:46.599
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9925, will wait for the garbage collector to delete the pods 08/11/23 14:26:46.599
    Aug 11 14:26:46.660: INFO: Deleting DaemonSet.extensions daemon-set took: 7.026517ms
    Aug 11 14:26:46.761: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.027389ms
    Aug 11 14:26:48.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:26:48.764: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:26:48.767: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22841"},"items":null}

    Aug 11 14:26:48.770: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22841"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:26:48.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9925" for this suite. 08/11/23 14:26:48.783
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:26:48.79
Aug 11 14:26:48.790: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename watch 08/11/23 14:26:48.791
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:48.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:48.811
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/11/23 14:26:48.813
STEP: creating a watch on configmaps with label B 08/11/23 14:26:48.814
STEP: creating a watch on configmaps with label A or B 08/11/23 14:26:48.815
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.816
Aug 11 14:26:48.825: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22846 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:48.825: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22846 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.825
Aug 11 14:26:48.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22847 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:48.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22847 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/11/23 14:26:48.832
Aug 11 14:26:48.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22848 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:48.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22848 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.841
Aug 11 14:26:48.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22849 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:48.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22849 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/11/23 14:26:48.848
Aug 11 14:26:48.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22850 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:48.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22850 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/11/23 14:26:58.855
Aug 11 14:26:58.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22939 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:26:58.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22939 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7678" for this suite. 08/11/23 14:27:08.871
------------------------------
• [SLOW TEST] [20.088 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:26:48.79
    Aug 11 14:26:48.790: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename watch 08/11/23 14:26:48.791
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:48.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:48.811
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/11/23 14:26:48.813
    STEP: creating a watch on configmaps with label B 08/11/23 14:26:48.814
    STEP: creating a watch on configmaps with label A or B 08/11/23 14:26:48.815
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.816
    Aug 11 14:26:48.825: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22846 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:48.825: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22846 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.825
    Aug 11 14:26:48.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22847 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:48.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22847 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/11/23 14:26:48.832
    Aug 11 14:26:48.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22848 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:48.841: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22848 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/11/23 14:26:48.841
    Aug 11 14:26:48.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22849 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:48.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7678  ea79ed08-c339-4dd4-b1c0-7be0e16fbc8a 22849 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/11/23 14:26:48.848
    Aug 11 14:26:48.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22850 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:48.854: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22850 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/11/23 14:26:58.855
    Aug 11 14:26:58.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22939 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:26:58.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7678  ffaaf218-9a7f-4ab3-978e-fd17564f954f 22939 0 2023-08-11 14:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7678" for this suite. 08/11/23 14:27:08.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:08.878
Aug 11 14:27:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:27:08.879
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:08.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:08.897
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/11/23 14:27:08.899
STEP: getting /apis/discovery.k8s.io 08/11/23 14:27:08.902
STEP: getting /apis/discovery.k8s.iov1 08/11/23 14:27:08.903
STEP: creating 08/11/23 14:27:08.904
STEP: getting 08/11/23 14:27:08.918
STEP: listing 08/11/23 14:27:08.921
STEP: watching 08/11/23 14:27:08.924
Aug 11 14:27:08.924: INFO: starting watch
STEP: cluster-wide listing 08/11/23 14:27:08.925
STEP: cluster-wide watching 08/11/23 14:27:08.928
Aug 11 14:27:08.928: INFO: starting watch
STEP: patching 08/11/23 14:27:08.929
STEP: updating 08/11/23 14:27:08.935
Aug 11 14:27:08.941: INFO: waiting for watch events with expected annotations
Aug 11 14:27:08.941: INFO: saw patched and updated annotations
STEP: deleting 08/11/23 14:27:08.941
STEP: deleting a collection 08/11/23 14:27:08.952
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-349" for this suite. 08/11/23 14:27:08.971
------------------------------
• [0.102 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:08.878
    Aug 11 14:27:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:27:08.879
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:08.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:08.897
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/11/23 14:27:08.899
    STEP: getting /apis/discovery.k8s.io 08/11/23 14:27:08.902
    STEP: getting /apis/discovery.k8s.iov1 08/11/23 14:27:08.903
    STEP: creating 08/11/23 14:27:08.904
    STEP: getting 08/11/23 14:27:08.918
    STEP: listing 08/11/23 14:27:08.921
    STEP: watching 08/11/23 14:27:08.924
    Aug 11 14:27:08.924: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 14:27:08.925
    STEP: cluster-wide watching 08/11/23 14:27:08.928
    Aug 11 14:27:08.928: INFO: starting watch
    STEP: patching 08/11/23 14:27:08.929
    STEP: updating 08/11/23 14:27:08.935
    Aug 11 14:27:08.941: INFO: waiting for watch events with expected annotations
    Aug 11 14:27:08.941: INFO: saw patched and updated annotations
    STEP: deleting 08/11/23 14:27:08.941
    STEP: deleting a collection 08/11/23 14:27:08.952
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-349" for this suite. 08/11/23 14:27:08.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:08.981
Aug 11 14:27:08.981: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:27:08.982
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:08.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:08.999
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/11/23 14:27:09.002
Aug 11 14:27:09.010: INFO: Waiting up to 5m0s for pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9" in namespace "var-expansion-5564" to be "Succeeded or Failed"
Aug 11 14:27:09.014: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803274ms
Aug 11 14:27:11.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008098064s
Aug 11 14:27:13.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008843561s
STEP: Saw pod success 08/11/23 14:27:13.019
Aug 11 14:27:13.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9" satisfied condition "Succeeded or Failed"
Aug 11 14:27:13.023: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:27:13.034
Aug 11 14:27:13.047: INFO: Waiting for pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 to disappear
Aug 11 14:27:13.050: INFO: Pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:13.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5564" for this suite. 08/11/23 14:27:13.053
------------------------------
• [4.079 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:08.981
    Aug 11 14:27:08.981: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:27:08.982
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:08.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:08.999
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/11/23 14:27:09.002
    Aug 11 14:27:09.010: INFO: Waiting up to 5m0s for pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9" in namespace "var-expansion-5564" to be "Succeeded or Failed"
    Aug 11 14:27:09.014: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803274ms
    Aug 11 14:27:11.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008098064s
    Aug 11 14:27:13.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008843561s
    STEP: Saw pod success 08/11/23 14:27:13.019
    Aug 11 14:27:13.019: INFO: Pod "var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9" satisfied condition "Succeeded or Failed"
    Aug 11 14:27:13.023: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:27:13.034
    Aug 11 14:27:13.047: INFO: Waiting for pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 to disappear
    Aug 11 14:27:13.050: INFO: Pod var-expansion-f6452502-ee86-47ee-8ab8-fbdfc6e99be9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:13.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5564" for this suite. 08/11/23 14:27:13.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:13.061
Aug 11 14:27:13.061: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:27:13.062
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:13.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:13.081
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 11 14:27:13.083: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:13.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3690" for this suite. 08/11/23 14:27:13.634
------------------------------
• [0.599 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:13.061
    Aug 11 14:27:13.061: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:27:13.062
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:13.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:13.081
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 11 14:27:13.083: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:13.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3690" for this suite. 08/11/23 14:27:13.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:13.661
Aug 11 14:27:13.662: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:27:13.662
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:13.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:13.686
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/11/23 14:27:13.689
Aug 11 14:27:13.697: INFO: Waiting up to 5m0s for pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae" in namespace "downward-api-7377" to be "Succeeded or Failed"
Aug 11 14:27:13.702: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365365ms
Aug 11 14:27:15.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009950846s
Aug 11 14:27:17.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009609791s
STEP: Saw pod success 08/11/23 14:27:17.707
Aug 11 14:27:17.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae" satisfied condition "Succeeded or Failed"
Aug 11 14:27:17.710: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:27:17.719
Aug 11 14:27:17.733: INFO: Waiting for pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae to disappear
Aug 11 14:27:17.735: INFO: Pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7377" for this suite. 08/11/23 14:27:17.74
------------------------------
• [4.085 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:13.661
    Aug 11 14:27:13.662: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:27:13.662
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:13.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:13.686
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/11/23 14:27:13.689
    Aug 11 14:27:13.697: INFO: Waiting up to 5m0s for pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae" in namespace "downward-api-7377" to be "Succeeded or Failed"
    Aug 11 14:27:13.702: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365365ms
    Aug 11 14:27:15.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009950846s
    Aug 11 14:27:17.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009609791s
    STEP: Saw pod success 08/11/23 14:27:17.707
    Aug 11 14:27:17.707: INFO: Pod "downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae" satisfied condition "Succeeded or Failed"
    Aug 11 14:27:17.710: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:27:17.719
    Aug 11 14:27:17.733: INFO: Waiting for pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae to disappear
    Aug 11 14:27:17.735: INFO: Pod downward-api-57f6406b-9545-4ff8-95ff-415917f3b0ae no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7377" for this suite. 08/11/23 14:27:17.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:17.747
Aug 11 14:27:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svc-latency 08/11/23 14:27:17.748
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:17.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:17.767
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 11 14:27:17.770: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1387 08/11/23 14:27:17.77
I0811 14:27:17.777905      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1387, replica count: 1
I0811 14:27:18.829109      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0811 14:27:19.829488      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:27:19.947: INFO: Created: latency-svc-gqtn9
Aug 11 14:27:19.954: INFO: Got endpoints: latency-svc-gqtn9 [24.291455ms]
Aug 11 14:27:19.973: INFO: Created: latency-svc-9qfhq
Aug 11 14:27:19.983: INFO: Got endpoints: latency-svc-9qfhq [28.467128ms]
Aug 11 14:27:19.989: INFO: Created: latency-svc-trvkr
Aug 11 14:27:19.996: INFO: Got endpoints: latency-svc-trvkr [41.402421ms]
Aug 11 14:27:20.000: INFO: Created: latency-svc-jfqb4
Aug 11 14:27:20.006: INFO: Got endpoints: latency-svc-jfqb4 [52.177421ms]
Aug 11 14:27:20.019: INFO: Created: latency-svc-cwlv8
Aug 11 14:27:20.027: INFO: Got endpoints: latency-svc-cwlv8 [72.03487ms]
Aug 11 14:27:20.037: INFO: Created: latency-svc-z8x2b
Aug 11 14:27:20.047: INFO: Got endpoints: latency-svc-z8x2b [92.0388ms]
Aug 11 14:27:20.065: INFO: Created: latency-svc-l9d7v
Aug 11 14:27:20.065: INFO: Got endpoints: latency-svc-l9d7v [110.430278ms]
Aug 11 14:27:20.077: INFO: Created: latency-svc-j9ldr
Aug 11 14:27:20.087: INFO: Got endpoints: latency-svc-j9ldr [132.274429ms]
Aug 11 14:27:20.092: INFO: Created: latency-svc-ct25m
Aug 11 14:27:20.095: INFO: Got endpoints: latency-svc-ct25m [140.605327ms]
Aug 11 14:27:20.111: INFO: Created: latency-svc-fgzrv
Aug 11 14:27:20.117: INFO: Got endpoints: latency-svc-fgzrv [162.294298ms]
Aug 11 14:27:20.127: INFO: Created: latency-svc-925jn
Aug 11 14:27:20.134: INFO: Got endpoints: latency-svc-925jn [179.013815ms]
Aug 11 14:27:20.145: INFO: Created: latency-svc-dpxkf
Aug 11 14:27:20.149: INFO: Got endpoints: latency-svc-dpxkf [194.562331ms]
Aug 11 14:27:20.159: INFO: Created: latency-svc-zww8k
Aug 11 14:27:20.183: INFO: Got endpoints: latency-svc-zww8k [227.938233ms]
Aug 11 14:27:20.189: INFO: Created: latency-svc-47wvn
Aug 11 14:27:20.191: INFO: Got endpoints: latency-svc-47wvn [236.631962ms]
Aug 11 14:27:20.201: INFO: Created: latency-svc-vkfnp
Aug 11 14:27:20.214: INFO: Got endpoints: latency-svc-vkfnp [258.719772ms]
Aug 11 14:27:20.216: INFO: Created: latency-svc-gvkns
Aug 11 14:27:20.222: INFO: Got endpoints: latency-svc-gvkns [267.711872ms]
Aug 11 14:27:20.234: INFO: Created: latency-svc-v94km
Aug 11 14:27:20.238: INFO: Got endpoints: latency-svc-v94km [254.802179ms]
Aug 11 14:27:20.248: INFO: Created: latency-svc-k26wk
Aug 11 14:27:20.257: INFO: Got endpoints: latency-svc-k26wk [261.103065ms]
Aug 11 14:27:20.270: INFO: Created: latency-svc-nnqkb
Aug 11 14:27:20.279: INFO: Got endpoints: latency-svc-nnqkb [272.952297ms]
Aug 11 14:27:20.307: INFO: Created: latency-svc-64n7w
Aug 11 14:27:20.307: INFO: Got endpoints: latency-svc-64n7w [280.707884ms]
Aug 11 14:27:20.320: INFO: Created: latency-svc-jjjdn
Aug 11 14:27:20.332: INFO: Got endpoints: latency-svc-jjjdn [284.902978ms]
Aug 11 14:27:20.336: INFO: Created: latency-svc-4vb96
Aug 11 14:27:20.345: INFO: Got endpoints: latency-svc-4vb96 [279.613103ms]
Aug 11 14:27:20.349: INFO: Created: latency-svc-x9l9b
Aug 11 14:27:20.357: INFO: Got endpoints: latency-svc-x9l9b [269.875744ms]
Aug 11 14:27:20.361: INFO: Created: latency-svc-gjwgx
Aug 11 14:27:20.364: INFO: Got endpoints: latency-svc-gjwgx [268.354203ms]
Aug 11 14:27:20.373: INFO: Created: latency-svc-98q5t
Aug 11 14:27:20.382: INFO: Got endpoints: latency-svc-98q5t [264.820909ms]
Aug 11 14:27:20.387: INFO: Created: latency-svc-8crg6
Aug 11 14:27:20.395: INFO: Got endpoints: latency-svc-8crg6 [260.850055ms]
Aug 11 14:27:20.400: INFO: Created: latency-svc-4qss7
Aug 11 14:27:20.405: INFO: Got endpoints: latency-svc-4qss7 [255.837801ms]
Aug 11 14:27:20.424: INFO: Created: latency-svc-pjngh
Aug 11 14:27:20.433: INFO: Got endpoints: latency-svc-pjngh [250.176584ms]
Aug 11 14:27:20.438: INFO: Created: latency-svc-ph2vr
Aug 11 14:27:20.449: INFO: Got endpoints: latency-svc-ph2vr [257.877392ms]
Aug 11 14:27:20.453: INFO: Created: latency-svc-ttdpc
Aug 11 14:27:20.459: INFO: Got endpoints: latency-svc-ttdpc [245.23167ms]
Aug 11 14:27:20.468: INFO: Created: latency-svc-77zm5
Aug 11 14:27:20.475: INFO: Got endpoints: latency-svc-77zm5 [252.177817ms]
Aug 11 14:27:20.485: INFO: Created: latency-svc-w687x
Aug 11 14:27:20.495: INFO: Got endpoints: latency-svc-w687x [257.554983ms]
Aug 11 14:27:20.501: INFO: Created: latency-svc-9bd54
Aug 11 14:27:20.506: INFO: Got endpoints: latency-svc-9bd54 [249.197634ms]
Aug 11 14:27:20.534: INFO: Created: latency-svc-f8kdq
Aug 11 14:27:20.534: INFO: Got endpoints: latency-svc-f8kdq [254.076038ms]
Aug 11 14:27:20.543: INFO: Created: latency-svc-prqjh
Aug 11 14:27:20.557: INFO: Created: latency-svc-5rhxl
Aug 11 14:27:20.559: INFO: Got endpoints: latency-svc-prqjh [251.308305ms]
Aug 11 14:27:20.561: INFO: Got endpoints: latency-svc-5rhxl [229.425205ms]
Aug 11 14:27:20.570: INFO: Created: latency-svc-t49d9
Aug 11 14:27:20.577: INFO: Got endpoints: latency-svc-t49d9 [231.933317ms]
Aug 11 14:27:20.587: INFO: Created: latency-svc-5tssz
Aug 11 14:27:20.591: INFO: Got endpoints: latency-svc-5tssz [234.143229ms]
Aug 11 14:27:20.600: INFO: Created: latency-svc-r8cpr
Aug 11 14:27:20.609: INFO: Got endpoints: latency-svc-r8cpr [245.63295ms]
Aug 11 14:27:20.613: INFO: Created: latency-svc-2rv8c
Aug 11 14:27:20.622: INFO: Got endpoints: latency-svc-2rv8c [239.658304ms]
Aug 11 14:27:20.627: INFO: Created: latency-svc-xf94g
Aug 11 14:27:20.631: INFO: Got endpoints: latency-svc-xf94g [236.568281ms]
Aug 11 14:27:20.650: INFO: Created: latency-svc-9jbk2
Aug 11 14:27:20.658: INFO: Got endpoints: latency-svc-9jbk2 [253.439338ms]
Aug 11 14:27:20.664: INFO: Created: latency-svc-ns9rt
Aug 11 14:27:20.674: INFO: Got endpoints: latency-svc-ns9rt [241.099906ms]
Aug 11 14:27:20.679: INFO: Created: latency-svc-pp2rp
Aug 11 14:27:20.683: INFO: Got endpoints: latency-svc-pp2rp [233.632409ms]
Aug 11 14:27:20.692: INFO: Created: latency-svc-gb2qm
Aug 11 14:27:20.703: INFO: Got endpoints: latency-svc-gb2qm [243.699988ms]
Aug 11 14:27:20.706: INFO: Created: latency-svc-mwrds
Aug 11 14:27:20.716: INFO: Created: latency-svc-9lmbb
Aug 11 14:27:20.728: INFO: Created: latency-svc-szgsc
Aug 11 14:27:20.739: INFO: Created: latency-svc-2qccz
Aug 11 14:27:20.759: INFO: Got endpoints: latency-svc-mwrds [284.664529ms]
Aug 11 14:27:20.762: INFO: Created: latency-svc-tv2zv
Aug 11 14:27:20.773: INFO: Created: latency-svc-jm7v5
Aug 11 14:27:20.784: INFO: Created: latency-svc-smn6q
Aug 11 14:27:20.796: INFO: Created: latency-svc-px7fm
Aug 11 14:27:20.806: INFO: Got endpoints: latency-svc-9lmbb [310.525945ms]
Aug 11 14:27:20.810: INFO: Created: latency-svc-8lvmg
Aug 11 14:27:20.824: INFO: Created: latency-svc-wcj7h
Aug 11 14:27:20.834: INFO: Created: latency-svc-np699
Aug 11 14:27:20.846: INFO: Created: latency-svc-hhjfx
Aug 11 14:27:20.875: INFO: Got endpoints: latency-svc-szgsc [368.995182ms]
Aug 11 14:27:20.879: INFO: Created: latency-svc-7frx2
Aug 11 14:27:20.895: INFO: Created: latency-svc-5k5qn
Aug 11 14:27:20.906: INFO: Got endpoints: latency-svc-2qccz [372.010424ms]
Aug 11 14:27:20.908: INFO: Created: latency-svc-62h2d
Aug 11 14:27:20.920: INFO: Created: latency-svc-n589d
Aug 11 14:27:20.932: INFO: Created: latency-svc-6j8mf
Aug 11 14:27:20.944: INFO: Created: latency-svc-8d4dl
Aug 11 14:27:20.954: INFO: Got endpoints: latency-svc-tv2zv [395.489958ms]
Aug 11 14:27:20.957: INFO: Created: latency-svc-9swxl
Aug 11 14:27:20.972: INFO: Created: latency-svc-5spbg
Aug 11 14:27:21.008: INFO: Got endpoints: latency-svc-jm7v5 [447.241948ms]
Aug 11 14:27:21.028: INFO: Created: latency-svc-pbgf4
Aug 11 14:27:21.052: INFO: Got endpoints: latency-svc-smn6q [475.484086ms]
Aug 11 14:27:21.069: INFO: Created: latency-svc-86rll
Aug 11 14:27:21.105: INFO: Got endpoints: latency-svc-px7fm [513.686714ms]
Aug 11 14:27:21.121: INFO: Created: latency-svc-87rvk
Aug 11 14:27:21.155: INFO: Got endpoints: latency-svc-8lvmg [545.025833ms]
Aug 11 14:27:21.171: INFO: Created: latency-svc-7db67
Aug 11 14:27:21.212: INFO: Got endpoints: latency-svc-wcj7h [590.343667ms]
Aug 11 14:27:21.231: INFO: Created: latency-svc-j6w7k
Aug 11 14:27:21.252: INFO: Got endpoints: latency-svc-np699 [621.013819ms]
Aug 11 14:27:21.269: INFO: Created: latency-svc-l7924
Aug 11 14:27:21.304: INFO: Got endpoints: latency-svc-hhjfx [645.353072ms]
Aug 11 14:27:21.325: INFO: Created: latency-svc-wmqcm
Aug 11 14:27:21.354: INFO: Got endpoints: latency-svc-7frx2 [679.744656ms]
Aug 11 14:27:21.374: INFO: Created: latency-svc-nsc4t
Aug 11 14:27:21.403: INFO: Got endpoints: latency-svc-5k5qn [720.215915ms]
Aug 11 14:27:21.418: INFO: Created: latency-svc-8pk2v
Aug 11 14:27:21.456: INFO: Got endpoints: latency-svc-62h2d [753.152527ms]
Aug 11 14:27:21.473: INFO: Created: latency-svc-vv7tm
Aug 11 14:27:21.504: INFO: Got endpoints: latency-svc-n589d [745.008369ms]
Aug 11 14:27:21.521: INFO: Created: latency-svc-smg5p
Aug 11 14:27:21.553: INFO: Got endpoints: latency-svc-6j8mf [747.105321ms]
Aug 11 14:27:21.568: INFO: Created: latency-svc-vpvks
Aug 11 14:27:21.604: INFO: Got endpoints: latency-svc-8d4dl [728.675862ms]
Aug 11 14:27:21.621: INFO: Created: latency-svc-rcbgr
Aug 11 14:27:21.659: INFO: Got endpoints: latency-svc-9swxl [753.360197ms]
Aug 11 14:27:21.674: INFO: Created: latency-svc-jv9vg
Aug 11 14:27:21.706: INFO: Got endpoints: latency-svc-5spbg [751.773015ms]
Aug 11 14:27:21.723: INFO: Created: latency-svc-vgjqj
Aug 11 14:27:21.753: INFO: Got endpoints: latency-svc-pbgf4 [744.787918ms]
Aug 11 14:27:21.789: INFO: Created: latency-svc-zbjdf
Aug 11 14:27:21.803: INFO: Got endpoints: latency-svc-86rll [750.861904ms]
Aug 11 14:27:21.821: INFO: Created: latency-svc-f76rk
Aug 11 14:27:21.853: INFO: Got endpoints: latency-svc-87rvk [748.630772ms]
Aug 11 14:27:21.880: INFO: Created: latency-svc-9z25s
Aug 11 14:27:21.903: INFO: Got endpoints: latency-svc-7db67 [748.936133ms]
Aug 11 14:27:21.918: INFO: Created: latency-svc-hrn8b
Aug 11 14:27:21.953: INFO: Got endpoints: latency-svc-j6w7k [741.307494ms]
Aug 11 14:27:21.971: INFO: Created: latency-svc-qpgbk
Aug 11 14:27:22.003: INFO: Got endpoints: latency-svc-l7924 [750.176023ms]
Aug 11 14:27:22.019: INFO: Created: latency-svc-rfcgd
Aug 11 14:27:22.053: INFO: Got endpoints: latency-svc-wmqcm [748.643242ms]
Aug 11 14:27:22.070: INFO: Created: latency-svc-hfgwh
Aug 11 14:27:22.104: INFO: Got endpoints: latency-svc-nsc4t [750.346533ms]
Aug 11 14:27:22.119: INFO: Created: latency-svc-l9d58
Aug 11 14:27:22.152: INFO: Got endpoints: latency-svc-8pk2v [749.059802ms]
Aug 11 14:27:22.170: INFO: Created: latency-svc-hmg8b
Aug 11 14:27:22.208: INFO: Got endpoints: latency-svc-vv7tm [752.271996ms]
Aug 11 14:27:22.224: INFO: Created: latency-svc-2ftzj
Aug 11 14:27:22.253: INFO: Got endpoints: latency-svc-smg5p [748.843062ms]
Aug 11 14:27:22.270: INFO: Created: latency-svc-mmm26
Aug 11 14:27:22.304: INFO: Got endpoints: latency-svc-vpvks [750.966114ms]
Aug 11 14:27:22.327: INFO: Created: latency-svc-hbtx5
Aug 11 14:27:22.353: INFO: Got endpoints: latency-svc-rcbgr [749.061292ms]
Aug 11 14:27:22.368: INFO: Created: latency-svc-jvbrj
Aug 11 14:27:22.404: INFO: Got endpoints: latency-svc-jv9vg [745.015119ms]
Aug 11 14:27:22.427: INFO: Created: latency-svc-x8g5t
Aug 11 14:27:22.454: INFO: Got endpoints: latency-svc-vgjqj [748.146602ms]
Aug 11 14:27:22.470: INFO: Created: latency-svc-fgtjl
Aug 11 14:27:22.504: INFO: Got endpoints: latency-svc-zbjdf [750.960135ms]
Aug 11 14:27:22.521: INFO: Created: latency-svc-gpxs9
Aug 11 14:27:22.553: INFO: Got endpoints: latency-svc-f76rk [750.173103ms]
Aug 11 14:27:22.569: INFO: Created: latency-svc-rqrnc
Aug 11 14:27:22.603: INFO: Got endpoints: latency-svc-9z25s [750.056354ms]
Aug 11 14:27:22.619: INFO: Created: latency-svc-r4q5f
Aug 11 14:27:22.653: INFO: Got endpoints: latency-svc-hrn8b [749.967673ms]
Aug 11 14:27:22.668: INFO: Created: latency-svc-br4tp
Aug 11 14:27:22.705: INFO: Got endpoints: latency-svc-qpgbk [752.121625ms]
Aug 11 14:27:22.722: INFO: Created: latency-svc-sqt4x
Aug 11 14:27:22.754: INFO: Got endpoints: latency-svc-rfcgd [751.636565ms]
Aug 11 14:27:22.771: INFO: Created: latency-svc-7w7cm
Aug 11 14:27:22.803: INFO: Got endpoints: latency-svc-hfgwh [750.922225ms]
Aug 11 14:27:22.819: INFO: Created: latency-svc-95dfx
Aug 11 14:27:22.860: INFO: Got endpoints: latency-svc-l9d58 [756.034971ms]
Aug 11 14:27:22.876: INFO: Created: latency-svc-8nxcg
Aug 11 14:27:22.902: INFO: Got endpoints: latency-svc-hmg8b [750.185804ms]
Aug 11 14:27:22.919: INFO: Created: latency-svc-cq6vc
Aug 11 14:27:22.952: INFO: Got endpoints: latency-svc-2ftzj [743.461448ms]
Aug 11 14:27:22.975: INFO: Created: latency-svc-q7scm
Aug 11 14:27:23.003: INFO: Got endpoints: latency-svc-mmm26 [749.686613ms]
Aug 11 14:27:23.018: INFO: Created: latency-svc-kq6pw
Aug 11 14:27:23.053: INFO: Got endpoints: latency-svc-hbtx5 [749.257122ms]
Aug 11 14:27:23.077: INFO: Created: latency-svc-lfxqj
Aug 11 14:27:23.104: INFO: Got endpoints: latency-svc-jvbrj [750.703474ms]
Aug 11 14:27:23.120: INFO: Created: latency-svc-569tz
Aug 11 14:27:23.153: INFO: Got endpoints: latency-svc-x8g5t [748.985682ms]
Aug 11 14:27:23.169: INFO: Created: latency-svc-rch9q
Aug 11 14:27:23.205: INFO: Got endpoints: latency-svc-fgtjl [750.352062ms]
Aug 11 14:27:23.220: INFO: Created: latency-svc-kvjtl
Aug 11 14:27:23.254: INFO: Got endpoints: latency-svc-gpxs9 [749.977933ms]
Aug 11 14:27:23.269: INFO: Created: latency-svc-h8lpj
Aug 11 14:27:23.303: INFO: Got endpoints: latency-svc-rqrnc [750.119633ms]
Aug 11 14:27:23.319: INFO: Created: latency-svc-cxp86
Aug 11 14:27:23.354: INFO: Got endpoints: latency-svc-r4q5f [750.438124ms]
Aug 11 14:27:23.370: INFO: Created: latency-svc-vb9gc
Aug 11 14:27:23.405: INFO: Got endpoints: latency-svc-br4tp [751.894936ms]
Aug 11 14:27:23.421: INFO: Created: latency-svc-g4d4n
Aug 11 14:27:23.455: INFO: Got endpoints: latency-svc-sqt4x [749.321234ms]
Aug 11 14:27:23.472: INFO: Created: latency-svc-bdqfr
Aug 11 14:27:23.514: INFO: Got endpoints: latency-svc-7w7cm [759.555533ms]
Aug 11 14:27:23.529: INFO: Created: latency-svc-8g92f
Aug 11 14:27:23.553: INFO: Got endpoints: latency-svc-95dfx [749.541382ms]
Aug 11 14:27:23.568: INFO: Created: latency-svc-hdprs
Aug 11 14:27:23.605: INFO: Got endpoints: latency-svc-8nxcg [744.767448ms]
Aug 11 14:27:23.628: INFO: Created: latency-svc-sk9bn
Aug 11 14:27:23.654: INFO: Got endpoints: latency-svc-cq6vc [751.987254ms]
Aug 11 14:27:23.670: INFO: Created: latency-svc-bznsm
Aug 11 14:27:23.703: INFO: Got endpoints: latency-svc-q7scm [751.554765ms]
Aug 11 14:27:23.720: INFO: Created: latency-svc-rptng
Aug 11 14:27:23.755: INFO: Got endpoints: latency-svc-kq6pw [752.199566ms]
Aug 11 14:27:23.772: INFO: Created: latency-svc-5zjdn
Aug 11 14:27:23.805: INFO: Got endpoints: latency-svc-lfxqj [751.779476ms]
Aug 11 14:27:23.828: INFO: Created: latency-svc-t65b8
Aug 11 14:27:23.854: INFO: Got endpoints: latency-svc-569tz [750.363203ms]
Aug 11 14:27:23.871: INFO: Created: latency-svc-9bf5n
Aug 11 14:27:23.904: INFO: Got endpoints: latency-svc-rch9q [750.416983ms]
Aug 11 14:27:23.920: INFO: Created: latency-svc-9hjjf
Aug 11 14:27:23.956: INFO: Got endpoints: latency-svc-kvjtl [751.020287ms]
Aug 11 14:27:23.972: INFO: Created: latency-svc-r58hz
Aug 11 14:27:24.004: INFO: Got endpoints: latency-svc-h8lpj [749.901183ms]
Aug 11 14:27:24.023: INFO: Created: latency-svc-s8hnm
Aug 11 14:27:24.053: INFO: Got endpoints: latency-svc-cxp86 [750.009714ms]
Aug 11 14:27:24.077: INFO: Created: latency-svc-n6r7g
Aug 11 14:27:24.104: INFO: Got endpoints: latency-svc-vb9gc [750.223573ms]
Aug 11 14:27:24.120: INFO: Created: latency-svc-njwtn
Aug 11 14:27:24.152: INFO: Got endpoints: latency-svc-g4d4n [746.712451ms]
Aug 11 14:27:24.179: INFO: Created: latency-svc-cv9dm
Aug 11 14:27:24.205: INFO: Got endpoints: latency-svc-bdqfr [750.086914ms]
Aug 11 14:27:24.223: INFO: Created: latency-svc-htkqj
Aug 11 14:27:24.254: INFO: Got endpoints: latency-svc-8g92f [740.206734ms]
Aug 11 14:27:24.272: INFO: Created: latency-svc-n4dcj
Aug 11 14:27:24.304: INFO: Got endpoints: latency-svc-hdprs [750.404123ms]
Aug 11 14:27:24.321: INFO: Created: latency-svc-954bj
Aug 11 14:27:24.355: INFO: Got endpoints: latency-svc-sk9bn [749.656603ms]
Aug 11 14:27:24.371: INFO: Created: latency-svc-h48lr
Aug 11 14:27:24.402: INFO: Got endpoints: latency-svc-bznsm [747.627199ms]
Aug 11 14:27:24.419: INFO: Created: latency-svc-4fjv5
Aug 11 14:27:24.455: INFO: Got endpoints: latency-svc-rptng [751.872635ms]
Aug 11 14:27:24.471: INFO: Created: latency-svc-9mkxj
Aug 11 14:27:24.507: INFO: Got endpoints: latency-svc-5zjdn [751.986485ms]
Aug 11 14:27:24.531: INFO: Created: latency-svc-njb9m
Aug 11 14:27:24.556: INFO: Got endpoints: latency-svc-t65b8 [750.460653ms]
Aug 11 14:27:24.572: INFO: Created: latency-svc-x9mx4
Aug 11 14:27:24.604: INFO: Got endpoints: latency-svc-9bf5n [749.942464ms]
Aug 11 14:27:24.626: INFO: Created: latency-svc-cw78j
Aug 11 14:27:24.654: INFO: Got endpoints: latency-svc-9hjjf [750.886462ms]
Aug 11 14:27:24.670: INFO: Created: latency-svc-q5t2g
Aug 11 14:27:24.705: INFO: Got endpoints: latency-svc-r58hz [748.930851ms]
Aug 11 14:27:24.728: INFO: Created: latency-svc-5dxhs
Aug 11 14:27:24.752: INFO: Got endpoints: latency-svc-s8hnm [747.9208ms]
Aug 11 14:27:24.767: INFO: Created: latency-svc-4cbn8
Aug 11 14:27:24.806: INFO: Got endpoints: latency-svc-n6r7g [752.494436ms]
Aug 11 14:27:24.836: INFO: Created: latency-svc-kbw5r
Aug 11 14:27:24.855: INFO: Got endpoints: latency-svc-njwtn [750.379683ms]
Aug 11 14:27:24.872: INFO: Created: latency-svc-9mnbx
Aug 11 14:27:24.902: INFO: Got endpoints: latency-svc-cv9dm [749.837993ms]
Aug 11 14:27:24.918: INFO: Created: latency-svc-fdphv
Aug 11 14:27:24.955: INFO: Got endpoints: latency-svc-htkqj [749.646684ms]
Aug 11 14:27:24.972: INFO: Created: latency-svc-4w2zn
Aug 11 14:27:25.002: INFO: Got endpoints: latency-svc-n4dcj [748.247241ms]
Aug 11 14:27:25.020: INFO: Created: latency-svc-rwfb2
Aug 11 14:27:25.058: INFO: Got endpoints: latency-svc-954bj [754.53652ms]
Aug 11 14:27:25.079: INFO: Created: latency-svc-5dxth
Aug 11 14:27:25.104: INFO: Got endpoints: latency-svc-h48lr [749.532013ms]
Aug 11 14:27:25.120: INFO: Created: latency-svc-4454z
Aug 11 14:27:25.153: INFO: Got endpoints: latency-svc-4fjv5 [751.152366ms]
Aug 11 14:27:25.178: INFO: Created: latency-svc-6dhhl
Aug 11 14:27:25.203: INFO: Got endpoints: latency-svc-9mkxj [747.661721ms]
Aug 11 14:27:25.218: INFO: Created: latency-svc-ddjbc
Aug 11 14:27:25.253: INFO: Got endpoints: latency-svc-njb9m [745.714189ms]
Aug 11 14:27:25.279: INFO: Created: latency-svc-h5gr9
Aug 11 14:27:25.303: INFO: Got endpoints: latency-svc-x9mx4 [747.644322ms]
Aug 11 14:27:25.321: INFO: Created: latency-svc-fb5rn
Aug 11 14:27:25.353: INFO: Got endpoints: latency-svc-cw78j [749.257203ms]
Aug 11 14:27:25.370: INFO: Created: latency-svc-kt7zk
Aug 11 14:27:25.404: INFO: Got endpoints: latency-svc-q5t2g [749.880076ms]
Aug 11 14:27:25.420: INFO: Created: latency-svc-lqhrg
Aug 11 14:27:25.453: INFO: Got endpoints: latency-svc-5dxhs [748.060852ms]
Aug 11 14:27:25.469: INFO: Created: latency-svc-kf2rw
Aug 11 14:27:25.503: INFO: Got endpoints: latency-svc-4cbn8 [750.733535ms]
Aug 11 14:27:25.519: INFO: Created: latency-svc-xrhxb
Aug 11 14:27:25.555: INFO: Got endpoints: latency-svc-kbw5r [749.308353ms]
Aug 11 14:27:25.572: INFO: Created: latency-svc-gg4fd
Aug 11 14:27:25.608: INFO: Got endpoints: latency-svc-9mnbx [753.113767ms]
Aug 11 14:27:25.623: INFO: Created: latency-svc-5cm2z
Aug 11 14:27:25.654: INFO: Got endpoints: latency-svc-fdphv [751.827364ms]
Aug 11 14:27:25.670: INFO: Created: latency-svc-9rdqx
Aug 11 14:27:25.703: INFO: Got endpoints: latency-svc-4w2zn [748.534391ms]
Aug 11 14:27:25.724: INFO: Created: latency-svc-5t2l8
Aug 11 14:27:25.753: INFO: Got endpoints: latency-svc-rwfb2 [750.813224ms]
Aug 11 14:27:25.769: INFO: Created: latency-svc-sp9hw
Aug 11 14:27:25.804: INFO: Got endpoints: latency-svc-5dxth [746.026409ms]
Aug 11 14:27:25.831: INFO: Created: latency-svc-mb2dg
Aug 11 14:27:25.855: INFO: Got endpoints: latency-svc-4454z [750.905394ms]
Aug 11 14:27:25.871: INFO: Created: latency-svc-nfmpg
Aug 11 14:27:25.905: INFO: Got endpoints: latency-svc-6dhhl [751.410484ms]
Aug 11 14:27:25.920: INFO: Created: latency-svc-4k7s4
Aug 11 14:27:25.952: INFO: Got endpoints: latency-svc-ddjbc [749.613663ms]
Aug 11 14:27:25.970: INFO: Created: latency-svc-74lvb
Aug 11 14:27:26.006: INFO: Got endpoints: latency-svc-h5gr9 [752.562526ms]
Aug 11 14:27:26.023: INFO: Created: latency-svc-hhfbh
Aug 11 14:27:26.054: INFO: Got endpoints: latency-svc-fb5rn [750.157473ms]
Aug 11 14:27:26.069: INFO: Created: latency-svc-wb8vl
Aug 11 14:27:26.104: INFO: Got endpoints: latency-svc-kt7zk [750.879173ms]
Aug 11 14:27:26.121: INFO: Created: latency-svc-64pc6
Aug 11 14:27:26.161: INFO: Got endpoints: latency-svc-lqhrg [756.582669ms]
Aug 11 14:27:26.178: INFO: Created: latency-svc-7tqvc
Aug 11 14:27:26.203: INFO: Got endpoints: latency-svc-kf2rw [749.826183ms]
Aug 11 14:27:26.219: INFO: Created: latency-svc-7ppmj
Aug 11 14:27:26.253: INFO: Got endpoints: latency-svc-xrhxb [749.806422ms]
Aug 11 14:27:26.275: INFO: Created: latency-svc-k7nmw
Aug 11 14:27:26.304: INFO: Got endpoints: latency-svc-gg4fd [748.530822ms]
Aug 11 14:27:26.320: INFO: Created: latency-svc-svg2t
Aug 11 14:27:26.354: INFO: Got endpoints: latency-svc-5cm2z [746.58417ms]
Aug 11 14:27:26.381: INFO: Created: latency-svc-2mkgh
Aug 11 14:27:26.405: INFO: Got endpoints: latency-svc-9rdqx [751.221966ms]
Aug 11 14:27:26.428: INFO: Created: latency-svc-grczz
Aug 11 14:27:26.453: INFO: Got endpoints: latency-svc-5t2l8 [750.144583ms]
Aug 11 14:27:26.468: INFO: Created: latency-svc-s7zjf
Aug 11 14:27:26.503: INFO: Got endpoints: latency-svc-sp9hw [749.534753ms]
Aug 11 14:27:26.520: INFO: Created: latency-svc-6v8hx
Aug 11 14:27:26.554: INFO: Got endpoints: latency-svc-mb2dg [750.116534ms]
Aug 11 14:27:26.571: INFO: Created: latency-svc-rh8hk
Aug 11 14:27:26.604: INFO: Got endpoints: latency-svc-nfmpg [749.042752ms]
Aug 11 14:27:26.619: INFO: Created: latency-svc-wmdl2
Aug 11 14:27:26.651: INFO: Got endpoints: latency-svc-4k7s4 [746.695191ms]
Aug 11 14:27:26.671: INFO: Created: latency-svc-9vsbd
Aug 11 14:27:26.710: INFO: Got endpoints: latency-svc-74lvb [758.076672ms]
Aug 11 14:27:26.726: INFO: Created: latency-svc-9hm5d
Aug 11 14:27:26.753: INFO: Got endpoints: latency-svc-hhfbh [747.512341ms]
Aug 11 14:27:26.769: INFO: Created: latency-svc-b9gbf
Aug 11 14:27:26.802: INFO: Got endpoints: latency-svc-wb8vl [748.920453ms]
Aug 11 14:27:26.825: INFO: Created: latency-svc-pnrkk
Aug 11 14:27:26.856: INFO: Got endpoints: latency-svc-64pc6 [751.144496ms]
Aug 11 14:27:26.872: INFO: Created: latency-svc-gzvnf
Aug 11 14:27:26.905: INFO: Got endpoints: latency-svc-7tqvc [743.945468ms]
Aug 11 14:27:26.933: INFO: Created: latency-svc-74lnr
Aug 11 14:27:26.954: INFO: Got endpoints: latency-svc-7ppmj [751.513036ms]
Aug 11 14:27:26.970: INFO: Created: latency-svc-g8r8l
Aug 11 14:27:27.008: INFO: Got endpoints: latency-svc-k7nmw [754.82113ms]
Aug 11 14:27:27.026: INFO: Created: latency-svc-7jc8b
Aug 11 14:27:27.054: INFO: Got endpoints: latency-svc-svg2t [749.671234ms]
Aug 11 14:27:27.071: INFO: Created: latency-svc-ljrsz
Aug 11 14:27:27.107: INFO: Got endpoints: latency-svc-2mkgh [752.434867ms]
Aug 11 14:27:27.129: INFO: Created: latency-svc-nr4d4
Aug 11 14:27:27.153: INFO: Got endpoints: latency-svc-grczz [748.125202ms]
Aug 11 14:27:27.169: INFO: Created: latency-svc-5fnpt
Aug 11 14:27:27.205: INFO: Got endpoints: latency-svc-s7zjf [751.824435ms]
Aug 11 14:27:27.221: INFO: Created: latency-svc-n4xmj
Aug 11 14:27:27.263: INFO: Got endpoints: latency-svc-6v8hx [759.965415ms]
Aug 11 14:27:27.289: INFO: Created: latency-svc-g49rl
Aug 11 14:27:27.304: INFO: Got endpoints: latency-svc-rh8hk [749.209535ms]
Aug 11 14:27:27.320: INFO: Created: latency-svc-tbs6p
Aug 11 14:27:27.354: INFO: Got endpoints: latency-svc-wmdl2 [749.506385ms]
Aug 11 14:27:27.377: INFO: Created: latency-svc-c9lgs
Aug 11 14:27:27.405: INFO: Got endpoints: latency-svc-9vsbd [753.453197ms]
Aug 11 14:27:27.419: INFO: Created: latency-svc-8nm72
Aug 11 14:27:27.452: INFO: Got endpoints: latency-svc-9hm5d [741.394945ms]
Aug 11 14:27:27.468: INFO: Created: latency-svc-8rsxm
Aug 11 14:27:27.502: INFO: Got endpoints: latency-svc-b9gbf [748.897493ms]
Aug 11 14:27:27.518: INFO: Created: latency-svc-pn8w8
Aug 11 14:27:27.553: INFO: Got endpoints: latency-svc-pnrkk [750.224814ms]
Aug 11 14:27:27.569: INFO: Created: latency-svc-gffmv
Aug 11 14:27:27.603: INFO: Got endpoints: latency-svc-gzvnf [747.50948ms]
Aug 11 14:27:27.629: INFO: Created: latency-svc-zv6x6
Aug 11 14:27:27.655: INFO: Got endpoints: latency-svc-74lnr [749.850104ms]
Aug 11 14:27:27.673: INFO: Created: latency-svc-bdljs
Aug 11 14:27:27.705: INFO: Got endpoints: latency-svc-g8r8l [750.831305ms]
Aug 11 14:27:27.722: INFO: Created: latency-svc-bb4fj
Aug 11 14:27:27.753: INFO: Got endpoints: latency-svc-7jc8b [745.299219ms]
Aug 11 14:27:27.771: INFO: Created: latency-svc-2sfln
Aug 11 14:27:27.803: INFO: Got endpoints: latency-svc-ljrsz [749.761763ms]
Aug 11 14:27:27.853: INFO: Got endpoints: latency-svc-nr4d4 [746.23806ms]
Aug 11 14:27:27.905: INFO: Got endpoints: latency-svc-5fnpt [751.527363ms]
Aug 11 14:27:27.954: INFO: Got endpoints: latency-svc-n4xmj [748.435333ms]
Aug 11 14:27:28.005: INFO: Got endpoints: latency-svc-g49rl [742.515756ms]
Aug 11 14:27:28.053: INFO: Got endpoints: latency-svc-tbs6p [749.437953ms]
Aug 11 14:27:28.105: INFO: Got endpoints: latency-svc-c9lgs [750.966064ms]
Aug 11 14:27:28.153: INFO: Got endpoints: latency-svc-8nm72 [748.124692ms]
Aug 11 14:27:28.203: INFO: Got endpoints: latency-svc-8rsxm [751.247094ms]
Aug 11 14:27:28.253: INFO: Got endpoints: latency-svc-pn8w8 [750.665145ms]
Aug 11 14:27:28.304: INFO: Got endpoints: latency-svc-gffmv [750.728315ms]
Aug 11 14:27:28.356: INFO: Got endpoints: latency-svc-zv6x6 [752.359627ms]
Aug 11 14:27:28.403: INFO: Got endpoints: latency-svc-bdljs [748.169622ms]
Aug 11 14:27:28.455: INFO: Got endpoints: latency-svc-bb4fj [750.005044ms]
Aug 11 14:27:28.507: INFO: Got endpoints: latency-svc-2sfln [753.967468ms]
Aug 11 14:27:28.507: INFO: Latencies: [28.467128ms 41.402421ms 52.177421ms 72.03487ms 92.0388ms 110.430278ms 132.274429ms 140.605327ms 162.294298ms 179.013815ms 194.562331ms 227.938233ms 229.425205ms 231.933317ms 233.632409ms 234.143229ms 236.568281ms 236.631962ms 239.658304ms 241.099906ms 243.699988ms 245.23167ms 245.63295ms 249.197634ms 250.176584ms 251.308305ms 252.177817ms 253.439338ms 254.076038ms 254.802179ms 255.837801ms 257.554983ms 257.877392ms 258.719772ms 260.850055ms 261.103065ms 264.820909ms 267.711872ms 268.354203ms 269.875744ms 272.952297ms 279.613103ms 280.707884ms 284.664529ms 284.902978ms 310.525945ms 368.995182ms 372.010424ms 395.489958ms 447.241948ms 475.484086ms 513.686714ms 545.025833ms 590.343667ms 621.013819ms 645.353072ms 679.744656ms 720.215915ms 728.675862ms 740.206734ms 741.307494ms 741.394945ms 742.515756ms 743.461448ms 743.945468ms 744.767448ms 744.787918ms 745.008369ms 745.015119ms 745.299219ms 745.714189ms 746.026409ms 746.23806ms 746.58417ms 746.695191ms 746.712451ms 747.105321ms 747.50948ms 747.512341ms 747.627199ms 747.644322ms 747.661721ms 747.9208ms 748.060852ms 748.124692ms 748.125202ms 748.146602ms 748.169622ms 748.247241ms 748.435333ms 748.530822ms 748.534391ms 748.630772ms 748.643242ms 748.843062ms 748.897493ms 748.920453ms 748.930851ms 748.936133ms 748.985682ms 749.042752ms 749.059802ms 749.061292ms 749.209535ms 749.257122ms 749.257203ms 749.308353ms 749.321234ms 749.437953ms 749.506385ms 749.532013ms 749.534753ms 749.541382ms 749.613663ms 749.646684ms 749.656603ms 749.671234ms 749.686613ms 749.761763ms 749.806422ms 749.826183ms 749.837993ms 749.850104ms 749.880076ms 749.901183ms 749.942464ms 749.967673ms 749.977933ms 750.005044ms 750.009714ms 750.056354ms 750.086914ms 750.116534ms 750.119633ms 750.144583ms 750.157473ms 750.173103ms 750.176023ms 750.185804ms 750.223573ms 750.224814ms 750.346533ms 750.352062ms 750.363203ms 750.379683ms 750.404123ms 750.416983ms 750.438124ms 750.460653ms 750.665145ms 750.703474ms 750.728315ms 750.733535ms 750.813224ms 750.831305ms 750.861904ms 750.879173ms 750.886462ms 750.905394ms 750.922225ms 750.960135ms 750.966064ms 750.966114ms 751.020287ms 751.144496ms 751.152366ms 751.221966ms 751.247094ms 751.410484ms 751.513036ms 751.527363ms 751.554765ms 751.636565ms 751.773015ms 751.779476ms 751.824435ms 751.827364ms 751.872635ms 751.894936ms 751.986485ms 751.987254ms 752.121625ms 752.199566ms 752.271996ms 752.359627ms 752.434867ms 752.494436ms 752.562526ms 753.113767ms 753.152527ms 753.360197ms 753.453197ms 753.967468ms 754.53652ms 754.82113ms 756.034971ms 756.582669ms 758.076672ms 759.555533ms 759.965415ms]
Aug 11 14:27:28.507: INFO: 50 %ile: 749.042752ms
Aug 11 14:27:28.507: INFO: 90 %ile: 751.987254ms
Aug 11 14:27:28.507: INFO: 99 %ile: 759.555533ms
Aug 11 14:27:28.507: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:28.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-1387" for this suite. 08/11/23 14:27:28.515
------------------------------
• [SLOW TEST] [10.774 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:17.747
    Aug 11 14:27:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svc-latency 08/11/23 14:27:17.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:17.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:17.767
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 11 14:27:17.770: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1387 08/11/23 14:27:17.77
    I0811 14:27:17.777905      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1387, replica count: 1
    I0811 14:27:18.829109      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0811 14:27:19.829488      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:27:19.947: INFO: Created: latency-svc-gqtn9
    Aug 11 14:27:19.954: INFO: Got endpoints: latency-svc-gqtn9 [24.291455ms]
    Aug 11 14:27:19.973: INFO: Created: latency-svc-9qfhq
    Aug 11 14:27:19.983: INFO: Got endpoints: latency-svc-9qfhq [28.467128ms]
    Aug 11 14:27:19.989: INFO: Created: latency-svc-trvkr
    Aug 11 14:27:19.996: INFO: Got endpoints: latency-svc-trvkr [41.402421ms]
    Aug 11 14:27:20.000: INFO: Created: latency-svc-jfqb4
    Aug 11 14:27:20.006: INFO: Got endpoints: latency-svc-jfqb4 [52.177421ms]
    Aug 11 14:27:20.019: INFO: Created: latency-svc-cwlv8
    Aug 11 14:27:20.027: INFO: Got endpoints: latency-svc-cwlv8 [72.03487ms]
    Aug 11 14:27:20.037: INFO: Created: latency-svc-z8x2b
    Aug 11 14:27:20.047: INFO: Got endpoints: latency-svc-z8x2b [92.0388ms]
    Aug 11 14:27:20.065: INFO: Created: latency-svc-l9d7v
    Aug 11 14:27:20.065: INFO: Got endpoints: latency-svc-l9d7v [110.430278ms]
    Aug 11 14:27:20.077: INFO: Created: latency-svc-j9ldr
    Aug 11 14:27:20.087: INFO: Got endpoints: latency-svc-j9ldr [132.274429ms]
    Aug 11 14:27:20.092: INFO: Created: latency-svc-ct25m
    Aug 11 14:27:20.095: INFO: Got endpoints: latency-svc-ct25m [140.605327ms]
    Aug 11 14:27:20.111: INFO: Created: latency-svc-fgzrv
    Aug 11 14:27:20.117: INFO: Got endpoints: latency-svc-fgzrv [162.294298ms]
    Aug 11 14:27:20.127: INFO: Created: latency-svc-925jn
    Aug 11 14:27:20.134: INFO: Got endpoints: latency-svc-925jn [179.013815ms]
    Aug 11 14:27:20.145: INFO: Created: latency-svc-dpxkf
    Aug 11 14:27:20.149: INFO: Got endpoints: latency-svc-dpxkf [194.562331ms]
    Aug 11 14:27:20.159: INFO: Created: latency-svc-zww8k
    Aug 11 14:27:20.183: INFO: Got endpoints: latency-svc-zww8k [227.938233ms]
    Aug 11 14:27:20.189: INFO: Created: latency-svc-47wvn
    Aug 11 14:27:20.191: INFO: Got endpoints: latency-svc-47wvn [236.631962ms]
    Aug 11 14:27:20.201: INFO: Created: latency-svc-vkfnp
    Aug 11 14:27:20.214: INFO: Got endpoints: latency-svc-vkfnp [258.719772ms]
    Aug 11 14:27:20.216: INFO: Created: latency-svc-gvkns
    Aug 11 14:27:20.222: INFO: Got endpoints: latency-svc-gvkns [267.711872ms]
    Aug 11 14:27:20.234: INFO: Created: latency-svc-v94km
    Aug 11 14:27:20.238: INFO: Got endpoints: latency-svc-v94km [254.802179ms]
    Aug 11 14:27:20.248: INFO: Created: latency-svc-k26wk
    Aug 11 14:27:20.257: INFO: Got endpoints: latency-svc-k26wk [261.103065ms]
    Aug 11 14:27:20.270: INFO: Created: latency-svc-nnqkb
    Aug 11 14:27:20.279: INFO: Got endpoints: latency-svc-nnqkb [272.952297ms]
    Aug 11 14:27:20.307: INFO: Created: latency-svc-64n7w
    Aug 11 14:27:20.307: INFO: Got endpoints: latency-svc-64n7w [280.707884ms]
    Aug 11 14:27:20.320: INFO: Created: latency-svc-jjjdn
    Aug 11 14:27:20.332: INFO: Got endpoints: latency-svc-jjjdn [284.902978ms]
    Aug 11 14:27:20.336: INFO: Created: latency-svc-4vb96
    Aug 11 14:27:20.345: INFO: Got endpoints: latency-svc-4vb96 [279.613103ms]
    Aug 11 14:27:20.349: INFO: Created: latency-svc-x9l9b
    Aug 11 14:27:20.357: INFO: Got endpoints: latency-svc-x9l9b [269.875744ms]
    Aug 11 14:27:20.361: INFO: Created: latency-svc-gjwgx
    Aug 11 14:27:20.364: INFO: Got endpoints: latency-svc-gjwgx [268.354203ms]
    Aug 11 14:27:20.373: INFO: Created: latency-svc-98q5t
    Aug 11 14:27:20.382: INFO: Got endpoints: latency-svc-98q5t [264.820909ms]
    Aug 11 14:27:20.387: INFO: Created: latency-svc-8crg6
    Aug 11 14:27:20.395: INFO: Got endpoints: latency-svc-8crg6 [260.850055ms]
    Aug 11 14:27:20.400: INFO: Created: latency-svc-4qss7
    Aug 11 14:27:20.405: INFO: Got endpoints: latency-svc-4qss7 [255.837801ms]
    Aug 11 14:27:20.424: INFO: Created: latency-svc-pjngh
    Aug 11 14:27:20.433: INFO: Got endpoints: latency-svc-pjngh [250.176584ms]
    Aug 11 14:27:20.438: INFO: Created: latency-svc-ph2vr
    Aug 11 14:27:20.449: INFO: Got endpoints: latency-svc-ph2vr [257.877392ms]
    Aug 11 14:27:20.453: INFO: Created: latency-svc-ttdpc
    Aug 11 14:27:20.459: INFO: Got endpoints: latency-svc-ttdpc [245.23167ms]
    Aug 11 14:27:20.468: INFO: Created: latency-svc-77zm5
    Aug 11 14:27:20.475: INFO: Got endpoints: latency-svc-77zm5 [252.177817ms]
    Aug 11 14:27:20.485: INFO: Created: latency-svc-w687x
    Aug 11 14:27:20.495: INFO: Got endpoints: latency-svc-w687x [257.554983ms]
    Aug 11 14:27:20.501: INFO: Created: latency-svc-9bd54
    Aug 11 14:27:20.506: INFO: Got endpoints: latency-svc-9bd54 [249.197634ms]
    Aug 11 14:27:20.534: INFO: Created: latency-svc-f8kdq
    Aug 11 14:27:20.534: INFO: Got endpoints: latency-svc-f8kdq [254.076038ms]
    Aug 11 14:27:20.543: INFO: Created: latency-svc-prqjh
    Aug 11 14:27:20.557: INFO: Created: latency-svc-5rhxl
    Aug 11 14:27:20.559: INFO: Got endpoints: latency-svc-prqjh [251.308305ms]
    Aug 11 14:27:20.561: INFO: Got endpoints: latency-svc-5rhxl [229.425205ms]
    Aug 11 14:27:20.570: INFO: Created: latency-svc-t49d9
    Aug 11 14:27:20.577: INFO: Got endpoints: latency-svc-t49d9 [231.933317ms]
    Aug 11 14:27:20.587: INFO: Created: latency-svc-5tssz
    Aug 11 14:27:20.591: INFO: Got endpoints: latency-svc-5tssz [234.143229ms]
    Aug 11 14:27:20.600: INFO: Created: latency-svc-r8cpr
    Aug 11 14:27:20.609: INFO: Got endpoints: latency-svc-r8cpr [245.63295ms]
    Aug 11 14:27:20.613: INFO: Created: latency-svc-2rv8c
    Aug 11 14:27:20.622: INFO: Got endpoints: latency-svc-2rv8c [239.658304ms]
    Aug 11 14:27:20.627: INFO: Created: latency-svc-xf94g
    Aug 11 14:27:20.631: INFO: Got endpoints: latency-svc-xf94g [236.568281ms]
    Aug 11 14:27:20.650: INFO: Created: latency-svc-9jbk2
    Aug 11 14:27:20.658: INFO: Got endpoints: latency-svc-9jbk2 [253.439338ms]
    Aug 11 14:27:20.664: INFO: Created: latency-svc-ns9rt
    Aug 11 14:27:20.674: INFO: Got endpoints: latency-svc-ns9rt [241.099906ms]
    Aug 11 14:27:20.679: INFO: Created: latency-svc-pp2rp
    Aug 11 14:27:20.683: INFO: Got endpoints: latency-svc-pp2rp [233.632409ms]
    Aug 11 14:27:20.692: INFO: Created: latency-svc-gb2qm
    Aug 11 14:27:20.703: INFO: Got endpoints: latency-svc-gb2qm [243.699988ms]
    Aug 11 14:27:20.706: INFO: Created: latency-svc-mwrds
    Aug 11 14:27:20.716: INFO: Created: latency-svc-9lmbb
    Aug 11 14:27:20.728: INFO: Created: latency-svc-szgsc
    Aug 11 14:27:20.739: INFO: Created: latency-svc-2qccz
    Aug 11 14:27:20.759: INFO: Got endpoints: latency-svc-mwrds [284.664529ms]
    Aug 11 14:27:20.762: INFO: Created: latency-svc-tv2zv
    Aug 11 14:27:20.773: INFO: Created: latency-svc-jm7v5
    Aug 11 14:27:20.784: INFO: Created: latency-svc-smn6q
    Aug 11 14:27:20.796: INFO: Created: latency-svc-px7fm
    Aug 11 14:27:20.806: INFO: Got endpoints: latency-svc-9lmbb [310.525945ms]
    Aug 11 14:27:20.810: INFO: Created: latency-svc-8lvmg
    Aug 11 14:27:20.824: INFO: Created: latency-svc-wcj7h
    Aug 11 14:27:20.834: INFO: Created: latency-svc-np699
    Aug 11 14:27:20.846: INFO: Created: latency-svc-hhjfx
    Aug 11 14:27:20.875: INFO: Got endpoints: latency-svc-szgsc [368.995182ms]
    Aug 11 14:27:20.879: INFO: Created: latency-svc-7frx2
    Aug 11 14:27:20.895: INFO: Created: latency-svc-5k5qn
    Aug 11 14:27:20.906: INFO: Got endpoints: latency-svc-2qccz [372.010424ms]
    Aug 11 14:27:20.908: INFO: Created: latency-svc-62h2d
    Aug 11 14:27:20.920: INFO: Created: latency-svc-n589d
    Aug 11 14:27:20.932: INFO: Created: latency-svc-6j8mf
    Aug 11 14:27:20.944: INFO: Created: latency-svc-8d4dl
    Aug 11 14:27:20.954: INFO: Got endpoints: latency-svc-tv2zv [395.489958ms]
    Aug 11 14:27:20.957: INFO: Created: latency-svc-9swxl
    Aug 11 14:27:20.972: INFO: Created: latency-svc-5spbg
    Aug 11 14:27:21.008: INFO: Got endpoints: latency-svc-jm7v5 [447.241948ms]
    Aug 11 14:27:21.028: INFO: Created: latency-svc-pbgf4
    Aug 11 14:27:21.052: INFO: Got endpoints: latency-svc-smn6q [475.484086ms]
    Aug 11 14:27:21.069: INFO: Created: latency-svc-86rll
    Aug 11 14:27:21.105: INFO: Got endpoints: latency-svc-px7fm [513.686714ms]
    Aug 11 14:27:21.121: INFO: Created: latency-svc-87rvk
    Aug 11 14:27:21.155: INFO: Got endpoints: latency-svc-8lvmg [545.025833ms]
    Aug 11 14:27:21.171: INFO: Created: latency-svc-7db67
    Aug 11 14:27:21.212: INFO: Got endpoints: latency-svc-wcj7h [590.343667ms]
    Aug 11 14:27:21.231: INFO: Created: latency-svc-j6w7k
    Aug 11 14:27:21.252: INFO: Got endpoints: latency-svc-np699 [621.013819ms]
    Aug 11 14:27:21.269: INFO: Created: latency-svc-l7924
    Aug 11 14:27:21.304: INFO: Got endpoints: latency-svc-hhjfx [645.353072ms]
    Aug 11 14:27:21.325: INFO: Created: latency-svc-wmqcm
    Aug 11 14:27:21.354: INFO: Got endpoints: latency-svc-7frx2 [679.744656ms]
    Aug 11 14:27:21.374: INFO: Created: latency-svc-nsc4t
    Aug 11 14:27:21.403: INFO: Got endpoints: latency-svc-5k5qn [720.215915ms]
    Aug 11 14:27:21.418: INFO: Created: latency-svc-8pk2v
    Aug 11 14:27:21.456: INFO: Got endpoints: latency-svc-62h2d [753.152527ms]
    Aug 11 14:27:21.473: INFO: Created: latency-svc-vv7tm
    Aug 11 14:27:21.504: INFO: Got endpoints: latency-svc-n589d [745.008369ms]
    Aug 11 14:27:21.521: INFO: Created: latency-svc-smg5p
    Aug 11 14:27:21.553: INFO: Got endpoints: latency-svc-6j8mf [747.105321ms]
    Aug 11 14:27:21.568: INFO: Created: latency-svc-vpvks
    Aug 11 14:27:21.604: INFO: Got endpoints: latency-svc-8d4dl [728.675862ms]
    Aug 11 14:27:21.621: INFO: Created: latency-svc-rcbgr
    Aug 11 14:27:21.659: INFO: Got endpoints: latency-svc-9swxl [753.360197ms]
    Aug 11 14:27:21.674: INFO: Created: latency-svc-jv9vg
    Aug 11 14:27:21.706: INFO: Got endpoints: latency-svc-5spbg [751.773015ms]
    Aug 11 14:27:21.723: INFO: Created: latency-svc-vgjqj
    Aug 11 14:27:21.753: INFO: Got endpoints: latency-svc-pbgf4 [744.787918ms]
    Aug 11 14:27:21.789: INFO: Created: latency-svc-zbjdf
    Aug 11 14:27:21.803: INFO: Got endpoints: latency-svc-86rll [750.861904ms]
    Aug 11 14:27:21.821: INFO: Created: latency-svc-f76rk
    Aug 11 14:27:21.853: INFO: Got endpoints: latency-svc-87rvk [748.630772ms]
    Aug 11 14:27:21.880: INFO: Created: latency-svc-9z25s
    Aug 11 14:27:21.903: INFO: Got endpoints: latency-svc-7db67 [748.936133ms]
    Aug 11 14:27:21.918: INFO: Created: latency-svc-hrn8b
    Aug 11 14:27:21.953: INFO: Got endpoints: latency-svc-j6w7k [741.307494ms]
    Aug 11 14:27:21.971: INFO: Created: latency-svc-qpgbk
    Aug 11 14:27:22.003: INFO: Got endpoints: latency-svc-l7924 [750.176023ms]
    Aug 11 14:27:22.019: INFO: Created: latency-svc-rfcgd
    Aug 11 14:27:22.053: INFO: Got endpoints: latency-svc-wmqcm [748.643242ms]
    Aug 11 14:27:22.070: INFO: Created: latency-svc-hfgwh
    Aug 11 14:27:22.104: INFO: Got endpoints: latency-svc-nsc4t [750.346533ms]
    Aug 11 14:27:22.119: INFO: Created: latency-svc-l9d58
    Aug 11 14:27:22.152: INFO: Got endpoints: latency-svc-8pk2v [749.059802ms]
    Aug 11 14:27:22.170: INFO: Created: latency-svc-hmg8b
    Aug 11 14:27:22.208: INFO: Got endpoints: latency-svc-vv7tm [752.271996ms]
    Aug 11 14:27:22.224: INFO: Created: latency-svc-2ftzj
    Aug 11 14:27:22.253: INFO: Got endpoints: latency-svc-smg5p [748.843062ms]
    Aug 11 14:27:22.270: INFO: Created: latency-svc-mmm26
    Aug 11 14:27:22.304: INFO: Got endpoints: latency-svc-vpvks [750.966114ms]
    Aug 11 14:27:22.327: INFO: Created: latency-svc-hbtx5
    Aug 11 14:27:22.353: INFO: Got endpoints: latency-svc-rcbgr [749.061292ms]
    Aug 11 14:27:22.368: INFO: Created: latency-svc-jvbrj
    Aug 11 14:27:22.404: INFO: Got endpoints: latency-svc-jv9vg [745.015119ms]
    Aug 11 14:27:22.427: INFO: Created: latency-svc-x8g5t
    Aug 11 14:27:22.454: INFO: Got endpoints: latency-svc-vgjqj [748.146602ms]
    Aug 11 14:27:22.470: INFO: Created: latency-svc-fgtjl
    Aug 11 14:27:22.504: INFO: Got endpoints: latency-svc-zbjdf [750.960135ms]
    Aug 11 14:27:22.521: INFO: Created: latency-svc-gpxs9
    Aug 11 14:27:22.553: INFO: Got endpoints: latency-svc-f76rk [750.173103ms]
    Aug 11 14:27:22.569: INFO: Created: latency-svc-rqrnc
    Aug 11 14:27:22.603: INFO: Got endpoints: latency-svc-9z25s [750.056354ms]
    Aug 11 14:27:22.619: INFO: Created: latency-svc-r4q5f
    Aug 11 14:27:22.653: INFO: Got endpoints: latency-svc-hrn8b [749.967673ms]
    Aug 11 14:27:22.668: INFO: Created: latency-svc-br4tp
    Aug 11 14:27:22.705: INFO: Got endpoints: latency-svc-qpgbk [752.121625ms]
    Aug 11 14:27:22.722: INFO: Created: latency-svc-sqt4x
    Aug 11 14:27:22.754: INFO: Got endpoints: latency-svc-rfcgd [751.636565ms]
    Aug 11 14:27:22.771: INFO: Created: latency-svc-7w7cm
    Aug 11 14:27:22.803: INFO: Got endpoints: latency-svc-hfgwh [750.922225ms]
    Aug 11 14:27:22.819: INFO: Created: latency-svc-95dfx
    Aug 11 14:27:22.860: INFO: Got endpoints: latency-svc-l9d58 [756.034971ms]
    Aug 11 14:27:22.876: INFO: Created: latency-svc-8nxcg
    Aug 11 14:27:22.902: INFO: Got endpoints: latency-svc-hmg8b [750.185804ms]
    Aug 11 14:27:22.919: INFO: Created: latency-svc-cq6vc
    Aug 11 14:27:22.952: INFO: Got endpoints: latency-svc-2ftzj [743.461448ms]
    Aug 11 14:27:22.975: INFO: Created: latency-svc-q7scm
    Aug 11 14:27:23.003: INFO: Got endpoints: latency-svc-mmm26 [749.686613ms]
    Aug 11 14:27:23.018: INFO: Created: latency-svc-kq6pw
    Aug 11 14:27:23.053: INFO: Got endpoints: latency-svc-hbtx5 [749.257122ms]
    Aug 11 14:27:23.077: INFO: Created: latency-svc-lfxqj
    Aug 11 14:27:23.104: INFO: Got endpoints: latency-svc-jvbrj [750.703474ms]
    Aug 11 14:27:23.120: INFO: Created: latency-svc-569tz
    Aug 11 14:27:23.153: INFO: Got endpoints: latency-svc-x8g5t [748.985682ms]
    Aug 11 14:27:23.169: INFO: Created: latency-svc-rch9q
    Aug 11 14:27:23.205: INFO: Got endpoints: latency-svc-fgtjl [750.352062ms]
    Aug 11 14:27:23.220: INFO: Created: latency-svc-kvjtl
    Aug 11 14:27:23.254: INFO: Got endpoints: latency-svc-gpxs9 [749.977933ms]
    Aug 11 14:27:23.269: INFO: Created: latency-svc-h8lpj
    Aug 11 14:27:23.303: INFO: Got endpoints: latency-svc-rqrnc [750.119633ms]
    Aug 11 14:27:23.319: INFO: Created: latency-svc-cxp86
    Aug 11 14:27:23.354: INFO: Got endpoints: latency-svc-r4q5f [750.438124ms]
    Aug 11 14:27:23.370: INFO: Created: latency-svc-vb9gc
    Aug 11 14:27:23.405: INFO: Got endpoints: latency-svc-br4tp [751.894936ms]
    Aug 11 14:27:23.421: INFO: Created: latency-svc-g4d4n
    Aug 11 14:27:23.455: INFO: Got endpoints: latency-svc-sqt4x [749.321234ms]
    Aug 11 14:27:23.472: INFO: Created: latency-svc-bdqfr
    Aug 11 14:27:23.514: INFO: Got endpoints: latency-svc-7w7cm [759.555533ms]
    Aug 11 14:27:23.529: INFO: Created: latency-svc-8g92f
    Aug 11 14:27:23.553: INFO: Got endpoints: latency-svc-95dfx [749.541382ms]
    Aug 11 14:27:23.568: INFO: Created: latency-svc-hdprs
    Aug 11 14:27:23.605: INFO: Got endpoints: latency-svc-8nxcg [744.767448ms]
    Aug 11 14:27:23.628: INFO: Created: latency-svc-sk9bn
    Aug 11 14:27:23.654: INFO: Got endpoints: latency-svc-cq6vc [751.987254ms]
    Aug 11 14:27:23.670: INFO: Created: latency-svc-bznsm
    Aug 11 14:27:23.703: INFO: Got endpoints: latency-svc-q7scm [751.554765ms]
    Aug 11 14:27:23.720: INFO: Created: latency-svc-rptng
    Aug 11 14:27:23.755: INFO: Got endpoints: latency-svc-kq6pw [752.199566ms]
    Aug 11 14:27:23.772: INFO: Created: latency-svc-5zjdn
    Aug 11 14:27:23.805: INFO: Got endpoints: latency-svc-lfxqj [751.779476ms]
    Aug 11 14:27:23.828: INFO: Created: latency-svc-t65b8
    Aug 11 14:27:23.854: INFO: Got endpoints: latency-svc-569tz [750.363203ms]
    Aug 11 14:27:23.871: INFO: Created: latency-svc-9bf5n
    Aug 11 14:27:23.904: INFO: Got endpoints: latency-svc-rch9q [750.416983ms]
    Aug 11 14:27:23.920: INFO: Created: latency-svc-9hjjf
    Aug 11 14:27:23.956: INFO: Got endpoints: latency-svc-kvjtl [751.020287ms]
    Aug 11 14:27:23.972: INFO: Created: latency-svc-r58hz
    Aug 11 14:27:24.004: INFO: Got endpoints: latency-svc-h8lpj [749.901183ms]
    Aug 11 14:27:24.023: INFO: Created: latency-svc-s8hnm
    Aug 11 14:27:24.053: INFO: Got endpoints: latency-svc-cxp86 [750.009714ms]
    Aug 11 14:27:24.077: INFO: Created: latency-svc-n6r7g
    Aug 11 14:27:24.104: INFO: Got endpoints: latency-svc-vb9gc [750.223573ms]
    Aug 11 14:27:24.120: INFO: Created: latency-svc-njwtn
    Aug 11 14:27:24.152: INFO: Got endpoints: latency-svc-g4d4n [746.712451ms]
    Aug 11 14:27:24.179: INFO: Created: latency-svc-cv9dm
    Aug 11 14:27:24.205: INFO: Got endpoints: latency-svc-bdqfr [750.086914ms]
    Aug 11 14:27:24.223: INFO: Created: latency-svc-htkqj
    Aug 11 14:27:24.254: INFO: Got endpoints: latency-svc-8g92f [740.206734ms]
    Aug 11 14:27:24.272: INFO: Created: latency-svc-n4dcj
    Aug 11 14:27:24.304: INFO: Got endpoints: latency-svc-hdprs [750.404123ms]
    Aug 11 14:27:24.321: INFO: Created: latency-svc-954bj
    Aug 11 14:27:24.355: INFO: Got endpoints: latency-svc-sk9bn [749.656603ms]
    Aug 11 14:27:24.371: INFO: Created: latency-svc-h48lr
    Aug 11 14:27:24.402: INFO: Got endpoints: latency-svc-bznsm [747.627199ms]
    Aug 11 14:27:24.419: INFO: Created: latency-svc-4fjv5
    Aug 11 14:27:24.455: INFO: Got endpoints: latency-svc-rptng [751.872635ms]
    Aug 11 14:27:24.471: INFO: Created: latency-svc-9mkxj
    Aug 11 14:27:24.507: INFO: Got endpoints: latency-svc-5zjdn [751.986485ms]
    Aug 11 14:27:24.531: INFO: Created: latency-svc-njb9m
    Aug 11 14:27:24.556: INFO: Got endpoints: latency-svc-t65b8 [750.460653ms]
    Aug 11 14:27:24.572: INFO: Created: latency-svc-x9mx4
    Aug 11 14:27:24.604: INFO: Got endpoints: latency-svc-9bf5n [749.942464ms]
    Aug 11 14:27:24.626: INFO: Created: latency-svc-cw78j
    Aug 11 14:27:24.654: INFO: Got endpoints: latency-svc-9hjjf [750.886462ms]
    Aug 11 14:27:24.670: INFO: Created: latency-svc-q5t2g
    Aug 11 14:27:24.705: INFO: Got endpoints: latency-svc-r58hz [748.930851ms]
    Aug 11 14:27:24.728: INFO: Created: latency-svc-5dxhs
    Aug 11 14:27:24.752: INFO: Got endpoints: latency-svc-s8hnm [747.9208ms]
    Aug 11 14:27:24.767: INFO: Created: latency-svc-4cbn8
    Aug 11 14:27:24.806: INFO: Got endpoints: latency-svc-n6r7g [752.494436ms]
    Aug 11 14:27:24.836: INFO: Created: latency-svc-kbw5r
    Aug 11 14:27:24.855: INFO: Got endpoints: latency-svc-njwtn [750.379683ms]
    Aug 11 14:27:24.872: INFO: Created: latency-svc-9mnbx
    Aug 11 14:27:24.902: INFO: Got endpoints: latency-svc-cv9dm [749.837993ms]
    Aug 11 14:27:24.918: INFO: Created: latency-svc-fdphv
    Aug 11 14:27:24.955: INFO: Got endpoints: latency-svc-htkqj [749.646684ms]
    Aug 11 14:27:24.972: INFO: Created: latency-svc-4w2zn
    Aug 11 14:27:25.002: INFO: Got endpoints: latency-svc-n4dcj [748.247241ms]
    Aug 11 14:27:25.020: INFO: Created: latency-svc-rwfb2
    Aug 11 14:27:25.058: INFO: Got endpoints: latency-svc-954bj [754.53652ms]
    Aug 11 14:27:25.079: INFO: Created: latency-svc-5dxth
    Aug 11 14:27:25.104: INFO: Got endpoints: latency-svc-h48lr [749.532013ms]
    Aug 11 14:27:25.120: INFO: Created: latency-svc-4454z
    Aug 11 14:27:25.153: INFO: Got endpoints: latency-svc-4fjv5 [751.152366ms]
    Aug 11 14:27:25.178: INFO: Created: latency-svc-6dhhl
    Aug 11 14:27:25.203: INFO: Got endpoints: latency-svc-9mkxj [747.661721ms]
    Aug 11 14:27:25.218: INFO: Created: latency-svc-ddjbc
    Aug 11 14:27:25.253: INFO: Got endpoints: latency-svc-njb9m [745.714189ms]
    Aug 11 14:27:25.279: INFO: Created: latency-svc-h5gr9
    Aug 11 14:27:25.303: INFO: Got endpoints: latency-svc-x9mx4 [747.644322ms]
    Aug 11 14:27:25.321: INFO: Created: latency-svc-fb5rn
    Aug 11 14:27:25.353: INFO: Got endpoints: latency-svc-cw78j [749.257203ms]
    Aug 11 14:27:25.370: INFO: Created: latency-svc-kt7zk
    Aug 11 14:27:25.404: INFO: Got endpoints: latency-svc-q5t2g [749.880076ms]
    Aug 11 14:27:25.420: INFO: Created: latency-svc-lqhrg
    Aug 11 14:27:25.453: INFO: Got endpoints: latency-svc-5dxhs [748.060852ms]
    Aug 11 14:27:25.469: INFO: Created: latency-svc-kf2rw
    Aug 11 14:27:25.503: INFO: Got endpoints: latency-svc-4cbn8 [750.733535ms]
    Aug 11 14:27:25.519: INFO: Created: latency-svc-xrhxb
    Aug 11 14:27:25.555: INFO: Got endpoints: latency-svc-kbw5r [749.308353ms]
    Aug 11 14:27:25.572: INFO: Created: latency-svc-gg4fd
    Aug 11 14:27:25.608: INFO: Got endpoints: latency-svc-9mnbx [753.113767ms]
    Aug 11 14:27:25.623: INFO: Created: latency-svc-5cm2z
    Aug 11 14:27:25.654: INFO: Got endpoints: latency-svc-fdphv [751.827364ms]
    Aug 11 14:27:25.670: INFO: Created: latency-svc-9rdqx
    Aug 11 14:27:25.703: INFO: Got endpoints: latency-svc-4w2zn [748.534391ms]
    Aug 11 14:27:25.724: INFO: Created: latency-svc-5t2l8
    Aug 11 14:27:25.753: INFO: Got endpoints: latency-svc-rwfb2 [750.813224ms]
    Aug 11 14:27:25.769: INFO: Created: latency-svc-sp9hw
    Aug 11 14:27:25.804: INFO: Got endpoints: latency-svc-5dxth [746.026409ms]
    Aug 11 14:27:25.831: INFO: Created: latency-svc-mb2dg
    Aug 11 14:27:25.855: INFO: Got endpoints: latency-svc-4454z [750.905394ms]
    Aug 11 14:27:25.871: INFO: Created: latency-svc-nfmpg
    Aug 11 14:27:25.905: INFO: Got endpoints: latency-svc-6dhhl [751.410484ms]
    Aug 11 14:27:25.920: INFO: Created: latency-svc-4k7s4
    Aug 11 14:27:25.952: INFO: Got endpoints: latency-svc-ddjbc [749.613663ms]
    Aug 11 14:27:25.970: INFO: Created: latency-svc-74lvb
    Aug 11 14:27:26.006: INFO: Got endpoints: latency-svc-h5gr9 [752.562526ms]
    Aug 11 14:27:26.023: INFO: Created: latency-svc-hhfbh
    Aug 11 14:27:26.054: INFO: Got endpoints: latency-svc-fb5rn [750.157473ms]
    Aug 11 14:27:26.069: INFO: Created: latency-svc-wb8vl
    Aug 11 14:27:26.104: INFO: Got endpoints: latency-svc-kt7zk [750.879173ms]
    Aug 11 14:27:26.121: INFO: Created: latency-svc-64pc6
    Aug 11 14:27:26.161: INFO: Got endpoints: latency-svc-lqhrg [756.582669ms]
    Aug 11 14:27:26.178: INFO: Created: latency-svc-7tqvc
    Aug 11 14:27:26.203: INFO: Got endpoints: latency-svc-kf2rw [749.826183ms]
    Aug 11 14:27:26.219: INFO: Created: latency-svc-7ppmj
    Aug 11 14:27:26.253: INFO: Got endpoints: latency-svc-xrhxb [749.806422ms]
    Aug 11 14:27:26.275: INFO: Created: latency-svc-k7nmw
    Aug 11 14:27:26.304: INFO: Got endpoints: latency-svc-gg4fd [748.530822ms]
    Aug 11 14:27:26.320: INFO: Created: latency-svc-svg2t
    Aug 11 14:27:26.354: INFO: Got endpoints: latency-svc-5cm2z [746.58417ms]
    Aug 11 14:27:26.381: INFO: Created: latency-svc-2mkgh
    Aug 11 14:27:26.405: INFO: Got endpoints: latency-svc-9rdqx [751.221966ms]
    Aug 11 14:27:26.428: INFO: Created: latency-svc-grczz
    Aug 11 14:27:26.453: INFO: Got endpoints: latency-svc-5t2l8 [750.144583ms]
    Aug 11 14:27:26.468: INFO: Created: latency-svc-s7zjf
    Aug 11 14:27:26.503: INFO: Got endpoints: latency-svc-sp9hw [749.534753ms]
    Aug 11 14:27:26.520: INFO: Created: latency-svc-6v8hx
    Aug 11 14:27:26.554: INFO: Got endpoints: latency-svc-mb2dg [750.116534ms]
    Aug 11 14:27:26.571: INFO: Created: latency-svc-rh8hk
    Aug 11 14:27:26.604: INFO: Got endpoints: latency-svc-nfmpg [749.042752ms]
    Aug 11 14:27:26.619: INFO: Created: latency-svc-wmdl2
    Aug 11 14:27:26.651: INFO: Got endpoints: latency-svc-4k7s4 [746.695191ms]
    Aug 11 14:27:26.671: INFO: Created: latency-svc-9vsbd
    Aug 11 14:27:26.710: INFO: Got endpoints: latency-svc-74lvb [758.076672ms]
    Aug 11 14:27:26.726: INFO: Created: latency-svc-9hm5d
    Aug 11 14:27:26.753: INFO: Got endpoints: latency-svc-hhfbh [747.512341ms]
    Aug 11 14:27:26.769: INFO: Created: latency-svc-b9gbf
    Aug 11 14:27:26.802: INFO: Got endpoints: latency-svc-wb8vl [748.920453ms]
    Aug 11 14:27:26.825: INFO: Created: latency-svc-pnrkk
    Aug 11 14:27:26.856: INFO: Got endpoints: latency-svc-64pc6 [751.144496ms]
    Aug 11 14:27:26.872: INFO: Created: latency-svc-gzvnf
    Aug 11 14:27:26.905: INFO: Got endpoints: latency-svc-7tqvc [743.945468ms]
    Aug 11 14:27:26.933: INFO: Created: latency-svc-74lnr
    Aug 11 14:27:26.954: INFO: Got endpoints: latency-svc-7ppmj [751.513036ms]
    Aug 11 14:27:26.970: INFO: Created: latency-svc-g8r8l
    Aug 11 14:27:27.008: INFO: Got endpoints: latency-svc-k7nmw [754.82113ms]
    Aug 11 14:27:27.026: INFO: Created: latency-svc-7jc8b
    Aug 11 14:27:27.054: INFO: Got endpoints: latency-svc-svg2t [749.671234ms]
    Aug 11 14:27:27.071: INFO: Created: latency-svc-ljrsz
    Aug 11 14:27:27.107: INFO: Got endpoints: latency-svc-2mkgh [752.434867ms]
    Aug 11 14:27:27.129: INFO: Created: latency-svc-nr4d4
    Aug 11 14:27:27.153: INFO: Got endpoints: latency-svc-grczz [748.125202ms]
    Aug 11 14:27:27.169: INFO: Created: latency-svc-5fnpt
    Aug 11 14:27:27.205: INFO: Got endpoints: latency-svc-s7zjf [751.824435ms]
    Aug 11 14:27:27.221: INFO: Created: latency-svc-n4xmj
    Aug 11 14:27:27.263: INFO: Got endpoints: latency-svc-6v8hx [759.965415ms]
    Aug 11 14:27:27.289: INFO: Created: latency-svc-g49rl
    Aug 11 14:27:27.304: INFO: Got endpoints: latency-svc-rh8hk [749.209535ms]
    Aug 11 14:27:27.320: INFO: Created: latency-svc-tbs6p
    Aug 11 14:27:27.354: INFO: Got endpoints: latency-svc-wmdl2 [749.506385ms]
    Aug 11 14:27:27.377: INFO: Created: latency-svc-c9lgs
    Aug 11 14:27:27.405: INFO: Got endpoints: latency-svc-9vsbd [753.453197ms]
    Aug 11 14:27:27.419: INFO: Created: latency-svc-8nm72
    Aug 11 14:27:27.452: INFO: Got endpoints: latency-svc-9hm5d [741.394945ms]
    Aug 11 14:27:27.468: INFO: Created: latency-svc-8rsxm
    Aug 11 14:27:27.502: INFO: Got endpoints: latency-svc-b9gbf [748.897493ms]
    Aug 11 14:27:27.518: INFO: Created: latency-svc-pn8w8
    Aug 11 14:27:27.553: INFO: Got endpoints: latency-svc-pnrkk [750.224814ms]
    Aug 11 14:27:27.569: INFO: Created: latency-svc-gffmv
    Aug 11 14:27:27.603: INFO: Got endpoints: latency-svc-gzvnf [747.50948ms]
    Aug 11 14:27:27.629: INFO: Created: latency-svc-zv6x6
    Aug 11 14:27:27.655: INFO: Got endpoints: latency-svc-74lnr [749.850104ms]
    Aug 11 14:27:27.673: INFO: Created: latency-svc-bdljs
    Aug 11 14:27:27.705: INFO: Got endpoints: latency-svc-g8r8l [750.831305ms]
    Aug 11 14:27:27.722: INFO: Created: latency-svc-bb4fj
    Aug 11 14:27:27.753: INFO: Got endpoints: latency-svc-7jc8b [745.299219ms]
    Aug 11 14:27:27.771: INFO: Created: latency-svc-2sfln
    Aug 11 14:27:27.803: INFO: Got endpoints: latency-svc-ljrsz [749.761763ms]
    Aug 11 14:27:27.853: INFO: Got endpoints: latency-svc-nr4d4 [746.23806ms]
    Aug 11 14:27:27.905: INFO: Got endpoints: latency-svc-5fnpt [751.527363ms]
    Aug 11 14:27:27.954: INFO: Got endpoints: latency-svc-n4xmj [748.435333ms]
    Aug 11 14:27:28.005: INFO: Got endpoints: latency-svc-g49rl [742.515756ms]
    Aug 11 14:27:28.053: INFO: Got endpoints: latency-svc-tbs6p [749.437953ms]
    Aug 11 14:27:28.105: INFO: Got endpoints: latency-svc-c9lgs [750.966064ms]
    Aug 11 14:27:28.153: INFO: Got endpoints: latency-svc-8nm72 [748.124692ms]
    Aug 11 14:27:28.203: INFO: Got endpoints: latency-svc-8rsxm [751.247094ms]
    Aug 11 14:27:28.253: INFO: Got endpoints: latency-svc-pn8w8 [750.665145ms]
    Aug 11 14:27:28.304: INFO: Got endpoints: latency-svc-gffmv [750.728315ms]
    Aug 11 14:27:28.356: INFO: Got endpoints: latency-svc-zv6x6 [752.359627ms]
    Aug 11 14:27:28.403: INFO: Got endpoints: latency-svc-bdljs [748.169622ms]
    Aug 11 14:27:28.455: INFO: Got endpoints: latency-svc-bb4fj [750.005044ms]
    Aug 11 14:27:28.507: INFO: Got endpoints: latency-svc-2sfln [753.967468ms]
    Aug 11 14:27:28.507: INFO: Latencies: [28.467128ms 41.402421ms 52.177421ms 72.03487ms 92.0388ms 110.430278ms 132.274429ms 140.605327ms 162.294298ms 179.013815ms 194.562331ms 227.938233ms 229.425205ms 231.933317ms 233.632409ms 234.143229ms 236.568281ms 236.631962ms 239.658304ms 241.099906ms 243.699988ms 245.23167ms 245.63295ms 249.197634ms 250.176584ms 251.308305ms 252.177817ms 253.439338ms 254.076038ms 254.802179ms 255.837801ms 257.554983ms 257.877392ms 258.719772ms 260.850055ms 261.103065ms 264.820909ms 267.711872ms 268.354203ms 269.875744ms 272.952297ms 279.613103ms 280.707884ms 284.664529ms 284.902978ms 310.525945ms 368.995182ms 372.010424ms 395.489958ms 447.241948ms 475.484086ms 513.686714ms 545.025833ms 590.343667ms 621.013819ms 645.353072ms 679.744656ms 720.215915ms 728.675862ms 740.206734ms 741.307494ms 741.394945ms 742.515756ms 743.461448ms 743.945468ms 744.767448ms 744.787918ms 745.008369ms 745.015119ms 745.299219ms 745.714189ms 746.026409ms 746.23806ms 746.58417ms 746.695191ms 746.712451ms 747.105321ms 747.50948ms 747.512341ms 747.627199ms 747.644322ms 747.661721ms 747.9208ms 748.060852ms 748.124692ms 748.125202ms 748.146602ms 748.169622ms 748.247241ms 748.435333ms 748.530822ms 748.534391ms 748.630772ms 748.643242ms 748.843062ms 748.897493ms 748.920453ms 748.930851ms 748.936133ms 748.985682ms 749.042752ms 749.059802ms 749.061292ms 749.209535ms 749.257122ms 749.257203ms 749.308353ms 749.321234ms 749.437953ms 749.506385ms 749.532013ms 749.534753ms 749.541382ms 749.613663ms 749.646684ms 749.656603ms 749.671234ms 749.686613ms 749.761763ms 749.806422ms 749.826183ms 749.837993ms 749.850104ms 749.880076ms 749.901183ms 749.942464ms 749.967673ms 749.977933ms 750.005044ms 750.009714ms 750.056354ms 750.086914ms 750.116534ms 750.119633ms 750.144583ms 750.157473ms 750.173103ms 750.176023ms 750.185804ms 750.223573ms 750.224814ms 750.346533ms 750.352062ms 750.363203ms 750.379683ms 750.404123ms 750.416983ms 750.438124ms 750.460653ms 750.665145ms 750.703474ms 750.728315ms 750.733535ms 750.813224ms 750.831305ms 750.861904ms 750.879173ms 750.886462ms 750.905394ms 750.922225ms 750.960135ms 750.966064ms 750.966114ms 751.020287ms 751.144496ms 751.152366ms 751.221966ms 751.247094ms 751.410484ms 751.513036ms 751.527363ms 751.554765ms 751.636565ms 751.773015ms 751.779476ms 751.824435ms 751.827364ms 751.872635ms 751.894936ms 751.986485ms 751.987254ms 752.121625ms 752.199566ms 752.271996ms 752.359627ms 752.434867ms 752.494436ms 752.562526ms 753.113767ms 753.152527ms 753.360197ms 753.453197ms 753.967468ms 754.53652ms 754.82113ms 756.034971ms 756.582669ms 758.076672ms 759.555533ms 759.965415ms]
    Aug 11 14:27:28.507: INFO: 50 %ile: 749.042752ms
    Aug 11 14:27:28.507: INFO: 90 %ile: 751.987254ms
    Aug 11 14:27:28.507: INFO: 99 %ile: 759.555533ms
    Aug 11 14:27:28.507: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:28.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-1387" for this suite. 08/11/23 14:27:28.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:28.524
Aug 11 14:27:28.524: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-pred 08/11/23 14:27:28.525
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:28.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:28.545
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 11 14:27:28.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 14:27:28.555: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 14:27:28.558: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
Aug 11 14:27:28.567: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:27:28.567: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container coredns ready: true, restart count 0
Aug 11 14:27:28.567: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:27:28.567: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:27:28.567: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:27:28.567: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:27:28.567: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:27:28.567: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:27:28.567: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 14:27:28.567: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container e2e ready: true, restart count 0
Aug 11 14:27:28.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:27:28.567: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:27:28.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:27:28.567: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:27:28.567: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
Aug 11 14:27:28.574: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.574: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 14:27:28.574: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.574: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:27:28.574: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:27:28.574: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:27:28.574: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:27:28.575: INFO: gcp-guest-agent-wrl88 from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:27:28.575: INFO: konnectivity-agent-sq7xs from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:27:28.575: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:27:28.575: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:27:28.575: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:27:28.575: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:27:28.575: INFO: svc-latency-rc-wgq8x from svc-latency-1387 started at 2023-08-11 14:27:17 +0000 UTC (1 container statuses recorded)
Aug 11 14:27:28.575: INFO: 	Container svc-latency-rc ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/11/23 14:27:28.575
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177a5a70590dafea], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 08/11/23 14:27:28.618
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:29.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1854" for this suite. 08/11/23 14:27:29.616
------------------------------
• [1.098 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:28.524
    Aug 11 14:27:28.524: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-pred 08/11/23 14:27:28.525
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:28.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:28.545
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 11 14:27:28.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 14:27:28.555: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 14:27:28.558: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
    Aug 11 14:27:28.567: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:27:28.567: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:27:28.567: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:27:28.567: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
    Aug 11 14:27:28.574: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.574: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 14:27:28.574: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.574: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:27:28.574: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:27:28.574: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:27:28.574: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: gcp-guest-agent-wrl88 from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: konnectivity-agent-sq7xs from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:27:28.575: INFO: svc-latency-rc-wgq8x from svc-latency-1387 started at 2023-08-11 14:27:17 +0000 UTC (1 container statuses recorded)
    Aug 11 14:27:28.575: INFO: 	Container svc-latency-rc ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/11/23 14:27:28.575
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177a5a70590dafea], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 08/11/23 14:27:28.618
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:29.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1854" for this suite. 08/11/23 14:27:29.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:29.622
Aug 11 14:27:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:27:29.623
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:29.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:29.641
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/11/23 14:27:29.643
STEP: Creating a ResourceQuota 08/11/23 14:27:34.647
STEP: Ensuring resource quota status is calculated 08/11/23 14:27:34.655
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:36.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9224" for this suite. 08/11/23 14:27:36.668
------------------------------
• [SLOW TEST] [7.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:29.622
    Aug 11 14:27:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:27:29.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:29.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:29.641
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/11/23 14:27:29.643
    STEP: Creating a ResourceQuota 08/11/23 14:27:34.647
    STEP: Ensuring resource quota status is calculated 08/11/23 14:27:34.655
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:36.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9224" for this suite. 08/11/23 14:27:36.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:36.689
Aug 11 14:27:36.690: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context 08/11/23 14:27:36.691
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:36.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:36.717
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:27:36.719
Aug 11 14:27:36.729: INFO: Waiting up to 5m0s for pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a" in namespace "security-context-1941" to be "Succeeded or Failed"
Aug 11 14:27:36.734: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394745ms
Aug 11 14:27:38.738: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008387175s
Aug 11 14:27:40.739: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009145212s
STEP: Saw pod success 08/11/23 14:27:40.739
Aug 11 14:27:40.739: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a" satisfied condition "Succeeded or Failed"
Aug 11 14:27:40.741: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a container test-container: <nil>
STEP: delete the pod 08/11/23 14:27:40.751
Aug 11 14:27:40.764: INFO: Waiting for pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a to disappear
Aug 11 14:27:40.767: INFO: Pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:40.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1941" for this suite. 08/11/23 14:27:40.771
------------------------------
• [4.088 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:36.689
    Aug 11 14:27:36.690: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context 08/11/23 14:27:36.691
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:36.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:36.717
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:27:36.719
    Aug 11 14:27:36.729: INFO: Waiting up to 5m0s for pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a" in namespace "security-context-1941" to be "Succeeded or Failed"
    Aug 11 14:27:36.734: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.394745ms
    Aug 11 14:27:38.738: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008387175s
    Aug 11 14:27:40.739: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009145212s
    STEP: Saw pod success 08/11/23 14:27:40.739
    Aug 11 14:27:40.739: INFO: Pod "security-context-88a497fb-87be-4941-bf12-d666aeb9be7a" satisfied condition "Succeeded or Failed"
    Aug 11 14:27:40.741: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a container test-container: <nil>
    STEP: delete the pod 08/11/23 14:27:40.751
    Aug 11 14:27:40.764: INFO: Waiting for pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a to disappear
    Aug 11 14:27:40.767: INFO: Pod security-context-88a497fb-87be-4941-bf12-d666aeb9be7a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:40.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1941" for this suite. 08/11/23 14:27:40.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:40.779
Aug 11 14:27:40.779: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:27:40.78
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:40.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:40.803
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/11/23 14:27:40.805
Aug 11 14:27:40.808: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/11/23 14:27:40.808
Aug 11 14:27:40.815: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/11/23 14:27:40.815
Aug 11 14:27:40.823: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:40.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2325" for this suite. 08/11/23 14:27:40.827
------------------------------
• [0.054 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:40.779
    Aug 11 14:27:40.779: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:27:40.78
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:40.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:40.803
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/11/23 14:27:40.805
    Aug 11 14:27:40.808: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/11/23 14:27:40.808
    Aug 11 14:27:40.815: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/11/23 14:27:40.815
    Aug 11 14:27:40.823: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:40.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2325" for this suite. 08/11/23 14:27:40.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:40.834
Aug 11 14:27:40.834: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:27:40.835
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:40.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:40.851
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-1c17b0be-42b1-4a94-a92f-40f3de702321 08/11/23 14:27:40.854
STEP: Creating a pod to test consume secrets 08/11/23 14:27:40.859
Aug 11 14:27:40.866: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863" in namespace "projected-4414" to be "Succeeded or Failed"
Aug 11 14:27:40.869: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617282ms
Aug 11 14:27:42.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089382s
Aug 11 14:27:44.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007393069s
STEP: Saw pod success 08/11/23 14:27:44.874
Aug 11 14:27:44.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863" satisfied condition "Succeeded or Failed"
Aug 11 14:27:44.877: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:27:44.886
Aug 11 14:27:44.897: INFO: Waiting for pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 to disappear
Aug 11 14:27:44.899: INFO: Pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:44.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4414" for this suite. 08/11/23 14:27:44.903
------------------------------
• [4.076 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:40.834
    Aug 11 14:27:40.834: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:27:40.835
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:40.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:40.851
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-1c17b0be-42b1-4a94-a92f-40f3de702321 08/11/23 14:27:40.854
    STEP: Creating a pod to test consume secrets 08/11/23 14:27:40.859
    Aug 11 14:27:40.866: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863" in namespace "projected-4414" to be "Succeeded or Failed"
    Aug 11 14:27:40.869: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.617282ms
    Aug 11 14:27:42.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089382s
    Aug 11 14:27:44.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007393069s
    STEP: Saw pod success 08/11/23 14:27:44.874
    Aug 11 14:27:44.874: INFO: Pod "pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863" satisfied condition "Succeeded or Failed"
    Aug 11 14:27:44.877: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:27:44.886
    Aug 11 14:27:44.897: INFO: Waiting for pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 to disappear
    Aug 11 14:27:44.899: INFO: Pod pod-projected-secrets-4ca828a0-3f6a-487a-a7b6-3957c1e33863 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:44.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4414" for this suite. 08/11/23 14:27:44.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:44.911
Aug 11 14:27:44.911: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:27:44.911
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:44.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:44.927
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7339 08/11/23 14:27:44.929
STEP: creating a selector 08/11/23 14:27:44.929
STEP: Creating the service pods in kubernetes 08/11/23 14:27:44.929
Aug 11 14:27:44.929: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 14:27:44.952: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7339" to be "running and ready"
Aug 11 14:27:44.958: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142705ms
Aug 11 14:27:44.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:27:46.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009770137s
Aug 11 14:27:46.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:27:48.963: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010411854s
Aug 11 14:27:48.963: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:27:50.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009684949s
Aug 11 14:27:50.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:27:52.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009237874s
Aug 11 14:27:52.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:27:54.961: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00891072s
Aug 11 14:27:54.961: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:27:56.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009644788s
Aug 11 14:27:56.962: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 14:27:56.962: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 14:27:56.965: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7339" to be "running and ready"
Aug 11 14:27:56.968: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.825243ms
Aug 11 14:27:56.968: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 14:27:56.968: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 14:27:56.971
Aug 11 14:27:56.978: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7339" to be "running"
Aug 11 14:27:56.981: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873442ms
Aug 11 14:27:58.985: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006923832s
Aug 11 14:27:58.985: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 14:27:58.988: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 14:27:58.988: INFO: Breadth first check of 10.10.0.200 on host 192.168.178.2...
Aug 11 14:27:58.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.69:9080/dial?request=hostname&protocol=udp&host=10.10.0.200&port=8081&tries=1'] Namespace:pod-network-test-7339 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:27:58.991: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:27:58.991: INFO: ExecWithOptions: Clientset creation
Aug 11 14:27:58.991: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7339/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.200%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:27:59.079: INFO: Waiting for responses: map[]
Aug 11 14:27:59.079: INFO: reached 10.10.0.200 after 0/1 tries
Aug 11 14:27:59.079: INFO: Breadth first check of 10.10.1.238 on host 192.168.178.3...
Aug 11 14:27:59.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.69:9080/dial?request=hostname&protocol=udp&host=10.10.1.238&port=8081&tries=1'] Namespace:pod-network-test-7339 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:27:59.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:27:59.083: INFO: ExecWithOptions: Clientset creation
Aug 11 14:27:59.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7339/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.238%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:27:59.159: INFO: Waiting for responses: map[]
Aug 11 14:27:59.159: INFO: reached 10.10.1.238 after 0/1 tries
Aug 11 14:27:59.159: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 11 14:27:59.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7339" for this suite. 08/11/23 14:27:59.164
------------------------------
• [SLOW TEST] [14.260 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:44.911
    Aug 11 14:27:44.911: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:27:44.911
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:44.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:44.927
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7339 08/11/23 14:27:44.929
    STEP: creating a selector 08/11/23 14:27:44.929
    STEP: Creating the service pods in kubernetes 08/11/23 14:27:44.929
    Aug 11 14:27:44.929: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 14:27:44.952: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7339" to be "running and ready"
    Aug 11 14:27:44.958: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142705ms
    Aug 11 14:27:44.958: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:27:46.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009770137s
    Aug 11 14:27:46.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:27:48.963: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010411854s
    Aug 11 14:27:48.963: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:27:50.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009684949s
    Aug 11 14:27:50.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:27:52.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009237874s
    Aug 11 14:27:52.962: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:27:54.961: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00891072s
    Aug 11 14:27:54.961: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:27:56.962: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009644788s
    Aug 11 14:27:56.962: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 14:27:56.962: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 14:27:56.965: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7339" to be "running and ready"
    Aug 11 14:27:56.968: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.825243ms
    Aug 11 14:27:56.968: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 14:27:56.968: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 14:27:56.971
    Aug 11 14:27:56.978: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7339" to be "running"
    Aug 11 14:27:56.981: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.873442ms
    Aug 11 14:27:58.985: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006923832s
    Aug 11 14:27:58.985: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 14:27:58.988: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 14:27:58.988: INFO: Breadth first check of 10.10.0.200 on host 192.168.178.2...
    Aug 11 14:27:58.991: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.69:9080/dial?request=hostname&protocol=udp&host=10.10.0.200&port=8081&tries=1'] Namespace:pod-network-test-7339 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:27:58.991: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:27:58.991: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:27:58.991: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7339/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.200%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:27:59.079: INFO: Waiting for responses: map[]
    Aug 11 14:27:59.079: INFO: reached 10.10.0.200 after 0/1 tries
    Aug 11 14:27:59.079: INFO: Breadth first check of 10.10.1.238 on host 192.168.178.3...
    Aug 11 14:27:59.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.69:9080/dial?request=hostname&protocol=udp&host=10.10.1.238&port=8081&tries=1'] Namespace:pod-network-test-7339 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:27:59.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:27:59.083: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:27:59.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7339/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.69%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.238%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:27:59.159: INFO: Waiting for responses: map[]
    Aug 11 14:27:59.159: INFO: reached 10.10.1.238 after 0/1 tries
    Aug 11 14:27:59.159: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:27:59.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7339" for this suite. 08/11/23 14:27:59.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:27:59.172
Aug 11 14:27:59.172: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 14:27:59.173
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:59.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:59.19
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/11/23 14:27:59.193
STEP: Verify that the required pods have come up 08/11/23 14:27:59.198
Aug 11 14:27:59.204: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 11 14:28:04.208: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/11/23 14:28:04.208
Aug 11 14:28:04.211: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/11/23 14:28:04.211
STEP: DeleteCollection of the ReplicaSets 08/11/23 14:28:04.215
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/11/23 14:28:04.224
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:04.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5719" for this suite. 08/11/23 14:28:04.234
------------------------------
• [SLOW TEST] [5.077 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:27:59.172
    Aug 11 14:27:59.172: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:27:59.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:59.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:59.19
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/11/23 14:27:59.193
    STEP: Verify that the required pods have come up 08/11/23 14:27:59.198
    Aug 11 14:27:59.204: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 11 14:28:04.208: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/11/23 14:28:04.208
    Aug 11 14:28:04.211: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/11/23 14:28:04.211
    STEP: DeleteCollection of the ReplicaSets 08/11/23 14:28:04.215
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/11/23 14:28:04.224
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:04.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5719" for this suite. 08/11/23 14:28:04.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:04.249
Aug 11 14:28:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:28:04.25
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:04.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:04.268
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/11/23 14:28:04.271
Aug 11 14:28:04.283: INFO: Waiting up to 5m0s for pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf" in namespace "downward-api-3446" to be "Succeeded or Failed"
Aug 11 14:28:04.289: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800036ms
Aug 11 14:28:06.294: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010398046s
Aug 11 14:28:08.294: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125133s
STEP: Saw pod success 08/11/23 14:28:08.295
Aug 11 14:28:08.295: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf" satisfied condition "Succeeded or Failed"
Aug 11 14:28:08.298: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:28:08.307
Aug 11 14:28:08.321: INFO: Waiting for pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf to disappear
Aug 11 14:28:08.324: INFO: Pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:08.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3446" for this suite. 08/11/23 14:28:08.328
------------------------------
• [4.085 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:04.249
    Aug 11 14:28:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:28:04.25
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:04.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:04.268
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/11/23 14:28:04.271
    Aug 11 14:28:04.283: INFO: Waiting up to 5m0s for pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf" in namespace "downward-api-3446" to be "Succeeded or Failed"
    Aug 11 14:28:04.289: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800036ms
    Aug 11 14:28:06.294: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010398046s
    Aug 11 14:28:08.294: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125133s
    STEP: Saw pod success 08/11/23 14:28:08.295
    Aug 11 14:28:08.295: INFO: Pod "downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf" satisfied condition "Succeeded or Failed"
    Aug 11 14:28:08.298: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:28:08.307
    Aug 11 14:28:08.321: INFO: Waiting for pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf to disappear
    Aug 11 14:28:08.324: INFO: Pod downward-api-678334cd-b4a3-4754-ae4e-6c984958f1cf no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:08.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3446" for this suite. 08/11/23 14:28:08.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:08.334
Aug 11 14:28:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:28:08.335
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:08.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:08.352
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:28:08.354
Aug 11 14:28:08.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f" in namespace "projected-8634" to be "Succeeded or Failed"
Aug 11 14:28:08.367: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826685ms
Aug 11 14:28:10.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009847584s
Aug 11 14:28:12.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010442232s
STEP: Saw pod success 08/11/23 14:28:12.373
Aug 11 14:28:12.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f" satisfied condition "Succeeded or Failed"
Aug 11 14:28:12.377: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f container client-container: <nil>
STEP: delete the pod 08/11/23 14:28:12.385
Aug 11 14:28:12.400: INFO: Waiting for pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f to disappear
Aug 11 14:28:12.403: INFO: Pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:12.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8634" for this suite. 08/11/23 14:28:12.406
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:08.334
    Aug 11 14:28:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:28:08.335
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:08.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:08.352
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:28:08.354
    Aug 11 14:28:08.363: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f" in namespace "projected-8634" to be "Succeeded or Failed"
    Aug 11 14:28:08.367: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826685ms
    Aug 11 14:28:10.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009847584s
    Aug 11 14:28:12.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010442232s
    STEP: Saw pod success 08/11/23 14:28:12.373
    Aug 11 14:28:12.373: INFO: Pod "downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f" satisfied condition "Succeeded or Failed"
    Aug 11 14:28:12.377: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f container client-container: <nil>
    STEP: delete the pod 08/11/23 14:28:12.385
    Aug 11 14:28:12.400: INFO: Waiting for pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f to disappear
    Aug 11 14:28:12.403: INFO: Pod downwardapi-volume-2080a12d-ab94-402f-bc89-a1f8534f8c1f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:12.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8634" for this suite. 08/11/23 14:28:12.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:12.413
Aug 11 14:28:12.413: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename certificates 08/11/23 14:28:12.414
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.431
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/11/23 14:28:12.699
STEP: getting /apis/certificates.k8s.io 08/11/23 14:28:12.702
STEP: getting /apis/certificates.k8s.io/v1 08/11/23 14:28:12.702
STEP: creating 08/11/23 14:28:12.703
STEP: getting 08/11/23 14:28:12.72
STEP: listing 08/11/23 14:28:12.722
STEP: watching 08/11/23 14:28:12.725
Aug 11 14:28:12.726: INFO: starting watch
STEP: patching 08/11/23 14:28:12.726
STEP: updating 08/11/23 14:28:12.733
Aug 11 14:28:12.738: INFO: waiting for watch events with expected annotations
Aug 11 14:28:12.739: INFO: saw patched and updated annotations
STEP: getting /approval 08/11/23 14:28:12.739
STEP: patching /approval 08/11/23 14:28:12.741
STEP: updating /approval 08/11/23 14:28:12.747
STEP: getting /status 08/11/23 14:28:12.753
STEP: patching /status 08/11/23 14:28:12.757
STEP: updating /status 08/11/23 14:28:12.769
STEP: deleting 08/11/23 14:28:12.776
STEP: deleting a collection 08/11/23 14:28:12.789
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-5192" for this suite. 08/11/23 14:28:12.808
------------------------------
• [0.401 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:12.413
    Aug 11 14:28:12.413: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename certificates 08/11/23 14:28:12.414
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.431
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/11/23 14:28:12.699
    STEP: getting /apis/certificates.k8s.io 08/11/23 14:28:12.702
    STEP: getting /apis/certificates.k8s.io/v1 08/11/23 14:28:12.702
    STEP: creating 08/11/23 14:28:12.703
    STEP: getting 08/11/23 14:28:12.72
    STEP: listing 08/11/23 14:28:12.722
    STEP: watching 08/11/23 14:28:12.725
    Aug 11 14:28:12.726: INFO: starting watch
    STEP: patching 08/11/23 14:28:12.726
    STEP: updating 08/11/23 14:28:12.733
    Aug 11 14:28:12.738: INFO: waiting for watch events with expected annotations
    Aug 11 14:28:12.739: INFO: saw patched and updated annotations
    STEP: getting /approval 08/11/23 14:28:12.739
    STEP: patching /approval 08/11/23 14:28:12.741
    STEP: updating /approval 08/11/23 14:28:12.747
    STEP: getting /status 08/11/23 14:28:12.753
    STEP: patching /status 08/11/23 14:28:12.757
    STEP: updating /status 08/11/23 14:28:12.769
    STEP: deleting 08/11/23 14:28:12.776
    STEP: deleting a collection 08/11/23 14:28:12.789
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-5192" for this suite. 08/11/23 14:28:12.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:12.815
Aug 11 14:28:12.815: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:28:12.816
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.833
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:28:12.848
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:28:13.033
STEP: Deploying the webhook pod 08/11/23 14:28:13.044
STEP: Wait for the deployment to be ready 08/11/23 14:28:13.057
Aug 11 14:28:13.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:28:15.074
STEP: Verifying the service has paired with the endpoint 08/11/23 14:28:15.11
Aug 11 14:28:16.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/11/23 14:28:16.113
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/11/23 14:28:16.144
STEP: Creating a configMap that should not be mutated 08/11/23 14:28:16.152
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/11/23 14:28:16.165
STEP: Creating a configMap that should be mutated 08/11/23 14:28:16.173
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4257" for this suite. 08/11/23 14:28:16.265
STEP: Destroying namespace "webhook-4257-markers" for this suite. 08/11/23 14:28:16.276
------------------------------
• [3.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:12.815
    Aug 11 14:28:12.815: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:28:12.816
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.833
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:28:12.848
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:28:13.033
    STEP: Deploying the webhook pod 08/11/23 14:28:13.044
    STEP: Wait for the deployment to be ready 08/11/23 14:28:13.057
    Aug 11 14:28:13.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:28:15.074
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:28:15.11
    Aug 11 14:28:16.110: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/11/23 14:28:16.113
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/11/23 14:28:16.144
    STEP: Creating a configMap that should not be mutated 08/11/23 14:28:16.152
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/11/23 14:28:16.165
    STEP: Creating a configMap that should be mutated 08/11/23 14:28:16.173
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4257" for this suite. 08/11/23 14:28:16.265
    STEP: Destroying namespace "webhook-4257-markers" for this suite. 08/11/23 14:28:16.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:16.286
Aug 11 14:28:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-pred 08/11/23 14:28:16.287
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:16.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:16.306
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 11 14:28:16.308: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 14:28:16.315: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 14:28:16.318: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
Aug 11 14:28:16.326: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:28:16.326: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container coredns ready: true, restart count 0
Aug 11 14:28:16.326: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:28:16.326: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:28:16.326: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:28:16.326: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:28:16.326: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:28:16.326: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:28:16.326: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 14:28:16.326: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container e2e ready: true, restart count 0
Aug 11 14:28:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:28:16.326: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:28:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:28:16.326: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:28:16.326: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
Aug 11 14:28:16.334: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 14:28:16.334: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:28:16.334: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:28:16.334: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:28:16.334: INFO: gcp-guest-agent-wrl88 from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:28:16.334: INFO: konnectivity-agent-sq7xs from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:28:16.334: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:28:16.334: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:28:16.334: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:28:16.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:28:16.334: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node constell-1cf5d931-worker-6381a7ba-mt98 08/11/23 14:28:16.359
STEP: verifying the node has the label node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 14:28:16.373
Aug 11 14:28:16.394: INFO: Pod cilium-k88wg requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod cilium-operator-86847fc955-vpgcn requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod cilium-rq95f requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod coredns-9ff5c7c6f-qtwv7 requesting resource cpu=100m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod csi-gce-pd-node-57ng9 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod csi-gce-pd-node-9mzs6 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod gcp-guest-agent-tncp7 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod gcp-guest-agent-wrl88 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod konnectivity-agent-g8nvt requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod konnectivity-agent-sq7xs requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod kube-proxy-kkq86 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod kube-proxy-mk54d requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod verification-service-swmj4 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.394: INFO: Pod verification-service-vwjx2 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod sonobuoy requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod sonobuoy-e2e-job-550936f75fcb40a8 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.394: INFO: Pod sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
STEP: Starting Pods to consume most of the cluster CPU. 08/11/23 14:28:16.394
Aug 11 14:28:16.394: INFO: Creating a pod which consumes cpu=2730m on Node constell-1cf5d931-worker-6381a7ba-mt98
Aug 11 14:28:16.403: INFO: Creating a pod which consumes cpu=2800m on Node constell-1cf5d931-worker-6381a7ba-nd80
Aug 11 14:28:16.411: INFO: Waiting up to 5m0s for pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f" in namespace "sched-pred-5996" to be "running"
Aug 11 14:28:16.417: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442207ms
Aug 11 14:28:18.422: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010923326s
Aug 11 14:28:18.422: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f" satisfied condition "running"
Aug 11 14:28:18.422: INFO: Waiting up to 5m0s for pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2" in namespace "sched-pred-5996" to be "running"
Aug 11 14:28:18.425: INFO: Pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.684143ms
Aug 11 14:28:18.425: INFO: Pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/11/23 14:28:18.425
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7b7ab44a1c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5996/filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2 to constell-1cf5d931-worker-6381a7ba-nd80] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7ba7361f19], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7ba8b3dbfd], Reason = [Created], Message = [Created container filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7bad47e96f], Reason = [Started], Message = [Started container filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7b7a498171], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5996/filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f to constell-1cf5d931-worker-6381a7ba-mt98] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7b9fdf3b87], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7ba13ffb67], Reason = [Created], Message = [Created container filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7ba6028c5d], Reason = [Started], Message = [Started container filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f] 08/11/23 14:28:18.429
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177a5a7bf3132593], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 08/11/23 14:28:18.443
STEP: removing the label node off the node constell-1cf5d931-worker-6381a7ba-mt98 08/11/23 14:28:19.441
STEP: verifying the node doesn't have the label node 08/11/23 14:28:19.453
STEP: removing the label node off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 14:28:19.456
STEP: verifying the node doesn't have the label node 08/11/23 14:28:19.469
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:28:19.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5996" for this suite. 08/11/23 14:28:19.482
------------------------------
• [3.204 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:16.286
    Aug 11 14:28:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-pred 08/11/23 14:28:16.287
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:16.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:16.306
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 11 14:28:16.308: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 14:28:16.315: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 14:28:16.318: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
    Aug 11 14:28:16.326: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:28:16.326: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:28:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:28:16.326: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
    Aug 11 14:28:16.334: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:28:16.334: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: gcp-guest-agent-wrl88 from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: konnectivity-agent-sq7xs from kube-system started at 2023-08-11 14:15:29 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:28:16.334: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:28:16.334: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node constell-1cf5d931-worker-6381a7ba-mt98 08/11/23 14:28:16.359
    STEP: verifying the node has the label node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 14:28:16.373
    Aug 11 14:28:16.394: INFO: Pod cilium-k88wg requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod cilium-operator-86847fc955-vpgcn requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod cilium-rq95f requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod coredns-9ff5c7c6f-qtwv7 requesting resource cpu=100m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod csi-gce-pd-node-57ng9 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod csi-gce-pd-node-9mzs6 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod gcp-guest-agent-tncp7 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod gcp-guest-agent-wrl88 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod konnectivity-agent-g8nvt requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod konnectivity-agent-sq7xs requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod kube-proxy-kkq86 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod kube-proxy-mk54d requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod verification-service-swmj4 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.394: INFO: Pod verification-service-vwjx2 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod sonobuoy requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod sonobuoy-e2e-job-550936f75fcb40a8 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.394: INFO: Pod sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 requesting resource cpu=0m on Node constell-1cf5d931-worker-6381a7ba-nd80
    STEP: Starting Pods to consume most of the cluster CPU. 08/11/23 14:28:16.394
    Aug 11 14:28:16.394: INFO: Creating a pod which consumes cpu=2730m on Node constell-1cf5d931-worker-6381a7ba-mt98
    Aug 11 14:28:16.403: INFO: Creating a pod which consumes cpu=2800m on Node constell-1cf5d931-worker-6381a7ba-nd80
    Aug 11 14:28:16.411: INFO: Waiting up to 5m0s for pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f" in namespace "sched-pred-5996" to be "running"
    Aug 11 14:28:16.417: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442207ms
    Aug 11 14:28:18.422: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010923326s
    Aug 11 14:28:18.422: INFO: Pod "filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f" satisfied condition "running"
    Aug 11 14:28:18.422: INFO: Waiting up to 5m0s for pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2" in namespace "sched-pred-5996" to be "running"
    Aug 11 14:28:18.425: INFO: Pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.684143ms
    Aug 11 14:28:18.425: INFO: Pod "filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/11/23 14:28:18.425
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7b7ab44a1c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5996/filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2 to constell-1cf5d931-worker-6381a7ba-nd80] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7ba7361f19], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7ba8b3dbfd], Reason = [Created], Message = [Created container filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2.177a5a7bad47e96f], Reason = [Started], Message = [Started container filler-pod-535fdfab-ce76-49ff-a9dc-abd87618a9b2] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7b7a498171], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5996/filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f to constell-1cf5d931-worker-6381a7ba-mt98] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7b9fdf3b87], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7ba13ffb67], Reason = [Created], Message = [Created container filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f.177a5a7ba6028c5d], Reason = [Started], Message = [Started container filler-pod-fb073e84-7b35-4a05-a7f6-d48c489fa88f] 08/11/23 14:28:18.429
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177a5a7bf3132593], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 08/11/23 14:28:18.443
    STEP: removing the label node off the node constell-1cf5d931-worker-6381a7ba-mt98 08/11/23 14:28:19.441
    STEP: verifying the node doesn't have the label node 08/11/23 14:28:19.453
    STEP: removing the label node off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 14:28:19.456
    STEP: verifying the node doesn't have the label node 08/11/23 14:28:19.469
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:28:19.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5996" for this suite. 08/11/23 14:28:19.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:28:19.491
Aug 11 14:28:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:28:19.492
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:19.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:19.51
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e in namespace container-probe-9749 08/11/23 14:28:19.513
Aug 11 14:28:19.520: INFO: Waiting up to 5m0s for pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e" in namespace "container-probe-9749" to be "not pending"
Aug 11 14:28:19.525: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.273964ms
Aug 11 14:28:21.529: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008934174s
Aug 11 14:28:21.529: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e" satisfied condition "not pending"
Aug 11 14:28:21.529: INFO: Started pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e in namespace container-probe-9749
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:28:21.529
Aug 11 14:28:21.532: INFO: Initial restart count of pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is 0
Aug 11 14:28:41.582: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 1 (20.04999986s elapsed)
Aug 11 14:29:01.628: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 2 (40.095438535s elapsed)
Aug 11 14:29:21.671: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 3 (1m0.139113088s elapsed)
Aug 11 14:29:41.717: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 4 (1m20.185029593s elapsed)
Aug 11 14:30:43.866: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 5 (2m22.333587873s elapsed)
STEP: deleting the pod 08/11/23 14:30:43.866
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:30:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9749" for this suite. 08/11/23 14:30:43.885
------------------------------
• [SLOW TEST] [144.402 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:28:19.491
    Aug 11 14:28:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:28:19.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:19.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:19.51
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e in namespace container-probe-9749 08/11/23 14:28:19.513
    Aug 11 14:28:19.520: INFO: Waiting up to 5m0s for pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e" in namespace "container-probe-9749" to be "not pending"
    Aug 11 14:28:19.525: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.273964ms
    Aug 11 14:28:21.529: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008934174s
    Aug 11 14:28:21.529: INFO: Pod "liveness-40a20aaf-ad36-4215-9777-ab450408bd6e" satisfied condition "not pending"
    Aug 11 14:28:21.529: INFO: Started pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e in namespace container-probe-9749
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:28:21.529
    Aug 11 14:28:21.532: INFO: Initial restart count of pod liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is 0
    Aug 11 14:28:41.582: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 1 (20.04999986s elapsed)
    Aug 11 14:29:01.628: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 2 (40.095438535s elapsed)
    Aug 11 14:29:21.671: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 3 (1m0.139113088s elapsed)
    Aug 11 14:29:41.717: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 4 (1m20.185029593s elapsed)
    Aug 11 14:30:43.866: INFO: Restart count of pod container-probe-9749/liveness-40a20aaf-ad36-4215-9777-ab450408bd6e is now 5 (2m22.333587873s elapsed)
    STEP: deleting the pod 08/11/23 14:30:43.866
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:30:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9749" for this suite. 08/11/23 14:30:43.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:30:43.893
Aug 11 14:30:43.893: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:30:43.894
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:43.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:43.911
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/11/23 14:30:43.914
Aug 11 14:30:43.922: INFO: Waiting up to 5m0s for pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73" in namespace "svcaccounts-77" to be "Succeeded or Failed"
Aug 11 14:30:43.924: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503112ms
Aug 11 14:30:45.929: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007051003s
Aug 11 14:30:47.928: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006859828s
STEP: Saw pod success 08/11/23 14:30:47.929
Aug 11 14:30:47.929: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73" satisfied condition "Succeeded or Failed"
Aug 11 14:30:47.932: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:30:47.952
Aug 11 14:30:47.967: INFO: Waiting for pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 to disappear
Aug 11 14:30:47.971: INFO: Pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 14:30:47.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-77" for this suite. 08/11/23 14:30:47.977
------------------------------
• [4.091 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:30:43.893
    Aug 11 14:30:43.893: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:30:43.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:43.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:43.911
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/11/23 14:30:43.914
    Aug 11 14:30:43.922: INFO: Waiting up to 5m0s for pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73" in namespace "svcaccounts-77" to be "Succeeded or Failed"
    Aug 11 14:30:43.924: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503112ms
    Aug 11 14:30:45.929: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007051003s
    Aug 11 14:30:47.928: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006859828s
    STEP: Saw pod success 08/11/23 14:30:47.929
    Aug 11 14:30:47.929: INFO: Pod "test-pod-adb95697-e128-4f31-bce9-212b245e8e73" satisfied condition "Succeeded or Failed"
    Aug 11 14:30:47.932: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:30:47.952
    Aug 11 14:30:47.967: INFO: Waiting for pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 to disappear
    Aug 11 14:30:47.971: INFO: Pod test-pod-adb95697-e128-4f31-bce9-212b245e8e73 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:30:47.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-77" for this suite. 08/11/23 14:30:47.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:30:47.985
Aug 11 14:30:47.985: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:30:47.986
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:47.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:48.002
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 14:30:48.004
Aug 11 14:30:48.013: INFO: Waiting up to 5m0s for pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e" in namespace "emptydir-1110" to be "Succeeded or Failed"
Aug 11 14:30:48.017: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950554ms
Aug 11 14:30:50.022: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008764164s
Aug 11 14:30:52.021: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008540999s
STEP: Saw pod success 08/11/23 14:30:52.021
Aug 11 14:30:52.021: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e" satisfied condition "Succeeded or Failed"
Aug 11 14:30:52.024: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e container test-container: <nil>
STEP: delete the pod 08/11/23 14:30:52.033
Aug 11 14:30:52.048: INFO: Waiting for pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e to disappear
Aug 11 14:30:52.051: INFO: Pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:30:52.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1110" for this suite. 08/11/23 14:30:52.056
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:30:47.985
    Aug 11 14:30:47.985: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:30:47.986
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:47.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:48.002
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 14:30:48.004
    Aug 11 14:30:48.013: INFO: Waiting up to 5m0s for pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e" in namespace "emptydir-1110" to be "Succeeded or Failed"
    Aug 11 14:30:48.017: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.950554ms
    Aug 11 14:30:50.022: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008764164s
    Aug 11 14:30:52.021: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008540999s
    STEP: Saw pod success 08/11/23 14:30:52.021
    Aug 11 14:30:52.021: INFO: Pod "pod-aee7dba8-1911-46cb-b2f5-11255e08597e" satisfied condition "Succeeded or Failed"
    Aug 11 14:30:52.024: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e container test-container: <nil>
    STEP: delete the pod 08/11/23 14:30:52.033
    Aug 11 14:30:52.048: INFO: Waiting for pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e to disappear
    Aug 11 14:30:52.051: INFO: Pod pod-aee7dba8-1911-46cb-b2f5-11255e08597e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:30:52.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1110" for this suite. 08/11/23 14:30:52.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:30:52.065
Aug 11 14:30:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:30:52.065
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:52.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:52.086
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:30:52.093
Aug 11 14:30:52.102: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9758" to be "running and ready"
Aug 11 14:30:52.108: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.739966ms
Aug 11 14:30:52.108: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:30:54.113: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010960627s
Aug 11 14:30:54.113: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 14:30:54.113: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/11/23 14:30:54.116
Aug 11 14:30:54.123: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9758" to be "running and ready"
Aug 11 14:30:54.126: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292212ms
Aug 11 14:30:54.126: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:30:56.131: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007885803s
Aug 11 14:30:56.131: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 11 14:30:56.131: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/11/23 14:30:56.134
STEP: delete the pod with lifecycle hook 08/11/23 14:30:56.157
Aug 11 14:30:56.165: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 11 14:30:56.168: INFO: Pod pod-with-poststart-http-hook still exists
Aug 11 14:30:58.168: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 11 14:30:58.172: INFO: Pod pod-with-poststart-http-hook still exists
Aug 11 14:31:00.168: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 11 14:31:00.172: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 11 14:31:00.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9758" for this suite. 08/11/23 14:31:00.177
------------------------------
• [SLOW TEST] [8.120 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:30:52.065
    Aug 11 14:30:52.065: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:30:52.065
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:30:52.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:30:52.086
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:30:52.093
    Aug 11 14:30:52.102: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9758" to be "running and ready"
    Aug 11 14:30:52.108: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.739966ms
    Aug 11 14:30:52.108: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:30:54.113: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010960627s
    Aug 11 14:30:54.113: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 14:30:54.113: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/11/23 14:30:54.116
    Aug 11 14:30:54.123: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9758" to be "running and ready"
    Aug 11 14:30:54.126: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292212ms
    Aug 11 14:30:54.126: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:30:56.131: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007885803s
    Aug 11 14:30:56.131: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 11 14:30:56.131: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/11/23 14:30:56.134
    STEP: delete the pod with lifecycle hook 08/11/23 14:30:56.157
    Aug 11 14:30:56.165: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 11 14:30:56.168: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 11 14:30:58.168: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 11 14:30:58.172: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 11 14:31:00.168: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 11 14:31:00.172: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:31:00.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9758" for this suite. 08/11/23 14:31:00.177
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:31:00.185
Aug 11 14:31:00.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:31:00.186
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:31:00.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:31:00.208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:31:00.223
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:31:00.424
STEP: Deploying the webhook pod 08/11/23 14:31:00.437
STEP: Wait for the deployment to be ready 08/11/23 14:31:00.451
Aug 11 14:31:00.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:31:02.47
STEP: Verifying the service has paired with the endpoint 08/11/23 14:31:02.484
Aug 11 14:31:03.485: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/11/23 14:31:03.488
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:03.488
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/11/23 14:31:03.517
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/11/23 14:31:04.53
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:04.53
STEP: Having no error when timeout is longer than webhook latency 08/11/23 14:31:05.563
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:05.563
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/11/23 14:31:10.609
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:10.609
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:31:15.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-859" for this suite. 08/11/23 14:31:15.696
STEP: Destroying namespace "webhook-859-markers" for this suite. 08/11/23 14:31:15.705
------------------------------
• [SLOW TEST] [15.527 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:31:00.185
    Aug 11 14:31:00.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:31:00.186
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:31:00.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:31:00.208
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:31:00.223
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:31:00.424
    STEP: Deploying the webhook pod 08/11/23 14:31:00.437
    STEP: Wait for the deployment to be ready 08/11/23 14:31:00.451
    Aug 11 14:31:00.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:31:02.47
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:31:02.484
    Aug 11 14:31:03.485: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/11/23 14:31:03.488
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:03.488
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/11/23 14:31:03.517
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/11/23 14:31:04.53
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:04.53
    STEP: Having no error when timeout is longer than webhook latency 08/11/23 14:31:05.563
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:05.563
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/11/23 14:31:10.609
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 14:31:10.609
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:31:15.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-859" for this suite. 08/11/23 14:31:15.696
    STEP: Destroying namespace "webhook-859-markers" for this suite. 08/11/23 14:31:15.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:31:15.713
Aug 11 14:31:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-watch 08/11/23 14:31:15.714
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:31:15.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:31:15.735
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 11 14:31:15.738: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Creating first CR  08/11/23 14:31:18.284
Aug 11 14:31:18.290: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:18Z]] name:name1 resourceVersion:27054 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/11/23 14:31:28.292
Aug 11 14:31:28.300: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:28Z]] name:name2 resourceVersion:27128 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/11/23 14:31:38.302
Aug 11 14:31:38.310: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:38Z]] name:name1 resourceVersion:27184 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/11/23 14:31:48.312
Aug 11 14:31:48.318: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:48Z]] name:name2 resourceVersion:27242 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/11/23 14:31:58.321
Aug 11 14:31:58.329: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:38Z]] name:name1 resourceVersion:27300 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/11/23 14:32:08.331
Aug 11 14:32:08.340: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:48Z]] name:name2 resourceVersion:27354 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:18.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-4694" for this suite. 08/11/23 14:32:18.858
------------------------------
• [SLOW TEST] [63.152 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:31:15.713
    Aug 11 14:31:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-watch 08/11/23 14:31:15.714
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:31:15.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:31:15.735
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 11 14:31:15.738: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Creating first CR  08/11/23 14:31:18.284
    Aug 11 14:31:18.290: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:18Z]] name:name1 resourceVersion:27054 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/11/23 14:31:28.292
    Aug 11 14:31:28.300: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:28Z]] name:name2 resourceVersion:27128 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/11/23 14:31:38.302
    Aug 11 14:31:38.310: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:38Z]] name:name1 resourceVersion:27184 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/11/23 14:31:48.312
    Aug 11 14:31:48.318: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:48Z]] name:name2 resourceVersion:27242 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/11/23 14:31:58.321
    Aug 11 14:31:58.329: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:38Z]] name:name1 resourceVersion:27300 uid:85011f14-aec5-453e-8e55-edca974f8c91] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/11/23 14:32:08.331
    Aug 11 14:32:08.340: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:31:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:31:48Z]] name:name2 resourceVersion:27354 uid:3c903d05-7716-451b-b9d3-bd56625f027a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:18.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-4694" for this suite. 08/11/23 14:32:18.858
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:18.865
Aug 11 14:32:18.865: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:32:18.866
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:18.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:18.885
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:32:18.888
Aug 11 14:32:18.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07" in namespace "downward-api-2923" to be "Succeeded or Failed"
Aug 11 14:32:18.902: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526155ms
Aug 11 14:32:20.906: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009598713s
Aug 11 14:32:22.908: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011315283s
STEP: Saw pod success 08/11/23 14:32:22.908
Aug 11 14:32:22.908: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07" satisfied condition "Succeeded or Failed"
Aug 11 14:32:22.911: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 container client-container: <nil>
STEP: delete the pod 08/11/23 14:32:22.934
Aug 11 14:32:22.946: INFO: Waiting for pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 to disappear
Aug 11 14:32:22.949: INFO: Pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:22.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2923" for this suite. 08/11/23 14:32:22.954
------------------------------
• [4.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:18.865
    Aug 11 14:32:18.865: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:32:18.866
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:18.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:18.885
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:32:18.888
    Aug 11 14:32:18.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07" in namespace "downward-api-2923" to be "Succeeded or Failed"
    Aug 11 14:32:18.902: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526155ms
    Aug 11 14:32:20.906: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009598713s
    Aug 11 14:32:22.908: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011315283s
    STEP: Saw pod success 08/11/23 14:32:22.908
    Aug 11 14:32:22.908: INFO: Pod "downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07" satisfied condition "Succeeded or Failed"
    Aug 11 14:32:22.911: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:32:22.934
    Aug 11 14:32:22.946: INFO: Waiting for pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 to disappear
    Aug 11 14:32:22.949: INFO: Pod downwardapi-volume-a86c2181-c0b6-4f9a-9782-6cc4279fac07 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:22.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2923" for this suite. 08/11/23 14:32:22.954
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:22.962
Aug 11 14:32:22.962: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:32:22.963
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:22.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:22.982
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-836105be-6b28-48fc-b849-37ccf62962b6 08/11/23 14:32:22.984
STEP: Creating a pod to test consume secrets 08/11/23 14:32:22.989
Aug 11 14:32:22.998: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98" in namespace "projected-4595" to be "Succeeded or Failed"
Aug 11 14:32:23.001: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017633ms
Aug 11 14:32:25.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008107634s
Aug 11 14:32:27.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007376679s
STEP: Saw pod success 08/11/23 14:32:27.006
Aug 11 14:32:27.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98" satisfied condition "Succeeded or Failed"
Aug 11 14:32:27.009: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:32:27.017
Aug 11 14:32:27.029: INFO: Waiting for pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 to disappear
Aug 11 14:32:27.031: INFO: Pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:27.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4595" for this suite. 08/11/23 14:32:27.036
------------------------------
• [4.080 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:22.962
    Aug 11 14:32:22.962: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:32:22.963
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:22.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:22.982
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-836105be-6b28-48fc-b849-37ccf62962b6 08/11/23 14:32:22.984
    STEP: Creating a pod to test consume secrets 08/11/23 14:32:22.989
    Aug 11 14:32:22.998: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98" in namespace "projected-4595" to be "Succeeded or Failed"
    Aug 11 14:32:23.001: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017633ms
    Aug 11 14:32:25.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008107634s
    Aug 11 14:32:27.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007376679s
    STEP: Saw pod success 08/11/23 14:32:27.006
    Aug 11 14:32:27.006: INFO: Pod "pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98" satisfied condition "Succeeded or Failed"
    Aug 11 14:32:27.009: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:32:27.017
    Aug 11 14:32:27.029: INFO: Waiting for pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 to disappear
    Aug 11 14:32:27.031: INFO: Pod pod-projected-secrets-7e2ec5bd-0b8c-4caa-9f4c-2bd8a7ddbc98 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:27.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4595" for this suite. 08/11/23 14:32:27.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:27.042
Aug 11 14:32:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:32:27.043
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:27.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:27.061
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 11 14:32:27.077: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24" in namespace "kubelet-test-4280" to be "running and ready"
Aug 11 14:32:27.081: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990684ms
Aug 11 14:32:27.081: INFO: The phase of Pod busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:32:29.085: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24": Phase="Running", Reason="", readiness=true. Elapsed: 2.008024384s
Aug 11 14:32:29.085: INFO: The phase of Pod busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24 is Running (Ready = true)
Aug 11 14:32:29.085: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:29.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4280" for this suite. 08/11/23 14:32:29.102
------------------------------
• [2.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:27.042
    Aug 11 14:32:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:32:27.043
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:27.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:27.061
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 11 14:32:27.077: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24" in namespace "kubelet-test-4280" to be "running and ready"
    Aug 11 14:32:27.081: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990684ms
    Aug 11 14:32:27.081: INFO: The phase of Pod busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:32:29.085: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24": Phase="Running", Reason="", readiness=true. Elapsed: 2.008024384s
    Aug 11 14:32:29.085: INFO: The phase of Pod busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24 is Running (Ready = true)
    Aug 11 14:32:29.085: INFO: Pod "busybox-scheduling-c262823c-8e68-49e0-94b5-fb4eb9333c24" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:29.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4280" for this suite. 08/11/23 14:32:29.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:29.113
Aug 11 14:32:29.113: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename controllerrevisions 08/11/23 14:32:29.114
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:29.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:29.138
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-4xjsh-daemon-set" 08/11/23 14:32:29.155
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:32:29.161
Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:29.168: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 0
Aug 11 14:32:29.168: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:30.176: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 1
Aug 11 14:32:30.176: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:32:31.177: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 2
Aug 11 14:32:31.177: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-4xjsh-daemon-set
STEP: Confirm DaemonSet "e2e-4xjsh-daemon-set" successfully created with "daemonset-name=e2e-4xjsh-daemon-set" label 08/11/23 14:32:31.181
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4xjsh-daemon-set" 08/11/23 14:32:31.188
Aug 11 14:32:31.191: INFO: Located ControllerRevision: "e2e-4xjsh-daemon-set-76fc446dd6"
STEP: Patching ControllerRevision "e2e-4xjsh-daemon-set-76fc446dd6" 08/11/23 14:32:31.194
Aug 11 14:32:31.201: INFO: e2e-4xjsh-daemon-set-76fc446dd6 has been patched
STEP: Create a new ControllerRevision 08/11/23 14:32:31.201
Aug 11 14:32:31.207: INFO: Created ControllerRevision: e2e-4xjsh-daemon-set-5b8fdb44b8
STEP: Confirm that there are two ControllerRevisions 08/11/23 14:32:31.207
Aug 11 14:32:31.207: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 14:32:31.210: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-4xjsh-daemon-set-76fc446dd6" 08/11/23 14:32:31.21
STEP: Confirm that there is only one ControllerRevision 08/11/23 14:32:31.215
Aug 11 14:32:31.215: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 14:32:31.218: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-4xjsh-daemon-set-5b8fdb44b8" 08/11/23 14:32:31.22
Aug 11 14:32:31.230: INFO: e2e-4xjsh-daemon-set-5b8fdb44b8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/11/23 14:32:31.23
W0811 14:32:31.236009      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/11/23 14:32:31.236
Aug 11 14:32:31.236: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 14:32:32.242: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 14:32:32.245: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4xjsh-daemon-set-5b8fdb44b8=updated" 08/11/23 14:32:32.245
STEP: Confirm that there is only one ControllerRevision 08/11/23 14:32:32.253
Aug 11 14:32:32.253: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 14:32:32.255: INFO: Found 1 ControllerRevisions
Aug 11 14:32:32.258: INFO: ControllerRevision "e2e-4xjsh-daemon-set-754469685c" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-4xjsh-daemon-set" 08/11/23 14:32:32.26
STEP: deleting DaemonSet.extensions e2e-4xjsh-daemon-set in namespace controllerrevisions-3342, will wait for the garbage collector to delete the pods 08/11/23 14:32:32.26
Aug 11 14:32:32.320: INFO: Deleting DaemonSet.extensions e2e-4xjsh-daemon-set took: 6.796396ms
Aug 11 14:32:32.421: INFO: Terminating DaemonSet.extensions e2e-4xjsh-daemon-set pods took: 101.010839ms
Aug 11 14:32:34.424: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 0
Aug 11 14:32:34.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4xjsh-daemon-set
Aug 11 14:32:34.427: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27659"},"items":null}

Aug 11 14:32:34.429: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27659"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:34.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3342" for this suite. 08/11/23 14:32:34.442
------------------------------
• [SLOW TEST] [5.336 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:29.113
    Aug 11 14:32:29.113: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename controllerrevisions 08/11/23 14:32:29.114
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:29.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:29.138
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-4xjsh-daemon-set" 08/11/23 14:32:29.155
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:32:29.161
    Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:29.165: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:29.168: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 0
    Aug 11 14:32:29.168: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:30.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:30.176: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 1
    Aug 11 14:32:30.176: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:31.173: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:32:31.177: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 2
    Aug 11 14:32:31.177: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-4xjsh-daemon-set
    STEP: Confirm DaemonSet "e2e-4xjsh-daemon-set" successfully created with "daemonset-name=e2e-4xjsh-daemon-set" label 08/11/23 14:32:31.181
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4xjsh-daemon-set" 08/11/23 14:32:31.188
    Aug 11 14:32:31.191: INFO: Located ControllerRevision: "e2e-4xjsh-daemon-set-76fc446dd6"
    STEP: Patching ControllerRevision "e2e-4xjsh-daemon-set-76fc446dd6" 08/11/23 14:32:31.194
    Aug 11 14:32:31.201: INFO: e2e-4xjsh-daemon-set-76fc446dd6 has been patched
    STEP: Create a new ControllerRevision 08/11/23 14:32:31.201
    Aug 11 14:32:31.207: INFO: Created ControllerRevision: e2e-4xjsh-daemon-set-5b8fdb44b8
    STEP: Confirm that there are two ControllerRevisions 08/11/23 14:32:31.207
    Aug 11 14:32:31.207: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 14:32:31.210: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-4xjsh-daemon-set-76fc446dd6" 08/11/23 14:32:31.21
    STEP: Confirm that there is only one ControllerRevision 08/11/23 14:32:31.215
    Aug 11 14:32:31.215: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 14:32:31.218: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-4xjsh-daemon-set-5b8fdb44b8" 08/11/23 14:32:31.22
    Aug 11 14:32:31.230: INFO: e2e-4xjsh-daemon-set-5b8fdb44b8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/11/23 14:32:31.23
    W0811 14:32:31.236009      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/11/23 14:32:31.236
    Aug 11 14:32:31.236: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 14:32:32.242: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 14:32:32.245: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4xjsh-daemon-set-5b8fdb44b8=updated" 08/11/23 14:32:32.245
    STEP: Confirm that there is only one ControllerRevision 08/11/23 14:32:32.253
    Aug 11 14:32:32.253: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 14:32:32.255: INFO: Found 1 ControllerRevisions
    Aug 11 14:32:32.258: INFO: ControllerRevision "e2e-4xjsh-daemon-set-754469685c" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-4xjsh-daemon-set" 08/11/23 14:32:32.26
    STEP: deleting DaemonSet.extensions e2e-4xjsh-daemon-set in namespace controllerrevisions-3342, will wait for the garbage collector to delete the pods 08/11/23 14:32:32.26
    Aug 11 14:32:32.320: INFO: Deleting DaemonSet.extensions e2e-4xjsh-daemon-set took: 6.796396ms
    Aug 11 14:32:32.421: INFO: Terminating DaemonSet.extensions e2e-4xjsh-daemon-set pods took: 101.010839ms
    Aug 11 14:32:34.424: INFO: Number of nodes with available pods controlled by daemonset e2e-4xjsh-daemon-set: 0
    Aug 11 14:32:34.424: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4xjsh-daemon-set
    Aug 11 14:32:34.427: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27659"},"items":null}

    Aug 11 14:32:34.429: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27659"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:34.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3342" for this suite. 08/11/23 14:32:34.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:34.452
Aug 11 14:32:34.452: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:32:34.452
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:34.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:34.471
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 11 14:32:34.481: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 11 14:32:39.485: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:32:39.485
Aug 11 14:32:39.485: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/11/23 14:32:39.496
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:32:39.511: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9374  26223c06-0245-4a91-922b-04448d4ec881 27707 1 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e0eb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 11 14:32:39.518: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9374  c487d81b-83cd-4801-8c37-0ae2f7300744 27709 1 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 26223c06-0245-4a91-922b-04448d4ec881 0xc003e0f137 0xc003e0f138}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26223c06-0245-4a91-922b-04448d4ec881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e0f1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:32:39.518: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 11 14:32:39.518: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9374  c9d96392-3fde-43d4-b248-c0f1b4518d1d 27708 1 2023-08-11 14:32:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 26223c06-0245-4a91-922b-04448d4ec881 0xc003e0eff7 0xc003e0eff8}] [] [{e2e.test Update apps/v1 2023-08-11 14:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:32:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"26223c06-0245-4a91-922b-04448d4ec881\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e0f0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:32:39.525: INFO: Pod "test-cleanup-controller-9zbvm" is available:
&Pod{ObjectMeta:{test-cleanup-controller-9zbvm test-cleanup-controller- deployment-9374  5339c763-45dd-4330-90ab-6fa09b5d2c4d 27690 0 2023-08-11 14:32:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller c9d96392-3fde-43d4-b248-c0f1b4518d1d 0xc004007eb7 0xc004007eb8}] [] [{kube-controller-manager Update v1 2023-08-11 14:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9d96392-3fde-43d4-b248-c0f1b4518d1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:32:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27zj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27zj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.152,StartTime:2023-08-11 14:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:32:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2b808e5a986138e1cf73e0b00ce050afefab0e4929a7a9cdf2724d789d93efa5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:32:39.525: INFO: Pod "test-cleanup-deployment-7698ff6f6b-ww6qt" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-ww6qt test-cleanup-deployment-7698ff6f6b- deployment-9374  7c3428f6-220e-4171-a1f3-17986328a367 27714 0 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b c487d81b-83cd-4801-8c37-0ae2f7300744 0xc0042c6097 0xc0042c6098}] [] [{kube-controller-manager Update v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c487d81b-83cd-4801-8c37-0ae2f7300744\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84thk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84thk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:39.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9374" for this suite. 08/11/23 14:32:39.533
------------------------------
• [SLOW TEST] [5.094 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:34.452
    Aug 11 14:32:34.452: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:32:34.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:34.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:34.471
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 11 14:32:34.481: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 11 14:32:39.485: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:32:39.485
    Aug 11 14:32:39.485: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/11/23 14:32:39.496
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:32:39.511: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9374  26223c06-0245-4a91-922b-04448d4ec881 27707 1 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e0eb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 11 14:32:39.518: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-9374  c487d81b-83cd-4801-8c37-0ae2f7300744 27709 1 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 26223c06-0245-4a91-922b-04448d4ec881 0xc003e0f137 0xc003e0f138}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26223c06-0245-4a91-922b-04448d4ec881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e0f1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:32:39.518: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 11 14:32:39.518: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9374  c9d96392-3fde-43d4-b248-c0f1b4518d1d 27708 1 2023-08-11 14:32:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 26223c06-0245-4a91-922b-04448d4ec881 0xc003e0eff7 0xc003e0eff8}] [] [{e2e.test Update apps/v1 2023-08-11 14:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:32:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"26223c06-0245-4a91-922b-04448d4ec881\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e0f0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:32:39.525: INFO: Pod "test-cleanup-controller-9zbvm" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-9zbvm test-cleanup-controller- deployment-9374  5339c763-45dd-4330-90ab-6fa09b5d2c4d 27690 0 2023-08-11 14:32:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller c9d96392-3fde-43d4-b248-c0f1b4518d1d 0xc004007eb7 0xc004007eb8}] [] [{kube-controller-manager Update v1 2023-08-11 14:32:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9d96392-3fde-43d4-b248-c0f1b4518d1d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:32:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27zj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27zj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.152,StartTime:2023-08-11 14:32:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:32:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2b808e5a986138e1cf73e0b00ce050afefab0e4929a7a9cdf2724d789d93efa5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:32:39.525: INFO: Pod "test-cleanup-deployment-7698ff6f6b-ww6qt" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-ww6qt test-cleanup-deployment-7698ff6f6b- deployment-9374  7c3428f6-220e-4171-a1f3-17986328a367 27714 0 2023-08-11 14:32:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b c487d81b-83cd-4801-8c37-0ae2f7300744 0xc0042c6097 0xc0042c6098}] [] [{kube-controller-manager Update v1 2023-08-11 14:32:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c487d81b-83cd-4801-8c37-0ae2f7300744\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84thk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84thk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:32:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:39.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9374" for this suite. 08/11/23 14:32:39.533
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:39.546
Aug 11 14:32:39.546: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:32:39.547
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:39.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:39.565
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/11/23 14:32:39.568
Aug 11 14:32:39.568: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-8060 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/11/23 14:32:39.603
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:39.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8060" for this suite. 08/11/23 14:32:39.613
------------------------------
• [0.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:39.546
    Aug 11 14:32:39.546: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:32:39.547
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:39.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:39.565
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/11/23 14:32:39.568
    Aug 11 14:32:39.568: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-8060 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/11/23 14:32:39.603
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:39.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8060" for this suite. 08/11/23 14:32:39.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:39.62
Aug 11 14:32:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:32:39.621
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:39.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:39.637
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-1073cbf6-41e6-47c6-8e85-536930ffdc81 08/11/23 14:32:39.639
STEP: Creating a pod to test consume configMaps 08/11/23 14:32:39.643
Aug 11 14:32:39.651: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439" in namespace "projected-8916" to be "Succeeded or Failed"
Aug 11 14:32:39.657: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Pending", Reason="", readiness=false. Elapsed: 5.603945ms
Aug 11 14:32:41.661: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009939175s
Aug 11 14:32:43.662: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010385652s
STEP: Saw pod success 08/11/23 14:32:43.662
Aug 11 14:32:43.662: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439" satisfied condition "Succeeded or Failed"
Aug 11 14:32:43.665: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:32:43.682
Aug 11 14:32:43.694: INFO: Waiting for pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 to disappear
Aug 11 14:32:43.697: INFO: Pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:43.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8916" for this suite. 08/11/23 14:32:43.701
------------------------------
• [4.088 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:39.62
    Aug 11 14:32:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:32:39.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:39.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:39.637
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-1073cbf6-41e6-47c6-8e85-536930ffdc81 08/11/23 14:32:39.639
    STEP: Creating a pod to test consume configMaps 08/11/23 14:32:39.643
    Aug 11 14:32:39.651: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439" in namespace "projected-8916" to be "Succeeded or Failed"
    Aug 11 14:32:39.657: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Pending", Reason="", readiness=false. Elapsed: 5.603945ms
    Aug 11 14:32:41.661: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009939175s
    Aug 11 14:32:43.662: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010385652s
    STEP: Saw pod success 08/11/23 14:32:43.662
    Aug 11 14:32:43.662: INFO: Pod "pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439" satisfied condition "Succeeded or Failed"
    Aug 11 14:32:43.665: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:32:43.682
    Aug 11 14:32:43.694: INFO: Waiting for pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 to disappear
    Aug 11 14:32:43.697: INFO: Pod pod-projected-configmaps-5917a2a5-69ee-4213-b536-07afd06df439 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:43.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8916" for this suite. 08/11/23 14:32:43.701
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:43.708
Aug 11 14:32:43.708: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:32:43.709
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:43.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:43.725
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/11/23 14:32:43.728
Aug 11 14:32:43.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 11 14:32:43.782: INFO: stderr: ""
Aug 11 14:32:43.782: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/11/23 14:32:43.782
Aug 11 14:32:43.782: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 11 14:32:43.782: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9038" to be "running and ready, or succeeded"
Aug 11 14:32:43.786: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511063ms
Aug 11 14:32:43.786: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'constell-1cf5d931-worker-6381a7ba-nd80' to be 'Running' but was 'Pending'
Aug 11 14:32:45.790: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007745884s
Aug 11 14:32:45.790: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 11 14:32:45.790: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/11/23 14:32:45.79
Aug 11 14:32:45.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator'
Aug 11 14:32:45.848: INFO: stderr: ""
Aug 11 14:32:45.848: INFO: stdout: "I0811 14:32:44.684097       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kcw8 311\nI0811 14:32:44.884523       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sxl 345\nI0811 14:32:45.084879       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/485 222\nI0811 14:32:45.284156       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/9dk4 532\nI0811 14:32:45.484541       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qvx 325\nI0811 14:32:45.684913       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/7bl 472\nI0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
STEP: limiting log lines 08/11/23 14:32:45.848
Aug 11 14:32:45.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --tail=1'
Aug 11 14:32:45.907: INFO: stderr: ""
Aug 11 14:32:45.907: INFO: stdout: "I0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
Aug 11 14:32:45.907: INFO: got output "I0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
STEP: limiting log bytes 08/11/23 14:32:45.907
Aug 11 14:32:45.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --limit-bytes=1'
Aug 11 14:32:45.964: INFO: stderr: ""
Aug 11 14:32:45.964: INFO: stdout: "I"
Aug 11 14:32:45.964: INFO: got output "I"
STEP: exposing timestamps 08/11/23 14:32:45.964
Aug 11 14:32:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 11 14:32:46.030: INFO: stderr: ""
Aug 11 14:32:46.030: INFO: stdout: "2023-08-11T14:32:46.084705634Z I0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\n"
Aug 11 14:32:46.030: INFO: got output "2023-08-11T14:32:46.084705634Z I0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\n"
STEP: restricting to a time range 08/11/23 14:32:46.03
Aug 11 14:32:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --since=1s'
Aug 11 14:32:48.605: INFO: stderr: ""
Aug 11 14:32:48.605: INFO: stdout: "I0811 14:32:47.884661       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rwd 200\nI0811 14:32:48.085086       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/n8f2 556\nI0811 14:32:48.284520       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jlw5 484\nI0811 14:32:48.484898       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/psz 597\nI0811 14:32:48.684240       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/wj5w 413\n"
Aug 11 14:32:48.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --since=24h'
Aug 11 14:32:48.667: INFO: stderr: ""
Aug 11 14:32:48.667: INFO: stdout: "I0811 14:32:44.684097       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kcw8 311\nI0811 14:32:44.884523       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sxl 345\nI0811 14:32:45.084879       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/485 222\nI0811 14:32:45.284156       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/9dk4 532\nI0811 14:32:45.484541       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qvx 325\nI0811 14:32:45.684913       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/7bl 472\nI0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\nI0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\nI0811 14:32:46.284900       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/546m 399\nI0811 14:32:46.484224       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/8nc 337\nI0811 14:32:46.684593       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/6blb 459\nI0811 14:32:46.884987       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/bqdv 480\nI0811 14:32:47.084278       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/b9l 565\nI0811 14:32:47.284671       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/zn7c 366\nI0811 14:32:47.485012       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/72pd 511\nI0811 14:32:47.684317       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/49f 370\nI0811 14:32:47.884661       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rwd 200\nI0811 14:32:48.085086       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/n8f2 556\nI0811 14:32:48.284520       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jlw5 484\nI0811 14:32:48.484898       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/psz 597\nI0811 14:32:48.684240       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/wj5w 413\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 11 14:32:48.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 delete pod logs-generator'
Aug 11 14:32:49.317: INFO: stderr: ""
Aug 11 14:32:49.317: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:49.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9038" for this suite. 08/11/23 14:32:49.323
------------------------------
• [SLOW TEST] [5.621 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:43.708
    Aug 11 14:32:43.708: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:32:43.709
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:43.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:43.725
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/11/23 14:32:43.728
    Aug 11 14:32:43.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 11 14:32:43.782: INFO: stderr: ""
    Aug 11 14:32:43.782: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/11/23 14:32:43.782
    Aug 11 14:32:43.782: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 11 14:32:43.782: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9038" to be "running and ready, or succeeded"
    Aug 11 14:32:43.786: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511063ms
    Aug 11 14:32:43.786: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'constell-1cf5d931-worker-6381a7ba-nd80' to be 'Running' but was 'Pending'
    Aug 11 14:32:45.790: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007745884s
    Aug 11 14:32:45.790: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 11 14:32:45.790: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/11/23 14:32:45.79
    Aug 11 14:32:45.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator'
    Aug 11 14:32:45.848: INFO: stderr: ""
    Aug 11 14:32:45.848: INFO: stdout: "I0811 14:32:44.684097       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kcw8 311\nI0811 14:32:44.884523       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sxl 345\nI0811 14:32:45.084879       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/485 222\nI0811 14:32:45.284156       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/9dk4 532\nI0811 14:32:45.484541       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qvx 325\nI0811 14:32:45.684913       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/7bl 472\nI0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
    STEP: limiting log lines 08/11/23 14:32:45.848
    Aug 11 14:32:45.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --tail=1'
    Aug 11 14:32:45.907: INFO: stderr: ""
    Aug 11 14:32:45.907: INFO: stdout: "I0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
    Aug 11 14:32:45.907: INFO: got output "I0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\n"
    STEP: limiting log bytes 08/11/23 14:32:45.907
    Aug 11 14:32:45.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --limit-bytes=1'
    Aug 11 14:32:45.964: INFO: stderr: ""
    Aug 11 14:32:45.964: INFO: stdout: "I"
    Aug 11 14:32:45.964: INFO: got output "I"
    STEP: exposing timestamps 08/11/23 14:32:45.964
    Aug 11 14:32:45.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 11 14:32:46.030: INFO: stderr: ""
    Aug 11 14:32:46.030: INFO: stdout: "2023-08-11T14:32:46.084705634Z I0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\n"
    Aug 11 14:32:46.030: INFO: got output "2023-08-11T14:32:46.084705634Z I0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\n"
    STEP: restricting to a time range 08/11/23 14:32:46.03
    Aug 11 14:32:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --since=1s'
    Aug 11 14:32:48.605: INFO: stderr: ""
    Aug 11 14:32:48.605: INFO: stdout: "I0811 14:32:47.884661       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rwd 200\nI0811 14:32:48.085086       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/n8f2 556\nI0811 14:32:48.284520       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jlw5 484\nI0811 14:32:48.484898       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/psz 597\nI0811 14:32:48.684240       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/wj5w 413\n"
    Aug 11 14:32:48.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 logs logs-generator logs-generator --since=24h'
    Aug 11 14:32:48.667: INFO: stderr: ""
    Aug 11 14:32:48.667: INFO: stdout: "I0811 14:32:44.684097       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/kcw8 311\nI0811 14:32:44.884523       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sxl 345\nI0811 14:32:45.084879       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/485 222\nI0811 14:32:45.284156       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/9dk4 532\nI0811 14:32:45.484541       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/qvx 325\nI0811 14:32:45.684913       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/7bl 472\nI0811 14:32:45.884225       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/kwx 514\nI0811 14:32:46.084581       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/6t4 591\nI0811 14:32:46.284900       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/546m 399\nI0811 14:32:46.484224       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/8nc 337\nI0811 14:32:46.684593       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/6blb 459\nI0811 14:32:46.884987       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/bqdv 480\nI0811 14:32:47.084278       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/b9l 565\nI0811 14:32:47.284671       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/zn7c 366\nI0811 14:32:47.485012       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/72pd 511\nI0811 14:32:47.684317       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/49f 370\nI0811 14:32:47.884661       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rwd 200\nI0811 14:32:48.085086       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/n8f2 556\nI0811 14:32:48.284520       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jlw5 484\nI0811 14:32:48.484898       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/psz 597\nI0811 14:32:48.684240       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/wj5w 413\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 11 14:32:48.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9038 delete pod logs-generator'
    Aug 11 14:32:49.317: INFO: stderr: ""
    Aug 11 14:32:49.317: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:49.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9038" for this suite. 08/11/23 14:32:49.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:49.329
Aug 11 14:32:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:32:49.33
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:49.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:49.346
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-4382f74b-9605-4dc4-ac4e-9e3f062b5edb 08/11/23 14:32:49.348
STEP: Creating a pod to test consume secrets 08/11/23 14:32:49.353
Aug 11 14:32:49.361: INFO: Waiting up to 5m0s for pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3" in namespace "secrets-6003" to be "Succeeded or Failed"
Aug 11 14:32:49.366: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960484ms
Aug 11 14:32:51.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011065746s
Aug 11 14:32:53.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010587023s
STEP: Saw pod success 08/11/23 14:32:53.372
Aug 11 14:32:53.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3" satisfied condition "Succeeded or Failed"
Aug 11 14:32:53.375: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:32:53.383
Aug 11 14:32:53.396: INFO: Waiting for pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 to disappear
Aug 11 14:32:53.398: INFO: Pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:53.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6003" for this suite. 08/11/23 14:32:53.402
------------------------------
• [4.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:49.329
    Aug 11 14:32:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:32:49.33
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:49.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:49.346
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-4382f74b-9605-4dc4-ac4e-9e3f062b5edb 08/11/23 14:32:49.348
    STEP: Creating a pod to test consume secrets 08/11/23 14:32:49.353
    Aug 11 14:32:49.361: INFO: Waiting up to 5m0s for pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3" in namespace "secrets-6003" to be "Succeeded or Failed"
    Aug 11 14:32:49.366: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960484ms
    Aug 11 14:32:51.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011065746s
    Aug 11 14:32:53.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010587023s
    STEP: Saw pod success 08/11/23 14:32:53.372
    Aug 11 14:32:53.372: INFO: Pod "pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3" satisfied condition "Succeeded or Failed"
    Aug 11 14:32:53.375: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:32:53.383
    Aug 11 14:32:53.396: INFO: Waiting for pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 to disappear
    Aug 11 14:32:53.398: INFO: Pod pod-secrets-27fb8791-8107-4e27-be7b-b55b02d429b3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:53.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6003" for this suite. 08/11/23 14:32:53.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:53.411
Aug 11 14:32:53.411: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 14:32:53.411
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:53.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:53.428
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/11/23 14:32:53.43
Aug 11 14:32:53.438: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 14:32:58.443: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:32:58.443
STEP: getting scale subresource 08/11/23 14:32:58.444
STEP: updating a scale subresource 08/11/23 14:32:58.447
STEP: verifying the replicaset Spec.Replicas was modified 08/11/23 14:32:58.453
STEP: Patch a scale subresource 08/11/23 14:32:58.458
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:32:58.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9281" for this suite. 08/11/23 14:32:58.496
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:53.411
    Aug 11 14:32:53.411: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:32:53.411
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:53.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:53.428
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/11/23 14:32:53.43
    Aug 11 14:32:53.438: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 14:32:58.443: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:32:58.443
    STEP: getting scale subresource 08/11/23 14:32:58.444
    STEP: updating a scale subresource 08/11/23 14:32:58.447
    STEP: verifying the replicaset Spec.Replicas was modified 08/11/23 14:32:58.453
    STEP: Patch a scale subresource 08/11/23 14:32:58.458
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:32:58.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9281" for this suite. 08/11/23 14:32:58.496
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:32:58.51
Aug 11 14:32:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:32:58.511
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:58.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:58.53
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/11/23 14:32:58.533
Aug 11 14:32:58.542: INFO: Waiting up to 5m0s for pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e" in namespace "projected-5443" to be "running and ready"
Aug 11 14:32:58.548: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.707936ms
Aug 11 14:32:58.548: INFO: The phase of Pod annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:33:00.553: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010772107s
Aug 11 14:33:00.553: INFO: The phase of Pod annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e is Running (Ready = true)
Aug 11 14:33:00.553: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e" satisfied condition "running and ready"
Aug 11 14:33:01.079: INFO: Successfully updated pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:33:03.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5443" for this suite. 08/11/23 14:33:03.104
------------------------------
• [4.601 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:32:58.51
    Aug 11 14:32:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:32:58.511
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:32:58.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:32:58.53
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/11/23 14:32:58.533
    Aug 11 14:32:58.542: INFO: Waiting up to 5m0s for pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e" in namespace "projected-5443" to be "running and ready"
    Aug 11 14:32:58.548: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.707936ms
    Aug 11 14:32:58.548: INFO: The phase of Pod annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:33:00.553: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010772107s
    Aug 11 14:33:00.553: INFO: The phase of Pod annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e is Running (Ready = true)
    Aug 11 14:33:00.553: INFO: Pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e" satisfied condition "running and ready"
    Aug 11 14:33:01.079: INFO: Successfully updated pod "annotationupdatef62d1cd0-759b-4efe-8a4f-93c1bd910f4e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:33:03.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5443" for this suite. 08/11/23 14:33:03.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:33:03.112
Aug 11 14:33:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename watch 08/11/23 14:33:03.113
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:03.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:03.131
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/11/23 14:33:03.133
STEP: creating a new configmap 08/11/23 14:33:03.134
STEP: modifying the configmap once 08/11/23 14:33:03.139
STEP: changing the label value of the configmap 08/11/23 14:33:03.146
STEP: Expecting to observe a delete notification for the watched object 08/11/23 14:33:03.153
Aug 11 14:33:03.153: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28064 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:33:03.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28065 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:33:03.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28066 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/11/23 14:33:03.154
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/11/23 14:33:03.162
STEP: changing the label value of the configmap back 08/11/23 14:33:13.163
STEP: modifying the configmap a third time 08/11/23 14:33:13.175
STEP: deleting the configmap 08/11/23 14:33:13.183
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/11/23 14:33:13.191
Aug 11 14:33:13.191: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28175 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:33:13.191: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28176 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:33:13.192: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28177 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:33:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3535" for this suite. 08/11/23 14:33:13.196
------------------------------
• [SLOW TEST] [10.090 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:33:03.112
    Aug 11 14:33:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename watch 08/11/23 14:33:03.113
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:03.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:03.131
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/11/23 14:33:03.133
    STEP: creating a new configmap 08/11/23 14:33:03.134
    STEP: modifying the configmap once 08/11/23 14:33:03.139
    STEP: changing the label value of the configmap 08/11/23 14:33:03.146
    STEP: Expecting to observe a delete notification for the watched object 08/11/23 14:33:03.153
    Aug 11 14:33:03.153: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28064 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:33:03.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28065 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:33:03.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28066 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/11/23 14:33:03.154
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/11/23 14:33:03.162
    STEP: changing the label value of the configmap back 08/11/23 14:33:13.163
    STEP: modifying the configmap a third time 08/11/23 14:33:13.175
    STEP: deleting the configmap 08/11/23 14:33:13.183
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/11/23 14:33:13.191
    Aug 11 14:33:13.191: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28175 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:33:13.191: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28176 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:33:13.192: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3535  e16a83d6-ff63-465f-9f20-62c9aef7e58a 28177 0 2023-08-11 14:33:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:33:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:33:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3535" for this suite. 08/11/23 14:33:13.196
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:33:13.202
Aug 11 14:33:13.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:33:13.203
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:13.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:13.219
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1561/secret-test-979e7460-5237-4cd3-aed2-aadbf6503b0f 08/11/23 14:33:13.222
STEP: Creating a pod to test consume secrets 08/11/23 14:33:13.229
Aug 11 14:33:13.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a" in namespace "secrets-1561" to be "Succeeded or Failed"
Aug 11 14:33:13.241: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798904ms
Aug 11 14:33:15.246: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009252925s
Aug 11 14:33:17.246: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009806412s
STEP: Saw pod success 08/11/23 14:33:17.246
Aug 11 14:33:17.247: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a" satisfied condition "Succeeded or Failed"
Aug 11 14:33:17.250: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a container env-test: <nil>
STEP: delete the pod 08/11/23 14:33:17.26
Aug 11 14:33:17.275: INFO: Waiting for pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a to disappear
Aug 11 14:33:17.281: INFO: Pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:33:17.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1561" for this suite. 08/11/23 14:33:17.285
------------------------------
• [4.089 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:33:13.202
    Aug 11 14:33:13.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:33:13.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:13.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:13.219
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1561/secret-test-979e7460-5237-4cd3-aed2-aadbf6503b0f 08/11/23 14:33:13.222
    STEP: Creating a pod to test consume secrets 08/11/23 14:33:13.229
    Aug 11 14:33:13.237: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a" in namespace "secrets-1561" to be "Succeeded or Failed"
    Aug 11 14:33:13.241: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798904ms
    Aug 11 14:33:15.246: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009252925s
    Aug 11 14:33:17.246: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009806412s
    STEP: Saw pod success 08/11/23 14:33:17.246
    Aug 11 14:33:17.247: INFO: Pod "pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a" satisfied condition "Succeeded or Failed"
    Aug 11 14:33:17.250: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a container env-test: <nil>
    STEP: delete the pod 08/11/23 14:33:17.26
    Aug 11 14:33:17.275: INFO: Waiting for pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a to disappear
    Aug 11 14:33:17.281: INFO: Pod pod-configmaps-9d7a14e7-91b4-4cf9-ba15-a306dda2649a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:33:17.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1561" for this suite. 08/11/23 14:33:17.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:33:17.292
Aug 11 14:33:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:33:17.293
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:17.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:17.312
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c in namespace container-probe-893 08/11/23 14:33:17.314
Aug 11 14:33:17.322: INFO: Waiting up to 5m0s for pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c" in namespace "container-probe-893" to be "not pending"
Aug 11 14:33:17.327: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910464ms
Aug 11 14:33:19.332: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009985863s
Aug 11 14:33:19.332: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c" satisfied condition "not pending"
Aug 11 14:33:19.332: INFO: Started pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c in namespace container-probe-893
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:33:19.332
Aug 11 14:33:19.335: INFO: Initial restart count of pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c is 0
STEP: deleting the pod 08/11/23 14:37:19.905
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:19.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-893" for this suite. 08/11/23 14:37:19.928
------------------------------
• [SLOW TEST] [242.643 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:33:17.292
    Aug 11 14:33:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:33:17.293
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:17.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:17.312
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c in namespace container-probe-893 08/11/23 14:33:17.314
    Aug 11 14:33:17.322: INFO: Waiting up to 5m0s for pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c" in namespace "container-probe-893" to be "not pending"
    Aug 11 14:33:17.327: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.910464ms
    Aug 11 14:33:19.332: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009985863s
    Aug 11 14:33:19.332: INFO: Pod "test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c" satisfied condition "not pending"
    Aug 11 14:33:19.332: INFO: Started pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c in namespace container-probe-893
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:33:19.332
    Aug 11 14:33:19.335: INFO: Initial restart count of pod test-webserver-fbe9822d-d8c6-479e-b385-f762ea30648c is 0
    STEP: deleting the pod 08/11/23 14:37:19.905
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:19.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-893" for this suite. 08/11/23 14:37:19.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:19.939
Aug 11 14:37:19.939: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:37:19.94
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:19.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:19.958
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/11/23 14:37:19.961
Aug 11 14:37:19.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 create -f -'
Aug 11 14:37:20.722: INFO: stderr: ""
Aug 11 14:37:20.722: INFO: stdout: "pod/pause created\n"
Aug 11 14:37:20.722: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 11 14:37:20.722: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-266" to be "running and ready"
Aug 11 14:37:20.728: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114417ms
Aug 11 14:37:20.728: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'constell-1cf5d931-worker-6381a7ba-nd80' to be 'Running' but was 'Pending'
Aug 11 14:37:22.733: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010752386s
Aug 11 14:37:22.733: INFO: Pod "pause" satisfied condition "running and ready"
Aug 11 14:37:22.733: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/11/23 14:37:22.733
Aug 11 14:37:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 label pods pause testing-label=testing-label-value'
Aug 11 14:37:22.791: INFO: stderr: ""
Aug 11 14:37:22.791: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/11/23 14:37:22.791
Aug 11 14:37:22.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pod pause -L testing-label'
Aug 11 14:37:22.849: INFO: stderr: ""
Aug 11 14:37:22.849: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/11/23 14:37:22.849
Aug 11 14:37:22.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 label pods pause testing-label-'
Aug 11 14:37:22.909: INFO: stderr: ""
Aug 11 14:37:22.909: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/11/23 14:37:22.909
Aug 11 14:37:22.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pod pause -L testing-label'
Aug 11 14:37:22.958: INFO: stderr: ""
Aug 11 14:37:22.958: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/11/23 14:37:22.958
Aug 11 14:37:22.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 delete --grace-period=0 --force -f -'
Aug 11 14:37:23.018: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 14:37:23.018: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 11 14:37:23.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get rc,svc -l name=pause --no-headers'
Aug 11 14:37:23.073: INFO: stderr: "No resources found in kubectl-266 namespace.\n"
Aug 11 14:37:23.073: INFO: stdout: ""
Aug 11 14:37:23.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 14:37:23.125: INFO: stderr: ""
Aug 11 14:37:23.125: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-266" for this suite. 08/11/23 14:37:23.13
------------------------------
• [3.198 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:19.939
    Aug 11 14:37:19.939: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:37:19.94
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:19.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:19.958
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/11/23 14:37:19.961
    Aug 11 14:37:19.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 create -f -'
    Aug 11 14:37:20.722: INFO: stderr: ""
    Aug 11 14:37:20.722: INFO: stdout: "pod/pause created\n"
    Aug 11 14:37:20.722: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 11 14:37:20.722: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-266" to be "running and ready"
    Aug 11 14:37:20.728: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.114417ms
    Aug 11 14:37:20.728: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'constell-1cf5d931-worker-6381a7ba-nd80' to be 'Running' but was 'Pending'
    Aug 11 14:37:22.733: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010752386s
    Aug 11 14:37:22.733: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 11 14:37:22.733: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/11/23 14:37:22.733
    Aug 11 14:37:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 label pods pause testing-label=testing-label-value'
    Aug 11 14:37:22.791: INFO: stderr: ""
    Aug 11 14:37:22.791: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/11/23 14:37:22.791
    Aug 11 14:37:22.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pod pause -L testing-label'
    Aug 11 14:37:22.849: INFO: stderr: ""
    Aug 11 14:37:22.849: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/11/23 14:37:22.849
    Aug 11 14:37:22.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 label pods pause testing-label-'
    Aug 11 14:37:22.909: INFO: stderr: ""
    Aug 11 14:37:22.909: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/11/23 14:37:22.909
    Aug 11 14:37:22.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pod pause -L testing-label'
    Aug 11 14:37:22.958: INFO: stderr: ""
    Aug 11 14:37:22.958: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/11/23 14:37:22.958
    Aug 11 14:37:22.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 delete --grace-period=0 --force -f -'
    Aug 11 14:37:23.018: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 14:37:23.018: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 11 14:37:23.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get rc,svc -l name=pause --no-headers'
    Aug 11 14:37:23.073: INFO: stderr: "No resources found in kubectl-266 namespace.\n"
    Aug 11 14:37:23.073: INFO: stdout: ""
    Aug 11 14:37:23.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-266 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 14:37:23.125: INFO: stderr: ""
    Aug 11 14:37:23.125: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:23.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-266" for this suite. 08/11/23 14:37:23.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:23.138
Aug 11 14:37:23.138: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename conformance-tests 08/11/23 14:37:23.139
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:23.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:23.157
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/11/23 14:37:23.16
Aug 11 14:37:23.160: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:23.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-2789" for this suite. 08/11/23 14:37:23.173
------------------------------
• [0.042 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:23.138
    Aug 11 14:37:23.138: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename conformance-tests 08/11/23 14:37:23.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:23.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:23.157
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/11/23 14:37:23.16
    Aug 11 14:37:23.160: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:23.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-2789" for this suite. 08/11/23 14:37:23.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:23.181
Aug 11 14:37:23.182: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:37:23.182
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:23.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:23.2
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 in namespace container-probe-915 08/11/23 14:37:23.203
Aug 11 14:37:23.212: INFO: Waiting up to 5m0s for pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61" in namespace "container-probe-915" to be "not pending"
Aug 11 14:37:23.216: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150914ms
Aug 11 14:37:25.222: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61": Phase="Running", Reason="", readiness=true. Elapsed: 2.009824636s
Aug 11 14:37:25.222: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61" satisfied condition "not pending"
Aug 11 14:37:25.222: INFO: Started pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 in namespace container-probe-915
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:37:25.222
Aug 11 14:37:25.225: INFO: Initial restart count of pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 is 0
Aug 11 14:37:45.280: INFO: Restart count of pod container-probe-915/liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 is now 1 (20.054537804s elapsed)
STEP: deleting the pod 08/11/23 14:37:45.28
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:45.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-915" for this suite. 08/11/23 14:37:45.295
------------------------------
• [SLOW TEST] [22.121 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:23.181
    Aug 11 14:37:23.182: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:37:23.182
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:23.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:23.2
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 in namespace container-probe-915 08/11/23 14:37:23.203
    Aug 11 14:37:23.212: INFO: Waiting up to 5m0s for pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61" in namespace "container-probe-915" to be "not pending"
    Aug 11 14:37:23.216: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150914ms
    Aug 11 14:37:25.222: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61": Phase="Running", Reason="", readiness=true. Elapsed: 2.009824636s
    Aug 11 14:37:25.222: INFO: Pod "liveness-02265b5c-85d3-4363-8f88-17e10bed9e61" satisfied condition "not pending"
    Aug 11 14:37:25.222: INFO: Started pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 in namespace container-probe-915
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:37:25.222
    Aug 11 14:37:25.225: INFO: Initial restart count of pod liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 is 0
    Aug 11 14:37:45.280: INFO: Restart count of pod container-probe-915/liveness-02265b5c-85d3-4363-8f88-17e10bed9e61 is now 1 (20.054537804s elapsed)
    STEP: deleting the pod 08/11/23 14:37:45.28
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:45.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-915" for this suite. 08/11/23 14:37:45.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:45.304
Aug 11 14:37:45.304: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename init-container 08/11/23 14:37:45.305
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:45.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:45.323
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/11/23 14:37:45.326
Aug 11 14:37:45.326: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:49.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3676" for this suite. 08/11/23 14:37:49.633
------------------------------
• [4.343 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:45.304
    Aug 11 14:37:45.304: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename init-container 08/11/23 14:37:45.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:45.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:45.323
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/11/23 14:37:45.326
    Aug 11 14:37:45.326: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:49.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3676" for this suite. 08/11/23 14:37:49.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:49.648
Aug 11 14:37:49.648: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:37:49.649
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:49.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:49.664
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Aug 11 14:37:49.684: INFO: Create a RollingUpdate DaemonSet
Aug 11 14:37:49.690: INFO: Check that daemon pods launch on every node of the cluster
Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:49.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:37:49.698: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:50.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:37:50.706: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:51.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:37:51.708: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Aug 11 14:37:51.708: INFO: Update the DaemonSet to trigger a rollout
Aug 11 14:37:51.717: INFO: Updating DaemonSet daemon-set
Aug 11 14:37:53.736: INFO: Roll back the DaemonSet before rollout is complete
Aug 11 14:37:53.746: INFO: Updating DaemonSet daemon-set
Aug 11 14:37:53.746: INFO: Make sure DaemonSet rollback is complete
Aug 11 14:37:53.749: INFO: Wrong image for pod: daemon-set-c89s6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 11 14:37:53.749: INFO: Pod daemon-set-c89s6 is not available
Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:55.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:55.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:55.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:56.761: INFO: Pod daemon-set-qmv6r is not available
Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:37:56.772
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4669, will wait for the garbage collector to delete the pods 08/11/23 14:37:56.772
Aug 11 14:37:56.832: INFO: Deleting DaemonSet.extensions daemon-set took: 6.285416ms
Aug 11 14:37:56.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.949879ms
Aug 11 14:37:58.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:37:58.738: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:37:58.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30066"},"items":null}

Aug 11 14:37:58.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30066"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:37:58.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4669" for this suite. 08/11/23 14:37:58.757
------------------------------
• [SLOW TEST] [9.115 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:49.648
    Aug 11 14:37:49.648: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:37:49.649
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:49.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:49.664
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Aug 11 14:37:49.684: INFO: Create a RollingUpdate DaemonSet
    Aug 11 14:37:49.690: INFO: Check that daemon pods launch on every node of the cluster
    Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:49.695: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:49.698: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:37:49.698: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:50.703: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:50.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:37:50.706: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:51.704: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:51.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:37:51.708: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Aug 11 14:37:51.708: INFO: Update the DaemonSet to trigger a rollout
    Aug 11 14:37:51.717: INFO: Updating DaemonSet daemon-set
    Aug 11 14:37:53.736: INFO: Roll back the DaemonSet before rollout is complete
    Aug 11 14:37:53.746: INFO: Updating DaemonSet daemon-set
    Aug 11 14:37:53.746: INFO: Make sure DaemonSet rollback is complete
    Aug 11 14:37:53.749: INFO: Wrong image for pod: daemon-set-c89s6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 11 14:37:53.749: INFO: Pod daemon-set-c89s6 is not available
    Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:53.755: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:54.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:55.764: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:55.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:55.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:56.761: INFO: Pod daemon-set-qmv6r is not available
    Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:37:56.765: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:37:56.772
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4669, will wait for the garbage collector to delete the pods 08/11/23 14:37:56.772
    Aug 11 14:37:56.832: INFO: Deleting DaemonSet.extensions daemon-set took: 6.285416ms
    Aug 11 14:37:56.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.949879ms
    Aug 11 14:37:58.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:37:58.738: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:37:58.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30066"},"items":null}

    Aug 11 14:37:58.743: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30066"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:37:58.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4669" for this suite. 08/11/23 14:37:58.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:37:58.764
Aug 11 14:37:58.764: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:37:58.765
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:58.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:58.78
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:37:58.783
Aug 11 14:37:58.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 11 14:37:58.837: INFO: stderr: ""
Aug 11 14:37:58.837: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/11/23 14:37:58.837
STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:38:03.888
Aug 11 14:38:03.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 get pod e2e-test-httpd-pod -o json'
Aug 11 14:38:03.940: INFO: stderr: ""
Aug 11 14:38:03.940: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-11T14:37:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7349\",\n        \"resourceVersion\": \"30087\",\n        \"uid\": \"52d008be-3cb8-4cc8-a0f5-ad0ab40618b7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vc6xk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"constell-1cf5d931-worker-6381a7ba-nd80\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vc6xk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1be2594e49eaeb11782458034e15eb82b42e3bdf598fd92ee125827a3e19d3f4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-11T14:37:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.178.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.1.229\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.1.229\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-11T14:37:58Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/11/23 14:38:03.94
Aug 11 14:38:03.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 replace -f -'
Aug 11 14:38:04.731: INFO: stderr: ""
Aug 11 14:38:04.731: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/11/23 14:38:04.731
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 11 14:38:04.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 delete pods e2e-test-httpd-pod'
Aug 11 14:38:06.655: INFO: stderr: ""
Aug 11 14:38:06.655: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:06.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7349" for this suite. 08/11/23 14:38:06.66
------------------------------
• [SLOW TEST] [7.902 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:37:58.764
    Aug 11 14:37:58.764: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:37:58.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:37:58.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:37:58.78
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:37:58.783
    Aug 11 14:37:58.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 11 14:37:58.837: INFO: stderr: ""
    Aug 11 14:37:58.837: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/11/23 14:37:58.837
    STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:38:03.888
    Aug 11 14:38:03.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 get pod e2e-test-httpd-pod -o json'
    Aug 11 14:38:03.940: INFO: stderr: ""
    Aug 11 14:38:03.940: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-11T14:37:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7349\",\n        \"resourceVersion\": \"30087\",\n        \"uid\": \"52d008be-3cb8-4cc8-a0f5-ad0ab40618b7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vc6xk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"constell-1cf5d931-worker-6381a7ba-nd80\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vc6xk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:37:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://1be2594e49eaeb11782458034e15eb82b42e3bdf598fd92ee125827a3e19d3f4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-11T14:37:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.178.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.1.229\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.1.229\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-11T14:37:58Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/11/23 14:38:03.94
    Aug 11 14:38:03.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 replace -f -'
    Aug 11 14:38:04.731: INFO: stderr: ""
    Aug 11 14:38:04.731: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/11/23 14:38:04.731
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 11 14:38:04.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-7349 delete pods e2e-test-httpd-pod'
    Aug 11 14:38:06.655: INFO: stderr: ""
    Aug 11 14:38:06.655: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:06.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7349" for this suite. 08/11/23 14:38:06.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:06.667
Aug 11 14:38:06.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename watch 08/11/23 14:38:06.668
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:06.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:06.684
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/11/23 14:38:06.686
STEP: starting a background goroutine to produce watch events 08/11/23 14:38:06.689
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/11/23 14:38:06.689
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1021" for this suite. 08/11/23 14:38:09.523
------------------------------
• [2.908 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:06.667
    Aug 11 14:38:06.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename watch 08/11/23 14:38:06.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:06.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:06.684
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/11/23 14:38:06.686
    STEP: starting a background goroutine to produce watch events 08/11/23 14:38:06.689
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/11/23 14:38:06.689
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1021" for this suite. 08/11/23 14:38:09.523
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:09.575
Aug 11 14:38:09.575: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:38:09.576
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:09.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:09.594
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/11/23 14:38:09.603
Aug 11 14:38:09.603: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40" in namespace "kubelet-test-1093" to be "completed"
Aug 11 14:38:09.606: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481263ms
Aug 11 14:38:11.610: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007247443s
Aug 11 14:38:13.610: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007338449s
Aug 11 14:38:13.611: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1093" for this suite. 08/11/23 14:38:13.641
------------------------------
• [4.074 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:09.575
    Aug 11 14:38:09.575: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:38:09.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:09.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:09.594
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/11/23 14:38:09.603
    Aug 11 14:38:09.603: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40" in namespace "kubelet-test-1093" to be "completed"
    Aug 11 14:38:09.606: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.481263ms
    Aug 11 14:38:11.610: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007247443s
    Aug 11 14:38:13.610: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007338449s
    Aug 11 14:38:13.611: INFO: Pod "agnhost-host-aliasesf936421c-1ad5-40c1-bebf-ce81176eaf40" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1093" for this suite. 08/11/23 14:38:13.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:13.649
Aug 11 14:38:13.649: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:38:13.65
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:13.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:13.667
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-ljvxn" 08/11/23 14:38:13.669
Aug 11 14:38:13.685: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-ljvxn-3547" 08/11/23 14:38:13.685
Aug 11 14:38:13.693: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-ljvxn-3547" 08/11/23 14:38:13.693
Aug 11 14:38:13.701: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:13.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1048" for this suite. 08/11/23 14:38:13.705
STEP: Destroying namespace "e2e-ns-ljvxn-3547" for this suite. 08/11/23 14:38:13.712
------------------------------
• [0.069 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:13.649
    Aug 11 14:38:13.649: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:38:13.65
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:13.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:13.667
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-ljvxn" 08/11/23 14:38:13.669
    Aug 11 14:38:13.685: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-ljvxn-3547" 08/11/23 14:38:13.685
    Aug 11 14:38:13.693: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-ljvxn-3547" 08/11/23 14:38:13.693
    Aug 11 14:38:13.701: INFO: Namespace "e2e-ns-ljvxn-3547" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:13.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1048" for this suite. 08/11/23 14:38:13.705
    STEP: Destroying namespace "e2e-ns-ljvxn-3547" for this suite. 08/11/23 14:38:13.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:13.72
Aug 11 14:38:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:38:13.721
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:13.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:13.736
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 14:38:13.738
Aug 11 14:38:13.746: INFO: Waiting up to 5m0s for pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972" in namespace "emptydir-7166" to be "Succeeded or Failed"
Aug 11 14:38:13.751: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878275ms
Aug 11 14:38:15.756: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009512455s
Aug 11 14:38:17.756: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010154752s
STEP: Saw pod success 08/11/23 14:38:17.756
Aug 11 14:38:17.757: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972" satisfied condition "Succeeded or Failed"
Aug 11 14:38:17.759: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 container test-container: <nil>
STEP: delete the pod 08/11/23 14:38:17.768
Aug 11 14:38:17.782: INFO: Waiting for pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 to disappear
Aug 11 14:38:17.785: INFO: Pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:17.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7166" for this suite. 08/11/23 14:38:17.789
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:13.72
    Aug 11 14:38:13.720: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:38:13.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:13.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:13.736
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 14:38:13.738
    Aug 11 14:38:13.746: INFO: Waiting up to 5m0s for pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972" in namespace "emptydir-7166" to be "Succeeded or Failed"
    Aug 11 14:38:13.751: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878275ms
    Aug 11 14:38:15.756: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009512455s
    Aug 11 14:38:17.756: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010154752s
    STEP: Saw pod success 08/11/23 14:38:17.756
    Aug 11 14:38:17.757: INFO: Pod "pod-06f47d03-3053-46e4-98ad-14ae6e23f972" satisfied condition "Succeeded or Failed"
    Aug 11 14:38:17.759: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:38:17.768
    Aug 11 14:38:17.782: INFO: Waiting for pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 to disappear
    Aug 11 14:38:17.785: INFO: Pod pod-06f47d03-3053-46e4-98ad-14ae6e23f972 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:17.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7166" for this suite. 08/11/23 14:38:17.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:17.795
Aug 11 14:38:17.795: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:38:17.796
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:17.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:17.814
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/11/23 14:38:17.817
STEP: submitting the pod to kubernetes 08/11/23 14:38:17.817
Aug 11 14:38:17.825: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" in namespace "pods-8947" to be "running and ready"
Aug 11 14:38:17.832: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794847ms
Aug 11 14:38:17.832: INFO: The phase of Pod pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:38:19.837: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=true. Elapsed: 2.011078223s
Aug 11 14:38:19.837: INFO: The phase of Pod pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163 is Running (Ready = true)
Aug 11 14:38:19.837: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/11/23 14:38:19.839
STEP: updating the pod 08/11/23 14:38:19.842
Aug 11 14:38:20.354: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163"
Aug 11 14:38:20.354: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" in namespace "pods-8947" to be "terminated with reason DeadlineExceeded"
Aug 11 14:38:20.357: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=true. Elapsed: 3.243753ms
Aug 11 14:38:22.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=false. Elapsed: 2.008087103s
Aug 11 14:38:24.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007838909s
Aug 11 14:38:24.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:24.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8947" for this suite. 08/11/23 14:38:24.366
------------------------------
• [SLOW TEST] [6.578 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:17.795
    Aug 11 14:38:17.795: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:38:17.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:17.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:17.814
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/11/23 14:38:17.817
    STEP: submitting the pod to kubernetes 08/11/23 14:38:17.817
    Aug 11 14:38:17.825: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" in namespace "pods-8947" to be "running and ready"
    Aug 11 14:38:17.832: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794847ms
    Aug 11 14:38:17.832: INFO: The phase of Pod pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:38:19.837: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=true. Elapsed: 2.011078223s
    Aug 11 14:38:19.837: INFO: The phase of Pod pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163 is Running (Ready = true)
    Aug 11 14:38:19.837: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/11/23 14:38:19.839
    STEP: updating the pod 08/11/23 14:38:19.842
    Aug 11 14:38:20.354: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163"
    Aug 11 14:38:20.354: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" in namespace "pods-8947" to be "terminated with reason DeadlineExceeded"
    Aug 11 14:38:20.357: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=true. Elapsed: 3.243753ms
    Aug 11 14:38:22.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Running", Reason="", readiness=false. Elapsed: 2.008087103s
    Aug 11 14:38:24.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007838909s
    Aug 11 14:38:24.362: INFO: Pod "pod-update-activedeadlineseconds-eb8bd24a-4ed7-4279-a22f-85417f926163" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:24.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8947" for this suite. 08/11/23 14:38:24.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:24.375
Aug 11 14:38:24.375: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename ingressclass 08/11/23 14:38:24.376
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:24.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:24.393
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/11/23 14:38:24.395
STEP: getting /apis/networking.k8s.io 08/11/23 14:38:24.397
STEP: getting /apis/networking.k8s.iov1 08/11/23 14:38:24.398
STEP: creating 08/11/23 14:38:24.399
STEP: getting 08/11/23 14:38:24.413
STEP: listing 08/11/23 14:38:24.415
STEP: watching 08/11/23 14:38:24.418
Aug 11 14:38:24.418: INFO: starting watch
STEP: patching 08/11/23 14:38:24.419
STEP: updating 08/11/23 14:38:24.423
Aug 11 14:38:24.428: INFO: waiting for watch events with expected annotations
Aug 11 14:38:24.428: INFO: saw patched and updated annotations
STEP: deleting 08/11/23 14:38:24.428
STEP: deleting a collection 08/11/23 14:38:24.438
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:24.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4085" for this suite. 08/11/23 14:38:24.457
------------------------------
• [0.087 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:24.375
    Aug 11 14:38:24.375: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename ingressclass 08/11/23 14:38:24.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:24.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:24.393
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/11/23 14:38:24.395
    STEP: getting /apis/networking.k8s.io 08/11/23 14:38:24.397
    STEP: getting /apis/networking.k8s.iov1 08/11/23 14:38:24.398
    STEP: creating 08/11/23 14:38:24.399
    STEP: getting 08/11/23 14:38:24.413
    STEP: listing 08/11/23 14:38:24.415
    STEP: watching 08/11/23 14:38:24.418
    Aug 11 14:38:24.418: INFO: starting watch
    STEP: patching 08/11/23 14:38:24.419
    STEP: updating 08/11/23 14:38:24.423
    Aug 11 14:38:24.428: INFO: waiting for watch events with expected annotations
    Aug 11 14:38:24.428: INFO: saw patched and updated annotations
    STEP: deleting 08/11/23 14:38:24.428
    STEP: deleting a collection 08/11/23 14:38:24.438
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:24.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4085" for this suite. 08/11/23 14:38:24.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:24.463
Aug 11 14:38:24.463: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:38:24.463
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:24.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:24.479
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:38:24.482
Aug 11 14:38:24.492: INFO: Waiting up to 5m0s for pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5" in namespace "emptydir-1454" to be "Succeeded or Failed"
Aug 11 14:38:24.498: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.674956ms
Aug 11 14:38:26.501: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145066s
Aug 11 14:38:28.503: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011144524s
STEP: Saw pod success 08/11/23 14:38:28.503
Aug 11 14:38:28.503: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5" satisfied condition "Succeeded or Failed"
Aug 11 14:38:28.506: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 container test-container: <nil>
STEP: delete the pod 08/11/23 14:38:28.517
Aug 11 14:38:28.532: INFO: Waiting for pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 to disappear
Aug 11 14:38:28.535: INFO: Pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:28.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1454" for this suite. 08/11/23 14:38:28.539
------------------------------
• [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:24.463
    Aug 11 14:38:24.463: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:38:24.463
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:24.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:24.479
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:38:24.482
    Aug 11 14:38:24.492: INFO: Waiting up to 5m0s for pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5" in namespace "emptydir-1454" to be "Succeeded or Failed"
    Aug 11 14:38:24.498: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.674956ms
    Aug 11 14:38:26.501: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009145066s
    Aug 11 14:38:28.503: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011144524s
    STEP: Saw pod success 08/11/23 14:38:28.503
    Aug 11 14:38:28.503: INFO: Pod "pod-e72e80fc-fffa-424c-b093-dae4342e08e5" satisfied condition "Succeeded or Failed"
    Aug 11 14:38:28.506: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:38:28.517
    Aug 11 14:38:28.532: INFO: Waiting for pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 to disappear
    Aug 11 14:38:28.535: INFO: Pod pod-e72e80fc-fffa-424c-b093-dae4342e08e5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:28.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1454" for this suite. 08/11/23 14:38:28.539
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:28.545
Aug 11 14:38:28.545: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:38:28.546
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:28.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:28.566
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-feec4e57-71d5-4fde-859f-c695c68ec4da 08/11/23 14:38:28.568
STEP: Creating a pod to test consume configMaps 08/11/23 14:38:28.574
Aug 11 14:38:28.584: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262" in namespace "configmap-2828" to be "Succeeded or Failed"
Aug 11 14:38:28.589: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174604ms
Aug 11 14:38:30.593: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009420865s
Aug 11 14:38:32.594: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00996109s
STEP: Saw pod success 08/11/23 14:38:32.594
Aug 11 14:38:32.594: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262" satisfied condition "Succeeded or Failed"
Aug 11 14:38:32.596: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:38:32.604
Aug 11 14:38:32.618: INFO: Waiting for pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 to disappear
Aug 11 14:38:32.620: INFO: Pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:32.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2828" for this suite. 08/11/23 14:38:32.625
------------------------------
• [4.086 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:28.545
    Aug 11 14:38:28.545: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:38:28.546
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:28.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:28.566
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-feec4e57-71d5-4fde-859f-c695c68ec4da 08/11/23 14:38:28.568
    STEP: Creating a pod to test consume configMaps 08/11/23 14:38:28.574
    Aug 11 14:38:28.584: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262" in namespace "configmap-2828" to be "Succeeded or Failed"
    Aug 11 14:38:28.589: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Pending", Reason="", readiness=false. Elapsed: 5.174604ms
    Aug 11 14:38:30.593: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009420865s
    Aug 11 14:38:32.594: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00996109s
    STEP: Saw pod success 08/11/23 14:38:32.594
    Aug 11 14:38:32.594: INFO: Pod "pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262" satisfied condition "Succeeded or Failed"
    Aug 11 14:38:32.596: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:38:32.604
    Aug 11 14:38:32.618: INFO: Waiting for pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 to disappear
    Aug 11 14:38:32.620: INFO: Pod pod-configmaps-c1a8ade3-fa9b-4735-8254-ac5ed6260262 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:32.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2828" for this suite. 08/11/23 14:38:32.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:32.631
Aug 11 14:38:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:38:32.632
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:32.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:32.648
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/11/23 14:38:32.65
Aug 11 14:38:32.650: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: mark a version not serverd 08/11/23 14:38:36.753
STEP: check the unserved version gets removed 08/11/23 14:38:36.768
STEP: check the other version is not changed 08/11/23 14:38:38.288
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:41.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3744" for this suite. 08/11/23 14:38:41.655
------------------------------
• [SLOW TEST] [9.028 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:32.631
    Aug 11 14:38:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:38:32.632
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:32.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:32.648
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/11/23 14:38:32.65
    Aug 11 14:38:32.650: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: mark a version not serverd 08/11/23 14:38:36.753
    STEP: check the unserved version gets removed 08/11/23 14:38:36.768
    STEP: check the other version is not changed 08/11/23 14:38:38.288
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:41.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3744" for this suite. 08/11/23 14:38:41.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:41.66
Aug 11 14:38:41.660: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:38:41.661
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:41.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:41.675
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/11/23 14:38:41.677
Aug 11 14:38:41.682: INFO: Waiting up to 5m0s for pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b" in namespace "downward-api-3535" to be "Succeeded or Failed"
Aug 11 14:38:41.684: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890962ms
Aug 11 14:38:43.687: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004928461s
Aug 11 14:38:45.688: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005598848s
STEP: Saw pod success 08/11/23 14:38:45.688
Aug 11 14:38:45.688: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b" satisfied condition "Succeeded or Failed"
Aug 11 14:38:45.690: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:38:45.713
Aug 11 14:38:45.722: INFO: Waiting for pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b to disappear
Aug 11 14:38:45.724: INFO: Pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:38:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3535" for this suite. 08/11/23 14:38:45.727
------------------------------
• [4.072 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:41.66
    Aug 11 14:38:41.660: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:38:41.661
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:41.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:41.675
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/11/23 14:38:41.677
    Aug 11 14:38:41.682: INFO: Waiting up to 5m0s for pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b" in namespace "downward-api-3535" to be "Succeeded or Failed"
    Aug 11 14:38:41.684: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890962ms
    Aug 11 14:38:43.687: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004928461s
    Aug 11 14:38:45.688: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005598848s
    STEP: Saw pod success 08/11/23 14:38:45.688
    Aug 11 14:38:45.688: INFO: Pod "downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b" satisfied condition "Succeeded or Failed"
    Aug 11 14:38:45.690: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:38:45.713
    Aug 11 14:38:45.722: INFO: Waiting for pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b to disappear
    Aug 11 14:38:45.724: INFO: Pod downward-api-c4c4cb4d-57b9-4d16-afad-2d8d0fd5170b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:38:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3535" for this suite. 08/11/23 14:38:45.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:38:45.732
Aug 11 14:38:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename hostport 08/11/23 14:38:45.733
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:45.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:45.746
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/11/23 14:38:45.751
Aug 11 14:38:45.758: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6525" to be "running and ready"
Aug 11 14:38:45.760: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144643ms
Aug 11 14:38:45.760: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:38:47.763: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005280641s
Aug 11 14:38:47.763: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 14:38:47.763: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.178.2 on the node which pod1 resides and expect scheduled 08/11/23 14:38:47.763
Aug 11 14:38:47.770: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6525" to be "running and ready"
Aug 11 14:38:47.772: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281402ms
Aug 11 14:38:47.772: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:38:49.775: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004804551s
Aug 11 14:38:49.775: INFO: The phase of Pod pod2 is Running (Ready = false)
Aug 11 14:38:51.776: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006150168s
Aug 11 14:38:51.776: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 14:38:51.776: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.178.2 but use UDP protocol on the node which pod2 resides 08/11/23 14:38:51.776
Aug 11 14:38:51.781: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6525" to be "running and ready"
Aug 11 14:38:51.784: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682943ms
Aug 11 14:38:51.784: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:38:53.790: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.008974165s
Aug 11 14:38:53.790: INFO: The phase of Pod pod3 is Running (Ready = false)
Aug 11 14:38:55.789: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00781823s
Aug 11 14:38:55.789: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 11 14:38:55.789: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 11 14:38:55.795: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6525" to be "running and ready"
Aug 11 14:38:55.797: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114632ms
Aug 11 14:38:55.797: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:38:57.800: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004958859s
Aug 11 14:38:57.800: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 11 14:38:57.800: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/11/23 14:38:57.802
Aug 11 14:38:57.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.178.2 http://127.0.0.1:54323/hostname] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:38:57.802: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:38:57.803: INFO: ExecWithOptions: Clientset creation
Aug 11 14:38:57.803: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.178.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 08/11/23 14:38:57.887
Aug 11 14:38:57.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.178.2:54323/hostname] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:38:57.887: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:38:57.888: INFO: ExecWithOptions: Clientset creation
Aug 11 14:38:57.888: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.178.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 UDP 08/11/23 14:38:57.953
Aug 11 14:38:57.954: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.178.2 54323] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:38:57.954: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:38:57.954: INFO: ExecWithOptions: Clientset creation
Aug 11 14:38:57.954: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.178.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:03.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-6525" for this suite. 08/11/23 14:39:03.046
------------------------------
• [SLOW TEST] [17.321 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:38:45.732
    Aug 11 14:38:45.732: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename hostport 08/11/23 14:38:45.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:38:45.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:38:45.746
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/11/23 14:38:45.751
    Aug 11 14:38:45.758: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6525" to be "running and ready"
    Aug 11 14:38:45.760: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144643ms
    Aug 11 14:38:45.760: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:38:47.763: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005280641s
    Aug 11 14:38:47.763: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 14:38:47.763: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.178.2 on the node which pod1 resides and expect scheduled 08/11/23 14:38:47.763
    Aug 11 14:38:47.770: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6525" to be "running and ready"
    Aug 11 14:38:47.772: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281402ms
    Aug 11 14:38:47.772: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:38:49.775: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004804551s
    Aug 11 14:38:49.775: INFO: The phase of Pod pod2 is Running (Ready = false)
    Aug 11 14:38:51.776: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006150168s
    Aug 11 14:38:51.776: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 14:38:51.776: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.178.2 but use UDP protocol on the node which pod2 resides 08/11/23 14:38:51.776
    Aug 11 14:38:51.781: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6525" to be "running and ready"
    Aug 11 14:38:51.784: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682943ms
    Aug 11 14:38:51.784: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:38:53.790: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.008974165s
    Aug 11 14:38:53.790: INFO: The phase of Pod pod3 is Running (Ready = false)
    Aug 11 14:38:55.789: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00781823s
    Aug 11 14:38:55.789: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 11 14:38:55.789: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 11 14:38:55.795: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6525" to be "running and ready"
    Aug 11 14:38:55.797: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114632ms
    Aug 11 14:38:55.797: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:38:57.800: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004958859s
    Aug 11 14:38:57.800: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 11 14:38:57.800: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/11/23 14:38:57.802
    Aug 11 14:38:57.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.178.2 http://127.0.0.1:54323/hostname] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:38:57.802: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:38:57.803: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:38:57.803: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.178.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 08/11/23 14:38:57.887
    Aug 11 14:38:57.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.178.2:54323/hostname] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:38:57.887: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:38:57.888: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:38:57.888: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.178.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 UDP 08/11/23 14:38:57.953
    Aug 11 14:38:57.954: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.178.2 54323] Namespace:hostport-6525 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:38:57.954: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:38:57.954: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:38:57.954: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6525/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.178.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:03.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-6525" for this suite. 08/11/23 14:39:03.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:03.053
Aug 11 14:39:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:39:03.054
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.068
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/11/23 14:39:03.072
STEP: waiting for available Endpoint 08/11/23 14:39:03.079
STEP: listing all Endpoints 08/11/23 14:39:03.081
STEP: updating the Endpoint 08/11/23 14:39:03.083
STEP: fetching the Endpoint 08/11/23 14:39:03.088
STEP: patching the Endpoint 08/11/23 14:39:03.091
STEP: fetching the Endpoint 08/11/23 14:39:03.098
STEP: deleting the Endpoint by Collection 08/11/23 14:39:03.1
STEP: waiting for Endpoint deletion 08/11/23 14:39:03.106
STEP: fetching the Endpoint 08/11/23 14:39:03.107
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:03.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3215" for this suite. 08/11/23 14:39:03.112
------------------------------
• [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:03.053
    Aug 11 14:39:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:39:03.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.068
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/11/23 14:39:03.072
    STEP: waiting for available Endpoint 08/11/23 14:39:03.079
    STEP: listing all Endpoints 08/11/23 14:39:03.081
    STEP: updating the Endpoint 08/11/23 14:39:03.083
    STEP: fetching the Endpoint 08/11/23 14:39:03.088
    STEP: patching the Endpoint 08/11/23 14:39:03.091
    STEP: fetching the Endpoint 08/11/23 14:39:03.098
    STEP: deleting the Endpoint by Collection 08/11/23 14:39:03.1
    STEP: waiting for Endpoint deletion 08/11/23 14:39:03.106
    STEP: fetching the Endpoint 08/11/23 14:39:03.107
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:03.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3215" for this suite. 08/11/23 14:39:03.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:03.12
Aug 11 14:39:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sysctl 08/11/23 14:39:03.121
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.135
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/11/23 14:39:03.137
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:03.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-48" for this suite. 08/11/23 14:39:03.143
------------------------------
• [0.028 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:03.12
    Aug 11 14:39:03.120: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sysctl 08/11/23 14:39:03.121
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.135
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/11/23 14:39:03.137
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:03.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-48" for this suite. 08/11/23 14:39:03.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:03.15
Aug 11 14:39:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:39:03.15
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.164
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 11 14:39:03.173: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 11 14:39:08.179: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:39:08.179
Aug 11 14:39:08.179: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 11 14:39:10.182: INFO: Creating deployment "test-rollover-deployment"
Aug 11 14:39:10.190: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 11 14:39:12.195: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 11 14:39:12.201: INFO: Ensure that both replica sets have 1 created replica
Aug 11 14:39:12.206: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 11 14:39:12.216: INFO: Updating deployment test-rollover-deployment
Aug 11 14:39:12.216: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 11 14:39:14.223: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 11 14:39:14.227: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 11 14:39:14.231: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:39:14.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:39:16.238: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:39:16.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:39:18.238: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:39:18.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:39:20.239: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:39:20.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:39:22.238: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:39:22.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:39:24.239: INFO: 
Aug 11 14:39:24.239: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:39:24.246: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2384  ac651bb6-286c-4cc2-9c44-3a484db8c035 31165 2 2023-08-11 14:39:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0015563b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:39:10 +0000 UTC,LastTransitionTime:2023-08-11 14:39:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-11 14:39:23 +0000 UTC,LastTransitionTime:2023-08-11 14:39:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:39:24.248: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2384  269d8c6c-79ab-4d44-b7b7-6677fff5911d 31154 2 2023-08-11 14:39:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc001557337 0xc001557338}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0015573f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:39:24.248: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 11 14:39:24.248: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2384  0aae250f-e4fc-4e77-906d-6bf279c50717 31163 2 2023-08-11 14:39:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc0015570c7 0xc0015570c8}] [] [{e2e.test Update apps/v1 2023-08-11 14:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0015572c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:39:24.248: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2384  4b9b13fc-9a63-4d06-89ac-078b8a197412 31067 2 2023-08-11 14:39:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc001557477 0xc001557478}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001557528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:39:24.251: INFO: Pod "test-rollover-deployment-6c6df9974f-9kc7j" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9kc7j test-rollover-deployment-6c6df9974f- deployment-2384  6d00535b-c92d-4fa3-b416-ec8ba92c558e 31097 0 2023-08-11 14:39:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 269d8c6c-79ab-4d44-b7b7-6677fff5911d 0xc005111ab7 0xc005111ab8}] [] [{kube-controller-manager Update v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"269d8c6c-79ab-4d44-b7b7-6677fff5911d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:39:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4jzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4jzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.92,StartTime:2023-08-11 14:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:39:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://ead8cdbf730e4bf1d34db8c947bd44a6462d4a21e1dfca163760845ce606f450,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2384" for this suite. 08/11/23 14:39:24.254
------------------------------
• [SLOW TEST] [21.111 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:03.15
    Aug 11 14:39:03.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:39:03.15
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:03.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:03.164
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 11 14:39:03.173: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 11 14:39:08.179: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:39:08.179
    Aug 11 14:39:08.179: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 11 14:39:10.182: INFO: Creating deployment "test-rollover-deployment"
    Aug 11 14:39:10.190: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 11 14:39:12.195: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 11 14:39:12.201: INFO: Ensure that both replica sets have 1 created replica
    Aug 11 14:39:12.206: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 11 14:39:12.216: INFO: Updating deployment test-rollover-deployment
    Aug 11 14:39:12.216: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 11 14:39:14.223: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 11 14:39:14.227: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 11 14:39:14.231: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:39:14.231: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:39:16.238: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:39:16.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:39:18.238: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:39:18.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:39:20.239: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:39:20.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:39:22.238: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:39:22.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 39, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 39, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:39:24.239: INFO: 
    Aug 11 14:39:24.239: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:39:24.246: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2384  ac651bb6-286c-4cc2-9c44-3a484db8c035 31165 2 2023-08-11 14:39:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0015563b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:39:10 +0000 UTC,LastTransitionTime:2023-08-11 14:39:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-11 14:39:23 +0000 UTC,LastTransitionTime:2023-08-11 14:39:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:39:24.248: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2384  269d8c6c-79ab-4d44-b7b7-6677fff5911d 31154 2 2023-08-11 14:39:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc001557337 0xc001557338}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0015573f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:39:24.248: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 11 14:39:24.248: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2384  0aae250f-e4fc-4e77-906d-6bf279c50717 31163 2 2023-08-11 14:39:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc0015570c7 0xc0015570c8}] [] [{e2e.test Update apps/v1 2023-08-11 14:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0015572c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:39:24.248: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2384  4b9b13fc-9a63-4d06-89ac-078b8a197412 31067 2 2023-08-11 14:39:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment ac651bb6-286c-4cc2-9c44-3a484db8c035 0xc001557477 0xc001557478}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac651bb6-286c-4cc2-9c44-3a484db8c035\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001557528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:39:24.251: INFO: Pod "test-rollover-deployment-6c6df9974f-9kc7j" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9kc7j test-rollover-deployment-6c6df9974f- deployment-2384  6d00535b-c92d-4fa3-b416-ec8ba92c558e 31097 0 2023-08-11 14:39:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 269d8c6c-79ab-4d44-b7b7-6677fff5911d 0xc005111ab7 0xc005111ab8}] [] [{kube-controller-manager Update v1 2023-08-11 14:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"269d8c6c-79ab-4d44-b7b7-6677fff5911d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:39:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n4jzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n4jzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:39:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.92,StartTime:2023-08-11 14:39:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:39:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://ead8cdbf730e4bf1d34db8c947bd44a6462d4a21e1dfca163760845ce606f450,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2384" for this suite. 08/11/23 14:39:24.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:24.262
Aug 11 14:39:24.262: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:39:24.263
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:24.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:24.275
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:39:24.278
Aug 11 14:39:24.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5" in namespace "downward-api-2110" to be "Succeeded or Failed"
Aug 11 14:39:24.287: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171283ms
Aug 11 14:39:26.291: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006038062s
Aug 11 14:39:28.290: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005134887s
STEP: Saw pod success 08/11/23 14:39:28.29
Aug 11 14:39:28.290: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5" satisfied condition "Succeeded or Failed"
Aug 11 14:39:28.292: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 container client-container: <nil>
STEP: delete the pod 08/11/23 14:39:28.3
Aug 11 14:39:28.311: INFO: Waiting for pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 to disappear
Aug 11 14:39:28.313: INFO: Pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:28.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2110" for this suite. 08/11/23 14:39:28.316
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:24.262
    Aug 11 14:39:24.262: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:39:24.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:24.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:24.275
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:39:24.278
    Aug 11 14:39:24.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5" in namespace "downward-api-2110" to be "Succeeded or Failed"
    Aug 11 14:39:24.287: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171283ms
    Aug 11 14:39:26.291: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006038062s
    Aug 11 14:39:28.290: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005134887s
    STEP: Saw pod success 08/11/23 14:39:28.29
    Aug 11 14:39:28.290: INFO: Pod "downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5" satisfied condition "Succeeded or Failed"
    Aug 11 14:39:28.292: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:39:28.3
    Aug 11 14:39:28.311: INFO: Waiting for pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 to disappear
    Aug 11 14:39:28.313: INFO: Pod downwardapi-volume-e3f4ea71-17c1-43f6-9200-80394fb6e2a5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:28.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2110" for this suite. 08/11/23 14:39:28.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:28.323
Aug 11 14:39:28.323: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:39:28.324
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:28.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:28.338
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/11/23 14:39:28.34
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:39:28.344
STEP: Creating a ResourceQuota with not best effort scope 08/11/23 14:39:30.347
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:39:30.352
STEP: Creating a best-effort pod 08/11/23 14:39:32.356
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/11/23 14:39:32.366
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/11/23 14:39:34.371
STEP: Deleting the pod 08/11/23 14:39:36.374
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:39:36.387
STEP: Creating a not best-effort pod 08/11/23 14:39:38.39
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/11/23 14:39:38.399
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/11/23 14:39:40.402
STEP: Deleting the pod 08/11/23 14:39:42.406
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:39:42.423
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7217" for this suite. 08/11/23 14:39:44.43
------------------------------
• [SLOW TEST] [16.111 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:28.323
    Aug 11 14:39:28.323: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:39:28.324
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:28.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:28.338
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/11/23 14:39:28.34
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:39:28.344
    STEP: Creating a ResourceQuota with not best effort scope 08/11/23 14:39:30.347
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:39:30.352
    STEP: Creating a best-effort pod 08/11/23 14:39:32.356
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/11/23 14:39:32.366
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/11/23 14:39:34.371
    STEP: Deleting the pod 08/11/23 14:39:36.374
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:39:36.387
    STEP: Creating a not best-effort pod 08/11/23 14:39:38.39
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/11/23 14:39:38.399
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/11/23 14:39:40.402
    STEP: Deleting the pod 08/11/23 14:39:42.406
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:39:42.423
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7217" for this suite. 08/11/23 14:39:44.43
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:44.435
Aug 11 14:39:44.435: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:39:44.436
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:44.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:44.45
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7082 08/11/23 14:39:44.452
STEP: creating replication controller nodeport-test in namespace services-7082 08/11/23 14:39:44.466
I0811 14:39:44.475963      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7082, replica count: 2
I0811 14:39:47.528450      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:39:47.528: INFO: Creating new exec pod
Aug 11 14:39:47.533: INFO: Waiting up to 5m0s for pod "execpodxmbtz" in namespace "services-7082" to be "running"
Aug 11 14:39:47.539: INFO: Pod "execpodxmbtz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.488605ms
Aug 11 14:39:49.542: INFO: Pod "execpodxmbtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008865403s
Aug 11 14:39:49.542: INFO: Pod "execpodxmbtz" satisfied condition "running"
Aug 11 14:39:50.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 11 14:39:50.675: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 11 14:39:50.675: INFO: stdout: ""
Aug 11 14:39:50.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 10.96.91.41 80'
Aug 11 14:39:50.793: INFO: stderr: "+ nc -v -z -w 2 10.96.91.41 80\nConnection to 10.96.91.41 80 port [tcp/http] succeeded!\n"
Aug 11 14:39:50.793: INFO: stdout: ""
Aug 11 14:39:50.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 32726'
Aug 11 14:39:50.933: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 32726\nConnection to 192.168.178.2 32726 port [tcp/*] succeeded!\n"
Aug 11 14:39:50.933: INFO: stdout: ""
Aug 11 14:39:50.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 32726'
Aug 11 14:39:51.051: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 32726\nConnection to 192.168.178.3 32726 port [tcp/*] succeeded!\n"
Aug 11 14:39:51.051: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:39:51.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7082" for this suite. 08/11/23 14:39:51.056
------------------------------
• [SLOW TEST] [6.626 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:44.435
    Aug 11 14:39:44.435: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:39:44.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:44.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:44.45
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7082 08/11/23 14:39:44.452
    STEP: creating replication controller nodeport-test in namespace services-7082 08/11/23 14:39:44.466
    I0811 14:39:44.475963      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7082, replica count: 2
    I0811 14:39:47.528450      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:39:47.528: INFO: Creating new exec pod
    Aug 11 14:39:47.533: INFO: Waiting up to 5m0s for pod "execpodxmbtz" in namespace "services-7082" to be "running"
    Aug 11 14:39:47.539: INFO: Pod "execpodxmbtz": Phase="Pending", Reason="", readiness=false. Elapsed: 5.488605ms
    Aug 11 14:39:49.542: INFO: Pod "execpodxmbtz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008865403s
    Aug 11 14:39:49.542: INFO: Pod "execpodxmbtz" satisfied condition "running"
    Aug 11 14:39:50.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 11 14:39:50.675: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 11 14:39:50.675: INFO: stdout: ""
    Aug 11 14:39:50.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 10.96.91.41 80'
    Aug 11 14:39:50.793: INFO: stderr: "+ nc -v -z -w 2 10.96.91.41 80\nConnection to 10.96.91.41 80 port [tcp/http] succeeded!\n"
    Aug 11 14:39:50.793: INFO: stdout: ""
    Aug 11 14:39:50.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 32726'
    Aug 11 14:39:50.933: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 32726\nConnection to 192.168.178.2 32726 port [tcp/*] succeeded!\n"
    Aug 11 14:39:50.933: INFO: stdout: ""
    Aug 11 14:39:50.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7082 exec execpodxmbtz -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 32726'
    Aug 11 14:39:51.051: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 32726\nConnection to 192.168.178.3 32726 port [tcp/*] succeeded!\n"
    Aug 11 14:39:51.051: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:39:51.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7082" for this suite. 08/11/23 14:39:51.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:39:51.062
Aug 11 14:39:51.062: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 14:39:51.063
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:51.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:51.077
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 11 14:39:51.079: INFO: Creating ReplicaSet my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19
Aug 11 14:39:51.085: INFO: Pod name my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Found 0 pods out of 1
Aug 11 14:39:56.089: INFO: Pod name my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Found 1 pods out of 1
Aug 11 14:39:56.089: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19" is running
Aug 11 14:39:56.089: INFO: Waiting up to 5m0s for pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" in namespace "replicaset-2808" to be "running"
Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf": Phase="Running", Reason="", readiness=true. Elapsed: 3.507823ms
Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" satisfied condition "running"
Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:51 +0000 UTC Reason: Message:}])
Aug 11 14:39:56.093: INFO: Trying to dial the pod
Aug 11 14:40:01.110: INFO: Controller my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Got expected result from replica 1 [my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf]: "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:01.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2808" for this suite. 08/11/23 14:40:01.114
------------------------------
• [SLOW TEST] [10.057 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:39:51.062
    Aug 11 14:39:51.062: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:39:51.063
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:51.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:51.077
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 11 14:39:51.079: INFO: Creating ReplicaSet my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19
    Aug 11 14:39:51.085: INFO: Pod name my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Found 0 pods out of 1
    Aug 11 14:39:56.089: INFO: Pod name my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Found 1 pods out of 1
    Aug 11 14:39:56.089: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19" is running
    Aug 11 14:39:56.089: INFO: Waiting up to 5m0s for pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" in namespace "replicaset-2808" to be "running"
    Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf": Phase="Running", Reason="", readiness=true. Elapsed: 3.507823ms
    Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" satisfied condition "running"
    Aug 11 14:39:56.093: INFO: Pod "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:39:51 +0000 UTC Reason: Message:}])
    Aug 11 14:39:56.093: INFO: Trying to dial the pod
    Aug 11 14:40:01.110: INFO: Controller my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19: Got expected result from replica 1 [my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf]: "my-hostname-basic-93b1abbf-266b-4851-b3b2-e68e133cfb19-sfrzf", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:01.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2808" for this suite. 08/11/23 14:40:01.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:01.121
Aug 11 14:40:01.121: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context-test 08/11/23 14:40:01.122
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:01.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:01.136
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 11 14:40:01.144: INFO: Waiting up to 5m0s for pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1" in namespace "security-context-test-7750" to be "Succeeded or Failed"
Aug 11 14:40:01.147: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301232ms
Aug 11 14:40:03.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006167701s
Aug 11 14:40:05.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005489966s
Aug 11 14:40:05.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:05.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7750" for this suite. 08/11/23 14:40:05.154
------------------------------
• [4.038 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:01.121
    Aug 11 14:40:01.121: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context-test 08/11/23 14:40:01.122
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:01.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:01.136
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 11 14:40:01.144: INFO: Waiting up to 5m0s for pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1" in namespace "security-context-test-7750" to be "Succeeded or Failed"
    Aug 11 14:40:01.147: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301232ms
    Aug 11 14:40:03.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006167701s
    Aug 11 14:40:05.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005489966s
    Aug 11 14:40:05.150: INFO: Pod "busybox-user-65534-034985f1-c262-4f3d-94ee-4a7239b84cf1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:05.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7750" for this suite. 08/11/23 14:40:05.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:05.16
Aug 11 14:40:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context-test 08/11/23 14:40:05.161
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:05.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:05.176
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 11 14:40:05.185: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307" in namespace "security-context-test-7658" to be "Succeeded or Failed"
Aug 11 14:40:05.188: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.200873ms
Aug 11 14:40:07.191: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Running", Reason="", readiness=false. Elapsed: 2.005673332s
Aug 11 14:40:09.192: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006595739s
Aug 11 14:40:09.192: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7658" for this suite. 08/11/23 14:40:09.197
------------------------------
• [4.042 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:05.16
    Aug 11 14:40:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context-test 08/11/23 14:40:05.161
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:05.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:05.176
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 11 14:40:05.185: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307" in namespace "security-context-test-7658" to be "Succeeded or Failed"
    Aug 11 14:40:05.188: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Pending", Reason="", readiness=false. Elapsed: 2.200873ms
    Aug 11 14:40:07.191: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Running", Reason="", readiness=false. Elapsed: 2.005673332s
    Aug 11 14:40:09.192: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006595739s
    Aug 11 14:40:09.192: INFO: Pod "busybox-readonly-false-95906556-7cf8-400f-a07c-cb32af0ef307" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7658" for this suite. 08/11/23 14:40:09.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:09.203
Aug 11 14:40:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename proxy 08/11/23 14:40:09.204
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:09.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:09.218
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 11 14:40:09.221: INFO: Creating pod...
Aug 11 14:40:09.227: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3889" to be "running"
Aug 11 14:40:09.229: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208372ms
Aug 11 14:40:11.232: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005876242s
Aug 11 14:40:11.233: INFO: Pod "agnhost" satisfied condition "running"
Aug 11 14:40:11.233: INFO: Creating service...
Aug 11 14:40:11.244: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=DELETE
Aug 11 14:40:11.258: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:40:11.258: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=OPTIONS
Aug 11 14:40:11.264: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:40:11.264: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=PATCH
Aug 11 14:40:11.270: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:40:11.270: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=POST
Aug 11 14:40:11.275: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:40:11.275: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=PUT
Aug 11 14:40:11.280: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 14:40:11.280: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 11 14:40:11.287: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:40:11.287: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 11 14:40:11.293: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:40:11.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 11 14:40:11.301: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:40:11.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=POST
Aug 11 14:40:11.307: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:40:11.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=PUT
Aug 11 14:40:11.314: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 14:40:11.314: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=GET
Aug 11 14:40:11.316: INFO: http.Client request:GET StatusCode:301
Aug 11 14:40:11.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=GET
Aug 11 14:40:11.319: INFO: http.Client request:GET StatusCode:301
Aug 11 14:40:11.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=HEAD
Aug 11 14:40:11.321: INFO: http.Client request:HEAD StatusCode:301
Aug 11 14:40:11.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 11 14:40:11.324: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3889" for this suite. 08/11/23 14:40:11.328
------------------------------
• [2.130 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:09.203
    Aug 11 14:40:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename proxy 08/11/23 14:40:09.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:09.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:09.218
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 11 14:40:09.221: INFO: Creating pod...
    Aug 11 14:40:09.227: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3889" to be "running"
    Aug 11 14:40:09.229: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208372ms
    Aug 11 14:40:11.232: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005876242s
    Aug 11 14:40:11.233: INFO: Pod "agnhost" satisfied condition "running"
    Aug 11 14:40:11.233: INFO: Creating service...
    Aug 11 14:40:11.244: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=DELETE
    Aug 11 14:40:11.258: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:40:11.258: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=OPTIONS
    Aug 11 14:40:11.264: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:40:11.264: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=PATCH
    Aug 11 14:40:11.270: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:40:11.270: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=POST
    Aug 11 14:40:11.275: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:40:11.275: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=PUT
    Aug 11 14:40:11.280: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 14:40:11.280: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 11 14:40:11.287: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:40:11.287: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 11 14:40:11.293: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:40:11.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 11 14:40:11.301: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:40:11.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=POST
    Aug 11 14:40:11.307: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:40:11.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 11 14:40:11.314: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 14:40:11.314: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=GET
    Aug 11 14:40:11.316: INFO: http.Client request:GET StatusCode:301
    Aug 11 14:40:11.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=GET
    Aug 11 14:40:11.319: INFO: http.Client request:GET StatusCode:301
    Aug 11 14:40:11.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/pods/agnhost/proxy?method=HEAD
    Aug 11 14:40:11.321: INFO: http.Client request:HEAD StatusCode:301
    Aug 11 14:40:11.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3889/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 11 14:40:11.324: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:11.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3889" for this suite. 08/11/23 14:40:11.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:11.334
Aug 11 14:40:11.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:40:11.335
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:11.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:11.35
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-675ff644-2a13-4ffa-b1f0-9d971bb43c17 08/11/23 14:40:11.352
STEP: Creating a pod to test consume secrets 08/11/23 14:40:11.356
Aug 11 14:40:11.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc" in namespace "projected-4441" to be "Succeeded or Failed"
Aug 11 14:40:11.364: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373572ms
Aug 11 14:40:13.371: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008750723s
Aug 11 14:40:15.368: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005815657s
STEP: Saw pod success 08/11/23 14:40:15.368
Aug 11 14:40:15.368: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc" satisfied condition "Succeeded or Failed"
Aug 11 14:40:15.370: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:40:15.377
Aug 11 14:40:15.390: INFO: Waiting for pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc to disappear
Aug 11 14:40:15.392: INFO: Pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:15.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4441" for this suite. 08/11/23 14:40:15.395
------------------------------
• [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:11.334
    Aug 11 14:40:11.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:40:11.335
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:11.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:11.35
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-675ff644-2a13-4ffa-b1f0-9d971bb43c17 08/11/23 14:40:11.352
    STEP: Creating a pod to test consume secrets 08/11/23 14:40:11.356
    Aug 11 14:40:11.362: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc" in namespace "projected-4441" to be "Succeeded or Failed"
    Aug 11 14:40:11.364: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.373572ms
    Aug 11 14:40:13.371: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008750723s
    Aug 11 14:40:15.368: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005815657s
    STEP: Saw pod success 08/11/23 14:40:15.368
    Aug 11 14:40:15.368: INFO: Pod "pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc" satisfied condition "Succeeded or Failed"
    Aug 11 14:40:15.370: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:40:15.377
    Aug 11 14:40:15.390: INFO: Waiting for pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc to disappear
    Aug 11 14:40:15.392: INFO: Pod pod-projected-secrets-9ba98a38-45d7-437a-b3fe-ee44494d14bc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:15.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4441" for this suite. 08/11/23 14:40:15.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:15.4
Aug 11 14:40:15.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:40:15.401
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:15.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:15.413
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/11/23 14:40:15.416
Aug 11 14:40:15.422: INFO: Waiting up to 5m0s for pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6" in namespace "var-expansion-3542" to be "Succeeded or Failed"
Aug 11 14:40:15.424: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089102ms
Aug 11 14:40:17.428: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006308052s
Aug 11 14:40:19.427: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005337476s
STEP: Saw pod success 08/11/23 14:40:19.427
Aug 11 14:40:19.428: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6" satisfied condition "Succeeded or Failed"
Aug 11 14:40:19.430: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:40:19.437
Aug 11 14:40:19.450: INFO: Waiting for pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 to disappear
Aug 11 14:40:19.452: INFO: Pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:19.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3542" for this suite. 08/11/23 14:40:19.455
------------------------------
• [4.061 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:15.4
    Aug 11 14:40:15.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:40:15.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:15.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:15.413
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/11/23 14:40:15.416
    Aug 11 14:40:15.422: INFO: Waiting up to 5m0s for pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6" in namespace "var-expansion-3542" to be "Succeeded or Failed"
    Aug 11 14:40:15.424: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089102ms
    Aug 11 14:40:17.428: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006308052s
    Aug 11 14:40:19.427: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005337476s
    STEP: Saw pod success 08/11/23 14:40:19.427
    Aug 11 14:40:19.428: INFO: Pod "var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6" satisfied condition "Succeeded or Failed"
    Aug 11 14:40:19.430: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:40:19.437
    Aug 11 14:40:19.450: INFO: Waiting for pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 to disappear
    Aug 11 14:40:19.452: INFO: Pod var-expansion-feeb51b4-343e-4f83-bc9e-c9e97c8d43d6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:19.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3542" for this suite. 08/11/23 14:40:19.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:19.462
Aug 11 14:40:19.462: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:40:19.463
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.475
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/11/23 14:40:19.477
Aug 11 14:40:19.477: INFO: Creating e2e-svc-a-rzqsg
Aug 11 14:40:19.488: INFO: Creating e2e-svc-b-wvlnx
Aug 11 14:40:19.500: INFO: Creating e2e-svc-c-rl6l9
STEP: deleting service collection 08/11/23 14:40:19.514
Aug 11 14:40:19.546: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4731" for this suite. 08/11/23 14:40:19.549
------------------------------
• [0.092 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:19.462
    Aug 11 14:40:19.462: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:40:19.463
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.475
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/11/23 14:40:19.477
    Aug 11 14:40:19.477: INFO: Creating e2e-svc-a-rzqsg
    Aug 11 14:40:19.488: INFO: Creating e2e-svc-b-wvlnx
    Aug 11 14:40:19.500: INFO: Creating e2e-svc-c-rl6l9
    STEP: deleting service collection 08/11/23 14:40:19.514
    Aug 11 14:40:19.546: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4731" for this suite. 08/11/23 14:40:19.549
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:19.555
Aug 11 14:40:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:40:19.555
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.575
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/11/23 14:40:19.578
STEP: watching for the ServiceAccount to be added 08/11/23 14:40:19.584
STEP: patching the ServiceAccount 08/11/23 14:40:19.586
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/11/23 14:40:19.59
STEP: deleting the ServiceAccount 08/11/23 14:40:19.593
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:19.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7471" for this suite. 08/11/23 14:40:19.606
------------------------------
• [0.057 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:19.555
    Aug 11 14:40:19.555: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:40:19.555
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.575
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/11/23 14:40:19.578
    STEP: watching for the ServiceAccount to be added 08/11/23 14:40:19.584
    STEP: patching the ServiceAccount 08/11/23 14:40:19.586
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/11/23 14:40:19.59
    STEP: deleting the ServiceAccount 08/11/23 14:40:19.593
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:19.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7471" for this suite. 08/11/23 14:40:19.606
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:19.612
Aug 11 14:40:19.612: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:40:19.613
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.624
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/11/23 14:40:36.629
STEP: Creating a ResourceQuota 08/11/23 14:40:41.632
STEP: Ensuring resource quota status is calculated 08/11/23 14:40:41.637
STEP: Creating a ConfigMap 08/11/23 14:40:43.641
STEP: Ensuring resource quota status captures configMap creation 08/11/23 14:40:43.651
STEP: Deleting a ConfigMap 08/11/23 14:40:45.655
STEP: Ensuring resource quota status released usage 08/11/23 14:40:45.66
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:47.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5678" for this suite. 08/11/23 14:40:47.668
------------------------------
• [SLOW TEST] [28.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:19.612
    Aug 11 14:40:19.612: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:40:19.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:19.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:19.624
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/11/23 14:40:36.629
    STEP: Creating a ResourceQuota 08/11/23 14:40:41.632
    STEP: Ensuring resource quota status is calculated 08/11/23 14:40:41.637
    STEP: Creating a ConfigMap 08/11/23 14:40:43.641
    STEP: Ensuring resource quota status captures configMap creation 08/11/23 14:40:43.651
    STEP: Deleting a ConfigMap 08/11/23 14:40:45.655
    STEP: Ensuring resource quota status released usage 08/11/23 14:40:45.66
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:47.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5678" for this suite. 08/11/23 14:40:47.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:47.674
Aug 11 14:40:47.674: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:40:47.675
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:47.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:47.696
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-1ac356bd-3d64-4ecf-aa4f-d6e9c6fa965c 08/11/23 14:40:47.698
STEP: Creating a pod to test consume configMaps 08/11/23 14:40:47.701
Aug 11 14:40:47.709: INFO: Waiting up to 5m0s for pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9" in namespace "configmap-8666" to be "Succeeded or Failed"
Aug 11 14:40:47.711: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121983ms
Aug 11 14:40:49.715: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005732113s
Aug 11 14:40:51.718: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008511962s
STEP: Saw pod success 08/11/23 14:40:51.718
Aug 11 14:40:51.718: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9" satisfied condition "Succeeded or Failed"
Aug 11 14:40:51.720: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 container configmap-volume-test: <nil>
STEP: delete the pod 08/11/23 14:40:51.728
Aug 11 14:40:51.740: INFO: Waiting for pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 to disappear
Aug 11 14:40:51.742: INFO: Pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:40:51.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8666" for this suite. 08/11/23 14:40:51.745
------------------------------
• [4.075 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:47.674
    Aug 11 14:40:47.674: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:40:47.675
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:47.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:47.696
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-1ac356bd-3d64-4ecf-aa4f-d6e9c6fa965c 08/11/23 14:40:47.698
    STEP: Creating a pod to test consume configMaps 08/11/23 14:40:47.701
    Aug 11 14:40:47.709: INFO: Waiting up to 5m0s for pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9" in namespace "configmap-8666" to be "Succeeded or Failed"
    Aug 11 14:40:47.711: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121983ms
    Aug 11 14:40:49.715: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005732113s
    Aug 11 14:40:51.718: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008511962s
    STEP: Saw pod success 08/11/23 14:40:51.718
    Aug 11 14:40:51.718: INFO: Pod "pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9" satisfied condition "Succeeded or Failed"
    Aug 11 14:40:51.720: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 container configmap-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:40:51.728
    Aug 11 14:40:51.740: INFO: Waiting for pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 to disappear
    Aug 11 14:40:51.742: INFO: Pod pod-configmaps-afc7c3ad-8070-4125-8feb-86bd784062f9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:40:51.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8666" for this suite. 08/11/23 14:40:51.745
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:40:51.75
Aug 11 14:40:51.750: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename taint-single-pod 08/11/23 14:40:51.751
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:51.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:51.764
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 11 14:40:51.766: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:41:51.801: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 11 14:41:51.808: INFO: Starting informer...
STEP: Starting pod... 08/11/23 14:41:51.809
Aug 11 14:41:52.023: INFO: Pod is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
STEP: Trying to apply a taint on the Node 08/11/23 14:41:52.023
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:52.034
STEP: Waiting short time to make sure Pod is queued for deletion 08/11/23 14:41:52.04
Aug 11 14:41:52.040: INFO: Pod wasn't evicted. Proceeding
Aug 11 14:41:52.040: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:52.054
STEP: Waiting some time to make sure that toleration time passed. 08/11/23 14:41:52.071
Aug 11 14:43:07.073: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-6586" for this suite. 08/11/23 14:43:07.076
------------------------------
• [SLOW TEST] [135.331 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:40:51.75
    Aug 11 14:40:51.750: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename taint-single-pod 08/11/23 14:40:51.751
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:51.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:51.764
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 11 14:40:51.766: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:41:51.801: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 11 14:41:51.808: INFO: Starting informer...
    STEP: Starting pod... 08/11/23 14:41:51.809
    Aug 11 14:41:52.023: INFO: Pod is running on constell-1cf5d931-worker-6381a7ba-nd80. Tainting Node
    STEP: Trying to apply a taint on the Node 08/11/23 14:41:52.023
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:52.034
    STEP: Waiting short time to make sure Pod is queued for deletion 08/11/23 14:41:52.04
    Aug 11 14:41:52.040: INFO: Pod wasn't evicted. Proceeding
    Aug 11 14:41:52.040: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:52.054
    STEP: Waiting some time to make sure that toleration time passed. 08/11/23 14:41:52.071
    Aug 11 14:43:07.073: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-6586" for this suite. 08/11/23 14:43:07.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:07.081
Aug 11 14:43:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:43:07.082
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:07.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:07.098
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/11/23 14:43:07.1
Aug 11 14:43:07.107: INFO: Waiting up to 5m0s for pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1" in namespace "pods-9608" to be "running and ready"
Aug 11 14:43:07.109: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866813ms
Aug 11 14:43:07.109: INFO: The phase of Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:43:09.113: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006321182s
Aug 11 14:43:09.113: INFO: The phase of Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 is Running (Ready = true)
Aug 11 14:43:09.113: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1" satisfied condition "running and ready"
Aug 11 14:43:09.118: INFO: Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 has hostIP: 192.168.178.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:09.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9608" for this suite. 08/11/23 14:43:09.121
------------------------------
• [2.045 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:07.081
    Aug 11 14:43:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:43:07.082
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:07.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:07.098
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/11/23 14:43:07.1
    Aug 11 14:43:07.107: INFO: Waiting up to 5m0s for pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1" in namespace "pods-9608" to be "running and ready"
    Aug 11 14:43:07.109: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866813ms
    Aug 11 14:43:07.109: INFO: The phase of Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:43:09.113: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006321182s
    Aug 11 14:43:09.113: INFO: The phase of Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 is Running (Ready = true)
    Aug 11 14:43:09.113: INFO: Pod "pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1" satisfied condition "running and ready"
    Aug 11 14:43:09.118: INFO: Pod pod-hostip-86cd14c2-466e-4844-9fe9-7d34d79e50c1 has hostIP: 192.168.178.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:09.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9608" for this suite. 08/11/23 14:43:09.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:09.127
Aug 11 14:43:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:43:09.127
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:09.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:09.14
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-6a4d501f-a87b-4fba-9ebb-c10d283d63fc 08/11/23 14:43:09.142
STEP: Creating a pod to test consume secrets 08/11/23 14:43:09.146
Aug 11 14:43:09.152: INFO: Waiting up to 5m0s for pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13" in namespace "secrets-977" to be "Succeeded or Failed"
Aug 11 14:43:09.154: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061242ms
Aug 11 14:43:11.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005411282s
Aug 11 14:43:13.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005446178s
STEP: Saw pod success 08/11/23 14:43:13.157
Aug 11 14:43:13.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13" satisfied condition "Succeeded or Failed"
Aug 11 14:43:13.160: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:43:13.178
Aug 11 14:43:13.187: INFO: Waiting for pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 to disappear
Aug 11 14:43:13.190: INFO: Pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:13.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-977" for this suite. 08/11/23 14:43:13.193
------------------------------
• [4.072 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:09.127
    Aug 11 14:43:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:43:09.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:09.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:09.14
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-6a4d501f-a87b-4fba-9ebb-c10d283d63fc 08/11/23 14:43:09.142
    STEP: Creating a pod to test consume secrets 08/11/23 14:43:09.146
    Aug 11 14:43:09.152: INFO: Waiting up to 5m0s for pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13" in namespace "secrets-977" to be "Succeeded or Failed"
    Aug 11 14:43:09.154: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061242ms
    Aug 11 14:43:11.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005411282s
    Aug 11 14:43:13.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005446178s
    STEP: Saw pod success 08/11/23 14:43:13.157
    Aug 11 14:43:13.157: INFO: Pod "pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13" satisfied condition "Succeeded or Failed"
    Aug 11 14:43:13.160: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:43:13.178
    Aug 11 14:43:13.187: INFO: Waiting for pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 to disappear
    Aug 11 14:43:13.190: INFO: Pod pod-secrets-87ce4b40-4f8c-422b-be07-122e601f1a13 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:13.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-977" for this suite. 08/11/23 14:43:13.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:13.199
Aug 11 14:43:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:43:13.2
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:13.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:13.213
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:43:13.219
Aug 11 14:43:13.226: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8172" to be "running and ready"
Aug 11 14:43:13.228: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.753183ms
Aug 11 14:43:13.228: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:43:15.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396782s
Aug 11 14:43:15.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 14:43:15.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/11/23 14:43:15.234
Aug 11 14:43:15.239: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8172" to be "running and ready"
Aug 11 14:43:15.242: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602882ms
Aug 11 14:43:15.242: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:43:17.246: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006967972s
Aug 11 14:43:17.246: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 11 14:43:17.246: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/11/23 14:43:17.248
Aug 11 14:43:17.255: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 14:43:17.257: INFO: Pod pod-with-prestop-http-hook still exists
Aug 11 14:43:19.258: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 14:43:19.264: INFO: Pod pod-with-prestop-http-hook still exists
Aug 11 14:43:21.258: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 14:43:21.261: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/11/23 14:43:21.261
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:21.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8172" for this suite. 08/11/23 14:43:21.277
------------------------------
• [SLOW TEST] [8.083 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:13.199
    Aug 11 14:43:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:43:13.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:13.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:13.213
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:43:13.219
    Aug 11 14:43:13.226: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8172" to be "running and ready"
    Aug 11 14:43:13.228: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.753183ms
    Aug 11 14:43:13.228: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:43:15.232: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006396782s
    Aug 11 14:43:15.232: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 14:43:15.232: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/11/23 14:43:15.234
    Aug 11 14:43:15.239: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8172" to be "running and ready"
    Aug 11 14:43:15.242: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602882ms
    Aug 11 14:43:15.242: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:43:17.246: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006967972s
    Aug 11 14:43:17.246: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 11 14:43:17.246: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/11/23 14:43:17.248
    Aug 11 14:43:17.255: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 14:43:17.257: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 11 14:43:19.258: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 14:43:19.264: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 11 14:43:21.258: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 14:43:21.261: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/11/23 14:43:21.261
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:21.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8172" for this suite. 08/11/23 14:43:21.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:21.282
Aug 11 14:43:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:43:21.283
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:21.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:21.298
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:21.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3926" for this suite. 08/11/23 14:43:21.332
------------------------------
• [0.054 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:21.282
    Aug 11 14:43:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:43:21.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:21.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:21.298
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:21.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3926" for this suite. 08/11/23 14:43:21.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:21.337
Aug 11 14:43:21.337: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption 08/11/23 14:43:21.338
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:21.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:21.349
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/11/23 14:43:21.356
STEP: Waiting for all pods to be running 08/11/23 14:43:23.381
Aug 11 14:43:23.386: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:25.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5593" for this suite. 08/11/23 14:43:25.396
------------------------------
• [4.064 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:21.337
    Aug 11 14:43:21.337: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption 08/11/23 14:43:21.338
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:21.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:21.349
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/11/23 14:43:21.356
    STEP: Waiting for all pods to be running 08/11/23 14:43:23.381
    Aug 11 14:43:23.386: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:25.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5593" for this suite. 08/11/23 14:43:25.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:25.403
Aug 11 14:43:25.403: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:43:25.404
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:25.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:25.416
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/11/23 14:43:25.419
STEP: Creating a ResourceQuota 08/11/23 14:43:30.422
STEP: Ensuring resource quota status is calculated 08/11/23 14:43:30.426
STEP: Creating a ReplicaSet 08/11/23 14:43:32.43
STEP: Ensuring resource quota status captures replicaset creation 08/11/23 14:43:32.441
STEP: Deleting a ReplicaSet 08/11/23 14:43:34.445
STEP: Ensuring resource quota status released usage 08/11/23 14:43:34.45
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4533" for this suite. 08/11/23 14:43:36.457
------------------------------
• [SLOW TEST] [11.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:25.403
    Aug 11 14:43:25.403: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:43:25.404
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:25.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:25.416
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/11/23 14:43:25.419
    STEP: Creating a ResourceQuota 08/11/23 14:43:30.422
    STEP: Ensuring resource quota status is calculated 08/11/23 14:43:30.426
    STEP: Creating a ReplicaSet 08/11/23 14:43:32.43
    STEP: Ensuring resource quota status captures replicaset creation 08/11/23 14:43:32.441
    STEP: Deleting a ReplicaSet 08/11/23 14:43:34.445
    STEP: Ensuring resource quota status released usage 08/11/23 14:43:34.45
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4533" for this suite. 08/11/23 14:43:36.457
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:36.463
Aug 11 14:43:36.463: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:43:36.464
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:36.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:36.478
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/11/23 14:43:36.48
Aug 11 14:43:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 create -f -'
Aug 11 14:43:36.985: INFO: stderr: ""
Aug 11 14:43:36.985: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:36.985
Aug 11 14:43:36.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:43:37.039: INFO: stderr: ""
Aug 11 14:43:37.039: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
Aug 11 14:43:37.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:37.089: INFO: stderr: ""
Aug 11 14:43:37.089: INFO: stdout: ""
Aug 11 14:43:37.089: INFO: update-demo-nautilus-9kjpx is created but not running
Aug 11 14:43:42.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:43:42.145: INFO: stderr: ""
Aug 11 14:43:42.145: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
Aug 11 14:43:42.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:42.192: INFO: stderr: ""
Aug 11 14:43:42.192: INFO: stdout: "true"
Aug 11 14:43:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:43:42.240: INFO: stderr: ""
Aug 11 14:43:42.240: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:43:42.240: INFO: validating pod update-demo-nautilus-9kjpx
Aug 11 14:43:42.251: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:43:42.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:43:42.251: INFO: update-demo-nautilus-9kjpx is verified up and running
Aug 11 14:43:42.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-dt4ll -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:42.301: INFO: stderr: ""
Aug 11 14:43:42.301: INFO: stdout: "true"
Aug 11 14:43:42.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-dt4ll -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:43:42.350: INFO: stderr: ""
Aug 11 14:43:42.350: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:43:42.350: INFO: validating pod update-demo-nautilus-dt4ll
Aug 11 14:43:42.360: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:43:42.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:43:42.360: INFO: update-demo-nautilus-dt4ll is verified up and running
STEP: scaling down the replication controller 08/11/23 14:43:42.36
Aug 11 14:43:42.361: INFO: scanned /root for discovery docs: <nil>
Aug 11 14:43:42.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 11 14:43:43.425: INFO: stderr: ""
Aug 11 14:43:43.425: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:43.425
Aug 11 14:43:43.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:43:43.476: INFO: stderr: ""
Aug 11 14:43:43.476: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/11/23 14:43:43.476
Aug 11 14:43:48.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:43:48.530: INFO: stderr: ""
Aug 11 14:43:48.530: INFO: stdout: "update-demo-nautilus-9kjpx "
Aug 11 14:43:48.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:48.580: INFO: stderr: ""
Aug 11 14:43:48.580: INFO: stdout: "true"
Aug 11 14:43:48.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:43:48.629: INFO: stderr: ""
Aug 11 14:43:48.629: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:43:48.629: INFO: validating pod update-demo-nautilus-9kjpx
Aug 11 14:43:48.637: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:43:48.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:43:48.638: INFO: update-demo-nautilus-9kjpx is verified up and running
STEP: scaling up the replication controller 08/11/23 14:43:48.638
Aug 11 14:43:48.639: INFO: scanned /root for discovery docs: <nil>
Aug 11 14:43:48.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 11 14:43:49.702: INFO: stderr: ""
Aug 11 14:43:49.703: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:49.703
Aug 11 14:43:49.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:43:49.753: INFO: stderr: ""
Aug 11 14:43:49.753: INFO: stdout: "update-demo-nautilus-64jss update-demo-nautilus-9kjpx "
Aug 11 14:43:49.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-64jss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:49.800: INFO: stderr: ""
Aug 11 14:43:49.800: INFO: stdout: "true"
Aug 11 14:43:49.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-64jss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:43:49.848: INFO: stderr: ""
Aug 11 14:43:49.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:43:49.848: INFO: validating pod update-demo-nautilus-64jss
Aug 11 14:43:49.861: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:43:49.861: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:43:49.861: INFO: update-demo-nautilus-64jss is verified up and running
Aug 11 14:43:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:43:49.911: INFO: stderr: ""
Aug 11 14:43:49.911: INFO: stdout: "true"
Aug 11 14:43:49.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:43:49.965: INFO: stderr: ""
Aug 11 14:43:49.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:43:49.965: INFO: validating pod update-demo-nautilus-9kjpx
Aug 11 14:43:49.971: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:43:49.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:43:49.971: INFO: update-demo-nautilus-9kjpx is verified up and running
STEP: using delete to clean up resources 08/11/23 14:43:49.971
Aug 11 14:43:49.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 delete --grace-period=0 --force -f -'
Aug 11 14:43:50.025: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 14:43:50.025: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 11 14:43:50.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get rc,svc -l name=update-demo --no-headers'
Aug 11 14:43:50.086: INFO: stderr: "No resources found in kubectl-9786 namespace.\n"
Aug 11 14:43:50.086: INFO: stdout: ""
Aug 11 14:43:50.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 14:43:50.140: INFO: stderr: ""
Aug 11 14:43:50.140: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9786" for this suite. 08/11/23 14:43:50.144
------------------------------
• [SLOW TEST] [13.687 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:36.463
    Aug 11 14:43:36.463: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:43:36.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:36.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:36.478
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/11/23 14:43:36.48
    Aug 11 14:43:36.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 create -f -'
    Aug 11 14:43:36.985: INFO: stderr: ""
    Aug 11 14:43:36.985: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:36.985
    Aug 11 14:43:36.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:43:37.039: INFO: stderr: ""
    Aug 11 14:43:37.039: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
    Aug 11 14:43:37.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:37.089: INFO: stderr: ""
    Aug 11 14:43:37.089: INFO: stdout: ""
    Aug 11 14:43:37.089: INFO: update-demo-nautilus-9kjpx is created but not running
    Aug 11 14:43:42.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:43:42.145: INFO: stderr: ""
    Aug 11 14:43:42.145: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
    Aug 11 14:43:42.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:42.192: INFO: stderr: ""
    Aug 11 14:43:42.192: INFO: stdout: "true"
    Aug 11 14:43:42.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:43:42.240: INFO: stderr: ""
    Aug 11 14:43:42.240: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:43:42.240: INFO: validating pod update-demo-nautilus-9kjpx
    Aug 11 14:43:42.251: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:43:42.251: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:43:42.251: INFO: update-demo-nautilus-9kjpx is verified up and running
    Aug 11 14:43:42.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-dt4ll -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:42.301: INFO: stderr: ""
    Aug 11 14:43:42.301: INFO: stdout: "true"
    Aug 11 14:43:42.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-dt4ll -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:43:42.350: INFO: stderr: ""
    Aug 11 14:43:42.350: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:43:42.350: INFO: validating pod update-demo-nautilus-dt4ll
    Aug 11 14:43:42.360: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:43:42.360: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:43:42.360: INFO: update-demo-nautilus-dt4ll is verified up and running
    STEP: scaling down the replication controller 08/11/23 14:43:42.36
    Aug 11 14:43:42.361: INFO: scanned /root for discovery docs: <nil>
    Aug 11 14:43:42.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 11 14:43:43.425: INFO: stderr: ""
    Aug 11 14:43:43.425: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:43.425
    Aug 11 14:43:43.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:43:43.476: INFO: stderr: ""
    Aug 11 14:43:43.476: INFO: stdout: "update-demo-nautilus-9kjpx update-demo-nautilus-dt4ll "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/11/23 14:43:43.476
    Aug 11 14:43:48.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:43:48.530: INFO: stderr: ""
    Aug 11 14:43:48.530: INFO: stdout: "update-demo-nautilus-9kjpx "
    Aug 11 14:43:48.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:48.580: INFO: stderr: ""
    Aug 11 14:43:48.580: INFO: stdout: "true"
    Aug 11 14:43:48.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:43:48.629: INFO: stderr: ""
    Aug 11 14:43:48.629: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:43:48.629: INFO: validating pod update-demo-nautilus-9kjpx
    Aug 11 14:43:48.637: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:43:48.637: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:43:48.638: INFO: update-demo-nautilus-9kjpx is verified up and running
    STEP: scaling up the replication controller 08/11/23 14:43:48.638
    Aug 11 14:43:48.639: INFO: scanned /root for discovery docs: <nil>
    Aug 11 14:43:48.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 11 14:43:49.702: INFO: stderr: ""
    Aug 11 14:43:49.703: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:43:49.703
    Aug 11 14:43:49.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:43:49.753: INFO: stderr: ""
    Aug 11 14:43:49.753: INFO: stdout: "update-demo-nautilus-64jss update-demo-nautilus-9kjpx "
    Aug 11 14:43:49.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-64jss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:49.800: INFO: stderr: ""
    Aug 11 14:43:49.800: INFO: stdout: "true"
    Aug 11 14:43:49.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-64jss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:43:49.848: INFO: stderr: ""
    Aug 11 14:43:49.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:43:49.848: INFO: validating pod update-demo-nautilus-64jss
    Aug 11 14:43:49.861: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:43:49.861: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:43:49.861: INFO: update-demo-nautilus-64jss is verified up and running
    Aug 11 14:43:49.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:43:49.911: INFO: stderr: ""
    Aug 11 14:43:49.911: INFO: stdout: "true"
    Aug 11 14:43:49.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods update-demo-nautilus-9kjpx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:43:49.965: INFO: stderr: ""
    Aug 11 14:43:49.965: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:43:49.965: INFO: validating pod update-demo-nautilus-9kjpx
    Aug 11 14:43:49.971: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:43:49.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:43:49.971: INFO: update-demo-nautilus-9kjpx is verified up and running
    STEP: using delete to clean up resources 08/11/23 14:43:49.971
    Aug 11 14:43:49.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 delete --grace-period=0 --force -f -'
    Aug 11 14:43:50.025: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 14:43:50.025: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 11 14:43:50.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get rc,svc -l name=update-demo --no-headers'
    Aug 11 14:43:50.086: INFO: stderr: "No resources found in kubectl-9786 namespace.\n"
    Aug 11 14:43:50.086: INFO: stdout: ""
    Aug 11 14:43:50.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9786 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 14:43:50.140: INFO: stderr: ""
    Aug 11 14:43:50.140: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:50.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9786" for this suite. 08/11/23 14:43:50.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:50.15
Aug 11 14:43:50.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:43:50.151
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:50.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:50.167
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/11/23 14:43:50.169
STEP: Getting a ResourceQuota 08/11/23 14:43:50.173
STEP: Updating a ResourceQuota 08/11/23 14:43:50.176
STEP: Verifying a ResourceQuota was modified 08/11/23 14:43:50.18
STEP: Deleting a ResourceQuota 08/11/23 14:43:50.185
STEP: Verifying the deleted ResourceQuota 08/11/23 14:43:50.19
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:50.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5693" for this suite. 08/11/23 14:43:50.195
------------------------------
• [0.050 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:50.15
    Aug 11 14:43:50.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:43:50.151
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:50.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:50.167
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/11/23 14:43:50.169
    STEP: Getting a ResourceQuota 08/11/23 14:43:50.173
    STEP: Updating a ResourceQuota 08/11/23 14:43:50.176
    STEP: Verifying a ResourceQuota was modified 08/11/23 14:43:50.18
    STEP: Deleting a ResourceQuota 08/11/23 14:43:50.185
    STEP: Verifying the deleted ResourceQuota 08/11/23 14:43:50.19
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:50.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5693" for this suite. 08/11/23 14:43:50.195
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:50.2
Aug 11 14:43:50.201: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption 08/11/23 14:43:50.201
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:50.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:50.215
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/11/23 14:43:50.218
STEP: Waiting for the pdb to be processed 08/11/23 14:43:50.222
STEP: updating the pdb 08/11/23 14:43:52.228
STEP: Waiting for the pdb to be processed 08/11/23 14:43:52.236
STEP: patching the pdb 08/11/23 14:43:54.244
STEP: Waiting for the pdb to be processed 08/11/23 14:43:54.252
STEP: Waiting for the pdb to be deleted 08/11/23 14:43:56.262
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:43:56.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6960" for this suite. 08/11/23 14:43:56.267
------------------------------
• [SLOW TEST] [6.073 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:50.2
    Aug 11 14:43:50.201: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption 08/11/23 14:43:50.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:50.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:50.215
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/11/23 14:43:50.218
    STEP: Waiting for the pdb to be processed 08/11/23 14:43:50.222
    STEP: updating the pdb 08/11/23 14:43:52.228
    STEP: Waiting for the pdb to be processed 08/11/23 14:43:52.236
    STEP: patching the pdb 08/11/23 14:43:54.244
    STEP: Waiting for the pdb to be processed 08/11/23 14:43:54.252
    STEP: Waiting for the pdb to be deleted 08/11/23 14:43:56.262
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:43:56.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6960" for this suite. 08/11/23 14:43:56.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:43:56.275
Aug 11 14:43:56.275: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:43:56.275
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:56.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:56.288
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 11 14:43:56.290: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 11 14:43:56.297: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 14:44:01.301: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:44:01.301
Aug 11 14:44:01.301: INFO: Creating deployment "test-rolling-update-deployment"
Aug 11 14:44:01.308: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 11 14:44:01.312: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 11 14:44:03.319: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 11 14:44:03.321: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:44:03.328: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2956  37f3b3b7-9e95-493a-87a7-102eba50b309 33735 1 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003663af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:44:01 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-11 14:44:02 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:44:03.330: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2956  ccd047ac-576a-4da0-9a02-03045e61b8f7 33724 1 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 37f3b3b7-9e95-493a-87a7-102eba50b309 0xc003663fe7 0xc003663fe8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37f3b3b7-9e95-493a-87a7-102eba50b309\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005616098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:44:03.330: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 11 14:44:03.330: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2956  3e20cada-fb2f-4b64-a9c5-1cb9912d30be 33733 2 2023-08-11 14:43:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 37f3b3b7-9e95-493a-87a7-102eba50b309 0xc003663eb7 0xc003663eb8}] [] [{e2e.test Update apps/v1 2023-08-11 14:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37f3b3b7-9e95-493a-87a7-102eba50b309\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003663f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:44:03.332: INFO: Pod "test-rolling-update-deployment-7549d9f46d-j89ng" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-j89ng test-rolling-update-deployment-7549d9f46d- deployment-2956  576c0962-17e0-42ef-b1a6-8e78fd5dff28 33723 0 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ccd047ac-576a-4da0-9a02-03045e61b8f7 0xc005b71377 0xc005b71378}] [] [{kube-controller-manager Update v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd047ac-576a-4da0-9a02-03045e61b8f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxgnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxgnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.141,StartTime:2023-08-11 14:44:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:44:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7b98ceb55f2a4ddff67a5f7642a875119982bb8fefbf08765d8a75ac3353e068,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:03.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2956" for this suite. 08/11/23 14:44:03.336
------------------------------
• [SLOW TEST] [7.066 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:43:56.275
    Aug 11 14:43:56.275: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:43:56.275
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:56.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:56.288
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 11 14:43:56.290: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 11 14:43:56.297: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 14:44:01.301: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:44:01.301
    Aug 11 14:44:01.301: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 11 14:44:01.308: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 11 14:44:01.312: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 11 14:44:03.319: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 11 14:44:03.321: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:44:03.328: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2956  37f3b3b7-9e95-493a-87a7-102eba50b309 33735 1 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003663af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:44:01 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-11 14:44:02 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:44:03.330: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2956  ccd047ac-576a-4da0-9a02-03045e61b8f7 33724 1 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 37f3b3b7-9e95-493a-87a7-102eba50b309 0xc003663fe7 0xc003663fe8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37f3b3b7-9e95-493a-87a7-102eba50b309\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005616098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:44:03.330: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 11 14:44:03.330: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2956  3e20cada-fb2f-4b64-a9c5-1cb9912d30be 33733 2 2023-08-11 14:43:56 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 37f3b3b7-9e95-493a-87a7-102eba50b309 0xc003663eb7 0xc003663eb8}] [] [{e2e.test Update apps/v1 2023-08-11 14:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37f3b3b7-9e95-493a-87a7-102eba50b309\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003663f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:44:03.332: INFO: Pod "test-rolling-update-deployment-7549d9f46d-j89ng" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-j89ng test-rolling-update-deployment-7549d9f46d- deployment-2956  576c0962-17e0-42ef-b1a6-8e78fd5dff28 33723 0 2023-08-11 14:44:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ccd047ac-576a-4da0-9a02-03045e61b8f7 0xc005b71377 0xc005b71378}] [] [{kube-controller-manager Update v1 2023-08-11 14:44:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccd047ac-576a-4da0-9a02-03045e61b8f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:44:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxgnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxgnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:44:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.141,StartTime:2023-08-11 14:44:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:44:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7b98ceb55f2a4ddff67a5f7642a875119982bb8fefbf08765d8a75ac3353e068,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:03.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2956" for this suite. 08/11/23 14:44:03.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:03.342
Aug 11 14:44:03.342: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:44:03.343
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:03.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:03.357
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6535 08/11/23 14:44:03.359
STEP: changing the ExternalName service to type=NodePort 08/11/23 14:44:03.363
STEP: creating replication controller externalname-service in namespace services-6535 08/11/23 14:44:03.384
I0811 14:44:03.390620      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6535, replica count: 2
I0811 14:44:06.442096      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:44:06.442: INFO: Creating new exec pod
Aug 11 14:44:06.448: INFO: Waiting up to 5m0s for pod "execpods7jkd" in namespace "services-6535" to be "running"
Aug 11 14:44:06.452: INFO: Pod "execpods7jkd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608774ms
Aug 11 14:44:08.456: INFO: Pod "execpods7jkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007973745s
Aug 11 14:44:08.456: INFO: Pod "execpods7jkd" satisfied condition "running"
Aug 11 14:44:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 11 14:44:09.595: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 14:44:09.595: INFO: stdout: ""
Aug 11 14:44:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 10.98.164.89 80'
Aug 11 14:44:09.719: INFO: stderr: "+ nc -v -z -w 2 10.98.164.89 80\nConnection to 10.98.164.89 80 port [tcp/http] succeeded!\n"
Aug 11 14:44:09.719: INFO: stdout: ""
Aug 11 14:44:09.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 30091'
Aug 11 14:44:09.841: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 30091\nConnection to 192.168.178.2 30091 port [tcp/*] succeeded!\n"
Aug 11 14:44:09.841: INFO: stdout: ""
Aug 11 14:44:09.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 30091'
Aug 11 14:44:09.960: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 30091\nConnection to 192.168.178.3 30091 port [tcp/*] succeeded!\n"
Aug 11 14:44:09.960: INFO: stdout: ""
Aug 11 14:44:09.960: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:09.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6535" for this suite. 08/11/23 14:44:09.988
------------------------------
• [SLOW TEST] [6.652 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:03.342
    Aug 11 14:44:03.342: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:44:03.343
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:03.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:03.357
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6535 08/11/23 14:44:03.359
    STEP: changing the ExternalName service to type=NodePort 08/11/23 14:44:03.363
    STEP: creating replication controller externalname-service in namespace services-6535 08/11/23 14:44:03.384
    I0811 14:44:03.390620      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6535, replica count: 2
    I0811 14:44:06.442096      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:44:06.442: INFO: Creating new exec pod
    Aug 11 14:44:06.448: INFO: Waiting up to 5m0s for pod "execpods7jkd" in namespace "services-6535" to be "running"
    Aug 11 14:44:06.452: INFO: Pod "execpods7jkd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608774ms
    Aug 11 14:44:08.456: INFO: Pod "execpods7jkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007973745s
    Aug 11 14:44:08.456: INFO: Pod "execpods7jkd" satisfied condition "running"
    Aug 11 14:44:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 11 14:44:09.595: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 14:44:09.595: INFO: stdout: ""
    Aug 11 14:44:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 10.98.164.89 80'
    Aug 11 14:44:09.719: INFO: stderr: "+ nc -v -z -w 2 10.98.164.89 80\nConnection to 10.98.164.89 80 port [tcp/http] succeeded!\n"
    Aug 11 14:44:09.719: INFO: stdout: ""
    Aug 11 14:44:09.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 30091'
    Aug 11 14:44:09.841: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 30091\nConnection to 192.168.178.2 30091 port [tcp/*] succeeded!\n"
    Aug 11 14:44:09.841: INFO: stdout: ""
    Aug 11 14:44:09.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6535 exec execpods7jkd -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 30091'
    Aug 11 14:44:09.960: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 30091\nConnection to 192.168.178.3 30091 port [tcp/*] succeeded!\n"
    Aug 11 14:44:09.960: INFO: stdout: ""
    Aug 11 14:44:09.960: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:09.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6535" for this suite. 08/11/23 14:44:09.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:09.994
Aug 11 14:44:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:44:09.995
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:10.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:10.009
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-4acdc819-a604-47aa-b4a4-08f064a0b73d 08/11/23 14:44:10.012
STEP: Creating a pod to test consume configMaps 08/11/23 14:44:10.016
Aug 11 14:44:10.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1" in namespace "configmap-6825" to be "Succeeded or Failed"
Aug 11 14:44:10.024: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257932ms
Aug 11 14:44:12.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006883423s
Aug 11 14:44:14.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006521437s
STEP: Saw pod success 08/11/23 14:44:14.029
Aug 11 14:44:14.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1" satisfied condition "Succeeded or Failed"
Aug 11 14:44:14.031: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:44:14.039
Aug 11 14:44:14.051: INFO: Waiting for pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 to disappear
Aug 11 14:44:14.053: INFO: Pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:14.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6825" for this suite. 08/11/23 14:44:14.057
------------------------------
• [4.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:09.994
    Aug 11 14:44:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:44:09.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:10.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:10.009
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-4acdc819-a604-47aa-b4a4-08f064a0b73d 08/11/23 14:44:10.012
    STEP: Creating a pod to test consume configMaps 08/11/23 14:44:10.016
    Aug 11 14:44:10.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1" in namespace "configmap-6825" to be "Succeeded or Failed"
    Aug 11 14:44:10.024: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257932ms
    Aug 11 14:44:12.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006883423s
    Aug 11 14:44:14.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006521437s
    STEP: Saw pod success 08/11/23 14:44:14.029
    Aug 11 14:44:14.029: INFO: Pod "pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:14.031: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:44:14.039
    Aug 11 14:44:14.051: INFO: Waiting for pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 to disappear
    Aug 11 14:44:14.053: INFO: Pod pod-configmaps-2edec07b-cc91-4f63-9096-e8ace9b15fb1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:14.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6825" for this suite. 08/11/23 14:44:14.057
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:14.062
Aug 11 14:44:14.062: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename namespaces 08/11/23 14:44:14.063
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:14.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:14.077
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/11/23 14:44:14.079
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:14.091
STEP: Creating a service in the namespace 08/11/23 14:44:14.093
STEP: Deleting the namespace 08/11/23 14:44:14.105
STEP: Waiting for the namespace to be removed. 08/11/23 14:44:14.114
STEP: Recreating the namespace 08/11/23 14:44:20.117
STEP: Verifying there is no service in the namespace 08/11/23 14:44:20.13
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:20.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1669" for this suite. 08/11/23 14:44:20.136
STEP: Destroying namespace "nsdeletetest-1051" for this suite. 08/11/23 14:44:20.143
Aug 11 14:44:20.144: INFO: Namespace nsdeletetest-1051 was already deleted
STEP: Destroying namespace "nsdeletetest-5404" for this suite. 08/11/23 14:44:20.144
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:14.062
    Aug 11 14:44:14.062: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:44:14.063
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:14.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:14.077
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/11/23 14:44:14.079
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:14.091
    STEP: Creating a service in the namespace 08/11/23 14:44:14.093
    STEP: Deleting the namespace 08/11/23 14:44:14.105
    STEP: Waiting for the namespace to be removed. 08/11/23 14:44:14.114
    STEP: Recreating the namespace 08/11/23 14:44:20.117
    STEP: Verifying there is no service in the namespace 08/11/23 14:44:20.13
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:20.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1669" for this suite. 08/11/23 14:44:20.136
    STEP: Destroying namespace "nsdeletetest-1051" for this suite. 08/11/23 14:44:20.143
    Aug 11 14:44:20.144: INFO: Namespace nsdeletetest-1051 was already deleted
    STEP: Destroying namespace "nsdeletetest-5404" for this suite. 08/11/23 14:44:20.144
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:20.149
Aug 11 14:44:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename podtemplate 08/11/23 14:44:20.15
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:20.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:20.163
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/11/23 14:44:20.165
Aug 11 14:44:20.169: INFO: created test-podtemplate-1
Aug 11 14:44:20.173: INFO: created test-podtemplate-2
Aug 11 14:44:20.177: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/11/23 14:44:20.177
STEP: delete collection of pod templates 08/11/23 14:44:20.179
Aug 11 14:44:20.179: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/11/23 14:44:20.192
Aug 11 14:44:20.192: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:20.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3349" for this suite. 08/11/23 14:44:20.197
------------------------------
• [0.052 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:20.149
    Aug 11 14:44:20.149: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename podtemplate 08/11/23 14:44:20.15
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:20.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:20.163
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/11/23 14:44:20.165
    Aug 11 14:44:20.169: INFO: created test-podtemplate-1
    Aug 11 14:44:20.173: INFO: created test-podtemplate-2
    Aug 11 14:44:20.177: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/11/23 14:44:20.177
    STEP: delete collection of pod templates 08/11/23 14:44:20.179
    Aug 11 14:44:20.179: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/11/23 14:44:20.192
    Aug 11 14:44:20.192: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:20.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3349" for this suite. 08/11/23 14:44:20.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:20.202
Aug 11 14:44:20.202: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:20.203
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:20.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:20.215
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:20.217
Aug 11 14:44:20.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c" in namespace "downward-api-1496" to be "Succeeded or Failed"
Aug 11 14:44:20.226: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137362ms
Aug 11 14:44:22.230: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005874812s
Aug 11 14:44:24.230: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006576399s
STEP: Saw pod success 08/11/23 14:44:24.23
Aug 11 14:44:24.231: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c" satisfied condition "Succeeded or Failed"
Aug 11 14:44:24.233: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c container client-container: <nil>
STEP: delete the pod 08/11/23 14:44:24.241
Aug 11 14:44:24.252: INFO: Waiting for pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c to disappear
Aug 11 14:44:24.254: INFO: Pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1496" for this suite. 08/11/23 14:44:24.257
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:20.202
    Aug 11 14:44:20.202: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:20.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:20.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:20.215
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:20.217
    Aug 11 14:44:20.224: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c" in namespace "downward-api-1496" to be "Succeeded or Failed"
    Aug 11 14:44:20.226: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137362ms
    Aug 11 14:44:22.230: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005874812s
    Aug 11 14:44:24.230: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006576399s
    STEP: Saw pod success 08/11/23 14:44:24.23
    Aug 11 14:44:24.231: INFO: Pod "downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:24.233: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c container client-container: <nil>
    STEP: delete the pod 08/11/23 14:44:24.241
    Aug 11 14:44:24.252: INFO: Waiting for pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c to disappear
    Aug 11 14:44:24.254: INFO: Pod downwardapi-volume-b0f0b1f0-852a-4ed0-a6ba-646cb8a4428c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1496" for this suite. 08/11/23 14:44:24.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:24.263
Aug 11 14:44:24.263: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename discovery 08/11/23 14:44:24.264
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.277
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/11/23 14:44:24.28
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 11 14:44:24.496: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 11 14:44:24.497: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 11 14:44:24.497: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 11 14:44:24.497: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 11 14:44:24.497: INFO: Checking APIGroup: apps
Aug 11 14:44:24.498: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 11 14:44:24.498: INFO: Versions found [{apps/v1 v1}]
Aug 11 14:44:24.498: INFO: apps/v1 matches apps/v1
Aug 11 14:44:24.498: INFO: Checking APIGroup: events.k8s.io
Aug 11 14:44:24.499: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 11 14:44:24.499: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 11 14:44:24.499: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 11 14:44:24.499: INFO: Checking APIGroup: authentication.k8s.io
Aug 11 14:44:24.500: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 11 14:44:24.500: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 11 14:44:24.500: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 11 14:44:24.500: INFO: Checking APIGroup: authorization.k8s.io
Aug 11 14:44:24.501: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 11 14:44:24.501: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 11 14:44:24.501: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 11 14:44:24.501: INFO: Checking APIGroup: autoscaling
Aug 11 14:44:24.502: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 11 14:44:24.502: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 11 14:44:24.502: INFO: autoscaling/v2 matches autoscaling/v2
Aug 11 14:44:24.502: INFO: Checking APIGroup: batch
Aug 11 14:44:24.502: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 11 14:44:24.502: INFO: Versions found [{batch/v1 v1}]
Aug 11 14:44:24.502: INFO: batch/v1 matches batch/v1
Aug 11 14:44:24.502: INFO: Checking APIGroup: certificates.k8s.io
Aug 11 14:44:24.503: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 11 14:44:24.503: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 11 14:44:24.503: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 11 14:44:24.503: INFO: Checking APIGroup: networking.k8s.io
Aug 11 14:44:24.504: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 11 14:44:24.504: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 11 14:44:24.504: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 11 14:44:24.504: INFO: Checking APIGroup: policy
Aug 11 14:44:24.505: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 11 14:44:24.505: INFO: Versions found [{policy/v1 v1}]
Aug 11 14:44:24.505: INFO: policy/v1 matches policy/v1
Aug 11 14:44:24.505: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 11 14:44:24.506: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 11 14:44:24.506: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 11 14:44:24.506: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 11 14:44:24.506: INFO: Checking APIGroup: storage.k8s.io
Aug 11 14:44:24.507: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 11 14:44:24.507: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 11 14:44:24.507: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 11 14:44:24.507: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 11 14:44:24.508: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 11 14:44:24.508: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 11 14:44:24.508: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 11 14:44:24.508: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 11 14:44:24.509: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 11 14:44:24.509: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 11 14:44:24.509: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 11 14:44:24.509: INFO: Checking APIGroup: scheduling.k8s.io
Aug 11 14:44:24.509: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 11 14:44:24.509: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 11 14:44:24.509: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 11 14:44:24.509: INFO: Checking APIGroup: coordination.k8s.io
Aug 11 14:44:24.510: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 11 14:44:24.510: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 11 14:44:24.510: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 11 14:44:24.510: INFO: Checking APIGroup: node.k8s.io
Aug 11 14:44:24.514: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 11 14:44:24.514: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 11 14:44:24.514: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 11 14:44:24.514: INFO: Checking APIGroup: discovery.k8s.io
Aug 11 14:44:24.515: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 11 14:44:24.515: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 11 14:44:24.515: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 11 14:44:24.515: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 11 14:44:24.516: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 11 14:44:24.516: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 11 14:44:24.516: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 11 14:44:24.516: INFO: Checking APIGroup: acme.cert-manager.io
Aug 11 14:44:24.517: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Aug 11 14:44:24.517: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Aug 11 14:44:24.517: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Aug 11 14:44:24.517: INFO: Checking APIGroup: cert-manager.io
Aug 11 14:44:24.517: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Aug 11 14:44:24.517: INFO: Versions found [{cert-manager.io/v1 v1}]
Aug 11 14:44:24.517: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Aug 11 14:44:24.517: INFO: Checking APIGroup: update.edgeless.systems
Aug 11 14:44:24.518: INFO: PreferredVersion.GroupVersion: update.edgeless.systems/v1alpha1
Aug 11 14:44:24.518: INFO: Versions found [{update.edgeless.systems/v1alpha1 v1alpha1}]
Aug 11 14:44:24.518: INFO: update.edgeless.systems/v1alpha1 matches update.edgeless.systems/v1alpha1
Aug 11 14:44:24.518: INFO: Checking APIGroup: nodemaintenance.medik8s.io
Aug 11 14:44:24.519: INFO: PreferredVersion.GroupVersion: nodemaintenance.medik8s.io/v1beta1
Aug 11 14:44:24.519: INFO: Versions found [{nodemaintenance.medik8s.io/v1beta1 v1beta1}]
Aug 11 14:44:24.519: INFO: nodemaintenance.medik8s.io/v1beta1 matches nodemaintenance.medik8s.io/v1beta1
Aug 11 14:44:24.519: INFO: Checking APIGroup: cilium.io
Aug 11 14:44:24.520: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Aug 11 14:44:24.520: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Aug 11 14:44:24.520: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 11 14:44:24.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3471" for this suite. 08/11/23 14:44:24.524
------------------------------
• [0.266 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:24.263
    Aug 11 14:44:24.263: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename discovery 08/11/23 14:44:24.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.277
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/11/23 14:44:24.28
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 11 14:44:24.496: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 11 14:44:24.497: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 11 14:44:24.497: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 11 14:44:24.497: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 11 14:44:24.497: INFO: Checking APIGroup: apps
    Aug 11 14:44:24.498: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 11 14:44:24.498: INFO: Versions found [{apps/v1 v1}]
    Aug 11 14:44:24.498: INFO: apps/v1 matches apps/v1
    Aug 11 14:44:24.498: INFO: Checking APIGroup: events.k8s.io
    Aug 11 14:44:24.499: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 11 14:44:24.499: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 11 14:44:24.499: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 11 14:44:24.499: INFO: Checking APIGroup: authentication.k8s.io
    Aug 11 14:44:24.500: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 11 14:44:24.500: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 11 14:44:24.500: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 11 14:44:24.500: INFO: Checking APIGroup: authorization.k8s.io
    Aug 11 14:44:24.501: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 11 14:44:24.501: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 11 14:44:24.501: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 11 14:44:24.501: INFO: Checking APIGroup: autoscaling
    Aug 11 14:44:24.502: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 11 14:44:24.502: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 11 14:44:24.502: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 11 14:44:24.502: INFO: Checking APIGroup: batch
    Aug 11 14:44:24.502: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 11 14:44:24.502: INFO: Versions found [{batch/v1 v1}]
    Aug 11 14:44:24.502: INFO: batch/v1 matches batch/v1
    Aug 11 14:44:24.502: INFO: Checking APIGroup: certificates.k8s.io
    Aug 11 14:44:24.503: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 11 14:44:24.503: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 11 14:44:24.503: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 11 14:44:24.503: INFO: Checking APIGroup: networking.k8s.io
    Aug 11 14:44:24.504: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 11 14:44:24.504: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 11 14:44:24.504: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 11 14:44:24.504: INFO: Checking APIGroup: policy
    Aug 11 14:44:24.505: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 11 14:44:24.505: INFO: Versions found [{policy/v1 v1}]
    Aug 11 14:44:24.505: INFO: policy/v1 matches policy/v1
    Aug 11 14:44:24.505: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 11 14:44:24.506: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 11 14:44:24.506: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 11 14:44:24.506: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 11 14:44:24.506: INFO: Checking APIGroup: storage.k8s.io
    Aug 11 14:44:24.507: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 11 14:44:24.507: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 11 14:44:24.507: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 11 14:44:24.507: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 11 14:44:24.508: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 11 14:44:24.508: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 11 14:44:24.508: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 11 14:44:24.508: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 11 14:44:24.509: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 11 14:44:24.509: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 11 14:44:24.509: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 11 14:44:24.509: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 11 14:44:24.509: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 11 14:44:24.509: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 11 14:44:24.509: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 11 14:44:24.509: INFO: Checking APIGroup: coordination.k8s.io
    Aug 11 14:44:24.510: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 11 14:44:24.510: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 11 14:44:24.510: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 11 14:44:24.510: INFO: Checking APIGroup: node.k8s.io
    Aug 11 14:44:24.514: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 11 14:44:24.514: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 11 14:44:24.514: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 11 14:44:24.514: INFO: Checking APIGroup: discovery.k8s.io
    Aug 11 14:44:24.515: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 11 14:44:24.515: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 11 14:44:24.515: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 11 14:44:24.515: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 11 14:44:24.516: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 11 14:44:24.516: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 11 14:44:24.516: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 11 14:44:24.516: INFO: Checking APIGroup: acme.cert-manager.io
    Aug 11 14:44:24.517: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Aug 11 14:44:24.517: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Aug 11 14:44:24.517: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Aug 11 14:44:24.517: INFO: Checking APIGroup: cert-manager.io
    Aug 11 14:44:24.517: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Aug 11 14:44:24.517: INFO: Versions found [{cert-manager.io/v1 v1}]
    Aug 11 14:44:24.517: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Aug 11 14:44:24.517: INFO: Checking APIGroup: update.edgeless.systems
    Aug 11 14:44:24.518: INFO: PreferredVersion.GroupVersion: update.edgeless.systems/v1alpha1
    Aug 11 14:44:24.518: INFO: Versions found [{update.edgeless.systems/v1alpha1 v1alpha1}]
    Aug 11 14:44:24.518: INFO: update.edgeless.systems/v1alpha1 matches update.edgeless.systems/v1alpha1
    Aug 11 14:44:24.518: INFO: Checking APIGroup: nodemaintenance.medik8s.io
    Aug 11 14:44:24.519: INFO: PreferredVersion.GroupVersion: nodemaintenance.medik8s.io/v1beta1
    Aug 11 14:44:24.519: INFO: Versions found [{nodemaintenance.medik8s.io/v1beta1 v1beta1}]
    Aug 11 14:44:24.519: INFO: nodemaintenance.medik8s.io/v1beta1 matches nodemaintenance.medik8s.io/v1beta1
    Aug 11 14:44:24.519: INFO: Checking APIGroup: cilium.io
    Aug 11 14:44:24.520: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Aug 11 14:44:24.520: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Aug 11 14:44:24.520: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:44:24.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3471" for this suite. 08/11/23 14:44:24.524
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:44:24.529
Aug 11 14:44:24.529: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename cronjob 08/11/23 14:44:24.53
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.546
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/11/23 14:44:24.548
STEP: Ensuring more than one job is running at a time 08/11/23 14:44:24.553
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/11/23 14:46:00.557
STEP: Removing cronjob 08/11/23 14:46:00.561
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:00.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7518" for this suite. 08/11/23 14:46:00.569
------------------------------
• [SLOW TEST] [96.045 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:44:24.529
    Aug 11 14:44:24.529: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:44:24.53
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.546
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/11/23 14:44:24.548
    STEP: Ensuring more than one job is running at a time 08/11/23 14:44:24.553
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/11/23 14:46:00.557
    STEP: Removing cronjob 08/11/23 14:46:00.561
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:00.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7518" for this suite. 08/11/23 14:46:00.569
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:00.575
Aug 11 14:46:00.575: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 14:46:00.576
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:00.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:00.593
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 11 14:46:00.612: INFO: Waiting up to 5m0s for pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893" in namespace "emptydir-wrapper-1417" to be "running and ready"
Aug 11 14:46:00.614: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327972ms
Aug 11 14:46:00.614: INFO: The phase of Pod pod-secrets-1165598f-ba66-4c44-babf-168a229cc893 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:46:02.619: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893": Phase="Running", Reason="", readiness=true. Elapsed: 2.006905282s
Aug 11 14:46:02.619: INFO: The phase of Pod pod-secrets-1165598f-ba66-4c44-babf-168a229cc893 is Running (Ready = true)
Aug 11 14:46:02.619: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/11/23 14:46:02.622
STEP: Cleaning up the configmap 08/11/23 14:46:02.627
STEP: Cleaning up the pod 08/11/23 14:46:02.632
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:02.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1417" for this suite. 08/11/23 14:46:02.646
------------------------------
• [2.078 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:00.575
    Aug 11 14:46:00.575: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 14:46:00.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:00.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:00.593
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 11 14:46:00.612: INFO: Waiting up to 5m0s for pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893" in namespace "emptydir-wrapper-1417" to be "running and ready"
    Aug 11 14:46:00.614: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327972ms
    Aug 11 14:46:00.614: INFO: The phase of Pod pod-secrets-1165598f-ba66-4c44-babf-168a229cc893 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:46:02.619: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893": Phase="Running", Reason="", readiness=true. Elapsed: 2.006905282s
    Aug 11 14:46:02.619: INFO: The phase of Pod pod-secrets-1165598f-ba66-4c44-babf-168a229cc893 is Running (Ready = true)
    Aug 11 14:46:02.619: INFO: Pod "pod-secrets-1165598f-ba66-4c44-babf-168a229cc893" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/11/23 14:46:02.622
    STEP: Cleaning up the configmap 08/11/23 14:46:02.627
    STEP: Cleaning up the pod 08/11/23 14:46:02.632
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:02.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1417" for this suite. 08/11/23 14:46:02.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:02.656
Aug 11 14:46:02.657: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:46:02.657
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:02.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:02.673
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-5bb931e6-510a-4fd2-a3de-4be77120331b 08/11/23 14:46:02.678
STEP: Creating the pod 08/11/23 14:46:02.682
Aug 11 14:46:02.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af" in namespace "configmap-9099" to be "running and ready"
Aug 11 14:46:02.690: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118953ms
Aug 11 14:46:02.690: INFO: The phase of Pod pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:46:04.694: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af": Phase="Running", Reason="", readiness=true. Elapsed: 2.005957422s
Aug 11 14:46:04.694: INFO: The phase of Pod pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af is Running (Ready = true)
Aug 11 14:46:04.694: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-5bb931e6-510a-4fd2-a3de-4be77120331b 08/11/23 14:46:04.714
STEP: waiting to observe update in volume 08/11/23 14:46:04.719
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:06.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9099" for this suite. 08/11/23 14:46:06.74
------------------------------
• [4.088 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:02.656
    Aug 11 14:46:02.657: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:46:02.657
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:02.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:02.673
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-5bb931e6-510a-4fd2-a3de-4be77120331b 08/11/23 14:46:02.678
    STEP: Creating the pod 08/11/23 14:46:02.682
    Aug 11 14:46:02.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af" in namespace "configmap-9099" to be "running and ready"
    Aug 11 14:46:02.690: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118953ms
    Aug 11 14:46:02.690: INFO: The phase of Pod pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:46:04.694: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af": Phase="Running", Reason="", readiness=true. Elapsed: 2.005957422s
    Aug 11 14:46:04.694: INFO: The phase of Pod pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af is Running (Ready = true)
    Aug 11 14:46:04.694: INFO: Pod "pod-configmaps-c435c859-52e0-44c8-b8a7-7f06b1c4e0af" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-5bb931e6-510a-4fd2-a3de-4be77120331b 08/11/23 14:46:04.714
    STEP: waiting to observe update in volume 08/11/23 14:46:04.719
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:06.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9099" for this suite. 08/11/23 14:46:06.74
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:06.745
Aug 11 14:46:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:46:06.746
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:06.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:06.761
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:46:06.777
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:46:06.781
Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:06.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:46:06.787: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:46:07.795: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:07.796: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:07.796: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:07.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:46:07.798: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:46:08.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:46:08.795: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 08/11/23 14:46:08.797
Aug 11 14:46:08.799: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/11/23 14:46:08.799
Aug 11 14:46:08.808: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/11/23 14:46:08.808
Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: ADDED
Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.810: INFO: Found daemon set daemon-set in namespace daemonsets-6568 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:46:08.810: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/11/23 14:46:08.81
STEP: watching for the daemon set status to be patched 08/11/23 14:46:08.817
Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: ADDED
Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.819: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.819: INFO: Observed daemon set daemon-set in namespace daemonsets-6568 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:46:08.819: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:46:08.819: INFO: Found daemon set daemon-set in namespace daemonsets-6568 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 11 14:46:08.819: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:46:08.822
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6568, will wait for the garbage collector to delete the pods 08/11/23 14:46:08.822
Aug 11 14:46:08.880: INFO: Deleting DaemonSet.extensions daemon-set took: 5.886965ms
Aug 11 14:46:08.981: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.334948ms
Aug 11 14:46:11.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:46:11.084: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:46:11.086: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34855"},"items":null}

Aug 11 14:46:11.088: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34855"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6568" for this suite. 08/11/23 14:46:11.099
------------------------------
• [4.360 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:06.745
    Aug 11 14:46:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:46:06.746
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:06.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:06.761
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:46:06.777
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:46:06.781
    Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:06.785: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:06.787: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:46:06.787: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:46:07.795: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:07.796: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:07.796: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:07.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:46:07.798: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:08.792: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:46:08.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:46:08.795: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 08/11/23 14:46:08.797
    Aug 11 14:46:08.799: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/11/23 14:46:08.799
    Aug 11 14:46:08.808: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/11/23 14:46:08.808
    Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: ADDED
    Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.810: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.810: INFO: Found daemon set daemon-set in namespace daemonsets-6568 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:46:08.810: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/11/23 14:46:08.81
    STEP: watching for the daemon set status to be patched 08/11/23 14:46:08.817
    Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: ADDED
    Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.818: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.819: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.819: INFO: Observed daemon set daemon-set in namespace daemonsets-6568 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:46:08.819: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:46:08.819: INFO: Found daemon set daemon-set in namespace daemonsets-6568 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 11 14:46:08.819: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:46:08.822
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6568, will wait for the garbage collector to delete the pods 08/11/23 14:46:08.822
    Aug 11 14:46:08.880: INFO: Deleting DaemonSet.extensions daemon-set took: 5.886965ms
    Aug 11 14:46:08.981: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.334948ms
    Aug 11 14:46:11.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:46:11.084: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:46:11.086: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34855"},"items":null}

    Aug 11 14:46:11.088: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34855"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:11.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6568" for this suite. 08/11/23 14:46:11.099
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:11.105
Aug 11 14:46:11.105: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:46:11.106
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:11.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:11.121
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/11/23 14:46:11.123
STEP: Creating a ResourceQuota 08/11/23 14:46:16.126
STEP: Ensuring resource quota status is calculated 08/11/23 14:46:16.13
STEP: Creating a Pod that fits quota 08/11/23 14:46:18.134
STEP: Ensuring ResourceQuota status captures the pod usage 08/11/23 14:46:18.148
STEP: Not allowing a pod to be created that exceeds remaining quota 08/11/23 14:46:20.153
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/11/23 14:46:20.155
STEP: Ensuring a pod cannot update its resource requirements 08/11/23 14:46:20.157
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/11/23 14:46:20.16
STEP: Deleting the pod 08/11/23 14:46:22.163
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:46:22.177
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:24.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1018" for this suite. 08/11/23 14:46:24.185
------------------------------
• [SLOW TEST] [13.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:11.105
    Aug 11 14:46:11.105: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:46:11.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:11.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:11.121
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/11/23 14:46:11.123
    STEP: Creating a ResourceQuota 08/11/23 14:46:16.126
    STEP: Ensuring resource quota status is calculated 08/11/23 14:46:16.13
    STEP: Creating a Pod that fits quota 08/11/23 14:46:18.134
    STEP: Ensuring ResourceQuota status captures the pod usage 08/11/23 14:46:18.148
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/11/23 14:46:20.153
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/11/23 14:46:20.155
    STEP: Ensuring a pod cannot update its resource requirements 08/11/23 14:46:20.157
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/11/23 14:46:20.16
    STEP: Deleting the pod 08/11/23 14:46:22.163
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:46:22.177
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:24.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1018" for this suite. 08/11/23 14:46:24.185
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:24.19
Aug 11 14:46:24.190: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:46:24.191
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:24.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:24.205
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:46:24.207
Aug 11 14:46:24.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1" in namespace "downward-api-1000" to be "Succeeded or Failed"
Aug 11 14:46:24.217: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065672ms
Aug 11 14:46:26.220: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005416089s
Aug 11 14:46:28.221: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006820687s
STEP: Saw pod success 08/11/23 14:46:28.221
Aug 11 14:46:28.221: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1" satisfied condition "Succeeded or Failed"
Aug 11 14:46:28.224: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 container client-container: <nil>
STEP: delete the pod 08/11/23 14:46:28.232
Aug 11 14:46:28.246: INFO: Waiting for pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 to disappear
Aug 11 14:46:28.248: INFO: Pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:46:28.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1000" for this suite. 08/11/23 14:46:28.251
------------------------------
• [4.067 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:24.19
    Aug 11 14:46:24.190: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:46:24.191
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:24.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:24.205
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:46:24.207
    Aug 11 14:46:24.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1" in namespace "downward-api-1000" to be "Succeeded or Failed"
    Aug 11 14:46:24.217: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065672ms
    Aug 11 14:46:26.220: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005416089s
    Aug 11 14:46:28.221: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006820687s
    STEP: Saw pod success 08/11/23 14:46:28.221
    Aug 11 14:46:28.221: INFO: Pod "downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1" satisfied condition "Succeeded or Failed"
    Aug 11 14:46:28.224: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:46:28.232
    Aug 11 14:46:28.246: INFO: Waiting for pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 to disappear
    Aug 11 14:46:28.248: INFO: Pod downwardapi-volume-b3c8939a-b6ca-4899-ba44-f9471e8041b1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:46:28.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1000" for this suite. 08/11/23 14:46:28.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:46:28.257
Aug 11 14:46:28.257: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:46:28.258
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:28.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:28.271
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1200 08/11/23 14:46:28.273
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/11/23 14:46:28.278
Aug 11 14:46:28.288: INFO: Found 0 stateful pods, waiting for 3
Aug 11 14:46:38.292: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:46:38.292: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:46:38.292: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:46:38.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:46:38.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:46:38.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:46:38.438: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/11/23 14:46:48.45
Aug 11 14:46:48.469: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/11/23 14:46:48.469
STEP: Updating Pods in reverse ordinal order 08/11/23 14:46:58.484
Aug 11 14:46:58.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:46:58.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:46:58.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:46:58.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/11/23 14:47:08.636
Aug 11 14:47:08.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:47:08.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:47:08.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:47:08.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:47:18.796: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/11/23 14:47:28.809
Aug 11 14:47:28.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:47:28.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:47:28.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:47:28.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:47:38.945: INFO: Deleting all statefulset in ns statefulset-1200
Aug 11 14:47:38.947: INFO: Scaling statefulset ss2 to 0
Aug 11 14:47:48.964: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:47:48.967: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:47:48.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1200" for this suite. 08/11/23 14:47:48.984
------------------------------
• [SLOW TEST] [80.733 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:46:28.257
    Aug 11 14:46:28.257: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:46:28.258
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:46:28.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:46:28.271
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1200 08/11/23 14:46:28.273
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/11/23 14:46:28.278
    Aug 11 14:46:28.288: INFO: Found 0 stateful pods, waiting for 3
    Aug 11 14:46:38.292: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:46:38.292: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:46:38.292: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:46:38.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:46:38.437: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:46:38.437: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:46:38.438: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/11/23 14:46:48.45
    Aug 11 14:46:48.469: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/11/23 14:46:48.469
    STEP: Updating Pods in reverse ordinal order 08/11/23 14:46:58.484
    Aug 11 14:46:58.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:46:58.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:46:58.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:46:58.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/11/23 14:47:08.636
    Aug 11 14:47:08.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:47:08.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:47:08.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:47:08.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:47:18.796: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/11/23 14:47:28.809
    Aug 11 14:47:28.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-1200 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:47:28.929: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:47:28.929: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:47:28.929: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:47:38.945: INFO: Deleting all statefulset in ns statefulset-1200
    Aug 11 14:47:38.947: INFO: Scaling statefulset ss2 to 0
    Aug 11 14:47:48.964: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:47:48.967: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:47:48.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1200" for this suite. 08/11/23 14:47:48.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:47:48.991
Aug 11 14:47:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:47:48.992
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:47:49.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:47:49.006
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/11/23 14:47:49.009
Aug 11 14:47:49.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 create -f -'
Aug 11 14:47:49.201: INFO: stderr: ""
Aug 11 14:47:49.202: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:47:49.202
Aug 11 14:47:49.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:47:49.255: INFO: stderr: ""
Aug 11 14:47:49.255: INFO: stdout: "update-demo-nautilus-hbgvf update-demo-nautilus-stn6v "
Aug 11 14:47:49.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:47:49.305: INFO: stderr: ""
Aug 11 14:47:49.305: INFO: stdout: ""
Aug 11 14:47:49.305: INFO: update-demo-nautilus-hbgvf is created but not running
Aug 11 14:47:54.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:47:54.365: INFO: stderr: ""
Aug 11 14:47:54.365: INFO: stdout: "update-demo-nautilus-hbgvf update-demo-nautilus-stn6v "
Aug 11 14:47:54.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:47:54.415: INFO: stderr: ""
Aug 11 14:47:54.415: INFO: stdout: "true"
Aug 11 14:47:54.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:47:54.462: INFO: stderr: ""
Aug 11 14:47:54.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:47:54.462: INFO: validating pod update-demo-nautilus-hbgvf
Aug 11 14:47:54.474: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:47:54.474: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:47:54.474: INFO: update-demo-nautilus-hbgvf is verified up and running
Aug 11 14:47:54.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-stn6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:47:54.527: INFO: stderr: ""
Aug 11 14:47:54.527: INFO: stdout: "true"
Aug 11 14:47:54.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-stn6v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:47:54.580: INFO: stderr: ""
Aug 11 14:47:54.580: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 11 14:47:54.580: INFO: validating pod update-demo-nautilus-stn6v
Aug 11 14:47:54.591: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:47:54.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:47:54.591: INFO: update-demo-nautilus-stn6v is verified up and running
STEP: using delete to clean up resources 08/11/23 14:47:54.591
Aug 11 14:47:54.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 delete --grace-period=0 --force -f -'
Aug 11 14:47:54.645: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 14:47:54.645: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 11 14:47:54.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get rc,svc -l name=update-demo --no-headers'
Aug 11 14:47:54.711: INFO: stderr: "No resources found in kubectl-6081 namespace.\n"
Aug 11 14:47:54.711: INFO: stdout: ""
Aug 11 14:47:54.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 14:47:54.778: INFO: stderr: ""
Aug 11 14:47:54.778: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:47:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6081" for this suite. 08/11/23 14:47:54.782
------------------------------
• [SLOW TEST] [5.796 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:47:48.991
    Aug 11 14:47:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:47:48.992
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:47:49.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:47:49.006
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/11/23 14:47:49.009
    Aug 11 14:47:49.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 create -f -'
    Aug 11 14:47:49.201: INFO: stderr: ""
    Aug 11 14:47:49.202: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:47:49.202
    Aug 11 14:47:49.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:47:49.255: INFO: stderr: ""
    Aug 11 14:47:49.255: INFO: stdout: "update-demo-nautilus-hbgvf update-demo-nautilus-stn6v "
    Aug 11 14:47:49.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:47:49.305: INFO: stderr: ""
    Aug 11 14:47:49.305: INFO: stdout: ""
    Aug 11 14:47:49.305: INFO: update-demo-nautilus-hbgvf is created but not running
    Aug 11 14:47:54.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:47:54.365: INFO: stderr: ""
    Aug 11 14:47:54.365: INFO: stdout: "update-demo-nautilus-hbgvf update-demo-nautilus-stn6v "
    Aug 11 14:47:54.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:47:54.415: INFO: stderr: ""
    Aug 11 14:47:54.415: INFO: stdout: "true"
    Aug 11 14:47:54.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-hbgvf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:47:54.462: INFO: stderr: ""
    Aug 11 14:47:54.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:47:54.462: INFO: validating pod update-demo-nautilus-hbgvf
    Aug 11 14:47:54.474: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:47:54.474: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:47:54.474: INFO: update-demo-nautilus-hbgvf is verified up and running
    Aug 11 14:47:54.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-stn6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:47:54.527: INFO: stderr: ""
    Aug 11 14:47:54.527: INFO: stdout: "true"
    Aug 11 14:47:54.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods update-demo-nautilus-stn6v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:47:54.580: INFO: stderr: ""
    Aug 11 14:47:54.580: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 11 14:47:54.580: INFO: validating pod update-demo-nautilus-stn6v
    Aug 11 14:47:54.591: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:47:54.591: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:47:54.591: INFO: update-demo-nautilus-stn6v is verified up and running
    STEP: using delete to clean up resources 08/11/23 14:47:54.591
    Aug 11 14:47:54.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 delete --grace-period=0 --force -f -'
    Aug 11 14:47:54.645: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 14:47:54.645: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 11 14:47:54.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get rc,svc -l name=update-demo --no-headers'
    Aug 11 14:47:54.711: INFO: stderr: "No resources found in kubectl-6081 namespace.\n"
    Aug 11 14:47:54.711: INFO: stdout: ""
    Aug 11 14:47:54.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-6081 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 14:47:54.778: INFO: stderr: ""
    Aug 11 14:47:54.778: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:47:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6081" for this suite. 08/11/23 14:47:54.782
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:47:54.788
Aug 11 14:47:54.788: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:47:54.789
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:47:54.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:47:54.801
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:47:54.815
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:47:55.007
STEP: Deploying the webhook pod 08/11/23 14:47:55.015
STEP: Wait for the deployment to be ready 08/11/23 14:47:55.027
Aug 11 14:47:55.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:47:57.044
STEP: Verifying the service has paired with the endpoint 08/11/23 14:47:57.057
Aug 11 14:47:58.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 11 14:47:58.060: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7171-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:47:58.569
STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 14:47:58.611
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-173" for this suite. 08/11/23 14:48:01.227
STEP: Destroying namespace "webhook-173-markers" for this suite. 08/11/23 14:48:01.235
------------------------------
• [SLOW TEST] [6.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:47:54.788
    Aug 11 14:47:54.788: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:47:54.789
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:47:54.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:47:54.801
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:47:54.815
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:47:55.007
    STEP: Deploying the webhook pod 08/11/23 14:47:55.015
    STEP: Wait for the deployment to be ready 08/11/23 14:47:55.027
    Aug 11 14:47:55.034: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:47:57.044
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:47:57.057
    Aug 11 14:47:58.057: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 11 14:47:58.060: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7171-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:47:58.569
    STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 14:47:58.611
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:01.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-173" for this suite. 08/11/23 14:48:01.227
    STEP: Destroying namespace "webhook-173-markers" for this suite. 08/11/23 14:48:01.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:01.245
Aug 11 14:48:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:48:01.246
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:01.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:01.265
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/11/23 14:48:01.267
Aug 11 14:48:01.267: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: rename a version 08/11/23 14:48:05.433
STEP: check the new version name is served 08/11/23 14:48:05.446
STEP: check the old version name is removed 08/11/23 14:48:07.001
STEP: check the other version is not changed 08/11/23 14:48:07.695
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:11.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4906" for this suite. 08/11/23 14:48:11.015
------------------------------
• [SLOW TEST] [9.776 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:01.245
    Aug 11 14:48:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:48:01.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:01.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:01.265
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/11/23 14:48:01.267
    Aug 11 14:48:01.267: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: rename a version 08/11/23 14:48:05.433
    STEP: check the new version name is served 08/11/23 14:48:05.446
    STEP: check the old version name is removed 08/11/23 14:48:07.001
    STEP: check the other version is not changed 08/11/23 14:48:07.695
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:11.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4906" for this suite. 08/11/23 14:48:11.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:11.022
Aug 11 14:48:11.022: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:48:11.023
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:11.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:11.042
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b75690f9-e6ac-424c-a1d8-ac3c87eee98d 08/11/23 14:48:11.044
STEP: Creating a pod to test consume secrets 08/11/23 14:48:11.049
Aug 11 14:48:11.060: INFO: Waiting up to 5m0s for pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb" in namespace "secrets-7349" to be "Succeeded or Failed"
Aug 11 14:48:11.065: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.127855ms
Aug 11 14:48:13.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011411607s
Aug 11 14:48:15.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010561342s
STEP: Saw pod success 08/11/23 14:48:15.071
Aug 11 14:48:15.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb" satisfied condition "Succeeded or Failed"
Aug 11 14:48:15.073: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:48:15.094
Aug 11 14:48:15.109: INFO: Waiting for pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb to disappear
Aug 11 14:48:15.112: INFO: Pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:15.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7349" for this suite. 08/11/23 14:48:15.116
------------------------------
• [4.101 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:11.022
    Aug 11 14:48:11.022: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:48:11.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:11.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:11.042
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b75690f9-e6ac-424c-a1d8-ac3c87eee98d 08/11/23 14:48:11.044
    STEP: Creating a pod to test consume secrets 08/11/23 14:48:11.049
    Aug 11 14:48:11.060: INFO: Waiting up to 5m0s for pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb" in namespace "secrets-7349" to be "Succeeded or Failed"
    Aug 11 14:48:11.065: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.127855ms
    Aug 11 14:48:13.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011411607s
    Aug 11 14:48:15.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010561342s
    STEP: Saw pod success 08/11/23 14:48:15.071
    Aug 11 14:48:15.071: INFO: Pod "pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb" satisfied condition "Succeeded or Failed"
    Aug 11 14:48:15.073: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:48:15.094
    Aug 11 14:48:15.109: INFO: Waiting for pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb to disappear
    Aug 11 14:48:15.112: INFO: Pod pod-secrets-41386d94-6e14-4ddf-baaf-7515e687dfeb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:15.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7349" for this suite. 08/11/23 14:48:15.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:15.124
Aug 11 14:48:15.124: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename watch 08/11/23 14:48:15.125
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.141
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/11/23 14:48:15.143
STEP: creating a new configmap 08/11/23 14:48:15.145
STEP: modifying the configmap once 08/11/23 14:48:15.151
STEP: closing the watch once it receives two notifications 08/11/23 14:48:15.159
Aug 11 14:48:15.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36276 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:48:15.159: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36277 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/11/23 14:48:15.159
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/11/23 14:48:15.167
STEP: deleting the configmap 08/11/23 14:48:15.168
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/11/23 14:48:15.174
Aug 11 14:48:15.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36278 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:48:15.174: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36279 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:15.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2383" for this suite. 08/11/23 14:48:15.178
------------------------------
• [0.060 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:15.124
    Aug 11 14:48:15.124: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename watch 08/11/23 14:48:15.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.141
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/11/23 14:48:15.143
    STEP: creating a new configmap 08/11/23 14:48:15.145
    STEP: modifying the configmap once 08/11/23 14:48:15.151
    STEP: closing the watch once it receives two notifications 08/11/23 14:48:15.159
    Aug 11 14:48:15.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36276 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:48:15.159: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36277 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/11/23 14:48:15.159
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/11/23 14:48:15.167
    STEP: deleting the configmap 08/11/23 14:48:15.168
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/11/23 14:48:15.174
    Aug 11 14:48:15.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36278 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:48:15.174: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2383  ac916ae6-c2f8-44b8-b38c-764a62cffa8a 36279 0 2023-08-11 14:48:15 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:48:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:15.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2383" for this suite. 08/11/23 14:48:15.178
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:15.185
Aug 11 14:48:15.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:48:15.185
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.202
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-0fb7adf2-1bdb-4cce-8a58-0301933da1cc 08/11/23 14:48:15.204
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:15.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7120" for this suite. 08/11/23 14:48:15.21
------------------------------
• [0.032 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:15.185
    Aug 11 14:48:15.185: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:48:15.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.202
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-0fb7adf2-1bdb-4cce-8a58-0301933da1cc 08/11/23 14:48:15.204
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:15.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7120" for this suite. 08/11/23 14:48:15.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:15.217
Aug 11 14:48:15.217: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:48:15.218
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.234
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/11/23 14:48:15.239
STEP: waiting for RC to be added 08/11/23 14:48:15.244
STEP: waiting for available Replicas 08/11/23 14:48:15.244
STEP: patching ReplicationController 08/11/23 14:48:16.009
STEP: waiting for RC to be modified 08/11/23 14:48:16.016
STEP: patching ReplicationController status 08/11/23 14:48:16.016
STEP: waiting for RC to be modified 08/11/23 14:48:16.021
STEP: waiting for available Replicas 08/11/23 14:48:16.022
STEP: fetching ReplicationController status 08/11/23 14:48:16.028
STEP: patching ReplicationController scale 08/11/23 14:48:16.03
STEP: waiting for RC to be modified 08/11/23 14:48:16.035
STEP: waiting for ReplicationController's scale to be the max amount 08/11/23 14:48:16.036
STEP: fetching ReplicationController; ensuring that it's patched 08/11/23 14:48:17.326
STEP: updating ReplicationController status 08/11/23 14:48:17.329
STEP: waiting for RC to be modified 08/11/23 14:48:17.337
STEP: listing all ReplicationControllers 08/11/23 14:48:17.337
STEP: checking that ReplicationController has expected values 08/11/23 14:48:17.341
STEP: deleting ReplicationControllers by collection 08/11/23 14:48:17.341
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/11/23 14:48:17.35
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:17.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2859" for this suite. 08/11/23 14:48:17.393
------------------------------
• [2.182 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:15.217
    Aug 11 14:48:15.217: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:48:15.218
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:15.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:15.234
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/11/23 14:48:15.239
    STEP: waiting for RC to be added 08/11/23 14:48:15.244
    STEP: waiting for available Replicas 08/11/23 14:48:15.244
    STEP: patching ReplicationController 08/11/23 14:48:16.009
    STEP: waiting for RC to be modified 08/11/23 14:48:16.016
    STEP: patching ReplicationController status 08/11/23 14:48:16.016
    STEP: waiting for RC to be modified 08/11/23 14:48:16.021
    STEP: waiting for available Replicas 08/11/23 14:48:16.022
    STEP: fetching ReplicationController status 08/11/23 14:48:16.028
    STEP: patching ReplicationController scale 08/11/23 14:48:16.03
    STEP: waiting for RC to be modified 08/11/23 14:48:16.035
    STEP: waiting for ReplicationController's scale to be the max amount 08/11/23 14:48:16.036
    STEP: fetching ReplicationController; ensuring that it's patched 08/11/23 14:48:17.326
    STEP: updating ReplicationController status 08/11/23 14:48:17.329
    STEP: waiting for RC to be modified 08/11/23 14:48:17.337
    STEP: listing all ReplicationControllers 08/11/23 14:48:17.337
    STEP: checking that ReplicationController has expected values 08/11/23 14:48:17.341
    STEP: deleting ReplicationControllers by collection 08/11/23 14:48:17.341
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/11/23 14:48:17.35
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:17.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2859" for this suite. 08/11/23 14:48:17.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:17.4
Aug 11 14:48:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:48:17.401
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:17.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:17.418
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 11 14:48:17.421: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/11/23 14:48:19.443
Aug 11 14:48:19.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
Aug 11 14:48:19.998: INFO: stderr: ""
Aug 11 14:48:19.998: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 11 14:48:19.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 delete e2e-test-crd-publish-openapi-4451-crds test-foo'
Aug 11 14:48:20.072: INFO: stderr: ""
Aug 11 14:48:20.072: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 11 14:48:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
Aug 11 14:48:20.271: INFO: stderr: ""
Aug 11 14:48:20.271: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 11 14:48:20.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 delete e2e-test-crd-publish-openapi-4451-crds test-foo'
Aug 11 14:48:20.330: INFO: stderr: ""
Aug 11 14:48:20.330: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/11/23 14:48:20.33
Aug 11 14:48:20.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
Aug 11 14:48:20.508: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/11/23 14:48:20.508
Aug 11 14:48:20.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
Aug 11 14:48:20.979: INFO: rc: 1
Aug 11 14:48:20.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
Aug 11 14:48:21.168: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/11/23 14:48:21.168
Aug 11 14:48:21.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
Aug 11 14:48:21.360: INFO: rc: 1
Aug 11 14:48:21.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
Aug 11 14:48:21.558: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/11/23 14:48:21.558
Aug 11 14:48:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds'
Aug 11 14:48:21.752: INFO: stderr: ""
Aug 11 14:48:21.752: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/11/23 14:48:21.753
Aug 11 14:48:21.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.metadata'
Aug 11 14:48:21.942: INFO: stderr: ""
Aug 11 14:48:21.942: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 11 14:48:21.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec'
Aug 11 14:48:22.444: INFO: stderr: ""
Aug 11 14:48:22.444: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 11 14:48:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec.bars'
Aug 11 14:48:22.657: INFO: stderr: ""
Aug 11 14:48:22.657: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/11/23 14:48:22.657
Aug 11 14:48:22.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec.bars2'
Aug 11 14:48:22.837: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:24.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7945" for this suite. 08/11/23 14:48:24.774
------------------------------
• [SLOW TEST] [7.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:17.4
    Aug 11 14:48:17.400: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:48:17.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:17.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:17.418
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 11 14:48:17.421: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/11/23 14:48:19.443
    Aug 11 14:48:19.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
    Aug 11 14:48:19.998: INFO: stderr: ""
    Aug 11 14:48:19.998: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 11 14:48:19.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 delete e2e-test-crd-publish-openapi-4451-crds test-foo'
    Aug 11 14:48:20.072: INFO: stderr: ""
    Aug 11 14:48:20.072: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 11 14:48:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
    Aug 11 14:48:20.271: INFO: stderr: ""
    Aug 11 14:48:20.271: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 11 14:48:20.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 delete e2e-test-crd-publish-openapi-4451-crds test-foo'
    Aug 11 14:48:20.330: INFO: stderr: ""
    Aug 11 14:48:20.330: INFO: stdout: "e2e-test-crd-publish-openapi-4451-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/11/23 14:48:20.33
    Aug 11 14:48:20.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
    Aug 11 14:48:20.508: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/11/23 14:48:20.508
    Aug 11 14:48:20.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
    Aug 11 14:48:20.979: INFO: rc: 1
    Aug 11 14:48:20.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
    Aug 11 14:48:21.168: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/11/23 14:48:21.168
    Aug 11 14:48:21.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 create -f -'
    Aug 11 14:48:21.360: INFO: rc: 1
    Aug 11 14:48:21.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 --namespace=crd-publish-openapi-7945 apply -f -'
    Aug 11 14:48:21.558: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/11/23 14:48:21.558
    Aug 11 14:48:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds'
    Aug 11 14:48:21.752: INFO: stderr: ""
    Aug 11 14:48:21.752: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/11/23 14:48:21.753
    Aug 11 14:48:21.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.metadata'
    Aug 11 14:48:21.942: INFO: stderr: ""
    Aug 11 14:48:21.942: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 11 14:48:21.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec'
    Aug 11 14:48:22.444: INFO: stderr: ""
    Aug 11 14:48:22.444: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 11 14:48:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec.bars'
    Aug 11 14:48:22.657: INFO: stderr: ""
    Aug 11 14:48:22.657: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4451-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/11/23 14:48:22.657
    Aug 11 14:48:22.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7945 explain e2e-test-crd-publish-openapi-4451-crds.spec.bars2'
    Aug 11 14:48:22.837: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:24.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7945" for this suite. 08/11/23 14:48:24.774
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:24.78
Aug 11 14:48:24.780: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:48:24.781
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:24.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:24.796
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 11 14:48:24.798: INFO: Creating simple deployment test-new-deployment
Aug 11 14:48:24.807: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 08/11/23 14:48:26.816
STEP: updating a scale subresource 08/11/23 14:48:26.818
STEP: verifying the deployment Spec.Replicas was modified 08/11/23 14:48:26.824
STEP: Patch a scale subresource 08/11/23 14:48:26.826
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:48:26.844: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2104  eaf51bd1-e1b9-45b4-8e54-77920404e192 36487 3 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-11 14:48:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ec6ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:48:26.847: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2104  e3eacb8a-2fd8-4bc8-962e-dd495821a548 36492 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment eaf51bd1-e1b9-45b4-8e54-77920404e192 0xc0043cd607 0xc0043cd608}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaf51bd1-e1b9-45b4-8e54-77920404e192\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043cd698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:48:26.852: INFO: Pod "test-new-deployment-7f5969cbc7-kq6w7" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kq6w7 test-new-deployment-7f5969cbc7- deployment-2104  fecb1325-c9b5-4a9e-a6c1-713ce0896196 36480 0 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e3eacb8a-2fd8-4bc8-962e-dd495821a548 0xc002ec7347 0xc002ec7348}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3eacb8a-2fd8-4bc8-962e-dd495821a548\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftkc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftkc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.139,StartTime:2023-08-11 14:48:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:48:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8155cb7bfc1f1546c67587019e6f7e01982fba66c3d064d72b4824e9c2c023cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:48:26.852: INFO: Pod "test-new-deployment-7f5969cbc7-pqktt" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-pqktt test-new-deployment-7f5969cbc7- deployment-2104  acbf971b-4f95-4547-9878-3b2c47d84971 36490 0 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e3eacb8a-2fd8-4bc8-962e-dd495821a548 0xc002ec7550 0xc002ec7551}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3eacb8a-2fd8-4bc8-962e-dd495821a548\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7skn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7skn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:48:26.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2104" for this suite. 08/11/23 14:48:26.855
------------------------------
• [2.083 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:24.78
    Aug 11 14:48:24.780: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:48:24.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:24.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:24.796
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 11 14:48:24.798: INFO: Creating simple deployment test-new-deployment
    Aug 11 14:48:24.807: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 08/11/23 14:48:26.816
    STEP: updating a scale subresource 08/11/23 14:48:26.818
    STEP: verifying the deployment Spec.Replicas was modified 08/11/23 14:48:26.824
    STEP: Patch a scale subresource 08/11/23 14:48:26.826
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:48:26.844: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2104  eaf51bd1-e1b9-45b4-8e54-77920404e192 36487 3 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-11 14:48:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ec6ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:48:26.847: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2104  e3eacb8a-2fd8-4bc8-962e-dd495821a548 36492 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment eaf51bd1-e1b9-45b4-8e54-77920404e192 0xc0043cd607 0xc0043cd608}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaf51bd1-e1b9-45b4-8e54-77920404e192\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043cd698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:48:26.852: INFO: Pod "test-new-deployment-7f5969cbc7-kq6w7" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kq6w7 test-new-deployment-7f5969cbc7- deployment-2104  fecb1325-c9b5-4a9e-a6c1-713ce0896196 36480 0 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e3eacb8a-2fd8-4bc8-962e-dd495821a548 0xc002ec7347 0xc002ec7348}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3eacb8a-2fd8-4bc8-962e-dd495821a548\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftkc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftkc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.139,StartTime:2023-08-11 14:48:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:48:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8155cb7bfc1f1546c67587019e6f7e01982fba66c3d064d72b4824e9c2c023cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:48:26.852: INFO: Pod "test-new-deployment-7f5969cbc7-pqktt" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-pqktt test-new-deployment-7f5969cbc7- deployment-2104  acbf971b-4f95-4547-9878-3b2c47d84971 36490 0 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e3eacb8a-2fd8-4bc8-962e-dd495821a548 0xc002ec7550 0xc002ec7551}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e3eacb8a-2fd8-4bc8-962e-dd495821a548\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7skn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7skn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:48:26.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2104" for this suite. 08/11/23 14:48:26.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:48:26.864
Aug 11 14:48:26.864: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:48:26.865
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:26.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:26.883
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae in namespace container-probe-1530 08/11/23 14:48:26.885
Aug 11 14:48:26.892: INFO: Waiting up to 5m0s for pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae" in namespace "container-probe-1530" to be "not pending"
Aug 11 14:48:26.894: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111493ms
Aug 11 14:48:28.903: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.010782887s
Aug 11 14:48:28.903: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae" satisfied condition "not pending"
Aug 11 14:48:28.903: INFO: Started pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae in namespace container-probe-1530
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:48:28.903
Aug 11 14:48:28.905: INFO: Initial restart count of pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae is 0
STEP: deleting the pod 08/11/23 14:52:29.358
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:52:29.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1530" for this suite. 08/11/23 14:52:29.375
------------------------------
• [SLOW TEST] [242.518 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:48:26.864
    Aug 11 14:48:26.864: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:48:26.865
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:26.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:26.883
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae in namespace container-probe-1530 08/11/23 14:48:26.885
    Aug 11 14:48:26.892: INFO: Waiting up to 5m0s for pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae" in namespace "container-probe-1530" to be "not pending"
    Aug 11 14:48:26.894: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111493ms
    Aug 11 14:48:28.903: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.010782887s
    Aug 11 14:48:28.903: INFO: Pod "busybox-81cb099a-a669-4e47-bb09-850aff68e0ae" satisfied condition "not pending"
    Aug 11 14:48:28.903: INFO: Started pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae in namespace container-probe-1530
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:48:28.903
    Aug 11 14:48:28.905: INFO: Initial restart count of pod busybox-81cb099a-a669-4e47-bb09-850aff68e0ae is 0
    STEP: deleting the pod 08/11/23 14:52:29.358
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:52:29.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1530" for this suite. 08/11/23 14:52:29.375
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:52:29.382
Aug 11 14:52:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:52:29.383
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:29.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:29.401
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-990 08/11/23 14:52:29.403
STEP: creating service affinity-nodeport in namespace services-990 08/11/23 14:52:29.403
STEP: creating replication controller affinity-nodeport in namespace services-990 08/11/23 14:52:29.419
I0811 14:52:29.427177      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-990, replica count: 3
I0811 14:52:32.478357      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:52:32.486: INFO: Creating new exec pod
Aug 11 14:52:32.490: INFO: Waiting up to 5m0s for pod "execpod-affinitybb2dl" in namespace "services-990" to be "running"
Aug 11 14:52:32.492: INFO: Pod "execpod-affinitybb2dl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129463ms
Aug 11 14:52:34.495: INFO: Pod "execpod-affinitybb2dl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004823379s
Aug 11 14:52:34.495: INFO: Pod "execpod-affinitybb2dl" satisfied condition "running"
Aug 11 14:52:35.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 11 14:52:35.622: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 11 14:52:35.622: INFO: stdout: ""
Aug 11 14:52:35.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 10.96.89.0 80'
Aug 11 14:52:35.740: INFO: stderr: "+ nc -v -z -w 2 10.96.89.0 80\nConnection to 10.96.89.0 80 port [tcp/http] succeeded!\n"
Aug 11 14:52:35.740: INFO: stdout: ""
Aug 11 14:52:35.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 31880'
Aug 11 14:52:35.862: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 31880\nConnection to 192.168.178.2 31880 port [tcp/*] succeeded!\n"
Aug 11 14:52:35.862: INFO: stdout: ""
Aug 11 14:52:35.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 31880'
Aug 11 14:52:35.995: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 31880\nConnection to 192.168.178.3 31880 port [tcp/*] succeeded!\n"
Aug 11 14:52:35.995: INFO: stdout: ""
Aug 11 14:52:35.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31880/ ; done'
Aug 11 14:52:36.182: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n"
Aug 11 14:52:36.182: INFO: stdout: "\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9"
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
Aug 11 14:52:36.182: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-990, will wait for the garbage collector to delete the pods 08/11/23 14:52:36.192
Aug 11 14:52:36.251: INFO: Deleting ReplicationController affinity-nodeport took: 4.461324ms
Aug 11 14:52:36.352: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.581839ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:52:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-990" for this suite. 08/11/23 14:52:37.879
------------------------------
• [SLOW TEST] [8.501 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:52:29.382
    Aug 11 14:52:29.382: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:52:29.383
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:29.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:29.401
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-990 08/11/23 14:52:29.403
    STEP: creating service affinity-nodeport in namespace services-990 08/11/23 14:52:29.403
    STEP: creating replication controller affinity-nodeport in namespace services-990 08/11/23 14:52:29.419
    I0811 14:52:29.427177      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-990, replica count: 3
    I0811 14:52:32.478357      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:52:32.486: INFO: Creating new exec pod
    Aug 11 14:52:32.490: INFO: Waiting up to 5m0s for pod "execpod-affinitybb2dl" in namespace "services-990" to be "running"
    Aug 11 14:52:32.492: INFO: Pod "execpod-affinitybb2dl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129463ms
    Aug 11 14:52:34.495: INFO: Pod "execpod-affinitybb2dl": Phase="Running", Reason="", readiness=true. Elapsed: 2.004823379s
    Aug 11 14:52:34.495: INFO: Pod "execpod-affinitybb2dl" satisfied condition "running"
    Aug 11 14:52:35.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 11 14:52:35.622: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 11 14:52:35.622: INFO: stdout: ""
    Aug 11 14:52:35.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 10.96.89.0 80'
    Aug 11 14:52:35.740: INFO: stderr: "+ nc -v -z -w 2 10.96.89.0 80\nConnection to 10.96.89.0 80 port [tcp/http] succeeded!\n"
    Aug 11 14:52:35.740: INFO: stdout: ""
    Aug 11 14:52:35.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 192.168.178.2 31880'
    Aug 11 14:52:35.862: INFO: stderr: "+ nc -v -z -w 2 192.168.178.2 31880\nConnection to 192.168.178.2 31880 port [tcp/*] succeeded!\n"
    Aug 11 14:52:35.862: INFO: stdout: ""
    Aug 11 14:52:35.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c nc -v -z -w 2 192.168.178.3 31880'
    Aug 11 14:52:35.995: INFO: stderr: "+ nc -v -z -w 2 192.168.178.3 31880\nConnection to 192.168.178.3 31880 port [tcp/*] succeeded!\n"
    Aug 11 14:52:35.995: INFO: stdout: ""
    Aug 11 14:52:35.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-990 exec execpod-affinitybb2dl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31880/ ; done'
    Aug 11 14:52:36.182: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31880/\n"
    Aug 11 14:52:36.182: INFO: stdout: "\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9\naffinity-nodeport-g5td9"
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Received response from host: affinity-nodeport-g5td9
    Aug 11 14:52:36.182: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-990, will wait for the garbage collector to delete the pods 08/11/23 14:52:36.192
    Aug 11 14:52:36.251: INFO: Deleting ReplicationController affinity-nodeport took: 4.461324ms
    Aug 11 14:52:36.352: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.581839ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:52:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-990" for this suite. 08/11/23 14:52:37.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:52:37.884
Aug 11 14:52:37.884: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:52:37.885
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:37.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:37.897
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/11/23 14:52:37.899
STEP: waiting for pod running 08/11/23 14:52:37.905
Aug 11 14:52:37.905: INFO: Waiting up to 2m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152" to be "running"
Aug 11 14:52:37.908: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088872ms
Aug 11 14:52:39.911: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Running", Reason="", readiness=true. Elapsed: 2.00544941s
Aug 11 14:52:39.911: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" satisfied condition "running"
STEP: creating a file in subpath 08/11/23 14:52:39.911
Aug 11 14:52:39.913: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7152 PodName:var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:52:39.914: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:52:39.914: INFO: ExecWithOptions: Clientset creation
Aug 11 14:52:39.914: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7152/pods/var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/11/23 14:52:39.999
Aug 11 14:52:40.002: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7152 PodName:var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:52:40.002: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:52:40.003: INFO: ExecWithOptions: Clientset creation
Aug 11 14:52:40.003: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7152/pods/var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/11/23 14:52:40.077
Aug 11 14:52:40.588: INFO: Successfully updated pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915"
STEP: waiting for annotated pod running 08/11/23 14:52:40.589
Aug 11 14:52:40.589: INFO: Waiting up to 2m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152" to be "running"
Aug 11 14:52:40.591: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Running", Reason="", readiness=true. Elapsed: 2.382032ms
Aug 11 14:52:40.591: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 14:52:40.591
Aug 11 14:52:40.591: INFO: Deleting pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152"
Aug 11 14:52:40.598: INFO: Wait up to 5m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7152" for this suite. 08/11/23 14:53:14.607
------------------------------
• [SLOW TEST] [36.728 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:52:37.884
    Aug 11 14:52:37.884: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:52:37.885
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:37.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:37.897
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/11/23 14:52:37.899
    STEP: waiting for pod running 08/11/23 14:52:37.905
    Aug 11 14:52:37.905: INFO: Waiting up to 2m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152" to be "running"
    Aug 11 14:52:37.908: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088872ms
    Aug 11 14:52:39.911: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Running", Reason="", readiness=true. Elapsed: 2.00544941s
    Aug 11 14:52:39.911: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" satisfied condition "running"
    STEP: creating a file in subpath 08/11/23 14:52:39.911
    Aug 11 14:52:39.913: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7152 PodName:var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:52:39.914: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:52:39.914: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:52:39.914: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7152/pods/var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/11/23 14:52:39.999
    Aug 11 14:52:40.002: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7152 PodName:var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:52:40.002: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:52:40.003: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:52:40.003: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7152/pods/var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/11/23 14:52:40.077
    Aug 11 14:52:40.588: INFO: Successfully updated pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915"
    STEP: waiting for annotated pod running 08/11/23 14:52:40.589
    Aug 11 14:52:40.589: INFO: Waiting up to 2m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152" to be "running"
    Aug 11 14:52:40.591: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915": Phase="Running", Reason="", readiness=true. Elapsed: 2.382032ms
    Aug 11 14:52:40.591: INFO: Pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 14:52:40.591
    Aug 11 14:52:40.591: INFO: Deleting pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" in namespace "var-expansion-7152"
    Aug 11 14:52:40.598: INFO: Wait up to 5m0s for pod "var-expansion-648f2655-e306-4ad2-843e-f18a2d7dc915" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:14.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7152" for this suite. 08/11/23 14:53:14.607
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:14.612
Aug 11 14:53:14.612: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 14:53:14.613
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:14.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:14.627
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:53:14.629
Aug 11 14:53:14.635: INFO: Waiting up to 5m0s for pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e" in namespace "emptydir-9700" to be "Succeeded or Failed"
Aug 11 14:53:14.637: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109802ms
Aug 11 14:53:16.640: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00515372s
Aug 11 14:53:18.640: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005435147s
STEP: Saw pod success 08/11/23 14:53:18.64
Aug 11 14:53:18.641: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e" satisfied condition "Succeeded or Failed"
Aug 11 14:53:18.643: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e container test-container: <nil>
STEP: delete the pod 08/11/23 14:53:18.666
Aug 11 14:53:18.675: INFO: Waiting for pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e to disappear
Aug 11 14:53:18.678: INFO: Pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:18.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9700" for this suite. 08/11/23 14:53:18.682
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:14.612
    Aug 11 14:53:14.612: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:53:14.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:14.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:14.627
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:53:14.629
    Aug 11 14:53:14.635: INFO: Waiting up to 5m0s for pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e" in namespace "emptydir-9700" to be "Succeeded or Failed"
    Aug 11 14:53:14.637: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109802ms
    Aug 11 14:53:16.640: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00515372s
    Aug 11 14:53:18.640: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005435147s
    STEP: Saw pod success 08/11/23 14:53:18.64
    Aug 11 14:53:18.641: INFO: Pod "pod-bc747e54-0cdd-4809-865b-247bcce10c5e" satisfied condition "Succeeded or Failed"
    Aug 11 14:53:18.643: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e container test-container: <nil>
    STEP: delete the pod 08/11/23 14:53:18.666
    Aug 11 14:53:18.675: INFO: Waiting for pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e to disappear
    Aug 11 14:53:18.678: INFO: Pod pod-bc747e54-0cdd-4809-865b-247bcce10c5e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:18.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9700" for this suite. 08/11/23 14:53:18.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:18.69
Aug 11 14:53:18.690: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:53:18.691
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:18.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:18.704
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-2514edc0-57d7-4ba2-a7ae-8c8fb4bf8f25 08/11/23 14:53:18.706
STEP: Creating a pod to test consume secrets 08/11/23 14:53:18.71
Aug 11 14:53:18.716: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d" in namespace "projected-5883" to be "Succeeded or Failed"
Aug 11 14:53:18.718: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155722ms
Aug 11 14:53:20.721: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0048862s
Aug 11 14:53:22.723: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006627868s
STEP: Saw pod success 08/11/23 14:53:22.723
Aug 11 14:53:22.723: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d" satisfied condition "Succeeded or Failed"
Aug 11 14:53:22.725: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:53:22.733
Aug 11 14:53:22.742: INFO: Waiting for pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d to disappear
Aug 11 14:53:22.744: INFO: Pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5883" for this suite. 08/11/23 14:53:22.747
------------------------------
• [4.065 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:18.69
    Aug 11 14:53:18.690: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:53:18.691
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:18.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:18.704
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-2514edc0-57d7-4ba2-a7ae-8c8fb4bf8f25 08/11/23 14:53:18.706
    STEP: Creating a pod to test consume secrets 08/11/23 14:53:18.71
    Aug 11 14:53:18.716: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d" in namespace "projected-5883" to be "Succeeded or Failed"
    Aug 11 14:53:18.718: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155722ms
    Aug 11 14:53:20.721: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0048862s
    Aug 11 14:53:22.723: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006627868s
    STEP: Saw pod success 08/11/23 14:53:22.723
    Aug 11 14:53:22.723: INFO: Pod "pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d" satisfied condition "Succeeded or Failed"
    Aug 11 14:53:22.725: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:53:22.733
    Aug 11 14:53:22.742: INFO: Waiting for pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d to disappear
    Aug 11 14:53:22.744: INFO: Pod pod-projected-secrets-adbae8ba-e119-4958-a74e-203a2dca091d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5883" for this suite. 08/11/23 14:53:22.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:22.755
Aug 11 14:53:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:53:22.756
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:22.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:22.77
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-681d4963-c531-4b2d-9dc1-22cba8761e9f 08/11/23 14:53:22.772
STEP: Creating a pod to test consume configMaps 08/11/23 14:53:22.776
Aug 11 14:53:22.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8" in namespace "configmap-4946" to be "Succeeded or Failed"
Aug 11 14:53:22.785: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188822ms
Aug 11 14:53:24.788: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493342s
Aug 11 14:53:26.789: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006323879s
STEP: Saw pod success 08/11/23 14:53:26.789
Aug 11 14:53:26.789: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8" satisfied condition "Succeeded or Failed"
Aug 11 14:53:26.791: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:53:26.8
Aug 11 14:53:26.811: INFO: Waiting for pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 to disappear
Aug 11 14:53:26.814: INFO: Pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:26.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4946" for this suite. 08/11/23 14:53:26.817
------------------------------
• [4.067 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:22.755
    Aug 11 14:53:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:53:22.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:22.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:22.77
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-681d4963-c531-4b2d-9dc1-22cba8761e9f 08/11/23 14:53:22.772
    STEP: Creating a pod to test consume configMaps 08/11/23 14:53:22.776
    Aug 11 14:53:22.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8" in namespace "configmap-4946" to be "Succeeded or Failed"
    Aug 11 14:53:22.785: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188822ms
    Aug 11 14:53:24.788: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493342s
    Aug 11 14:53:26.789: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006323879s
    STEP: Saw pod success 08/11/23 14:53:26.789
    Aug 11 14:53:26.789: INFO: Pod "pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8" satisfied condition "Succeeded or Failed"
    Aug 11 14:53:26.791: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:53:26.8
    Aug 11 14:53:26.811: INFO: Waiting for pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 to disappear
    Aug 11 14:53:26.814: INFO: Pod pod-configmaps-f767765f-c32f-4d43-85d9-92307ca72ce8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:26.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4946" for this suite. 08/11/23 14:53:26.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:26.822
Aug 11 14:53:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:53:26.823
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:26.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:26.836
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/11/23 14:53:26.838
STEP: Creating a ResourceQuota 08/11/23 14:53:31.841
STEP: Ensuring resource quota status is calculated 08/11/23 14:53:31.846
STEP: Creating a ReplicationController 08/11/23 14:53:33.849
STEP: Ensuring resource quota status captures replication controller creation 08/11/23 14:53:33.86
STEP: Deleting a ReplicationController 08/11/23 14:53:35.864
STEP: Ensuring resource quota status released usage 08/11/23 14:53:35.869
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-448" for this suite. 08/11/23 14:53:37.876
------------------------------
• [SLOW TEST] [11.059 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:26.822
    Aug 11 14:53:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:53:26.823
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:26.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:26.836
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/11/23 14:53:26.838
    STEP: Creating a ResourceQuota 08/11/23 14:53:31.841
    STEP: Ensuring resource quota status is calculated 08/11/23 14:53:31.846
    STEP: Creating a ReplicationController 08/11/23 14:53:33.849
    STEP: Ensuring resource quota status captures replication controller creation 08/11/23 14:53:33.86
    STEP: Deleting a ReplicationController 08/11/23 14:53:35.864
    STEP: Ensuring resource quota status released usage 08/11/23 14:53:35.869
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:37.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-448" for this suite. 08/11/23 14:53:37.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:37.883
Aug 11 14:53:37.884: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 14:53:37.884
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:37.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:37.897
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4195 08/11/23 14:53:37.899
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4195 08/11/23 14:53:37.905
Aug 11 14:53:37.912: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:53:47.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/11/23 14:53:47.921
STEP: Getting /status 08/11/23 14:53:47.927
Aug 11 14:53:47.930: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/11/23 14:53:47.93
Aug 11 14:53:47.941: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/11/23 14:53:47.941
Aug 11 14:53:47.942: INFO: Observed &StatefulSet event: ADDED
Aug 11 14:53:47.942: INFO: Found Statefulset ss in namespace statefulset-4195 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 14:53:47.942: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/11/23 14:53:47.942
Aug 11 14:53:47.942: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 14:53:47.948: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/11/23 14:53:47.948
Aug 11 14:53:47.950: INFO: Observed &StatefulSet event: ADDED
Aug 11 14:53:47.950: INFO: Observed Statefulset ss in namespace statefulset-4195 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 14:53:47.950: INFO: Observed &StatefulSet event: MODIFIED
Aug 11 14:53:47.950: INFO: Found Statefulset ss in namespace statefulset-4195 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 14:53:47.950: INFO: Deleting all statefulset in ns statefulset-4195
Aug 11 14:53:47.952: INFO: Scaling statefulset ss to 0
Aug 11 14:53:57.968: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:53:57.970: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:53:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4195" for this suite. 08/11/23 14:53:57.985
------------------------------
• [SLOW TEST] [20.108 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:37.883
    Aug 11 14:53:37.884: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:53:37.884
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:37.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:37.897
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4195 08/11/23 14:53:37.899
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4195 08/11/23 14:53:37.905
    Aug 11 14:53:37.912: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:53:47.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/11/23 14:53:47.921
    STEP: Getting /status 08/11/23 14:53:47.927
    Aug 11 14:53:47.930: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/11/23 14:53:47.93
    Aug 11 14:53:47.941: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/11/23 14:53:47.941
    Aug 11 14:53:47.942: INFO: Observed &StatefulSet event: ADDED
    Aug 11 14:53:47.942: INFO: Found Statefulset ss in namespace statefulset-4195 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 14:53:47.942: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/11/23 14:53:47.942
    Aug 11 14:53:47.942: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 14:53:47.948: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/11/23 14:53:47.948
    Aug 11 14:53:47.950: INFO: Observed &StatefulSet event: ADDED
    Aug 11 14:53:47.950: INFO: Observed Statefulset ss in namespace statefulset-4195 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 14:53:47.950: INFO: Observed &StatefulSet event: MODIFIED
    Aug 11 14:53:47.950: INFO: Found Statefulset ss in namespace statefulset-4195 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 14:53:47.950: INFO: Deleting all statefulset in ns statefulset-4195
    Aug 11 14:53:47.952: INFO: Scaling statefulset ss to 0
    Aug 11 14:53:57.968: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:53:57.970: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:53:57.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4195" for this suite. 08/11/23 14:53:57.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:53:57.994
Aug 11 14:53:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 14:53:57.994
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:58.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:58.007
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 11 14:53:58.018: INFO: Waiting up to 5m0s for pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0" in namespace "container-probe-9930" to be "running and ready"
Aug 11 14:53:58.020: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045832ms
Aug 11 14:53:58.020: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:54:00.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006290701s
Aug 11 14:54:00.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:02.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006129079s
Aug 11 14:54:02.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:04.025: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007243786s
Aug 11 14:54:04.025: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:06.023: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005574137s
Aug 11 14:54:06.023: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:08.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006063835s
Aug 11 14:54:08.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:10.025: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006861962s
Aug 11 14:54:10.025: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:12.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005681384s
Aug 11 14:54:12.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:14.023: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005382131s
Aug 11 14:54:14.023: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:16.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005950127s
Aug 11 14:54:16.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:18.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006276082s
Aug 11 14:54:18.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
Aug 11 14:54:20.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006259448s
Aug 11 14:54:20.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = true)
Aug 11 14:54:20.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0" satisfied condition "running and ready"
Aug 11 14:54:20.027: INFO: Container started at 2023-08-11 14:53:58 +0000 UTC, pod became ready at 2023-08-11 14:54:18 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:20.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9930" for this suite. 08/11/23 14:54:20.03
------------------------------
• [SLOW TEST] [22.042 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:53:57.994
    Aug 11 14:53:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:53:57.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:53:58.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:53:58.007
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 11 14:53:58.018: INFO: Waiting up to 5m0s for pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0" in namespace "container-probe-9930" to be "running and ready"
    Aug 11 14:53:58.020: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045832ms
    Aug 11 14:53:58.020: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:54:00.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006290701s
    Aug 11 14:54:00.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:02.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006129079s
    Aug 11 14:54:02.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:04.025: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007243786s
    Aug 11 14:54:04.025: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:06.023: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005574137s
    Aug 11 14:54:06.023: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:08.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006063835s
    Aug 11 14:54:08.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:10.025: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006861962s
    Aug 11 14:54:10.025: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:12.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 14.005681384s
    Aug 11 14:54:12.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:14.023: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005382131s
    Aug 11 14:54:14.023: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:16.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005950127s
    Aug 11 14:54:16.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:18.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=false. Elapsed: 20.006276082s
    Aug 11 14:54:18.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = false)
    Aug 11 14:54:20.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006259448s
    Aug 11 14:54:20.024: INFO: The phase of Pod test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0 is Running (Ready = true)
    Aug 11 14:54:20.024: INFO: Pod "test-webserver-75943681-5897-4d3b-94c0-ae87d25d24e0" satisfied condition "running and ready"
    Aug 11 14:54:20.027: INFO: Container started at 2023-08-11 14:53:58 +0000 UTC, pod became ready at 2023-08-11 14:54:18 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:20.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9930" for this suite. 08/11/23 14:54:20.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:20.038
Aug 11 14:54:20.038: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:54:20.039
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:20.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:20.052
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/11/23 14:54:20.054
Aug 11 14:54:20.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-5989 api-versions'
Aug 11 14:54:20.114: INFO: stderr: ""
Aug 11 14:54:20.114: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemaintenance.medik8s.io/v1beta1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nupdate.edgeless.systems/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:20.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5989" for this suite. 08/11/23 14:54:20.118
------------------------------
• [0.086 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:20.038
    Aug 11 14:54:20.038: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:54:20.039
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:20.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:20.052
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/11/23 14:54:20.054
    Aug 11 14:54:20.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-5989 api-versions'
    Aug 11 14:54:20.114: INFO: stderr: ""
    Aug 11 14:54:20.114: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemaintenance.medik8s.io/v1beta1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nupdate.edgeless.systems/v1alpha1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:20.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5989" for this suite. 08/11/23 14:54:20.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:20.124
Aug 11 14:54:20.124: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename containers 08/11/23 14:54:20.125
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:20.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:20.137
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/11/23 14:54:20.139
Aug 11 14:54:20.145: INFO: Waiting up to 5m0s for pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d" in namespace "containers-7164" to be "Succeeded or Failed"
Aug 11 14:54:20.147: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059422ms
Aug 11 14:54:22.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00590976s
Aug 11 14:54:24.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006436716s
STEP: Saw pod success 08/11/23 14:54:24.151
Aug 11 14:54:24.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d" satisfied condition "Succeeded or Failed"
Aug 11 14:54:24.154: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:54:24.164
Aug 11 14:54:24.176: INFO: Waiting for pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d to disappear
Aug 11 14:54:24.178: INFO: Pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:24.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7164" for this suite. 08/11/23 14:54:24.182
------------------------------
• [4.064 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:20.124
    Aug 11 14:54:20.124: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename containers 08/11/23 14:54:20.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:20.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:20.137
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/11/23 14:54:20.139
    Aug 11 14:54:20.145: INFO: Waiting up to 5m0s for pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d" in namespace "containers-7164" to be "Succeeded or Failed"
    Aug 11 14:54:20.147: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059422ms
    Aug 11 14:54:22.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00590976s
    Aug 11 14:54:24.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006436716s
    STEP: Saw pod success 08/11/23 14:54:24.151
    Aug 11 14:54:24.151: INFO: Pod "client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d" satisfied condition "Succeeded or Failed"
    Aug 11 14:54:24.154: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:54:24.164
    Aug 11 14:54:24.176: INFO: Waiting for pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d to disappear
    Aug 11 14:54:24.178: INFO: Pod client-containers-b4cf3edd-399d-40ee-b109-0bc08fe6c11d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:24.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7164" for this suite. 08/11/23 14:54:24.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:24.189
Aug 11 14:54:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:54:24.19
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:24.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:24.203
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/11/23 14:54:24.205
Aug 11 14:54:24.212: INFO: Waiting up to 5m0s for pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b" in namespace "downward-api-9203" to be "Succeeded or Failed"
Aug 11 14:54:24.215: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210512ms
Aug 11 14:54:26.218: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005626891s
Aug 11 14:54:28.219: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006846918s
STEP: Saw pod success 08/11/23 14:54:28.219
Aug 11 14:54:28.219: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b" satisfied condition "Succeeded or Failed"
Aug 11 14:54:28.222: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:54:28.23
Aug 11 14:54:28.242: INFO: Waiting for pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b to disappear
Aug 11 14:54:28.244: INFO: Pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:28.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9203" for this suite. 08/11/23 14:54:28.247
------------------------------
• [4.064 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:24.189
    Aug 11 14:54:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:54:24.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:24.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:24.203
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/11/23 14:54:24.205
    Aug 11 14:54:24.212: INFO: Waiting up to 5m0s for pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b" in namespace "downward-api-9203" to be "Succeeded or Failed"
    Aug 11 14:54:24.215: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210512ms
    Aug 11 14:54:26.218: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005626891s
    Aug 11 14:54:28.219: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006846918s
    STEP: Saw pod success 08/11/23 14:54:28.219
    Aug 11 14:54:28.219: INFO: Pod "downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b" satisfied condition "Succeeded or Failed"
    Aug 11 14:54:28.222: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:54:28.23
    Aug 11 14:54:28.242: INFO: Waiting for pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b to disappear
    Aug 11 14:54:28.244: INFO: Pod downward-api-8e63d7de-b8a7-4285-9f99-55d9ae66475b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:28.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9203" for this suite. 08/11/23 14:54:28.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:28.254
Aug 11 14:54:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:54:28.255
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:28.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:28.266
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:54:28.268
Aug 11 14:54:28.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1" in namespace "projected-2113" to be "Succeeded or Failed"
Aug 11 14:54:28.277: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929002ms
Aug 11 14:54:30.280: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004893969s
Aug 11 14:54:32.281: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006302967s
STEP: Saw pod success 08/11/23 14:54:32.281
Aug 11 14:54:32.282: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1" satisfied condition "Succeeded or Failed"
Aug 11 14:54:32.284: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 container client-container: <nil>
STEP: delete the pod 08/11/23 14:54:32.292
Aug 11 14:54:32.301: INFO: Waiting for pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 to disappear
Aug 11 14:54:32.303: INFO: Pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:32.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2113" for this suite. 08/11/23 14:54:32.306
------------------------------
• [4.057 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:28.254
    Aug 11 14:54:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:54:28.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:28.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:28.266
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:54:28.268
    Aug 11 14:54:28.275: INFO: Waiting up to 5m0s for pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1" in namespace "projected-2113" to be "Succeeded or Failed"
    Aug 11 14:54:28.277: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929002ms
    Aug 11 14:54:30.280: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004893969s
    Aug 11 14:54:32.281: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006302967s
    STEP: Saw pod success 08/11/23 14:54:32.281
    Aug 11 14:54:32.282: INFO: Pod "downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1" satisfied condition "Succeeded or Failed"
    Aug 11 14:54:32.284: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:54:32.292
    Aug 11 14:54:32.301: INFO: Waiting for pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 to disappear
    Aug 11 14:54:32.303: INFO: Pod downwardapi-volume-479789af-c2f1-49fe-9bbc-817d9d798dc1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:32.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2113" for this suite. 08/11/23 14:54:32.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:32.312
Aug 11 14:54:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 14:54:32.313
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:32.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:32.326
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5409 08/11/23 14:54:32.328
STEP: creating service affinity-clusterip-transition in namespace services-5409 08/11/23 14:54:32.328
STEP: creating replication controller affinity-clusterip-transition in namespace services-5409 08/11/23 14:54:32.34
I0811 14:54:32.348030      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5409, replica count: 3
I0811 14:54:35.399383      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:54:35.403: INFO: Creating new exec pod
Aug 11 14:54:35.408: INFO: Waiting up to 5m0s for pod "execpod-affinityk66dk" in namespace "services-5409" to be "running"
Aug 11 14:54:35.410: INFO: Pod "execpod-affinityk66dk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478432ms
Aug 11 14:54:37.413: INFO: Pod "execpod-affinityk66dk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005523851s
Aug 11 14:54:37.413: INFO: Pod "execpod-affinityk66dk" satisfied condition "running"
Aug 11 14:54:38.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 11 14:54:38.544: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 11 14:54:38.544: INFO: stdout: ""
Aug 11 14:54:38.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c nc -v -z -w 2 10.106.40.133 80'
Aug 11 14:54:38.669: INFO: stderr: "+ nc -v -z -w 2 10.106.40.133 80\nConnection to 10.106.40.133 80 port [tcp/http] succeeded!\n"
Aug 11 14:54:38.669: INFO: stdout: ""
Aug 11 14:54:38.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.40.133:80/ ; done'
Aug 11 14:54:38.858: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n"
Aug 11 14:54:38.858: INFO: stdout: "\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-pmf2s"
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
Aug 11 14:54:38.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.40.133:80/ ; done'
Aug 11 14:54:39.039: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n"
Aug 11 14:54:39.039: INFO: stdout: "\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn"
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
Aug 11 14:54:39.039: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5409, will wait for the garbage collector to delete the pods 08/11/23 14:54:39.053
Aug 11 14:54:39.111: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.782325ms
Aug 11 14:54:39.211: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.106998ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 14:54:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5409" for this suite. 08/11/23 14:54:41.133
------------------------------
• [SLOW TEST] [8.825 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:32.312
    Aug 11 14:54:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 14:54:32.313
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:32.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:32.326
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5409 08/11/23 14:54:32.328
    STEP: creating service affinity-clusterip-transition in namespace services-5409 08/11/23 14:54:32.328
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5409 08/11/23 14:54:32.34
    I0811 14:54:32.348030      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5409, replica count: 3
    I0811 14:54:35.399383      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:54:35.403: INFO: Creating new exec pod
    Aug 11 14:54:35.408: INFO: Waiting up to 5m0s for pod "execpod-affinityk66dk" in namespace "services-5409" to be "running"
    Aug 11 14:54:35.410: INFO: Pod "execpod-affinityk66dk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478432ms
    Aug 11 14:54:37.413: INFO: Pod "execpod-affinityk66dk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005523851s
    Aug 11 14:54:37.413: INFO: Pod "execpod-affinityk66dk" satisfied condition "running"
    Aug 11 14:54:38.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 11 14:54:38.544: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 11 14:54:38.544: INFO: stdout: ""
    Aug 11 14:54:38.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c nc -v -z -w 2 10.106.40.133 80'
    Aug 11 14:54:38.669: INFO: stderr: "+ nc -v -z -w 2 10.106.40.133 80\nConnection to 10.106.40.133 80 port [tcp/http] succeeded!\n"
    Aug 11 14:54:38.669: INFO: stdout: ""
    Aug 11 14:54:38.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.40.133:80/ ; done'
    Aug 11 14:54:38.858: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n"
    Aug 11 14:54:38.858: INFO: stdout: "\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-5kx6n\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-pmf2s\naffinity-clusterip-transition-pmf2s"
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-5kx6n
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.858: INFO: Received response from host: affinity-clusterip-transition-pmf2s
    Aug 11 14:54:38.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-5409 exec execpod-affinityk66dk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.106.40.133:80/ ; done'
    Aug 11 14:54:39.039: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.106.40.133:80/\n"
    Aug 11 14:54:39.039: INFO: stdout: "\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn\naffinity-clusterip-transition-gpnbn"
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Received response from host: affinity-clusterip-transition-gpnbn
    Aug 11 14:54:39.039: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5409, will wait for the garbage collector to delete the pods 08/11/23 14:54:39.053
    Aug 11 14:54:39.111: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.782325ms
    Aug 11 14:54:39.211: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.106998ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:54:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5409" for this suite. 08/11/23 14:54:41.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:54:41.138
Aug 11 14:54:41.138: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:54:41.139
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:41.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:41.15
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 11 14:54:41.162: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:55:41.199: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:55:41.202
Aug 11 14:55:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:55:41.203
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:55:41.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:55:41.216
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/11/23 14:55:41.218
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:55:41.218
Aug 11 14:55:41.225: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9757" to be "running"
Aug 11 14:55:41.227: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106262ms
Aug 11 14:55:43.231: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005851751s
Aug 11 14:55:43.231: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:55:43.233
Aug 11 14:55:43.244: INFO: found a healthy node: constell-1cf5d931-worker-6381a7ba-nd80
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 11 14:55:49.313: INFO: pods created so far: [1 1 1]
Aug 11 14:55:49.313: INFO: length of pods created so far: 3
Aug 11 14:55:53.324: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9757" for this suite. 08/11/23 14:56:00.395
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6578" for this suite. 08/11/23 14:56:00.4
------------------------------
• [SLOW TEST] [79.270 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:54:41.138
    Aug 11 14:54:41.138: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:54:41.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:41.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:41.15
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 11 14:54:41.162: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:55:41.199: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:55:41.202
    Aug 11 14:55:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:55:41.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:55:41.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:55:41.216
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/11/23 14:55:41.218
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:55:41.218
    Aug 11 14:55:41.225: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9757" to be "running"
    Aug 11 14:55:41.227: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.106262ms
    Aug 11 14:55:43.231: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005851751s
    Aug 11 14:55:43.231: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:55:43.233
    Aug 11 14:55:43.244: INFO: found a healthy node: constell-1cf5d931-worker-6381a7ba-nd80
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 11 14:55:49.313: INFO: pods created so far: [1 1 1]
    Aug 11 14:55:49.313: INFO: length of pods created so far: 3
    Aug 11 14:55:53.324: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:00.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9757" for this suite. 08/11/23 14:56:00.395
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6578" for this suite. 08/11/23 14:56:00.4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:00.408
Aug 11 14:56:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:56:00.409
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:00.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:00.423
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/11/23 14:56:00.425
Aug 11 14:56:00.431: INFO: created test-pod-1
Aug 11 14:56:00.437: INFO: created test-pod-2
Aug 11 14:56:00.442: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/11/23 14:56:00.442
Aug 11 14:56:00.442: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6046' to be running and ready
Aug 11 14:56:00.454: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 14:56:00.454: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 14:56:00.454: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 14:56:00.454: INFO: 0 / 3 pods in namespace 'pods-6046' are running and ready (0 seconds elapsed)
Aug 11 14:56:00.454: INFO: expected 0 pod replicas in namespace 'pods-6046', 0 are Running and Ready.
Aug 11 14:56:00.454: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 14:56:00.454: INFO: test-pod-1  constell-1cf5d931-worker-6381a7ba-nd80  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
Aug 11 14:56:00.454: INFO: test-pod-2  constell-1cf5d931-worker-6381a7ba-nd80  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
Aug 11 14:56:00.454: INFO: test-pod-3  constell-1cf5d931-worker-6381a7ba-mt98  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
Aug 11 14:56:00.454: INFO: 
Aug 11 14:56:02.462: INFO: 3 / 3 pods in namespace 'pods-6046' are running and ready (2 seconds elapsed)
Aug 11 14:56:02.462: INFO: expected 0 pod replicas in namespace 'pods-6046', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/11/23 14:56:02.48
Aug 11 14:56:02.482: INFO: Pod quantity 3 is different from expected quantity 0
Aug 11 14:56:03.486: INFO: Pod quantity 3 is different from expected quantity 0
Aug 11 14:56:04.486: INFO: Pod quantity 2 is different from expected quantity 0
Aug 11 14:56:05.488: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:06.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6046" for this suite. 08/11/23 14:56:06.489
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:00.408
    Aug 11 14:56:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:56:00.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:00.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:00.423
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/11/23 14:56:00.425
    Aug 11 14:56:00.431: INFO: created test-pod-1
    Aug 11 14:56:00.437: INFO: created test-pod-2
    Aug 11 14:56:00.442: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/11/23 14:56:00.442
    Aug 11 14:56:00.442: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6046' to be running and ready
    Aug 11 14:56:00.454: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 14:56:00.454: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 14:56:00.454: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 14:56:00.454: INFO: 0 / 3 pods in namespace 'pods-6046' are running and ready (0 seconds elapsed)
    Aug 11 14:56:00.454: INFO: expected 0 pod replicas in namespace 'pods-6046', 0 are Running and Ready.
    Aug 11 14:56:00.454: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 14:56:00.454: INFO: test-pod-1  constell-1cf5d931-worker-6381a7ba-nd80  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
    Aug 11 14:56:00.454: INFO: test-pod-2  constell-1cf5d931-worker-6381a7ba-nd80  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
    Aug 11 14:56:00.454: INFO: test-pod-3  constell-1cf5d931-worker-6381a7ba-mt98  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:56:00 +0000 UTC  }]
    Aug 11 14:56:00.454: INFO: 
    Aug 11 14:56:02.462: INFO: 3 / 3 pods in namespace 'pods-6046' are running and ready (2 seconds elapsed)
    Aug 11 14:56:02.462: INFO: expected 0 pod replicas in namespace 'pods-6046', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/11/23 14:56:02.48
    Aug 11 14:56:02.482: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 11 14:56:03.486: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 11 14:56:04.486: INFO: Pod quantity 2 is different from expected quantity 0
    Aug 11 14:56:05.488: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:06.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6046" for this suite. 08/11/23 14:56:06.489
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:06.495
Aug 11 14:56:06.495: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename server-version 08/11/23 14:56:06.496
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.51
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/11/23 14:56:06.512
STEP: Confirm major version 08/11/23 14:56:06.513
Aug 11 14:56:06.513: INFO: Major version: 1
STEP: Confirm minor version 08/11/23 14:56:06.513
Aug 11 14:56:06.513: INFO: cleanMinorVersion: 26
Aug 11 14:56:06.513: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3334" for this suite. 08/11/23 14:56:06.516
------------------------------
• [0.026 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:06.495
    Aug 11 14:56:06.495: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename server-version 08/11/23 14:56:06.496
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.51
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/11/23 14:56:06.512
    STEP: Confirm major version 08/11/23 14:56:06.513
    Aug 11 14:56:06.513: INFO: Major version: 1
    STEP: Confirm minor version 08/11/23 14:56:06.513
    Aug 11 14:56:06.513: INFO: cleanMinorVersion: 26
    Aug 11 14:56:06.513: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3334" for this suite. 08/11/23 14:56:06.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:06.523
Aug 11 14:56:06.523: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename ingress 08/11/23 14:56:06.523
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.536
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/11/23 14:56:06.539
STEP: getting /apis/networking.k8s.io 08/11/23 14:56:06.541
STEP: getting /apis/networking.k8s.iov1 08/11/23 14:56:06.542
STEP: creating 08/11/23 14:56:06.542
STEP: getting 08/11/23 14:56:06.557
STEP: listing 08/11/23 14:56:06.559
STEP: watching 08/11/23 14:56:06.562
Aug 11 14:56:06.562: INFO: starting watch
STEP: cluster-wide listing 08/11/23 14:56:06.563
STEP: cluster-wide watching 08/11/23 14:56:06.565
Aug 11 14:56:06.565: INFO: starting watch
STEP: patching 08/11/23 14:56:06.566
STEP: updating 08/11/23 14:56:06.57
Aug 11 14:56:06.578: INFO: waiting for watch events with expected annotations
Aug 11 14:56:06.578: INFO: saw patched and updated annotations
STEP: patching /status 08/11/23 14:56:06.578
STEP: updating /status 08/11/23 14:56:06.582
STEP: get /status 08/11/23 14:56:06.588
STEP: deleting 08/11/23 14:56:06.59
STEP: deleting a collection 08/11/23 14:56:06.598
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5165" for this suite. 08/11/23 14:56:06.613
------------------------------
• [0.096 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:06.523
    Aug 11 14:56:06.523: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename ingress 08/11/23 14:56:06.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.536
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/11/23 14:56:06.539
    STEP: getting /apis/networking.k8s.io 08/11/23 14:56:06.541
    STEP: getting /apis/networking.k8s.iov1 08/11/23 14:56:06.542
    STEP: creating 08/11/23 14:56:06.542
    STEP: getting 08/11/23 14:56:06.557
    STEP: listing 08/11/23 14:56:06.559
    STEP: watching 08/11/23 14:56:06.562
    Aug 11 14:56:06.562: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 14:56:06.563
    STEP: cluster-wide watching 08/11/23 14:56:06.565
    Aug 11 14:56:06.565: INFO: starting watch
    STEP: patching 08/11/23 14:56:06.566
    STEP: updating 08/11/23 14:56:06.57
    Aug 11 14:56:06.578: INFO: waiting for watch events with expected annotations
    Aug 11 14:56:06.578: INFO: saw patched and updated annotations
    STEP: patching /status 08/11/23 14:56:06.578
    STEP: updating /status 08/11/23 14:56:06.582
    STEP: get /status 08/11/23 14:56:06.588
    STEP: deleting 08/11/23 14:56:06.59
    STEP: deleting a collection 08/11/23 14:56:06.598
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5165" for this suite. 08/11/23 14:56:06.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:06.619
Aug 11 14:56:06.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 14:56:06.62
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.631
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/11/23 14:56:06.634
Aug 11 14:56:06.640: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7089  a8b83079-2a2c-43a6-9338-d3e0250c8917 40138 0 2023-08-11 14:56:06 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-11 14:56:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8gt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8gt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:56:06.641: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7089" to be "running and ready"
Aug 11 14:56:06.643: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207412ms
Aug 11 14:56:06.643: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:08.647: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.006175501s
Aug 11 14:56:08.647: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 11 14:56:08.647: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/11/23 14:56:08.647
Aug 11 14:56:08.647: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7089 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:56:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:56:08.648: INFO: ExecWithOptions: Clientset creation
Aug 11 14:56:08.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7089/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/11/23 14:56:08.735
Aug 11 14:56:08.736: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7089 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:56:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 14:56:08.736: INFO: ExecWithOptions: Clientset creation
Aug 11 14:56:08.736: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7089/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 14:56:08.814: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:08.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7089" for this suite. 08/11/23 14:56:08.83
------------------------------
• [2.217 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:06.619
    Aug 11 14:56:06.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 14:56:06.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:06.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:06.631
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/11/23 14:56:06.634
    Aug 11 14:56:06.640: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7089  a8b83079-2a2c-43a6-9338-d3e0250c8917 40138 0 2023-08-11 14:56:06 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-11 14:56:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8gt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8gt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:56:06.641: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7089" to be "running and ready"
    Aug 11 14:56:06.643: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.207412ms
    Aug 11 14:56:06.643: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:08.647: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.006175501s
    Aug 11 14:56:08.647: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 11 14:56:08.647: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/11/23 14:56:08.647
    Aug 11 14:56:08.647: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7089 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:56:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:56:08.648: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:56:08.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7089/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/11/23 14:56:08.735
    Aug 11 14:56:08.736: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7089 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:56:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 14:56:08.736: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:56:08.736: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7089/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 14:56:08.814: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:08.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7089" for this suite. 08/11/23 14:56:08.83
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:08.836
Aug 11 14:56:08.836: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:56:08.837
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:08.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:08.853
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:56:08.866
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:56:09.157
STEP: Deploying the webhook pod 08/11/23 14:56:09.165
STEP: Wait for the deployment to be ready 08/11/23 14:56:09.175
Aug 11 14:56:09.182: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:56:11.19
STEP: Verifying the service has paired with the endpoint 08/11/23 14:56:11.202
Aug 11 14:56:12.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 11 14:56:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6650-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:56:12.717
STEP: Creating a custom resource while v1 is storage version 08/11/23 14:56:12.741
STEP: Patching Custom Resource Definition to set v2 as storage 08/11/23 14:56:14.804
STEP: Patching the custom resource while v2 is storage version 08/11/23 14:56:14.821
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:15.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7116" for this suite. 08/11/23 14:56:15.421
STEP: Destroying namespace "webhook-7116-markers" for this suite. 08/11/23 14:56:15.428
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:08.836
    Aug 11 14:56:08.836: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:56:08.837
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:08.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:08.853
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:56:08.866
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:56:09.157
    STEP: Deploying the webhook pod 08/11/23 14:56:09.165
    STEP: Wait for the deployment to be ready 08/11/23 14:56:09.175
    Aug 11 14:56:09.182: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:56:11.19
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:56:11.202
    Aug 11 14:56:12.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 11 14:56:12.207: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6650-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:56:12.717
    STEP: Creating a custom resource while v1 is storage version 08/11/23 14:56:12.741
    STEP: Patching Custom Resource Definition to set v2 as storage 08/11/23 14:56:14.804
    STEP: Patching the custom resource while v2 is storage version 08/11/23 14:56:14.821
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:15.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7116" for this suite. 08/11/23 14:56:15.421
    STEP: Destroying namespace "webhook-7116-markers" for this suite. 08/11/23 14:56:15.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:15.435
Aug 11 14:56:15.435: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:56:15.436
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:15.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:15.455
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:15.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9828" for this suite. 08/11/23 14:56:15.506
------------------------------
• [0.075 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:15.435
    Aug 11 14:56:15.435: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:56:15.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:15.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:15.455
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:15.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9828" for this suite. 08/11/23 14:56:15.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:15.512
Aug 11 14:56:15.512: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:56:15.513
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:15.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:15.527
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/11/23 14:56:15.529
Aug 11 14:56:15.535: INFO: Waiting up to 5m0s for pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6" in namespace "downward-api-1537" to be "running and ready"
Aug 11 14:56:15.537: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.531232ms
Aug 11 14:56:15.537: INFO: The phase of Pod labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:17.541: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005910491s
Aug 11 14:56:17.541: INFO: The phase of Pod labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6 is Running (Ready = true)
Aug 11 14:56:17.541: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6" satisfied condition "running and ready"
Aug 11 14:56:18.072: INFO: Successfully updated pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1537" for this suite. 08/11/23 14:56:20.092
------------------------------
• [4.585 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:15.512
    Aug 11 14:56:15.512: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:56:15.513
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:15.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:15.527
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/11/23 14:56:15.529
    Aug 11 14:56:15.535: INFO: Waiting up to 5m0s for pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6" in namespace "downward-api-1537" to be "running and ready"
    Aug 11 14:56:15.537: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.531232ms
    Aug 11 14:56:15.537: INFO: The phase of Pod labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:17.541: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005910491s
    Aug 11 14:56:17.541: INFO: The phase of Pod labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6 is Running (Ready = true)
    Aug 11 14:56:17.541: INFO: Pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6" satisfied condition "running and ready"
    Aug 11 14:56:18.072: INFO: Successfully updated pod "labelsupdate241c5c6b-949b-42cb-a05b-7eab828806d6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1537" for this suite. 08/11/23 14:56:20.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:20.099
Aug 11 14:56:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:56:20.1
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:20.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:20.113
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 11 14:56:20.115: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: creating the pod 08/11/23 14:56:20.116
STEP: submitting the pod to kubernetes 08/11/23 14:56:20.116
Aug 11 14:56:20.123: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f" in namespace "pods-3702" to be "running and ready"
Aug 11 14:56:20.125: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866993ms
Aug 11 14:56:20.125: INFO: The phase of Pod pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:22.130: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007204314s
Aug 11 14:56:22.130: INFO: The phase of Pod pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f is Running (Ready = true)
Aug 11 14:56:22.130: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3702" for this suite. 08/11/23 14:56:22.146
------------------------------
• [2.053 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:20.099
    Aug 11 14:56:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:56:20.1
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:20.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:20.113
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 11 14:56:20.115: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: creating the pod 08/11/23 14:56:20.116
    STEP: submitting the pod to kubernetes 08/11/23 14:56:20.116
    Aug 11 14:56:20.123: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f" in namespace "pods-3702" to be "running and ready"
    Aug 11 14:56:20.125: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866993ms
    Aug 11 14:56:20.125: INFO: The phase of Pod pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:22.130: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007204314s
    Aug 11 14:56:22.130: INFO: The phase of Pod pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f is Running (Ready = true)
    Aug 11 14:56:22.130: INFO: Pod "pod-logs-websocket-844f1a82-4029-4229-aef0-3c5af6e0502f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:22.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3702" for this suite. 08/11/23 14:56:22.146
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:22.152
Aug 11 14:56:22.152: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:56:22.153
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:22.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:22.166
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/11/23 14:56:22.169
STEP: getting /apis/node.k8s.io 08/11/23 14:56:22.171
STEP: getting /apis/node.k8s.io/v1 08/11/23 14:56:22.172
STEP: creating 08/11/23 14:56:22.172
STEP: watching 08/11/23 14:56:22.188
Aug 11 14:56:22.188: INFO: starting watch
STEP: getting 08/11/23 14:56:22.192
STEP: listing 08/11/23 14:56:22.194
STEP: patching 08/11/23 14:56:22.196
STEP: updating 08/11/23 14:56:22.2
Aug 11 14:56:22.206: INFO: waiting for watch events with expected annotations
STEP: deleting 08/11/23 14:56:22.206
STEP: deleting a collection 08/11/23 14:56:22.214
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:22.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7945" for this suite. 08/11/23 14:56:22.227
------------------------------
• [0.081 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:22.152
    Aug 11 14:56:22.152: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:56:22.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:22.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:22.166
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/11/23 14:56:22.169
    STEP: getting /apis/node.k8s.io 08/11/23 14:56:22.171
    STEP: getting /apis/node.k8s.io/v1 08/11/23 14:56:22.172
    STEP: creating 08/11/23 14:56:22.172
    STEP: watching 08/11/23 14:56:22.188
    Aug 11 14:56:22.188: INFO: starting watch
    STEP: getting 08/11/23 14:56:22.192
    STEP: listing 08/11/23 14:56:22.194
    STEP: patching 08/11/23 14:56:22.196
    STEP: updating 08/11/23 14:56:22.2
    Aug 11 14:56:22.206: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/11/23 14:56:22.206
    STEP: deleting a collection 08/11/23 14:56:22.214
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:22.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7945" for this suite. 08/11/23 14:56:22.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:22.233
Aug 11 14:56:22.233: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:56:22.234
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:22.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:22.247
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 11 14:56:22.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2734" for this suite. 08/11/23 14:56:23.273
------------------------------
• [1.046 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:22.233
    Aug 11 14:56:22.233: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:56:22.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:22.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:22.247
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 11 14:56:22.249: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:23.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2734" for this suite. 08/11/23 14:56:23.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:23.279
Aug 11 14:56:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:56:23.28
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:23.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:23.294
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 11 14:56:23.304: INFO: Waiting up to 2m0s for pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" in namespace "var-expansion-1320" to be "container 0 failed with reason CreateContainerConfigError"
Aug 11 14:56:23.307: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658892ms
Aug 11 14:56:25.311: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006307362s
Aug 11 14:56:25.311: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 11 14:56:25.311: INFO: Deleting pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" in namespace "var-expansion-1320"
Aug 11 14:56:25.317: INFO: Wait up to 5m0s for pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1320" for this suite. 08/11/23 14:56:27.326
------------------------------
• [4.053 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:23.279
    Aug 11 14:56:23.279: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:56:23.28
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:23.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:23.294
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 11 14:56:23.304: INFO: Waiting up to 2m0s for pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" in namespace "var-expansion-1320" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 11 14:56:23.307: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658892ms
    Aug 11 14:56:25.311: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006307362s
    Aug 11 14:56:25.311: INFO: Pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 11 14:56:25.311: INFO: Deleting pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" in namespace "var-expansion-1320"
    Aug 11 14:56:25.317: INFO: Wait up to 5m0s for pod "var-expansion-7dcf2f84-7ba3-48e3-94db-5a3e95006515" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1320" for this suite. 08/11/23 14:56:27.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:27.334
Aug 11 14:56:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 14:56:27.334
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:27.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:27.351
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/11/23 14:56:27.356
Aug 11 14:56:27.356: INFO: Creating simple deployment test-deployment-jkkpj
Aug 11 14:56:27.369: INFO: deployment "test-deployment-jkkpj" doesn't have the required revision set
STEP: Getting /status 08/11/23 14:56:29.38
Aug 11 14:56:29.382: INFO: Deployment test-deployment-jkkpj has Conditions: [{Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/11/23 14:56:29.382
Aug 11 14:56:29.391: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 56, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jkkpj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/11/23 14:56:29.391
Aug 11 14:56:29.392: INFO: Observed &Deployment event: ADDED
Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
Aug 11 14:56:29.392: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 14:56:29.392: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jkkpj-54bc444df" is progressing.}
Aug 11 14:56:29.393: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
Aug 11 14:56:29.393: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
Aug 11 14:56:29.393: INFO: Found Deployment test-deployment-jkkpj in namespace deployment-580 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 14:56:29.393: INFO: Deployment test-deployment-jkkpj has an updated status
STEP: patching the Statefulset Status 08/11/23 14:56:29.393
Aug 11 14:56:29.393: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 14:56:29.399: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/11/23 14:56:29.399
Aug 11 14:56:29.400: INFO: Observed &Deployment event: ADDED
Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
Aug 11 14:56:29.400: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 14:56:29.400: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jkkpj-54bc444df" is progressing.}
Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
Aug 11 14:56:29.401: INFO: Found deployment test-deployment-jkkpj in namespace deployment-580 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 11 14:56:29.401: INFO: Deployment test-deployment-jkkpj has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:56:29.404: INFO: Deployment "test-deployment-jkkpj":
&Deployment{ObjectMeta:{test-deployment-jkkpj  deployment-580  0b6eb176-f83f-4849-80f2-5ba383920381 40562 1 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ec6878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:56:29.407: INFO: New ReplicaSet "test-deployment-jkkpj-54bc444df" of Deployment "test-deployment-jkkpj":
&ReplicaSet{ObjectMeta:{test-deployment-jkkpj-54bc444df  deployment-580  cd617737-4d21-400d-be59-4918e5863835 40557 1 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jkkpj 0b6eb176-f83f-4849-80f2-5ba383920381 0xc004cda720 0xc004cda721}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b6eb176-f83f-4849-80f2-5ba383920381\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cda7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:56:29.409: INFO: Pod "test-deployment-jkkpj-54bc444df-d49w7" is available:
&Pod{ObjectMeta:{test-deployment-jkkpj-54bc444df-d49w7 test-deployment-jkkpj-54bc444df- deployment-580  cd1ca9e3-1593-42d4-996a-4f77dd573429 40556 0 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-jkkpj-54bc444df cd617737-4d21-400d-be59-4918e5863835 0xc004cdaba0 0xc004cdaba1}] [] [{kube-controller-manager Update v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd617737-4d21-400d-be59-4918e5863835\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnl62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnl62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.28,StartTime:2023-08-11 14:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://de8b816decc25d63cf50cee7cb231812495d44651ca5377488af95cd3f8637c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:29.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-580" for this suite. 08/11/23 14:56:29.414
------------------------------
• [2.085 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:27.334
    Aug 11 14:56:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 14:56:27.334
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:27.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:27.351
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/11/23 14:56:27.356
    Aug 11 14:56:27.356: INFO: Creating simple deployment test-deployment-jkkpj
    Aug 11 14:56:27.369: INFO: deployment "test-deployment-jkkpj" doesn't have the required revision set
    STEP: Getting /status 08/11/23 14:56:29.38
    Aug 11 14:56:29.382: INFO: Deployment test-deployment-jkkpj has Conditions: [{Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/11/23 14:56:29.382
    Aug 11 14:56:29.391: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 56, 27, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jkkpj-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/11/23 14:56:29.391
    Aug 11 14:56:29.392: INFO: Observed &Deployment event: ADDED
    Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
    Aug 11 14:56:29.392: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
    Aug 11 14:56:29.392: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 14:56:29.392: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jkkpj-54bc444df" is progressing.}
    Aug 11 14:56:29.393: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
    Aug 11 14:56:29.393: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 14:56:29.393: INFO: Observed Deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
    Aug 11 14:56:29.393: INFO: Found Deployment test-deployment-jkkpj in namespace deployment-580 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 14:56:29.393: INFO: Deployment test-deployment-jkkpj has an updated status
    STEP: patching the Statefulset Status 08/11/23 14:56:29.393
    Aug 11 14:56:29.393: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 14:56:29.399: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/11/23 14:56:29.399
    Aug 11 14:56:29.400: INFO: Observed &Deployment event: ADDED
    Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
    Aug 11 14:56:29.400: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jkkpj-54bc444df"}
    Aug 11 14:56:29.400: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 14:56:29.400: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:27 +0000 UTC 2023-08-11 14:56:27 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jkkpj-54bc444df" is progressing.}
    Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
    Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 14:56:29 +0000 UTC 2023-08-11 14:56:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jkkpj-54bc444df" has successfully progressed.}
    Aug 11 14:56:29.401: INFO: Observed deployment test-deployment-jkkpj in namespace deployment-580 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 14:56:29.401: INFO: Observed &Deployment event: MODIFIED
    Aug 11 14:56:29.401: INFO: Found deployment test-deployment-jkkpj in namespace deployment-580 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 11 14:56:29.401: INFO: Deployment test-deployment-jkkpj has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:56:29.404: INFO: Deployment "test-deployment-jkkpj":
    &Deployment{ObjectMeta:{test-deployment-jkkpj  deployment-580  0b6eb176-f83f-4849-80f2-5ba383920381 40562 1 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ec6878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:56:29.407: INFO: New ReplicaSet "test-deployment-jkkpj-54bc444df" of Deployment "test-deployment-jkkpj":
    &ReplicaSet{ObjectMeta:{test-deployment-jkkpj-54bc444df  deployment-580  cd617737-4d21-400d-be59-4918e5863835 40557 1 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jkkpj 0b6eb176-f83f-4849-80f2-5ba383920381 0xc004cda720 0xc004cda721}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b6eb176-f83f-4849-80f2-5ba383920381\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cda7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:56:29.409: INFO: Pod "test-deployment-jkkpj-54bc444df-d49w7" is available:
    &Pod{ObjectMeta:{test-deployment-jkkpj-54bc444df-d49w7 test-deployment-jkkpj-54bc444df- deployment-580  cd1ca9e3-1593-42d4-996a-4f77dd573429 40556 0 2023-08-11 14:56:27 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-jkkpj-54bc444df cd617737-4d21-400d-be59-4918e5863835 0xc004cdaba0 0xc004cdaba1}] [] [{kube-controller-manager Update v1 2023-08-11 14:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd617737-4d21-400d-be59-4918e5863835\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnl62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnl62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.28,StartTime:2023-08-11 14:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://de8b816decc25d63cf50cee7cb231812495d44651ca5377488af95cd3f8637c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:29.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-580" for this suite. 08/11/23 14:56:29.414
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:29.419
Aug 11 14:56:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:56:29.42
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.433
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:56:29.435
Aug 11 14:56:29.442: INFO: Waiting up to 5m0s for pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42" in namespace "projected-8382" to be "Succeeded or Failed"
Aug 11 14:56:29.444: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049392ms
Aug 11 14:56:31.448: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00585432s
Aug 11 14:56:33.447: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004651464s
STEP: Saw pod success 08/11/23 14:56:33.447
Aug 11 14:56:33.447: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42" satisfied condition "Succeeded or Failed"
Aug 11 14:56:33.449: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 container client-container: <nil>
STEP: delete the pod 08/11/23 14:56:33.457
Aug 11 14:56:33.468: INFO: Waiting for pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 to disappear
Aug 11 14:56:33.471: INFO: Pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:33.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8382" for this suite. 08/11/23 14:56:33.474
------------------------------
• [4.060 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:29.419
    Aug 11 14:56:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:56:29.42
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.433
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:56:29.435
    Aug 11 14:56:29.442: INFO: Waiting up to 5m0s for pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42" in namespace "projected-8382" to be "Succeeded or Failed"
    Aug 11 14:56:29.444: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049392ms
    Aug 11 14:56:31.448: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00585432s
    Aug 11 14:56:33.447: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004651464s
    STEP: Saw pod success 08/11/23 14:56:33.447
    Aug 11 14:56:33.447: INFO: Pod "downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42" satisfied condition "Succeeded or Failed"
    Aug 11 14:56:33.449: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:56:33.457
    Aug 11 14:56:33.468: INFO: Waiting for pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 to disappear
    Aug 11 14:56:33.471: INFO: Pod downwardapi-volume-164e3ec2-cff0-4b6a-8c02-e42943ef4f42 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:33.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8382" for this suite. 08/11/23 14:56:33.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:33.479
Aug 11 14:56:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:56:33.48
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:33.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:33.493
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-b6012dd6-1bc5-45b3-a97f-f4876018f589 08/11/23 14:56:33.498
STEP: Creating configMap with name cm-test-opt-upd-22fdcc8e-3688-4233-b9ab-fe7ce49ba3ec 08/11/23 14:56:33.502
STEP: Creating the pod 08/11/23 14:56:33.507
Aug 11 14:56:33.516: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9" in namespace "projected-7815" to be "running and ready"
Aug 11 14:56:33.518: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563352ms
Aug 11 14:56:33.518: INFO: The phase of Pod pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:35.522: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006623402s
Aug 11 14:56:35.522: INFO: The phase of Pod pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9 is Running (Ready = true)
Aug 11 14:56:35.523: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b6012dd6-1bc5-45b3-a97f-f4876018f589 08/11/23 14:56:35.547
STEP: Updating configmap cm-test-opt-upd-22fdcc8e-3688-4233-b9ab-fe7ce49ba3ec 08/11/23 14:56:35.552
STEP: Creating configMap with name cm-test-opt-create-48d11da3-3631-4254-8b9a-e3c1d8aed136 08/11/23 14:56:35.556
STEP: waiting to observe update in volume 08/11/23 14:56:35.561
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7815" for this suite. 08/11/23 14:56:37.595
------------------------------
• [4.120 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:33.479
    Aug 11 14:56:33.479: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:56:33.48
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:33.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:33.493
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-b6012dd6-1bc5-45b3-a97f-f4876018f589 08/11/23 14:56:33.498
    STEP: Creating configMap with name cm-test-opt-upd-22fdcc8e-3688-4233-b9ab-fe7ce49ba3ec 08/11/23 14:56:33.502
    STEP: Creating the pod 08/11/23 14:56:33.507
    Aug 11 14:56:33.516: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9" in namespace "projected-7815" to be "running and ready"
    Aug 11 14:56:33.518: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.563352ms
    Aug 11 14:56:33.518: INFO: The phase of Pod pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:35.522: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006623402s
    Aug 11 14:56:35.522: INFO: The phase of Pod pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9 is Running (Ready = true)
    Aug 11 14:56:35.523: INFO: Pod "pod-projected-configmaps-05bcd6f5-37c0-48df-8fad-7f0f428618f9" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b6012dd6-1bc5-45b3-a97f-f4876018f589 08/11/23 14:56:35.547
    STEP: Updating configmap cm-test-opt-upd-22fdcc8e-3688-4233-b9ab-fe7ce49ba3ec 08/11/23 14:56:35.552
    STEP: Creating configMap with name cm-test-opt-create-48d11da3-3631-4254-8b9a-e3c1d8aed136 08/11/23 14:56:35.556
    STEP: waiting to observe update in volume 08/11/23 14:56:35.561
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7815" for this suite. 08/11/23 14:56:37.595
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:37.599
Aug 11 14:56:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:56:37.6
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.615
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 11 14:56:37.631: INFO: created pod pod-service-account-defaultsa
Aug 11 14:56:37.631: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 11 14:56:37.637: INFO: created pod pod-service-account-mountsa
Aug 11 14:56:37.637: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 11 14:56:37.643: INFO: created pod pod-service-account-nomountsa
Aug 11 14:56:37.643: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 11 14:56:37.649: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 11 14:56:37.649: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 11 14:56:37.659: INFO: created pod pod-service-account-mountsa-mountspec
Aug 11 14:56:37.659: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 11 14:56:37.667: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 11 14:56:37.667: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 11 14:56:37.675: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 11 14:56:37.675: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 11 14:56:37.682: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 11 14:56:37.682: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 11 14:56:37.689: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 11 14:56:37.689: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:37.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7445" for this suite. 08/11/23 14:56:37.698
------------------------------
• [0.105 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:37.599
    Aug 11 14:56:37.600: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:56:37.6
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.615
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 11 14:56:37.631: INFO: created pod pod-service-account-defaultsa
    Aug 11 14:56:37.631: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 11 14:56:37.637: INFO: created pod pod-service-account-mountsa
    Aug 11 14:56:37.637: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 11 14:56:37.643: INFO: created pod pod-service-account-nomountsa
    Aug 11 14:56:37.643: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 11 14:56:37.649: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 11 14:56:37.649: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 11 14:56:37.659: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 11 14:56:37.659: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 11 14:56:37.667: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 11 14:56:37.667: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 11 14:56:37.675: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 11 14:56:37.675: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 11 14:56:37.682: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 11 14:56:37.682: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 11 14:56:37.689: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 11 14:56:37.689: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:37.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7445" for this suite. 08/11/23 14:56:37.698
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:37.704
Aug 11 14:56:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:56:37.705
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.73
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-7d5c9d6e-8037-487e-aae2-7e9b7cbb8842 08/11/23 14:56:37.733
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:37.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7847" for this suite. 08/11/23 14:56:37.738
------------------------------
• [0.040 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:37.704
    Aug 11 14:56:37.704: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:56:37.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.73
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-7d5c9d6e-8037-487e-aae2-7e9b7cbb8842 08/11/23 14:56:37.733
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:37.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7847" for this suite. 08/11/23 14:56:37.738
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:37.746
Aug 11 14:56:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:56:37.747
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.761
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 11 14:56:37.772: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6" in namespace "kubelet-test-9312" to be "running and ready"
Aug 11 14:56:37.776: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696795ms
Aug 11 14:56:37.777: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:39.782: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009970146s
Aug 11 14:56:39.782: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:41.780: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008086589s
Aug 11 14:56:41.780: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Running (Ready = true)
Aug 11 14:56:41.780: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:41.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9312" for this suite. 08/11/23 14:56:41.795
------------------------------
• [4.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:37.746
    Aug 11 14:56:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 14:56:37.747
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:37.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:37.761
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 11 14:56:37.772: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6" in namespace "kubelet-test-9312" to be "running and ready"
    Aug 11 14:56:37.776: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.696795ms
    Aug 11 14:56:37.777: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:39.782: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009970146s
    Aug 11 14:56:39.782: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:41.780: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008086589s
    Aug 11 14:56:41.780: INFO: The phase of Pod busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6 is Running (Ready = true)
    Aug 11 14:56:41.780: INFO: Pod "busybox-readonly-fs220b265a-e0b6-4e54-9775-a9d9134738a6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:41.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9312" for this suite. 08/11/23 14:56:41.795
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:41.801
Aug 11 14:56:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:56:41.801
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:41.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:41.815
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/11/23 14:56:41.818
Aug 11 14:56:41.818: INFO: namespace kubectl-1587
Aug 11 14:56:41.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 create -f -'
Aug 11 14:56:42.618: INFO: stderr: ""
Aug 11 14:56:42.618: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 14:56:42.618
Aug 11 14:56:43.621: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:56:43.621: INFO: Found 0 / 1
Aug 11 14:56:44.622: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:56:44.622: INFO: Found 0 / 1
Aug 11 14:56:45.622: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:56:45.622: INFO: Found 1 / 1
Aug 11 14:56:45.622: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 11 14:56:45.625: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:56:45.625: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 14:56:45.625: INFO: wait on agnhost-primary startup in kubectl-1587 
Aug 11 14:56:45.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 logs agnhost-primary-2mbvp agnhost-primary'
Aug 11 14:56:45.705: INFO: stderr: ""
Aug 11 14:56:45.705: INFO: stdout: "Paused\n"
STEP: exposing RC 08/11/23 14:56:45.705
Aug 11 14:56:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 11 14:56:45.782: INFO: stderr: ""
Aug 11 14:56:45.782: INFO: stdout: "service/rm2 exposed\n"
Aug 11 14:56:45.787: INFO: Service rm2 in namespace kubectl-1587 found.
STEP: exposing service 08/11/23 14:56:47.793
Aug 11 14:56:47.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 11 14:56:47.867: INFO: stderr: ""
Aug 11 14:56:47.867: INFO: stdout: "service/rm3 exposed\n"
Aug 11 14:56:47.872: INFO: Service rm3 in namespace kubectl-1587 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:49.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1587" for this suite. 08/11/23 14:56:49.882
------------------------------
• [SLOW TEST] [8.087 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:41.801
    Aug 11 14:56:41.801: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:56:41.801
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:41.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:41.815
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/11/23 14:56:41.818
    Aug 11 14:56:41.818: INFO: namespace kubectl-1587
    Aug 11 14:56:41.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 create -f -'
    Aug 11 14:56:42.618: INFO: stderr: ""
    Aug 11 14:56:42.618: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 14:56:42.618
    Aug 11 14:56:43.621: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:56:43.621: INFO: Found 0 / 1
    Aug 11 14:56:44.622: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:56:44.622: INFO: Found 0 / 1
    Aug 11 14:56:45.622: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:56:45.622: INFO: Found 1 / 1
    Aug 11 14:56:45.622: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 11 14:56:45.625: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:56:45.625: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 14:56:45.625: INFO: wait on agnhost-primary startup in kubectl-1587 
    Aug 11 14:56:45.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 logs agnhost-primary-2mbvp agnhost-primary'
    Aug 11 14:56:45.705: INFO: stderr: ""
    Aug 11 14:56:45.705: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/11/23 14:56:45.705
    Aug 11 14:56:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 11 14:56:45.782: INFO: stderr: ""
    Aug 11 14:56:45.782: INFO: stdout: "service/rm2 exposed\n"
    Aug 11 14:56:45.787: INFO: Service rm2 in namespace kubectl-1587 found.
    STEP: exposing service 08/11/23 14:56:47.793
    Aug 11 14:56:47.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1587 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 11 14:56:47.867: INFO: stderr: ""
    Aug 11 14:56:47.867: INFO: stdout: "service/rm3 exposed\n"
    Aug 11 14:56:47.872: INFO: Service rm3 in namespace kubectl-1587 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:49.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1587" for this suite. 08/11/23 14:56:49.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:49.888
Aug 11 14:56:49.888: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 14:56:49.888
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:49.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:49.903
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-14ea9c19-7725-43ff-976a-696e22b5c4d0 08/11/23 14:56:49.906
STEP: Creating a pod to test consume configMaps 08/11/23 14:56:49.91
Aug 11 14:56:49.916: INFO: Waiting up to 5m0s for pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909" in namespace "configmap-611" to be "Succeeded or Failed"
Aug 11 14:56:49.918: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110122ms
Aug 11 14:56:51.922: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005805041s
Aug 11 14:56:53.923: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006460347s
STEP: Saw pod success 08/11/23 14:56:53.923
Aug 11 14:56:53.923: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909" satisfied condition "Succeeded or Failed"
Aug 11 14:56:53.925: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:56:53.935
Aug 11 14:56:53.947: INFO: Waiting for pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 to disappear
Aug 11 14:56:53.949: INFO: Pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-611" for this suite. 08/11/23 14:56:53.952
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:49.888
    Aug 11 14:56:49.888: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 14:56:49.888
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:49.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:49.903
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-14ea9c19-7725-43ff-976a-696e22b5c4d0 08/11/23 14:56:49.906
    STEP: Creating a pod to test consume configMaps 08/11/23 14:56:49.91
    Aug 11 14:56:49.916: INFO: Waiting up to 5m0s for pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909" in namespace "configmap-611" to be "Succeeded or Failed"
    Aug 11 14:56:49.918: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110122ms
    Aug 11 14:56:51.922: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005805041s
    Aug 11 14:56:53.923: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006460347s
    STEP: Saw pod success 08/11/23 14:56:53.923
    Aug 11 14:56:53.923: INFO: Pod "pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909" satisfied condition "Succeeded or Failed"
    Aug 11 14:56:53.925: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:56:53.935
    Aug 11 14:56:53.947: INFO: Waiting for pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 to disappear
    Aug 11 14:56:53.949: INFO: Pod pod-configmaps-dcf34765-cee0-423d-b206-194fbfdae909 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-611" for this suite. 08/11/23 14:56:53.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:53.957
Aug 11 14:56:53.957: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 14:56:53.958
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:53.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:53.973
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 11 14:56:53.975: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: creating the pod 08/11/23 14:56:53.976
STEP: submitting the pod to kubernetes 08/11/23 14:56:53.976
Aug 11 14:56:53.983: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502" in namespace "pods-3198" to be "running and ready"
Aug 11 14:56:53.985: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210662ms
Aug 11 14:56:53.985: INFO: The phase of Pod pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:56:55.989: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502": Phase="Running", Reason="", readiness=true. Elapsed: 2.006169212s
Aug 11 14:56:55.989: INFO: The phase of Pod pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502 is Running (Ready = true)
Aug 11 14:56:55.989: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 14:56:56.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3198" for this suite. 08/11/23 14:56:56.073
------------------------------
• [2.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:53.957
    Aug 11 14:56:53.957: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 14:56:53.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:53.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:53.973
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 11 14:56:53.975: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: creating the pod 08/11/23 14:56:53.976
    STEP: submitting the pod to kubernetes 08/11/23 14:56:53.976
    Aug 11 14:56:53.983: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502" in namespace "pods-3198" to be "running and ready"
    Aug 11 14:56:53.985: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502": Phase="Pending", Reason="", readiness=false. Elapsed: 2.210662ms
    Aug 11 14:56:53.985: INFO: The phase of Pod pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:56:55.989: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502": Phase="Running", Reason="", readiness=true. Elapsed: 2.006169212s
    Aug 11 14:56:55.989: INFO: The phase of Pod pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502 is Running (Ready = true)
    Aug 11 14:56:55.989: INFO: Pod "pod-exec-websocket-0d03227e-f66d-4bb3-8509-0179136f3502" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:56:56.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3198" for this suite. 08/11/23 14:56:56.073
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:56:56.078
Aug 11 14:56:56.079: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:56:56.079
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:56.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:56.092
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 11 14:56:56.107: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:57:56.147: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:57:56.149
Aug 11 14:57:56.149: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:57:56.15
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:56.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:56.164
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 11 14:57:56.178: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 11 14:57:56.181: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 11 14:57:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:57:56.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4646" for this suite. 08/11/23 14:57:56.234
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1095" for this suite. 08/11/23 14:57:56.239
------------------------------
• [SLOW TEST] [60.166 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:56:56.078
    Aug 11 14:56:56.079: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:56:56.079
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:56.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:56.092
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 11 14:56:56.107: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:57:56.147: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:57:56.149
    Aug 11 14:57:56.149: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:57:56.15
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:56.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:56.164
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 11 14:57:56.178: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 11 14:57:56.181: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:57:56.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:57:56.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4646" for this suite. 08/11/23 14:57:56.234
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1095" for this suite. 08/11/23 14:57:56.239
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:57:56.245
Aug 11 14:57:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:57:56.246
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:56.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:56.259
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:57:56.272
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:57:56.498
STEP: Deploying the webhook pod 08/11/23 14:57:56.506
STEP: Wait for the deployment to be ready 08/11/23 14:57:56.518
Aug 11 14:57:56.524: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/11/23 14:57:58.532
STEP: Verifying the service has paired with the endpoint 08/11/23 14:57:58.543
Aug 11 14:57:59.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/11/23 14:57:59.546
STEP: create a namespace for the webhook 08/11/23 14:57:59.572
STEP: create a configmap should be unconditionally rejected by the webhook 08/11/23 14:57:59.578
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:57:59.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6362" for this suite. 08/11/23 14:57:59.651
STEP: Destroying namespace "webhook-6362-markers" for this suite. 08/11/23 14:57:59.66
------------------------------
• [3.422 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:57:56.245
    Aug 11 14:57:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:57:56.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:56.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:56.259
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:57:56.272
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:57:56.498
    STEP: Deploying the webhook pod 08/11/23 14:57:56.506
    STEP: Wait for the deployment to be ready 08/11/23 14:57:56.518
    Aug 11 14:57:56.524: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/11/23 14:57:58.532
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:57:58.543
    Aug 11 14:57:59.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/11/23 14:57:59.546
    STEP: create a namespace for the webhook 08/11/23 14:57:59.572
    STEP: create a configmap should be unconditionally rejected by the webhook 08/11/23 14:57:59.578
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:57:59.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6362" for this suite. 08/11/23 14:57:59.651
    STEP: Destroying namespace "webhook-6362-markers" for this suite. 08/11/23 14:57:59.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:57:59.667
Aug 11 14:57:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:57:59.668
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:59.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:59.686
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 11 14:57:59.694: INFO: Endpoints addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
Aug 11 14:57:59.694: INFO: EndpointSlices addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 11 14:57:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3918" for this suite. 08/11/23 14:57:59.698
------------------------------
• [0.036 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:57:59.667
    Aug 11 14:57:59.667: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:57:59.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:59.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:59.686
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 11 14:57:59.694: INFO: Endpoints addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
    Aug 11 14:57:59.694: INFO: EndpointSlices addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:57:59.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3918" for this suite. 08/11/23 14:57:59.698
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:57:59.703
Aug 11 14:57:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:57:59.704
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:59.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:59.718
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-003a1184-0978-446b-a6f6-734c2d07451a 08/11/23 14:57:59.723
STEP: Creating secret with name s-test-opt-upd-7d8d2b37-34ab-4b6c-8b32-8f2c8ddf217f 08/11/23 14:57:59.729
STEP: Creating the pod 08/11/23 14:57:59.733
Aug 11 14:57:59.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9" in namespace "projected-2305" to be "running and ready"
Aug 11 14:57:59.745: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671554ms
Aug 11 14:57:59.745: INFO: The phase of Pod pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:58:01.749: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008220543s
Aug 11 14:58:01.749: INFO: The phase of Pod pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9 is Running (Ready = true)
Aug 11 14:58:01.749: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-003a1184-0978-446b-a6f6-734c2d07451a 08/11/23 14:58:01.774
STEP: Updating secret s-test-opt-upd-7d8d2b37-34ab-4b6c-8b32-8f2c8ddf217f 08/11/23 14:58:01.779
STEP: Creating secret with name s-test-opt-create-f2593302-fb79-4297-8ca8-9b60cc8f593a 08/11/23 14:58:01.783
STEP: waiting to observe update in volume 08/11/23 14:58:01.786
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:03.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2305" for this suite. 08/11/23 14:58:03.822
------------------------------
• [4.125 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:57:59.703
    Aug 11 14:57:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:57:59.704
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:59.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:59.718
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-003a1184-0978-446b-a6f6-734c2d07451a 08/11/23 14:57:59.723
    STEP: Creating secret with name s-test-opt-upd-7d8d2b37-34ab-4b6c-8b32-8f2c8ddf217f 08/11/23 14:57:59.729
    STEP: Creating the pod 08/11/23 14:57:59.733
    Aug 11 14:57:59.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9" in namespace "projected-2305" to be "running and ready"
    Aug 11 14:57:59.745: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.671554ms
    Aug 11 14:57:59.745: INFO: The phase of Pod pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:58:01.749: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008220543s
    Aug 11 14:58:01.749: INFO: The phase of Pod pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9 is Running (Ready = true)
    Aug 11 14:58:01.749: INFO: Pod "pod-projected-secrets-1b44690f-53ca-4876-b305-e208f3350ce9" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-003a1184-0978-446b-a6f6-734c2d07451a 08/11/23 14:58:01.774
    STEP: Updating secret s-test-opt-upd-7d8d2b37-34ab-4b6c-8b32-8f2c8ddf217f 08/11/23 14:58:01.779
    STEP: Creating secret with name s-test-opt-create-f2593302-fb79-4297-8ca8-9b60cc8f593a 08/11/23 14:58:01.783
    STEP: waiting to observe update in volume 08/11/23 14:58:01.786
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:03.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2305" for this suite. 08/11/23 14:58:03.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:03.829
Aug 11 14:58:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename tables 08/11/23 14:58:03.83
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:03.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:03.843
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-9320" for this suite. 08/11/23 14:58:03.85
------------------------------
• [0.026 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:03.829
    Aug 11 14:58:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename tables 08/11/23 14:58:03.83
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:03.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:03.843
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:03.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-9320" for this suite. 08/11/23 14:58:03.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:03.855
Aug 11 14:58:03.855: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-webhook 08/11/23 14:58:03.856
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:03.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:03.869
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/11/23 14:58:03.871
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 14:58:04.034
STEP: Deploying the custom resource conversion webhook pod 08/11/23 14:58:04.039
STEP: Wait for the deployment to be ready 08/11/23 14:58:04.049
Aug 11 14:58:04.055: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Aug 11 14:58:06.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/11/23 14:58:08.066
STEP: Verifying the service has paired with the endpoint 08/11/23 14:58:08.078
Aug 11 14:58:09.079: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 11 14:58:09.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Creating a v1 custom resource 08/11/23 14:58:11.669
STEP: v2 custom resource should be converted 08/11/23 14:58:11.674
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4155" for this suite. 08/11/23 14:58:12.232
------------------------------
• [SLOW TEST] [8.385 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:03.855
    Aug 11 14:58:03.855: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-webhook 08/11/23 14:58:03.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:03.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:03.869
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/11/23 14:58:03.871
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 14:58:04.034
    STEP: Deploying the custom resource conversion webhook pod 08/11/23 14:58:04.039
    STEP: Wait for the deployment to be ready 08/11/23 14:58:04.049
    Aug 11 14:58:04.055: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Aug 11 14:58:06.063: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/11/23 14:58:08.066
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:58:08.078
    Aug 11 14:58:09.079: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 11 14:58:09.082: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Creating a v1 custom resource 08/11/23 14:58:11.669
    STEP: v2 custom resource should be converted 08/11/23 14:58:11.674
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4155" for this suite. 08/11/23 14:58:12.232
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:12.241
Aug 11 14:58:12.241: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 14:58:12.241
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:12.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:12.26
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-1e0a3137-23f8-4bef-b6e3-a53fc994024f 08/11/23 14:58:12.263
STEP: Creating a pod to test consume secrets 08/11/23 14:58:12.269
Aug 11 14:58:12.276: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b" in namespace "projected-6954" to be "Succeeded or Failed"
Aug 11 14:58:12.278: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.495072ms
Aug 11 14:58:14.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006181502s
Aug 11 14:58:16.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006196018s
STEP: Saw pod success 08/11/23 14:58:16.282
Aug 11 14:58:16.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b" satisfied condition "Succeeded or Failed"
Aug 11 14:58:16.284: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:58:16.292
Aug 11 14:58:16.301: INFO: Waiting for pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b to disappear
Aug 11 14:58:16.304: INFO: Pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:16.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6954" for this suite. 08/11/23 14:58:16.307
------------------------------
• [4.073 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:12.241
    Aug 11 14:58:12.241: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 14:58:12.241
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:12.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:12.26
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-1e0a3137-23f8-4bef-b6e3-a53fc994024f 08/11/23 14:58:12.263
    STEP: Creating a pod to test consume secrets 08/11/23 14:58:12.269
    Aug 11 14:58:12.276: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b" in namespace "projected-6954" to be "Succeeded or Failed"
    Aug 11 14:58:12.278: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.495072ms
    Aug 11 14:58:14.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006181502s
    Aug 11 14:58:16.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006196018s
    STEP: Saw pod success 08/11/23 14:58:16.282
    Aug 11 14:58:16.282: INFO: Pod "pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b" satisfied condition "Succeeded or Failed"
    Aug 11 14:58:16.284: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:58:16.292
    Aug 11 14:58:16.301: INFO: Waiting for pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b to disappear
    Aug 11 14:58:16.304: INFO: Pod pod-projected-secrets-c0831e6c-2d37-4d02-a9cd-a1f1492b955b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:16.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6954" for this suite. 08/11/23 14:58:16.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:16.314
Aug 11 14:58:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:16.315
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:16.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:16.328
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:58:16.331
Aug 11 14:58:16.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1350 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 11 14:58:16.391: INFO: stderr: ""
Aug 11 14:58:16.391: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:58:16.391
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 11 14:58:16.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1350 delete pods e2e-test-httpd-pod'
Aug 11 14:58:18.323: INFO: stderr: ""
Aug 11 14:58:18.323: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:18.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1350" for this suite. 08/11/23 14:58:18.326
------------------------------
• [2.019 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:16.314
    Aug 11 14:58:16.315: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:16.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:16.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:16.328
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/11/23 14:58:16.331
    Aug 11 14:58:16.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1350 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 11 14:58:16.391: INFO: stderr: ""
    Aug 11 14:58:16.391: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:58:16.391
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 11 14:58:16.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1350 delete pods e2e-test-httpd-pod'
    Aug 11 14:58:18.323: INFO: stderr: ""
    Aug 11 14:58:18.323: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:18.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1350" for this suite. 08/11/23 14:58:18.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:18.334
Aug 11 14:58:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:58:18.335
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:18.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:18.348
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/11/23 14:58:18.35
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/11/23 14:58:18.351
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:58:18.351
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/11/23 14:58:18.351
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/11/23 14:58:18.352
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:58:18.352
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:58:18.353
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:18.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1351" for this suite. 08/11/23 14:58:18.356
------------------------------
• [0.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:18.334
    Aug 11 14:58:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:58:18.335
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:18.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:18.348
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/11/23 14:58:18.35
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/11/23 14:58:18.351
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:58:18.351
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/11/23 14:58:18.351
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/11/23 14:58:18.352
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:58:18.352
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:58:18.353
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:18.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1351" for this suite. 08/11/23 14:58:18.356
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:18.362
Aug 11 14:58:18.362: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 14:58:18.363
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:18.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:18.375
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-051a691e-63c1-40fc-9e82-730b40416d55 08/11/23 14:58:18.377
STEP: Creating a pod to test consume secrets 08/11/23 14:58:18.382
Aug 11 14:58:18.389: INFO: Waiting up to 5m0s for pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b" in namespace "secrets-8036" to be "Succeeded or Failed"
Aug 11 14:58:18.391: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034942ms
Aug 11 14:58:20.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005716651s
Aug 11 14:58:22.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005317806s
STEP: Saw pod success 08/11/23 14:58:22.394
Aug 11 14:58:22.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b" satisfied condition "Succeeded or Failed"
Aug 11 14:58:22.396: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:58:22.41
Aug 11 14:58:22.423: INFO: Waiting for pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b to disappear
Aug 11 14:58:22.425: INFO: Pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:22.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8036" for this suite. 08/11/23 14:58:22.428
------------------------------
• [4.072 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:18.362
    Aug 11 14:58:18.362: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 14:58:18.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:18.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:18.375
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-051a691e-63c1-40fc-9e82-730b40416d55 08/11/23 14:58:18.377
    STEP: Creating a pod to test consume secrets 08/11/23 14:58:18.382
    Aug 11 14:58:18.389: INFO: Waiting up to 5m0s for pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b" in namespace "secrets-8036" to be "Succeeded or Failed"
    Aug 11 14:58:18.391: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034942ms
    Aug 11 14:58:20.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005716651s
    Aug 11 14:58:22.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005317806s
    STEP: Saw pod success 08/11/23 14:58:22.394
    Aug 11 14:58:22.394: INFO: Pod "pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b" satisfied condition "Succeeded or Failed"
    Aug 11 14:58:22.396: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:58:22.41
    Aug 11 14:58:22.423: INFO: Waiting for pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b to disappear
    Aug 11 14:58:22.425: INFO: Pod pod-secrets-6ced9045-8152-4c7e-9d2d-146dbae00a6b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:22.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8036" for this suite. 08/11/23 14:58:22.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:22.436
Aug 11 14:58:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 14:58:22.436
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:22.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:22.451
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:58:22.453
Aug 11 14:58:22.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b" in namespace "downward-api-8239" to be "Succeeded or Failed"
Aug 11 14:58:22.463: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173032ms
Aug 11 14:58:24.466: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005789331s
Aug 11 14:58:26.467: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006473067s
STEP: Saw pod success 08/11/23 14:58:26.467
Aug 11 14:58:26.467: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b" satisfied condition "Succeeded or Failed"
Aug 11 14:58:26.469: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b container client-container: <nil>
STEP: delete the pod 08/11/23 14:58:26.477
Aug 11 14:58:26.488: INFO: Waiting for pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b to disappear
Aug 11 14:58:26.490: INFO: Pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:26.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8239" for this suite. 08/11/23 14:58:26.496
------------------------------
• [4.065 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:22.436
    Aug 11 14:58:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:58:22.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:22.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:22.451
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:58:22.453
    Aug 11 14:58:22.461: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b" in namespace "downward-api-8239" to be "Succeeded or Failed"
    Aug 11 14:58:22.463: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173032ms
    Aug 11 14:58:24.466: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005789331s
    Aug 11 14:58:26.467: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006473067s
    STEP: Saw pod success 08/11/23 14:58:26.467
    Aug 11 14:58:26.467: INFO: Pod "downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b" satisfied condition "Succeeded or Failed"
    Aug 11 14:58:26.469: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b container client-container: <nil>
    STEP: delete the pod 08/11/23 14:58:26.477
    Aug 11 14:58:26.488: INFO: Waiting for pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b to disappear
    Aug 11 14:58:26.490: INFO: Pod downwardapi-volume-8799f394-e8a1-4529-8d5c-85dc3f0aeb1b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:26.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8239" for this suite. 08/11/23 14:58:26.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:26.501
Aug 11 14:58:26.501: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 14:58:26.502
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:26.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:26.515
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 14:58:26.528
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:58:26.841
STEP: Deploying the webhook pod 08/11/23 14:58:26.849
STEP: Wait for the deployment to be ready 08/11/23 14:58:26.861
Aug 11 14:58:26.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:58:28.876
STEP: Verifying the service has paired with the endpoint 08/11/23 14:58:28.888
Aug 11 14:58:29.889: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/11/23 14:58:29.892
STEP: create a pod that should be updated by the webhook 08/11/23 14:58:29.916
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 14:58:29.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-391" for this suite. 08/11/23 14:58:29.988
STEP: Destroying namespace "webhook-391-markers" for this suite. 08/11/23 14:58:29.997
------------------------------
• [3.503 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:26.501
    Aug 11 14:58:26.501: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 14:58:26.502
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:26.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:26.515
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 14:58:26.528
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:58:26.841
    STEP: Deploying the webhook pod 08/11/23 14:58:26.849
    STEP: Wait for the deployment to be ready 08/11/23 14:58:26.861
    Aug 11 14:58:26.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:58:28.876
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:58:28.888
    Aug 11 14:58:29.889: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/11/23 14:58:29.892
    STEP: create a pod that should be updated by the webhook 08/11/23 14:58:29.916
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 14:58:29.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-391" for this suite. 08/11/23 14:58:29.988
    STEP: Destroying namespace "webhook-391-markers" for this suite. 08/11/23 14:58:29.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 14:58:30.005
Aug 11 14:58:30.005: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-pred 08/11/23 14:58:30.006
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:30.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:30.023
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 11 14:58:30.025: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 14:58:30.036: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 14:58:30.038: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
Aug 11 14:58:30.048: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:58:30.048: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container coredns ready: true, restart count 0
Aug 11 14:58:30.048: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:58:30.048: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:58:30.048: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:58:30.048: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:58:30.048: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:58:30.048: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:58:30.048: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 14:58:30.048: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container e2e ready: true, restart count 0
Aug 11 14:58:30.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:58:30.048: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:58:30.048: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:58:30.048: INFO: pod-service-account-defaultsa from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container token-test ready: false, restart count 0
Aug 11 14:58:30.048: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.048: INFO: 	Container token-test ready: false, restart count 0
Aug 11 14:58:30.048: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
Aug 11 14:58:30.057: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 14:58:30.057: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:58:30.057: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:58:30.057: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:58:30.057: INFO: gcp-guest-agent-rscx6 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:58:30.057: INFO: konnectivity-agent-5vq55 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:58:30.057: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:58:30.057: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:58:30.057: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:58:30.057: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:58:30.057: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
Aug 11 14:58:30.057: INFO: pod-service-account-mountsa from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
Aug 11 14:58:30.057: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
Aug 11 14:58:30.057: INFO: webhook-to-be-mutated from webhook-391 started at 2023-08-11 14:58:30 +0000 UTC (1 container statuses recorded)
Aug 11 14:58:30.057: INFO: 	Container example ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:58:30.057
Aug 11 14:58:30.064: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8431" to be "running"
Aug 11 14:58:30.066: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061672ms
Aug 11 14:58:32.071: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006655442s
Aug 11 14:58:32.071: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:58:32.074
STEP: Trying to apply a random label on the found node. 08/11/23 14:58:32.089
STEP: verifying the node has the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 95 08/11/23 14:58:32.101
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/11/23 14:58:32.103
Aug 11 14:58:32.111: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8431" to be "not pending"
Aug 11 14:58:32.116: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409715ms
Aug 11 14:58:34.120: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.009548595s
Aug 11 14:58:34.120: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.178.3 on the node which pod4 resides and expect not scheduled 08/11/23 14:58:34.12
Aug 11 14:58:34.126: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8431" to be "not pending"
Aug 11 14:58:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358152ms
Aug 11 14:58:36.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005633341s
Aug 11 14:58:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006874607s
Aug 11 14:58:40.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005795844s
Aug 11 14:58:42.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006269051s
Aug 11 14:58:44.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005361715s
Aug 11 14:58:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006567981s
Aug 11 14:58:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006788527s
Aug 11 14:58:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006990253s
Aug 11 14:58:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006776079s
Aug 11 14:58:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006885995s
Aug 11 14:58:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005920671s
Aug 11 14:58:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006669298s
Aug 11 14:59:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006920032s
Aug 11 14:59:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006388258s
Aug 11 14:59:04.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007844313s
Aug 11 14:59:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0061267s
Aug 11 14:59:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007084884s
Aug 11 14:59:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006813912s
Aug 11 14:59:12.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006561937s
Aug 11 14:59:14.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006878653s
Aug 11 14:59:16.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005534468s
Aug 11 14:59:18.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005550872s
Aug 11 14:59:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00719932s
Aug 11 14:59:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006294505s
Aug 11 14:59:24.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007306231s
Aug 11 14:59:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006812406s
Aug 11 14:59:28.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008211523s
Aug 11 14:59:30.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006975638s
Aug 11 14:59:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006144012s
Aug 11 14:59:34.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009886572s
Aug 11 14:59:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006597754s
Aug 11 14:59:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006852679s
Aug 11 14:59:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006116805s
Aug 11 14:59:42.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006517021s
Aug 11 14:59:44.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005292864s
Aug 11 14:59:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006044341s
Aug 11 14:59:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006354199s
Aug 11 14:59:50.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009486986s
Aug 11 14:59:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006237538s
Aug 11 14:59:54.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007372975s
Aug 11 14:59:56.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00672717s
Aug 11 14:59:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006959845s
Aug 11 15:00:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006369611s
Aug 11 15:00:02.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007160546s
Aug 11 15:00:04.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006069321s
Aug 11 15:00:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006290247s
Aug 11 15:00:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007912105s
Aug 11 15:00:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006235279s
Aug 11 15:00:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007494545s
Aug 11 15:00:14.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007490891s
Aug 11 15:00:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006378477s
Aug 11 15:00:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007302742s
Aug 11 15:00:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007158859s
Aug 11 15:00:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006506924s
Aug 11 15:00:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006930228s
Aug 11 15:00:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006330485s
Aug 11 15:00:28.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011559636s
Aug 11 15:00:30.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005395566s
Aug 11 15:00:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006435202s
Aug 11 15:00:34.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007738549s
Aug 11 15:00:36.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005687764s
Aug 11 15:00:38.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005621539s
Aug 11 15:00:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006276046s
Aug 11 15:00:42.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005211941s
Aug 11 15:00:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006335708s
Aug 11 15:00:46.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005840233s
Aug 11 15:00:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006561117s
Aug 11 15:00:50.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005959785s
Aug 11 15:00:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006840969s
Aug 11 15:00:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006118036s
Aug 11 15:00:56.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006708932s
Aug 11 15:00:58.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005699995s
Aug 11 15:01:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006769764s
Aug 11 15:01:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006778299s
Aug 11 15:01:04.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007018906s
Aug 11 15:01:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00624459s
Aug 11 15:01:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007097658s
Aug 11 15:01:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006475681s
Aug 11 15:01:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007158248s
Aug 11 15:01:14.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007249365s
Aug 11 15:01:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00641956s
Aug 11 15:01:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007470776s
Aug 11 15:01:20.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006809841s
Aug 11 15:01:22.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005647145s
Aug 11 15:01:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006274232s
Aug 11 15:01:26.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005700845s
Aug 11 15:01:28.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005560771s
Aug 11 15:01:30.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005369767s
Aug 11 15:01:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006959385s
Aug 11 15:01:34.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004980258s
Aug 11 15:01:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006138096s
Aug 11 15:01:38.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007105502s
Aug 11 15:01:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.006583587s
Aug 11 15:01:42.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007795003s
Aug 11 15:01:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006488787s
Aug 11 15:01:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006599374s
Aug 11 15:01:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00675947s
Aug 11 15:01:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006622345s
Aug 11 15:01:52.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007085502s
Aug 11 15:01:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006688005s
Aug 11 15:01:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005496141s
Aug 11 15:01:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006149398s
Aug 11 15:02:00.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007785435s
Aug 11 15:02:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00623238s
Aug 11 15:02:04.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007466027s
Aug 11 15:02:06.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005625221s
Aug 11 15:02:08.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006710947s
Aug 11 15:02:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006521023s
Aug 11 15:02:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007052259s
Aug 11 15:02:14.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005868273s
Aug 11 15:02:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.0065419s
Aug 11 15:02:18.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006270976s
Aug 11 15:02:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007120643s
Aug 11 15:02:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006710198s
Aug 11 15:02:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006642323s
Aug 11 15:02:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006223798s
Aug 11 15:02:28.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007717604s
Aug 11 15:02:30.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006909008s
Aug 11 15:02:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006744994s
Aug 11 15:02:34.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00745434s
Aug 11 15:02:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006739155s
Aug 11 15:02:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00655824s
Aug 11 15:02:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006341346s
Aug 11 15:02:42.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007090322s
Aug 11 15:02:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006703217s
Aug 11 15:02:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006052112s
Aug 11 15:02:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007029348s
Aug 11 15:02:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006662444s
Aug 11 15:02:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.006604068s
Aug 11 15:02:54.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005480923s
Aug 11 15:02:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005399288s
Aug 11 15:02:58.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005534124s
Aug 11 15:03:00.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007059111s
Aug 11 15:03:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006271026s
Aug 11 15:03:04.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008162192s
Aug 11 15:03:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006739608s
Aug 11 15:03:08.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005683723s
Aug 11 15:03:10.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005518328s
Aug 11 15:03:12.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006591115s
Aug 11 15:03:14.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00630761s
Aug 11 15:03:16.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006013815s
Aug 11 15:03:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007397662s
Aug 11 15:03:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007305328s
Aug 11 15:03:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006965832s
Aug 11 15:03:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006980099s
Aug 11 15:03:26.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005987914s
Aug 11 15:03:28.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005271558s
Aug 11 15:03:30.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007260366s
Aug 11 15:03:32.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.0058439s
Aug 11 15:03:34.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006411147s
Aug 11 15:03:34.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008540439s
STEP: removing the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 15:03:34.135
STEP: verifying the node doesn't have the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 08/11/23 15:03:34.145
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:03:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8431" for this suite. 08/11/23 15:03:34.151
------------------------------
• [SLOW TEST] [304.154 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 14:58:30.005
    Aug 11 14:58:30.005: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-pred 08/11/23 14:58:30.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:30.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:30.023
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 11 14:58:30.025: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 14:58:30.036: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 14:58:30.038: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
    Aug 11 14:58:30.048: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:58:30.048: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:58:30.048: INFO: pod-service-account-defaultsa from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container token-test ready: false, restart count 0
    Aug 11 14:58:30.048: INFO: pod-service-account-mountsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.048: INFO: 	Container token-test ready: false, restart count 0
    Aug 11 14:58:30.048: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
    Aug 11 14:58:30.057: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:58:30.057: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: gcp-guest-agent-rscx6 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: konnectivity-agent-5vq55 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:58:30.057: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
    Aug 11 14:58:30.057: INFO: pod-service-account-mountsa from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
    Aug 11 14:58:30.057: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-7445 started at 2023-08-11 14:56:37 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container token-test ready: false, restart count 0
    Aug 11 14:58:30.057: INFO: webhook-to-be-mutated from webhook-391 started at 2023-08-11 14:58:30 +0000 UTC (1 container statuses recorded)
    Aug 11 14:58:30.057: INFO: 	Container example ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:58:30.057
    Aug 11 14:58:30.064: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8431" to be "running"
    Aug 11 14:58:30.066: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061672ms
    Aug 11 14:58:32.071: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006655442s
    Aug 11 14:58:32.071: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:58:32.074
    STEP: Trying to apply a random label on the found node. 08/11/23 14:58:32.089
    STEP: verifying the node has the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 95 08/11/23 14:58:32.101
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/11/23 14:58:32.103
    Aug 11 14:58:32.111: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8431" to be "not pending"
    Aug 11 14:58:32.116: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.409715ms
    Aug 11 14:58:34.120: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.009548595s
    Aug 11 14:58:34.120: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.178.3 on the node which pod4 resides and expect not scheduled 08/11/23 14:58:34.12
    Aug 11 14:58:34.126: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8431" to be "not pending"
    Aug 11 14:58:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358152ms
    Aug 11 14:58:36.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005633341s
    Aug 11 14:58:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006874607s
    Aug 11 14:58:40.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005795844s
    Aug 11 14:58:42.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006269051s
    Aug 11 14:58:44.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005361715s
    Aug 11 14:58:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006567981s
    Aug 11 14:58:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006788527s
    Aug 11 14:58:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006990253s
    Aug 11 14:58:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006776079s
    Aug 11 14:58:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006885995s
    Aug 11 14:58:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005920671s
    Aug 11 14:58:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006669298s
    Aug 11 14:59:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006920032s
    Aug 11 14:59:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006388258s
    Aug 11 14:59:04.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007844313s
    Aug 11 14:59:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.0061267s
    Aug 11 14:59:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007084884s
    Aug 11 14:59:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006813912s
    Aug 11 14:59:12.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006561937s
    Aug 11 14:59:14.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006878653s
    Aug 11 14:59:16.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005534468s
    Aug 11 14:59:18.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005550872s
    Aug 11 14:59:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.00719932s
    Aug 11 14:59:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006294505s
    Aug 11 14:59:24.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007306231s
    Aug 11 14:59:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006812406s
    Aug 11 14:59:28.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008211523s
    Aug 11 14:59:30.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006975638s
    Aug 11 14:59:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006144012s
    Aug 11 14:59:34.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009886572s
    Aug 11 14:59:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006597754s
    Aug 11 14:59:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006852679s
    Aug 11 14:59:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006116805s
    Aug 11 14:59:42.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006517021s
    Aug 11 14:59:44.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005292864s
    Aug 11 14:59:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006044341s
    Aug 11 14:59:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006354199s
    Aug 11 14:59:50.136: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.009486986s
    Aug 11 14:59:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006237538s
    Aug 11 14:59:54.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007372975s
    Aug 11 14:59:56.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00672717s
    Aug 11 14:59:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006959845s
    Aug 11 15:00:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006369611s
    Aug 11 15:00:02.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007160546s
    Aug 11 15:00:04.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006069321s
    Aug 11 15:00:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006290247s
    Aug 11 15:00:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007912105s
    Aug 11 15:00:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006235279s
    Aug 11 15:00:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.007494545s
    Aug 11 15:00:14.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007490891s
    Aug 11 15:00:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006378477s
    Aug 11 15:00:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007302742s
    Aug 11 15:00:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007158859s
    Aug 11 15:00:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006506924s
    Aug 11 15:00:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006930228s
    Aug 11 15:00:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006330485s
    Aug 11 15:00:28.138: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011559636s
    Aug 11 15:00:30.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005395566s
    Aug 11 15:00:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006435202s
    Aug 11 15:00:34.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007738549s
    Aug 11 15:00:36.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005687764s
    Aug 11 15:00:38.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005621539s
    Aug 11 15:00:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006276046s
    Aug 11 15:00:42.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005211941s
    Aug 11 15:00:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006335708s
    Aug 11 15:00:46.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005840233s
    Aug 11 15:00:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006561117s
    Aug 11 15:00:50.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005959785s
    Aug 11 15:00:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006840969s
    Aug 11 15:00:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006118036s
    Aug 11 15:00:56.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006708932s
    Aug 11 15:00:58.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005699995s
    Aug 11 15:01:00.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006769764s
    Aug 11 15:01:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006778299s
    Aug 11 15:01:04.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007018906s
    Aug 11 15:01:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.00624459s
    Aug 11 15:01:08.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007097658s
    Aug 11 15:01:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006475681s
    Aug 11 15:01:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.007158248s
    Aug 11 15:01:14.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.007249365s
    Aug 11 15:01:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00641956s
    Aug 11 15:01:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007470776s
    Aug 11 15:01:20.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.006809841s
    Aug 11 15:01:22.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005647145s
    Aug 11 15:01:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006274232s
    Aug 11 15:01:26.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005700845s
    Aug 11 15:01:28.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005560771s
    Aug 11 15:01:30.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005369767s
    Aug 11 15:01:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006959385s
    Aug 11 15:01:34.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004980258s
    Aug 11 15:01:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006138096s
    Aug 11 15:01:38.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007105502s
    Aug 11 15:01:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.006583587s
    Aug 11 15:01:42.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007795003s
    Aug 11 15:01:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006488787s
    Aug 11 15:01:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.006599374s
    Aug 11 15:01:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00675947s
    Aug 11 15:01:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006622345s
    Aug 11 15:01:52.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007085502s
    Aug 11 15:01:54.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006688005s
    Aug 11 15:01:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005496141s
    Aug 11 15:01:58.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.006149398s
    Aug 11 15:02:00.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007785435s
    Aug 11 15:02:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00623238s
    Aug 11 15:02:04.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007466027s
    Aug 11 15:02:06.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005625221s
    Aug 11 15:02:08.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006710947s
    Aug 11 15:02:10.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006521023s
    Aug 11 15:02:12.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007052259s
    Aug 11 15:02:14.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005868273s
    Aug 11 15:02:16.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.0065419s
    Aug 11 15:02:18.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006270976s
    Aug 11 15:02:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007120643s
    Aug 11 15:02:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006710198s
    Aug 11 15:02:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.006642323s
    Aug 11 15:02:26.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.006223798s
    Aug 11 15:02:28.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007717604s
    Aug 11 15:02:30.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006909008s
    Aug 11 15:02:32.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006744994s
    Aug 11 15:02:34.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.00745434s
    Aug 11 15:02:36.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006739155s
    Aug 11 15:02:38.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.00655824s
    Aug 11 15:02:40.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006341346s
    Aug 11 15:02:42.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007090322s
    Aug 11 15:02:44.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006703217s
    Aug 11 15:02:46.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006052112s
    Aug 11 15:02:48.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007029348s
    Aug 11 15:02:50.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.006662444s
    Aug 11 15:02:52.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.006604068s
    Aug 11 15:02:54.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.005480923s
    Aug 11 15:02:56.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005399288s
    Aug 11 15:02:58.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.005534124s
    Aug 11 15:03:00.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007059111s
    Aug 11 15:03:02.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006271026s
    Aug 11 15:03:04.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008162192s
    Aug 11 15:03:06.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006739608s
    Aug 11 15:03:08.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005683723s
    Aug 11 15:03:10.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005518328s
    Aug 11 15:03:12.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006591115s
    Aug 11 15:03:14.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00630761s
    Aug 11 15:03:16.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006013815s
    Aug 11 15:03:18.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007397662s
    Aug 11 15:03:20.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007305328s
    Aug 11 15:03:22.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006965832s
    Aug 11 15:03:24.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006980099s
    Aug 11 15:03:26.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005987914s
    Aug 11 15:03:28.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005271558s
    Aug 11 15:03:30.134: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007260366s
    Aug 11 15:03:32.132: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.0058439s
    Aug 11 15:03:34.133: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006411147s
    Aug 11 15:03:34.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008540439s
    STEP: removing the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 15:03:34.135
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-31af19a9-0886-4254-bf89-8779c02120a6 08/11/23 15:03:34.145
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:03:34.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8431" for this suite. 08/11/23 15:03:34.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:03:34.159
Aug 11 15:03:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 15:03:34.16
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:34.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:34.175
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:03:34.177
Aug 11 15:03:34.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01" in namespace "projected-5374" to be "Succeeded or Failed"
Aug 11 15:03:34.187: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121022ms
Aug 11 15:03:36.191: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Running", Reason="", readiness=false. Elapsed: 2.00543398s
Aug 11 15:03:38.192: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006475037s
STEP: Saw pod success 08/11/23 15:03:38.192
Aug 11 15:03:38.192: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01" satisfied condition "Succeeded or Failed"
Aug 11 15:03:38.194: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 container client-container: <nil>
STEP: delete the pod 08/11/23 15:03:38.216
Aug 11 15:03:38.226: INFO: Waiting for pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 to disappear
Aug 11 15:03:38.234: INFO: Pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 15:03:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5374" for this suite. 08/11/23 15:03:38.241
------------------------------
• [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:03:34.159
    Aug 11 15:03:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 15:03:34.16
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:34.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:34.175
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:03:34.177
    Aug 11 15:03:34.185: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01" in namespace "projected-5374" to be "Succeeded or Failed"
    Aug 11 15:03:34.187: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121022ms
    Aug 11 15:03:36.191: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Running", Reason="", readiness=false. Elapsed: 2.00543398s
    Aug 11 15:03:38.192: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006475037s
    STEP: Saw pod success 08/11/23 15:03:38.192
    Aug 11 15:03:38.192: INFO: Pod "downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01" satisfied condition "Succeeded or Failed"
    Aug 11 15:03:38.194: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:03:38.216
    Aug 11 15:03:38.226: INFO: Waiting for pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 to disappear
    Aug 11 15:03:38.234: INFO: Pod downwardapi-volume-51086ab2-5dfc-499c-8d40-9fecff553a01 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:03:38.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5374" for this suite. 08/11/23 15:03:38.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:03:38.254
Aug 11 15:03:38.254: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 15:03:38.255
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:38.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:38.283
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 15:03:38.295
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:03:38.556
STEP: Deploying the webhook pod 08/11/23 15:03:38.563
STEP: Wait for the deployment to be ready 08/11/23 15:03:38.573
Aug 11 15:03:38.580: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/11/23 15:03:40.59
STEP: Verifying the service has paired with the endpoint 08/11/23 15:03:40.607
Aug 11 15:03:41.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:03:41.611
STEP: create a pod 08/11/23 15:03:41.64
Aug 11 15:03:41.645: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8060" to be "running"
Aug 11 15:03:41.647: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252842ms
Aug 11 15:03:43.651: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006325292s
Aug 11 15:03:43.651: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/11/23 15:03:43.651
Aug 11 15:03:43.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=webhook-8060 attach --namespace=webhook-8060 to-be-attached-pod -i -c=container1'
Aug 11 15:03:43.726: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:03:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8060" for this suite. 08/11/23 15:03:43.776
STEP: Destroying namespace "webhook-8060-markers" for this suite. 08/11/23 15:03:43.785
------------------------------
• [SLOW TEST] [5.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:03:38.254
    Aug 11 15:03:38.254: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 15:03:38.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:38.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:38.283
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 15:03:38.295
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:03:38.556
    STEP: Deploying the webhook pod 08/11/23 15:03:38.563
    STEP: Wait for the deployment to be ready 08/11/23 15:03:38.573
    Aug 11 15:03:38.580: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/11/23 15:03:40.59
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:03:40.607
    Aug 11 15:03:41.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:03:41.611
    STEP: create a pod 08/11/23 15:03:41.64
    Aug 11 15:03:41.645: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8060" to be "running"
    Aug 11 15:03:41.647: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252842ms
    Aug 11 15:03:43.651: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006325292s
    Aug 11 15:03:43.651: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/11/23 15:03:43.651
    Aug 11 15:03:43.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=webhook-8060 attach --namespace=webhook-8060 to-be-attached-pod -i -c=container1'
    Aug 11 15:03:43.726: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:03:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8060" for this suite. 08/11/23 15:03:43.776
    STEP: Destroying namespace "webhook-8060-markers" for this suite. 08/11/23 15:03:43.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:03:43.803
Aug 11 15:03:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 15:03:43.804
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:43.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:43.821
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-612 08/11/23 15:03:43.823
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-612 08/11/23 15:03:43.828
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-612 08/11/23 15:03:43.836
Aug 11 15:03:43.838: INFO: Found 0 stateful pods, waiting for 1
Aug 11 15:03:53.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/11/23 15:03:53.842
Aug 11 15:03:53.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:03:53.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:03:53.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:03:53.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:03:53.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 11 15:04:03.998: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:04:03.998: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:04:04.012: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 15:04:04.012: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
Aug 11 15:04:04.012: INFO: 
Aug 11 15:04:04.012: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 11 15:04:05.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99694861s
Aug 11 15:04:06.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991547007s
Aug 11 15:04:07.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987949503s
Aug 11 15:04:08.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984512161s
Aug 11 15:04:09.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981264871s
Aug 11 15:04:10.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97776393s
Aug 11 15:04:11.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973358607s
Aug 11 15:04:12.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968420898s
Aug 11 15:04:13.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.329444ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-612 08/11/23 15:04:14.048
Aug 11 15:04:14.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:04:14.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:04:14.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:04:14.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:04:14.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:04:14.320: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 11 15:04:14.320: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:04:14.320: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:04:14.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:04:14.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 11 15:04:14.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:04:14.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:04:14.462: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:04:14.462: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:04:14.462: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/11/23 15:04:14.462
Aug 11 15:04:14.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:04:14.588: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:04:14.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:04:14.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:04:14.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:04:14.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:04:14.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:04:14.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:04:14.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:04:14.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:04:14.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:04:14.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:04:14.849: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:04:14.851: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 11 15:04:24.861: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:04:24.861: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:04:24.861: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:04:24.872: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 15:04:24.872: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
Aug 11 15:04:24.872: INFO: ss-1  constell-1cf5d931-worker-6381a7ba-mt98  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
Aug 11 15:04:24.872: INFO: ss-2  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
Aug 11 15:04:24.872: INFO: 
Aug 11 15:04:24.872: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 11 15:04:25.875: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 15:04:25.875: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
Aug 11 15:04:25.875: INFO: ss-2  constell-1cf5d931-worker-6381a7ba-nd80  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
Aug 11 15:04:25.875: INFO: 
Aug 11 15:04:25.875: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 11 15:04:26.878: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993843908s
Aug 11 15:04:27.881: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990900108s
Aug 11 15:04:28.884: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987775328s
Aug 11 15:04:29.887: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984812745s
Aug 11 15:04:30.890: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981987135s
Aug 11 15:04:31.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979165206s
Aug 11 15:04:32.895: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976308373s
Aug 11 15:04:33.899: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.612603ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-612 08/11/23 15:04:34.899
Aug 11 15:04:34.903: INFO: Scaling statefulset ss to 0
Aug 11 15:04:34.911: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 15:04:34.913: INFO: Deleting all statefulset in ns statefulset-612
Aug 11 15:04:34.915: INFO: Scaling statefulset ss to 0
Aug 11 15:04:34.922: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:04:34.924: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 15:04:34.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-612" for this suite. 08/11/23 15:04:34.939
------------------------------
• [SLOW TEST] [51.144 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:03:43.803
    Aug 11 15:03:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:03:43.804
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:03:43.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:03:43.821
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-612 08/11/23 15:03:43.823
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-612 08/11/23 15:03:43.828
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-612 08/11/23 15:03:43.836
    Aug 11 15:03:43.838: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 15:03:53.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/11/23 15:03:53.842
    Aug 11 15:03:53.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:03:53.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:03:53.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:03:53.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:03:53.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 11 15:04:03.998: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:04:03.998: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:04:04.012: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 15:04:04.012: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:54 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
    Aug 11 15:04:04.012: INFO: 
    Aug 11 15:04:04.012: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 11 15:04:05.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99694861s
    Aug 11 15:04:06.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991547007s
    Aug 11 15:04:07.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987949503s
    Aug 11 15:04:08.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984512161s
    Aug 11 15:04:09.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981264871s
    Aug 11 15:04:10.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97776393s
    Aug 11 15:04:11.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973358607s
    Aug 11 15:04:12.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968420898s
    Aug 11 15:04:13.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.329444ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-612 08/11/23 15:04:14.048
    Aug 11 15:04:14.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:04:14.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:04:14.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:04:14.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:04:14.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:04:14.320: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 11 15:04:14.320: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:04:14.320: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:04:14.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:04:14.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 11 15:04:14.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:04:14.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:04:14.462: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:04:14.462: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:04:14.462: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/11/23 15:04:14.462
    Aug 11 15:04:14.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:04:14.588: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:04:14.588: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:04:14.588: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:04:14.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:04:14.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:04:14.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:04:14.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:04:14.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=statefulset-612 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:04:14.849: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:04:14.849: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:04:14.849: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:04:14.849: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:04:14.851: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 11 15:04:24.861: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:04:24.861: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:04:24.861: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:04:24.872: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 15:04:24.872: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
    Aug 11 15:04:24.872: INFO: ss-1  constell-1cf5d931-worker-6381a7ba-mt98  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
    Aug 11 15:04:24.872: INFO: ss-2  constell-1cf5d931-worker-6381a7ba-nd80  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
    Aug 11 15:04:24.872: INFO: 
    Aug 11 15:04:24.872: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 11 15:04:25.875: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 15:04:25.875: INFO: ss-0  constell-1cf5d931-worker-6381a7ba-nd80  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:03:43 +0000 UTC  }]
    Aug 11 15:04:25.875: INFO: ss-2  constell-1cf5d931-worker-6381a7ba-nd80  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:04:04 +0000 UTC  }]
    Aug 11 15:04:25.875: INFO: 
    Aug 11 15:04:25.875: INFO: StatefulSet ss has not reached scale 0, at 2
    Aug 11 15:04:26.878: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993843908s
    Aug 11 15:04:27.881: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.990900108s
    Aug 11 15:04:28.884: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987775328s
    Aug 11 15:04:29.887: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.984812745s
    Aug 11 15:04:30.890: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.981987135s
    Aug 11 15:04:31.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979165206s
    Aug 11 15:04:32.895: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976308373s
    Aug 11 15:04:33.899: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.612603ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-612 08/11/23 15:04:34.899
    Aug 11 15:04:34.903: INFO: Scaling statefulset ss to 0
    Aug 11 15:04:34.911: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 15:04:34.913: INFO: Deleting all statefulset in ns statefulset-612
    Aug 11 15:04:34.915: INFO: Scaling statefulset ss to 0
    Aug 11 15:04:34.922: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:04:34.924: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:04:34.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-612" for this suite. 08/11/23 15:04:34.939
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:04:34.948
Aug 11 15:04:34.948: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename csistoragecapacity 08/11/23 15:04:34.949
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:34.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:34.963
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/11/23 15:04:34.966
STEP: getting /apis/storage.k8s.io 08/11/23 15:04:34.968
STEP: getting /apis/storage.k8s.io/v1 08/11/23 15:04:34.969
STEP: creating 08/11/23 15:04:34.969
STEP: watching 08/11/23 15:04:34.982
Aug 11 15:04:34.982: INFO: starting watch
STEP: getting 08/11/23 15:04:34.987
STEP: listing in namespace 08/11/23 15:04:34.989
STEP: listing across namespaces 08/11/23 15:04:34.991
STEP: patching 08/11/23 15:04:34.993
STEP: updating 08/11/23 15:04:34.997
Aug 11 15:04:35.001: INFO: waiting for watch events with expected annotations in namespace
Aug 11 15:04:35.001: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/11/23 15:04:35.001
STEP: deleting a collection 08/11/23 15:04:35.011
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 11 15:04:35.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-8048" for this suite. 08/11/23 15:04:35.027
------------------------------
• [0.084 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:04:34.948
    Aug 11 15:04:34.948: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename csistoragecapacity 08/11/23 15:04:34.949
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:34.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:34.963
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/11/23 15:04:34.966
    STEP: getting /apis/storage.k8s.io 08/11/23 15:04:34.968
    STEP: getting /apis/storage.k8s.io/v1 08/11/23 15:04:34.969
    STEP: creating 08/11/23 15:04:34.969
    STEP: watching 08/11/23 15:04:34.982
    Aug 11 15:04:34.982: INFO: starting watch
    STEP: getting 08/11/23 15:04:34.987
    STEP: listing in namespace 08/11/23 15:04:34.989
    STEP: listing across namespaces 08/11/23 15:04:34.991
    STEP: patching 08/11/23 15:04:34.993
    STEP: updating 08/11/23 15:04:34.997
    Aug 11 15:04:35.001: INFO: waiting for watch events with expected annotations in namespace
    Aug 11 15:04:35.001: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/11/23 15:04:35.001
    STEP: deleting a collection 08/11/23 15:04:35.011
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:04:35.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-8048" for this suite. 08/11/23 15:04:35.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:04:35.033
Aug 11 15:04:35.033: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename init-container 08/11/23 15:04:35.034
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:35.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:35.048
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/11/23 15:04:35.051
Aug 11 15:04:35.051: INFO: PodSpec: initContainers in spec.initContainers
Aug 11 15:05:18.145: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e4a3740e-be39-431f-a82c-a7c8575fe68f", GenerateName:"", Namespace:"init-container-565", SelfLink:"", UID:"c8102602-d410-4dbb-83db-d0d36c5da48a", ResourceVersion:"44820", Generation:0, CreationTimestamp:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"51204654"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ee4d98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ee4dc8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-66h9r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005f1be40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025c0ec8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"constell-1cf5d931-worker-6381a7ba-nd80", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000aef570), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c0f40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c0f60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025c0f68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0025c0f6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ec3500), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.178.3", PodIP:"10.10.1.247", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.1.247"}}, StartTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aef650)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aef6c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://dcf1169ff0d6d5d74b2e244c59f7ab320c9e7ebcc73687ce3ecb8d72ffe12f7f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005f1bec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005f1bea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0025c0fff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:05:18.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-565" for this suite. 08/11/23 15:05:18.149
------------------------------
• [SLOW TEST] [43.121 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:04:35.033
    Aug 11 15:04:35.033: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename init-container 08/11/23 15:04:35.034
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:35.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:35.048
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/11/23 15:04:35.051
    Aug 11 15:04:35.051: INFO: PodSpec: initContainers in spec.initContainers
    Aug 11 15:05:18.145: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e4a3740e-be39-431f-a82c-a7c8575fe68f", GenerateName:"", Namespace:"init-container-565", SelfLink:"", UID:"c8102602-d410-4dbb-83db-d0d36c5da48a", ResourceVersion:"44820", Generation:0, CreationTimestamp:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"51204654"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ee4d98), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 5, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005ee4dc8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-66h9r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005f1be40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-66h9r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025c0ec8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"constell-1cf5d931-worker-6381a7ba-nd80", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000aef570), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c0f40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0025c0f60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0025c0f68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0025c0f6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ec3500), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.178.3", PodIP:"10.10.1.247", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.1.247"}}, StartTime:time.Date(2023, time.August, 11, 15, 4, 35, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aef650)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000aef6c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://dcf1169ff0d6d5d74b2e244c59f7ab320c9e7ebcc73687ce3ecb8d72ffe12f7f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005f1bec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005f1bea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0025c0fff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:05:18.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-565" for this suite. 08/11/23 15:05:18.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:05:18.155
Aug 11 15:05:18.155: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 15:05:18.156
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:18.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:18.168
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6925 08/11/23 15:05:18.17
STEP: creating service affinity-clusterip in namespace services-6925 08/11/23 15:05:18.17
STEP: creating replication controller affinity-clusterip in namespace services-6925 08/11/23 15:05:18.182
I0811 15:05:18.189517      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6925, replica count: 3
I0811 15:05:21.240771      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:05:21.246: INFO: Creating new exec pod
Aug 11 15:05:21.253: INFO: Waiting up to 5m0s for pod "execpod-affinityxpzcb" in namespace "services-6925" to be "running"
Aug 11 15:05:21.256: INFO: Pod "execpod-affinityxpzcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459752ms
Aug 11 15:05:23.261: INFO: Pod "execpod-affinityxpzcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007451724s
Aug 11 15:05:23.261: INFO: Pod "execpod-affinityxpzcb" satisfied condition "running"
Aug 11 15:05:24.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 11 15:05:24.394: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 11 15:05:24.394: INFO: stdout: ""
Aug 11 15:05:24.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c nc -v -z -w 2 10.111.165.175 80'
Aug 11 15:05:24.531: INFO: stderr: "+ nc -v -z -w 2 10.111.165.175 80\nConnection to 10.111.165.175 80 port [tcp/http] succeeded!\n"
Aug 11 15:05:24.531: INFO: stdout: ""
Aug 11 15:05:24.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.165.175:80/ ; done'
Aug 11 15:05:24.703: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n"
Aug 11 15:05:24.703: INFO: stdout: "\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf"
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
Aug 11 15:05:24.703: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6925, will wait for the garbage collector to delete the pods 08/11/23 15:05:24.716
Aug 11 15:05:24.774: INFO: Deleting ReplicationController affinity-clusterip took: 4.867795ms
Aug 11 15:05:24.875: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.204039ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 15:05:26.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6925" for this suite. 08/11/23 15:05:26.497
------------------------------
• [SLOW TEST] [8.348 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:05:18.155
    Aug 11 15:05:18.155: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 15:05:18.156
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:18.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:18.168
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6925 08/11/23 15:05:18.17
    STEP: creating service affinity-clusterip in namespace services-6925 08/11/23 15:05:18.17
    STEP: creating replication controller affinity-clusterip in namespace services-6925 08/11/23 15:05:18.182
    I0811 15:05:18.189517      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6925, replica count: 3
    I0811 15:05:21.240771      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:05:21.246: INFO: Creating new exec pod
    Aug 11 15:05:21.253: INFO: Waiting up to 5m0s for pod "execpod-affinityxpzcb" in namespace "services-6925" to be "running"
    Aug 11 15:05:21.256: INFO: Pod "execpod-affinityxpzcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459752ms
    Aug 11 15:05:23.261: INFO: Pod "execpod-affinityxpzcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007451724s
    Aug 11 15:05:23.261: INFO: Pod "execpod-affinityxpzcb" satisfied condition "running"
    Aug 11 15:05:24.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 11 15:05:24.394: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 11 15:05:24.394: INFO: stdout: ""
    Aug 11 15:05:24.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c nc -v -z -w 2 10.111.165.175 80'
    Aug 11 15:05:24.531: INFO: stderr: "+ nc -v -z -w 2 10.111.165.175 80\nConnection to 10.111.165.175 80 port [tcp/http] succeeded!\n"
    Aug 11 15:05:24.531: INFO: stdout: ""
    Aug 11 15:05:24.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6925 exec execpod-affinityxpzcb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.165.175:80/ ; done'
    Aug 11 15:05:24.703: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.165.175:80/\n"
    Aug 11 15:05:24.703: INFO: stdout: "\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf\naffinity-clusterip-8nrjf"
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Received response from host: affinity-clusterip-8nrjf
    Aug 11 15:05:24.703: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6925, will wait for the garbage collector to delete the pods 08/11/23 15:05:24.716
    Aug 11 15:05:24.774: INFO: Deleting ReplicationController affinity-clusterip took: 4.867795ms
    Aug 11 15:05:24.875: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.204039ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:05:26.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6925" for this suite. 08/11/23 15:05:26.497
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:05:26.503
Aug 11 15:05:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename proxy 08/11/23 15:05:26.504
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:26.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:26.517
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 11 15:05:26.520: INFO: Creating pod...
Aug 11 15:05:26.526: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9399" to be "running"
Aug 11 15:05:26.528: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172822ms
Aug 11 15:05:28.534: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008381194s
Aug 11 15:05:28.534: INFO: Pod "agnhost" satisfied condition "running"
Aug 11 15:05:28.534: INFO: Creating service...
Aug 11 15:05:28.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/DELETE
Aug 11 15:05:28.570: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 15:05:28.570: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/GET
Aug 11 15:05:28.577: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 11 15:05:28.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/HEAD
Aug 11 15:05:28.581: INFO: http.Client request:HEAD | StatusCode:200
Aug 11 15:05:28.582: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 11 15:05:28.586: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 15:05:28.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/PATCH
Aug 11 15:05:28.591: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 15:05:28.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/POST
Aug 11 15:05:28.597: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 15:05:28.597: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/PUT
Aug 11 15:05:28.601: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 15:05:28.601: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/DELETE
Aug 11 15:05:28.607: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 15:05:28.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/GET
Aug 11 15:05:28.613: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 11 15:05:28.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/HEAD
Aug 11 15:05:28.619: INFO: http.Client request:HEAD | StatusCode:200
Aug 11 15:05:28.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/OPTIONS
Aug 11 15:05:28.629: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 15:05:28.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/PATCH
Aug 11 15:05:28.635: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 15:05:28.635: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/POST
Aug 11 15:05:28.642: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 15:05:28.642: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/PUT
Aug 11 15:05:28.648: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 11 15:05:28.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9399" for this suite. 08/11/23 15:05:28.652
------------------------------
• [2.155 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:05:26.503
    Aug 11 15:05:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename proxy 08/11/23 15:05:26.504
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:26.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:26.517
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 11 15:05:26.520: INFO: Creating pod...
    Aug 11 15:05:26.526: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9399" to be "running"
    Aug 11 15:05:26.528: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172822ms
    Aug 11 15:05:28.534: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.008381194s
    Aug 11 15:05:28.534: INFO: Pod "agnhost" satisfied condition "running"
    Aug 11 15:05:28.534: INFO: Creating service...
    Aug 11 15:05:28.555: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/DELETE
    Aug 11 15:05:28.570: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 15:05:28.570: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/GET
    Aug 11 15:05:28.577: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 11 15:05:28.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/HEAD
    Aug 11 15:05:28.581: INFO: http.Client request:HEAD | StatusCode:200
    Aug 11 15:05:28.582: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 11 15:05:28.586: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 15:05:28.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/PATCH
    Aug 11 15:05:28.591: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 15:05:28.591: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/POST
    Aug 11 15:05:28.597: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 15:05:28.597: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/pods/agnhost/proxy/some/path/with/PUT
    Aug 11 15:05:28.601: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 15:05:28.601: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/DELETE
    Aug 11 15:05:28.607: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 15:05:28.607: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/GET
    Aug 11 15:05:28.613: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 11 15:05:28.613: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/HEAD
    Aug 11 15:05:28.619: INFO: http.Client request:HEAD | StatusCode:200
    Aug 11 15:05:28.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/OPTIONS
    Aug 11 15:05:28.629: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 15:05:28.629: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/PATCH
    Aug 11 15:05:28.635: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 15:05:28.635: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/POST
    Aug 11 15:05:28.642: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 15:05:28.642: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9399/services/test-service/proxy/some/path/with/PUT
    Aug 11 15:05:28.648: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:05:28.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9399" for this suite. 08/11/23 15:05:28.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:05:28.659
Aug 11 15:05:28.659: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:05:28.66
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:28.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:28.673
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-c6e84af1-2dce-4993-a234-b284f6b46a89 08/11/23 15:05:28.678
STEP: Creating the pod 08/11/23 15:05:28.682
Aug 11 15:05:28.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af" in namespace "configmap-9721" to be "running"
Aug 11 15:05:28.691: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264432ms
Aug 11 15:05:30.694: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af": Phase="Running", Reason="", readiness=false. Elapsed: 2.005596341s
Aug 11 15:05:30.694: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af" satisfied condition "running"
STEP: Waiting for pod with text data 08/11/23 15:05:30.694
STEP: Waiting for pod with binary data 08/11/23 15:05:30.712
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:05:30.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9721" for this suite. 08/11/23 15:05:30.722
------------------------------
• [2.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:05:28.659
    Aug 11 15:05:28.659: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:05:28.66
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:28.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:28.673
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-c6e84af1-2dce-4993-a234-b284f6b46a89 08/11/23 15:05:28.678
    STEP: Creating the pod 08/11/23 15:05:28.682
    Aug 11 15:05:28.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af" in namespace "configmap-9721" to be "running"
    Aug 11 15:05:28.691: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.264432ms
    Aug 11 15:05:30.694: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af": Phase="Running", Reason="", readiness=false. Elapsed: 2.005596341s
    Aug 11 15:05:30.694: INFO: Pod "pod-configmaps-abd6a080-7069-48e7-ac8b-4ae7cab452af" satisfied condition "running"
    STEP: Waiting for pod with text data 08/11/23 15:05:30.694
    STEP: Waiting for pod with binary data 08/11/23 15:05:30.712
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:05:30.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9721" for this suite. 08/11/23 15:05:30.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:05:30.729
Aug 11 15:05:30.729: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-preemption 08/11/23 15:05:30.73
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:30.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:30.743
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 11 15:05:30.757: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 15:06:30.796: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/11/23 15:06:30.798
Aug 11 15:06:30.817: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 11 15:06:30.824: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 11 15:06:30.844: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 11 15:06:30.851: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/11/23 15:06:30.851
Aug 11 15:06:30.851: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8295" to be "running"
Aug 11 15:06:30.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.657276ms
Aug 11 15:06:32.861: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009400594s
Aug 11 15:06:32.861: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 11 15:06:32.861: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
Aug 11 15:06:32.863: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.959082ms
Aug 11 15:06:32.863: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 15:06:32.863: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
Aug 11 15:06:32.865: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.050152ms
Aug 11 15:06:32.865: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 15:06:32.865: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
Aug 11 15:06:32.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.928182ms
Aug 11 15:06:32.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/11/23 15:06:32.867
Aug 11 15:06:32.875: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 11 15:06:32.877: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136272ms
Aug 11 15:06:34.880: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005487941s
Aug 11 15:06:36.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006484786s
Aug 11 15:06:38.881: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00602381s
Aug 11 15:06:38.881: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:38.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8295" for this suite. 08/11/23 15:06:38.936
------------------------------
• [SLOW TEST] [68.215 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:05:30.729
    Aug 11 15:05:30.729: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 15:05:30.73
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:30.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:30.743
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 11 15:05:30.757: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 15:06:30.796: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/11/23 15:06:30.798
    Aug 11 15:06:30.817: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 11 15:06:30.824: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 11 15:06:30.844: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 11 15:06:30.851: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/11/23 15:06:30.851
    Aug 11 15:06:30.851: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8295" to be "running"
    Aug 11 15:06:30.857: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.657276ms
    Aug 11 15:06:32.861: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009400594s
    Aug 11 15:06:32.861: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 11 15:06:32.861: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
    Aug 11 15:06:32.863: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.959082ms
    Aug 11 15:06:32.863: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 15:06:32.863: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
    Aug 11 15:06:32.865: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.050152ms
    Aug 11 15:06:32.865: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 15:06:32.865: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8295" to be "running"
    Aug 11 15:06:32.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.928182ms
    Aug 11 15:06:32.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/11/23 15:06:32.867
    Aug 11 15:06:32.875: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 11 15:06:32.877: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136272ms
    Aug 11 15:06:34.880: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005487941s
    Aug 11 15:06:36.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006484786s
    Aug 11 15:06:38.881: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00602381s
    Aug 11 15:06:38.881: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:38.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8295" for this suite. 08/11/23 15:06:38.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:38.946
Aug 11 15:06:38.946: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename runtimeclass 08/11/23 15:06:38.947
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.959
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:38.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8749" for this suite. 08/11/23 15:06:38.968
------------------------------
• [0.028 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:38.946
    Aug 11 15:06:38.946: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 15:06:38.947
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.959
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:38.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8749" for this suite. 08/11/23 15:06:38.968
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:38.974
Aug 11 15:06:38.974: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context-test 08/11/23 15:06:38.975
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.986
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 11 15:06:38.995: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05" in namespace "security-context-test-5708" to be "Succeeded or Failed"
Aug 11 15:06:38.997: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879802ms
Aug 11 15:06:41.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00536963s
Aug 11 15:06:43.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005538766s
Aug 11 15:06:43.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05" satisfied condition "Succeeded or Failed"
Aug 11 15:06:43.008: INFO: Got logs for pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:43.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5708" for this suite. 08/11/23 15:06:43.012
------------------------------
• [4.044 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:38.974
    Aug 11 15:06:38.974: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context-test 08/11/23 15:06:38.975
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.986
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 11 15:06:38.995: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05" in namespace "security-context-test-5708" to be "Succeeded or Failed"
    Aug 11 15:06:38.997: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Pending", Reason="", readiness=false. Elapsed: 1.879802ms
    Aug 11 15:06:41.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00536963s
    Aug 11 15:06:43.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005538766s
    Aug 11 15:06:43.000: INFO: Pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05" satisfied condition "Succeeded or Failed"
    Aug 11 15:06:43.008: INFO: Got logs for pod "busybox-privileged-false-7f98840d-649c-4b30-9876-16fa1c68cd05": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:43.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5708" for this suite. 08/11/23 15:06:43.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:43.019
Aug 11 15:06:43.019: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 15:06:43.02
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:43.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:43.035
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/11/23 15:06:43.037
Aug 11 15:06:43.045: INFO: Waiting up to 5m0s for pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad" in namespace "downward-api-9248" to be "Succeeded or Failed"
Aug 11 15:06:43.047: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314902ms
Aug 11 15:06:45.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00594021s
Aug 11 15:06:47.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006278507s
STEP: Saw pod success 08/11/23 15:06:47.051
Aug 11 15:06:47.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad" satisfied condition "Succeeded or Failed"
Aug 11 15:06:47.054: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad container dapi-container: <nil>
STEP: delete the pod 08/11/23 15:06:47.061
Aug 11 15:06:47.073: INFO: Waiting for pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad to disappear
Aug 11 15:06:47.075: INFO: Pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:47.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9248" for this suite. 08/11/23 15:06:47.078
------------------------------
• [4.064 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:43.019
    Aug 11 15:06:43.019: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:06:43.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:43.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:43.035
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/11/23 15:06:43.037
    Aug 11 15:06:43.045: INFO: Waiting up to 5m0s for pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad" in namespace "downward-api-9248" to be "Succeeded or Failed"
    Aug 11 15:06:43.047: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314902ms
    Aug 11 15:06:45.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00594021s
    Aug 11 15:06:47.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006278507s
    STEP: Saw pod success 08/11/23 15:06:47.051
    Aug 11 15:06:47.051: INFO: Pod "downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad" satisfied condition "Succeeded or Failed"
    Aug 11 15:06:47.054: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad container dapi-container: <nil>
    STEP: delete the pod 08/11/23 15:06:47.061
    Aug 11 15:06:47.073: INFO: Waiting for pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad to disappear
    Aug 11 15:06:47.075: INFO: Pod downward-api-08c0643e-a081-4aea-8ff3-d7c6654b84ad no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:47.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9248" for this suite. 08/11/23 15:06:47.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:47.084
Aug 11 15:06:47.084: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/11/23 15:06:47.085
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:47.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:47.097
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/11/23 15:06:47.099
STEP: Creating hostNetwork=false pod 08/11/23 15:06:47.099
Aug 11 15:06:47.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3847" to be "running and ready"
Aug 11 15:06:47.108: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305002ms
Aug 11 15:06:47.108: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:06:49.112: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005598759s
Aug 11 15:06:49.112: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 11 15:06:49.112: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/11/23 15:06:49.114
Aug 11 15:06:49.120: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3847" to be "running and ready"
Aug 11 15:06:49.122: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091893ms
Aug 11 15:06:49.122: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:06:51.126: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006248174s
Aug 11 15:06:51.126: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 11 15:06:51.126: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/11/23 15:06:51.129
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/11/23 15:06:51.129
Aug 11 15:06:51.129: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.129: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.130: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 15:06:51.207: INFO: Exec stderr: ""
Aug 11 15:06:51.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.208: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 15:06:51.287: INFO: Exec stderr: ""
Aug 11 15:06:51.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.287: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.296: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 15:06:51.367: INFO: Exec stderr: ""
Aug 11 15:06:51.367: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.367: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.368: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 15:06:51.453: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/11/23 15:06:51.453
Aug 11 15:06:51.453: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.453: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.453: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.453: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 11 15:06:51.532: INFO: Exec stderr: ""
Aug 11 15:06:51.532: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.532: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.533: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.533: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 11 15:06:51.604: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/11/23 15:06:51.604
Aug 11 15:06:51.604: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.604: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.605: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.605: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 15:06:51.678: INFO: Exec stderr: ""
Aug 11 15:06:51.678: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.678: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.679: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.679: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 15:06:51.754: INFO: Exec stderr: ""
Aug 11 15:06:51.754: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.754: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.754: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 15:06:51.834: INFO: Exec stderr: ""
Aug 11 15:06:51.834: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:06:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:06:51.834: INFO: ExecWithOptions: Clientset creation
Aug 11 15:06:51.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 15:06:51.902: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:51.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3847" for this suite. 08/11/23 15:06:51.906
------------------------------
• [4.827 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:47.084
    Aug 11 15:06:47.084: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/11/23 15:06:47.085
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:47.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:47.097
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/11/23 15:06:47.099
    STEP: Creating hostNetwork=false pod 08/11/23 15:06:47.099
    Aug 11 15:06:47.106: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3847" to be "running and ready"
    Aug 11 15:06:47.108: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305002ms
    Aug 11 15:06:47.108: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:06:49.112: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005598759s
    Aug 11 15:06:49.112: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 11 15:06:49.112: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/11/23 15:06:49.114
    Aug 11 15:06:49.120: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3847" to be "running and ready"
    Aug 11 15:06:49.122: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091893ms
    Aug 11 15:06:49.122: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:06:51.126: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006248174s
    Aug 11 15:06:51.126: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 11 15:06:51.126: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/11/23 15:06:51.129
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/11/23 15:06:51.129
    Aug 11 15:06:51.129: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.129: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.130: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 15:06:51.207: INFO: Exec stderr: ""
    Aug 11 15:06:51.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.208: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 15:06:51.287: INFO: Exec stderr: ""
    Aug 11 15:06:51.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.287: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.296: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.296: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 15:06:51.367: INFO: Exec stderr: ""
    Aug 11 15:06:51.367: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.367: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.368: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.368: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 15:06:51.453: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/11/23 15:06:51.453
    Aug 11 15:06:51.453: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.453: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.453: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.453: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 11 15:06:51.532: INFO: Exec stderr: ""
    Aug 11 15:06:51.532: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.532: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.533: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.533: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 11 15:06:51.604: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/11/23 15:06:51.604
    Aug 11 15:06:51.604: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.604: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.605: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.605: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 15:06:51.678: INFO: Exec stderr: ""
    Aug 11 15:06:51.678: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.678: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.679: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.679: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 15:06:51.754: INFO: Exec stderr: ""
    Aug 11 15:06:51.754: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.754: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.754: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 15:06:51.834: INFO: Exec stderr: ""
    Aug 11 15:06:51.834: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3847 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:06:51.834: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:06:51.834: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:06:51.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3847/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 15:06:51.902: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:51.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3847" for this suite. 08/11/23 15:06:51.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:51.913
Aug 11 15:06:51.913: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:06:51.914
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:51.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:51.927
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 15:06:51.929
Aug 11 15:06:51.936: INFO: Waiting up to 5m0s for pod "pod-c76b5777-8860-4e10-9354-99b21654ff58" in namespace "emptydir-2998" to be "Succeeded or Failed"
Aug 11 15:06:51.939: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348703ms
Aug 11 15:06:53.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005754581s
Aug 11 15:06:55.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005927426s
STEP: Saw pod success 08/11/23 15:06:55.942
Aug 11 15:06:55.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58" satisfied condition "Succeeded or Failed"
Aug 11 15:06:55.945: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c76b5777-8860-4e10-9354-99b21654ff58 container test-container: <nil>
STEP: delete the pod 08/11/23 15:06:55.953
Aug 11 15:06:55.964: INFO: Waiting for pod pod-c76b5777-8860-4e10-9354-99b21654ff58 to disappear
Aug 11 15:06:55.966: INFO: Pod pod-c76b5777-8860-4e10-9354-99b21654ff58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:06:55.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2998" for this suite. 08/11/23 15:06:55.969
------------------------------
• [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:51.913
    Aug 11 15:06:51.913: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:06:51.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:51.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:51.927
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 15:06:51.929
    Aug 11 15:06:51.936: INFO: Waiting up to 5m0s for pod "pod-c76b5777-8860-4e10-9354-99b21654ff58" in namespace "emptydir-2998" to be "Succeeded or Failed"
    Aug 11 15:06:51.939: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348703ms
    Aug 11 15:06:53.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005754581s
    Aug 11 15:06:55.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005927426s
    STEP: Saw pod success 08/11/23 15:06:55.942
    Aug 11 15:06:55.942: INFO: Pod "pod-c76b5777-8860-4e10-9354-99b21654ff58" satisfied condition "Succeeded or Failed"
    Aug 11 15:06:55.945: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c76b5777-8860-4e10-9354-99b21654ff58 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:06:55.953
    Aug 11 15:06:55.964: INFO: Waiting for pod pod-c76b5777-8860-4e10-9354-99b21654ff58 to disappear
    Aug 11 15:06:55.966: INFO: Pod pod-c76b5777-8860-4e10-9354-99b21654ff58 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:06:55.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2998" for this suite. 08/11/23 15:06:55.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:06:55.975
Aug 11 15:06:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:06:55.976
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:55.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:55.989
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-43 08/11/23 15:06:55.992
STEP: creating a selector 08/11/23 15:06:55.992
STEP: Creating the service pods in kubernetes 08/11/23 15:06:55.992
Aug 11 15:06:55.992: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 15:06:56.010: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-43" to be "running and ready"
Aug 11 15:06:56.013: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939113ms
Aug 11 15:06:56.013: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:06:58.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007416082s
Aug 11 15:06:58.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:07:00.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008067057s
Aug 11 15:07:00.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:07:02.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006976573s
Aug 11 15:07:02.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:07:04.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006047587s
Aug 11 15:07:04.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:07:06.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007106144s
Aug 11 15:07:06.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:07:08.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00824546s
Aug 11 15:07:08.018: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 15:07:08.018: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 15:07:08.021: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-43" to be "running and ready"
Aug 11 15:07:08.023: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.251403ms
Aug 11 15:07:08.023: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 15:07:08.023: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 15:07:08.025
Aug 11 15:07:08.035: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-43" to be "running"
Aug 11 15:07:08.040: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.133964ms
Aug 11 15:07:10.044: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008808083s
Aug 11 15:07:10.044: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 15:07:10.047: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-43" to be "running"
Aug 11 15:07:10.049: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.081032ms
Aug 11 15:07:10.049: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 11 15:07:10.051: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 15:07:10.051: INFO: Going to poll 10.10.0.10 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:07:10.053: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-43 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:07:10.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:07:10.053: INFO: ExecWithOptions: Clientset creation
Aug 11 15:07:10.053: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-43/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:07:11.132: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 11 15:07:11.132: INFO: Going to poll 10.10.1.178 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:07:11.135: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-43 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:07:11.135: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:07:11.135: INFO: ExecWithOptions: Clientset creation
Aug 11 15:07:11.136: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-43/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:07:12.211: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 11 15:07:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-43" for this suite. 08/11/23 15:07:12.215
------------------------------
• [SLOW TEST] [16.246 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:06:55.975
    Aug 11 15:06:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:06:55.976
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:55.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:55.989
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-43 08/11/23 15:06:55.992
    STEP: creating a selector 08/11/23 15:06:55.992
    STEP: Creating the service pods in kubernetes 08/11/23 15:06:55.992
    Aug 11 15:06:55.992: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 15:06:56.010: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-43" to be "running and ready"
    Aug 11 15:06:56.013: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939113ms
    Aug 11 15:06:56.013: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:06:58.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007416082s
    Aug 11 15:06:58.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:07:00.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008067057s
    Aug 11 15:07:00.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:07:02.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006976573s
    Aug 11 15:07:02.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:07:04.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006047587s
    Aug 11 15:07:04.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:07:06.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007106144s
    Aug 11 15:07:06.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:07:08.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.00824546s
    Aug 11 15:07:08.018: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 15:07:08.018: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 15:07:08.021: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-43" to be "running and ready"
    Aug 11 15:07:08.023: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.251403ms
    Aug 11 15:07:08.023: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 15:07:08.023: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 15:07:08.025
    Aug 11 15:07:08.035: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-43" to be "running"
    Aug 11 15:07:08.040: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.133964ms
    Aug 11 15:07:10.044: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008808083s
    Aug 11 15:07:10.044: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 15:07:10.047: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-43" to be "running"
    Aug 11 15:07:10.049: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.081032ms
    Aug 11 15:07:10.049: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 11 15:07:10.051: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 15:07:10.051: INFO: Going to poll 10.10.0.10 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:07:10.053: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.10 8081 | grep -v '^\s*$'] Namespace:pod-network-test-43 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:07:10.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:07:10.053: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:07:10.053: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-43/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.10+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:07:11.132: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 11 15:07:11.132: INFO: Going to poll 10.10.1.178 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:07:11.135: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-43 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:07:11.135: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:07:11.135: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:07:11.136: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-43/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:07:12.211: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:07:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-43" for this suite. 08/11/23 15:07:12.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:07:12.221
Aug 11 15:07:12.221: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 15:07:12.222
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:12.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:12.234
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/11/23 15:07:12.238
STEP: watching for the Service to be added 08/11/23 15:07:12.249
Aug 11 15:07:12.251: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 11 15:07:12.251: INFO: Service test-service-gvr2p created
STEP: Getting /status 08/11/23 15:07:12.251
Aug 11 15:07:12.254: INFO: Service test-service-gvr2p has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/11/23 15:07:12.254
STEP: watching for the Service to be patched 08/11/23 15:07:12.259
Aug 11 15:07:12.261: INFO: observed Service test-service-gvr2p in namespace services-737 with annotations: map[] & LoadBalancer: {[]}
Aug 11 15:07:12.261: INFO: Found Service test-service-gvr2p in namespace services-737 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 11 15:07:12.261: INFO: Service test-service-gvr2p has service status patched
STEP: updating the ServiceStatus 08/11/23 15:07:12.261
Aug 11 15:07:12.269: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/11/23 15:07:12.269
Aug 11 15:07:12.271: INFO: Observed Service test-service-gvr2p in namespace services-737 with annotations: map[] & Conditions: {[]}
Aug 11 15:07:12.271: INFO: Observed event: &Service{ObjectMeta:{test-service-gvr2p  services-737  5752e316-7493-4b69-8ea7-00c87a74cba5 46031 0 2023-08-11 15:07:12 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-11 15:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-11 15:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.194.3,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.194.3],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 11 15:07:12.271: INFO: Found Service test-service-gvr2p in namespace services-737 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 15:07:12.271: INFO: Service test-service-gvr2p has service status updated
STEP: patching the service 08/11/23 15:07:12.271
STEP: watching for the Service to be patched 08/11/23 15:07:12.282
Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
Aug 11 15:07:12.284: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service:patched test-service-static:true]
Aug 11 15:07:12.284: INFO: Service test-service-gvr2p patched
STEP: deleting the service 08/11/23 15:07:12.284
STEP: watching for the Service to be deleted 08/11/23 15:07:12.298
Aug 11 15:07:12.300: INFO: Observed event: ADDED
Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
Aug 11 15:07:12.300: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 11 15:07:12.300: INFO: Service test-service-gvr2p deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 15:07:12.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-737" for this suite. 08/11/23 15:07:12.304
------------------------------
• [0.088 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:07:12.221
    Aug 11 15:07:12.221: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 15:07:12.222
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:12.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:12.234
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/11/23 15:07:12.238
    STEP: watching for the Service to be added 08/11/23 15:07:12.249
    Aug 11 15:07:12.251: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 11 15:07:12.251: INFO: Service test-service-gvr2p created
    STEP: Getting /status 08/11/23 15:07:12.251
    Aug 11 15:07:12.254: INFO: Service test-service-gvr2p has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/11/23 15:07:12.254
    STEP: watching for the Service to be patched 08/11/23 15:07:12.259
    Aug 11 15:07:12.261: INFO: observed Service test-service-gvr2p in namespace services-737 with annotations: map[] & LoadBalancer: {[]}
    Aug 11 15:07:12.261: INFO: Found Service test-service-gvr2p in namespace services-737 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 11 15:07:12.261: INFO: Service test-service-gvr2p has service status patched
    STEP: updating the ServiceStatus 08/11/23 15:07:12.261
    Aug 11 15:07:12.269: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/11/23 15:07:12.269
    Aug 11 15:07:12.271: INFO: Observed Service test-service-gvr2p in namespace services-737 with annotations: map[] & Conditions: {[]}
    Aug 11 15:07:12.271: INFO: Observed event: &Service{ObjectMeta:{test-service-gvr2p  services-737  5752e316-7493-4b69-8ea7-00c87a74cba5 46031 0 2023-08-11 15:07:12 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-11 15:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-11 15:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.194.3,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.194.3],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 11 15:07:12.271: INFO: Found Service test-service-gvr2p in namespace services-737 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 15:07:12.271: INFO: Service test-service-gvr2p has service status updated
    STEP: patching the service 08/11/23 15:07:12.271
    STEP: watching for the Service to be patched 08/11/23 15:07:12.282
    Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
    Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
    Aug 11 15:07:12.284: INFO: observed Service test-service-gvr2p in namespace services-737 with labels: map[test-service-static:true]
    Aug 11 15:07:12.284: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service:patched test-service-static:true]
    Aug 11 15:07:12.284: INFO: Service test-service-gvr2p patched
    STEP: deleting the service 08/11/23 15:07:12.284
    STEP: watching for the Service to be deleted 08/11/23 15:07:12.298
    Aug 11 15:07:12.300: INFO: Observed event: ADDED
    Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
    Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
    Aug 11 15:07:12.300: INFO: Observed event: MODIFIED
    Aug 11 15:07:12.300: INFO: Found Service test-service-gvr2p in namespace services-737 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 11 15:07:12.300: INFO: Service test-service-gvr2p deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:07:12.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-737" for this suite. 08/11/23 15:07:12.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:07:12.31
Aug 11 15:07:12.310: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 15:07:12.311
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:12.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:12.325
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/11/23 15:07:12.328
Aug 11 15:07:12.335: INFO: Waiting up to 5m0s for pod "pod-wpwcz" in namespace "pods-6533" to be "running"
Aug 11 15:07:12.337: INFO: Pod "pod-wpwcz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236583ms
Aug 11 15:07:14.341: INFO: Pod "pod-wpwcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005899741s
Aug 11 15:07:14.341: INFO: Pod "pod-wpwcz" satisfied condition "running"
STEP: patching /status 08/11/23 15:07:14.341
Aug 11 15:07:14.349: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 15:07:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6533" for this suite. 08/11/23 15:07:14.352
------------------------------
• [2.048 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:07:12.31
    Aug 11 15:07:12.310: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 15:07:12.311
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:12.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:12.325
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/11/23 15:07:12.328
    Aug 11 15:07:12.335: INFO: Waiting up to 5m0s for pod "pod-wpwcz" in namespace "pods-6533" to be "running"
    Aug 11 15:07:12.337: INFO: Pod "pod-wpwcz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236583ms
    Aug 11 15:07:14.341: INFO: Pod "pod-wpwcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005899741s
    Aug 11 15:07:14.341: INFO: Pod "pod-wpwcz" satisfied condition "running"
    STEP: patching /status 08/11/23 15:07:14.341
    Aug 11 15:07:14.349: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:07:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6533" for this suite. 08/11/23 15:07:14.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:07:14.359
Aug 11 15:07:14.359: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename cronjob 08/11/23 15:07:14.36
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:14.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:14.372
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/11/23 15:07:14.374
STEP: Ensuring no jobs are scheduled 08/11/23 15:07:14.383
STEP: Ensuring no job exists by listing jobs explicitly 08/11/23 15:12:14.39
STEP: Removing cronjob 08/11/23 15:12:14.392
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:14.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3486" for this suite. 08/11/23 15:12:14.403
------------------------------
• [SLOW TEST] [300.048 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:07:14.359
    Aug 11 15:07:14.359: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename cronjob 08/11/23 15:07:14.36
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:14.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:14.372
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/11/23 15:07:14.374
    STEP: Ensuring no jobs are scheduled 08/11/23 15:07:14.383
    STEP: Ensuring no job exists by listing jobs explicitly 08/11/23 15:12:14.39
    STEP: Removing cronjob 08/11/23 15:12:14.392
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:14.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3486" for this suite. 08/11/23 15:12:14.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:14.409
Aug 11 15:12:14.409: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-webhook 08/11/23 15:12:14.409
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:14.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:14.422
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/11/23 15:12:14.424
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 15:12:15.111
STEP: Deploying the custom resource conversion webhook pod 08/11/23 15:12:15.119
STEP: Wait for the deployment to be ready 08/11/23 15:12:15.131
Aug 11 15:12:15.138: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:12:17.147
STEP: Verifying the service has paired with the endpoint 08/11/23 15:12:17.16
Aug 11 15:12:18.161: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 11 15:12:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Creating a v1 custom resource 08/11/23 15:12:20.748
STEP: Create a v2 custom resource 08/11/23 15:12:20.762
STEP: List CRs in v1 08/11/23 15:12:20.812
STEP: List CRs in v2 08/11/23 15:12:20.818
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:21.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9147" for this suite. 08/11/23 15:12:21.381
------------------------------
• [SLOW TEST] [6.981 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:14.409
    Aug 11 15:12:14.409: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-webhook 08/11/23 15:12:14.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:14.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:14.422
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/11/23 15:12:14.424
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 15:12:15.111
    STEP: Deploying the custom resource conversion webhook pod 08/11/23 15:12:15.119
    STEP: Wait for the deployment to be ready 08/11/23 15:12:15.131
    Aug 11 15:12:15.138: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:12:17.147
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:12:17.16
    Aug 11 15:12:18.161: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 11 15:12:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Creating a v1 custom resource 08/11/23 15:12:20.748
    STEP: Create a v2 custom resource 08/11/23 15:12:20.762
    STEP: List CRs in v1 08/11/23 15:12:20.812
    STEP: List CRs in v2 08/11/23 15:12:20.818
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:21.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9147" for this suite. 08/11/23 15:12:21.381
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:21.389
Aug 11 15:12:21.389: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:12:21.39
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:21.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:21.41
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 15:12:21.413
Aug 11 15:12:21.420: INFO: Waiting up to 5m0s for pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc" in namespace "emptydir-982" to be "Succeeded or Failed"
Aug 11 15:12:21.426: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.474366ms
Aug 11 15:12:23.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009476775s
Aug 11 15:12:25.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009427781s
STEP: Saw pod success 08/11/23 15:12:25.43
Aug 11 15:12:25.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc" satisfied condition "Succeeded or Failed"
Aug 11 15:12:25.432: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc container test-container: <nil>
STEP: delete the pod 08/11/23 15:12:25.452
Aug 11 15:12:25.464: INFO: Waiting for pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc to disappear
Aug 11 15:12:25.466: INFO: Pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:25.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-982" for this suite. 08/11/23 15:12:25.47
------------------------------
• [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:21.389
    Aug 11 15:12:21.389: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:12:21.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:21.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:21.41
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 15:12:21.413
    Aug 11 15:12:21.420: INFO: Waiting up to 5m0s for pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc" in namespace "emptydir-982" to be "Succeeded or Failed"
    Aug 11 15:12:21.426: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.474366ms
    Aug 11 15:12:23.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009476775s
    Aug 11 15:12:25.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009427781s
    STEP: Saw pod success 08/11/23 15:12:25.43
    Aug 11 15:12:25.430: INFO: Pod "pod-59d7780b-e94d-44a9-8121-876f8bcf01fc" satisfied condition "Succeeded or Failed"
    Aug 11 15:12:25.432: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc container test-container: <nil>
    STEP: delete the pod 08/11/23 15:12:25.452
    Aug 11 15:12:25.464: INFO: Waiting for pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc to disappear
    Aug 11 15:12:25.466: INFO: Pod pod-59d7780b-e94d-44a9-8121-876f8bcf01fc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:25.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-982" for this suite. 08/11/23 15:12:25.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:25.476
Aug 11 15:12:25.476: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename security-context-test 08/11/23 15:12:25.476
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:25.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:25.492
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 11 15:12:25.500: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297" in namespace "security-context-test-1004" to be "Succeeded or Failed"
Aug 11 15:12:25.502: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085321ms
Aug 11 15:12:27.507: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006893833s
Aug 11 15:12:29.507: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006751968s
Aug 11 15:12:31.506: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005248141s
Aug 11 15:12:31.506: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:31.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-1004" for this suite. 08/11/23 15:12:31.517
------------------------------
• [SLOW TEST] [6.046 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:25.476
    Aug 11 15:12:25.476: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename security-context-test 08/11/23 15:12:25.476
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:25.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:25.492
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 11 15:12:25.500: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297" in namespace "security-context-test-1004" to be "Succeeded or Failed"
    Aug 11 15:12:25.502: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085321ms
    Aug 11 15:12:27.507: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006893833s
    Aug 11 15:12:29.507: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006751968s
    Aug 11 15:12:31.506: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005248141s
    Aug 11 15:12:31.506: INFO: Pod "alpine-nnp-false-eb9be701-2798-4eea-8532-4a99a764a297" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:31.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-1004" for this suite. 08/11/23 15:12:31.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:31.524
Aug 11 15:12:31.524: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 15:12:31.525
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:31.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:31.538
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:12:31.54
Aug 11 15:12:31.547: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e" in namespace "downward-api-4940" to be "Succeeded or Failed"
Aug 11 15:12:31.551: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751073ms
Aug 11 15:12:33.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007463461s
Aug 11 15:12:35.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007331239s
STEP: Saw pod success 08/11/23 15:12:35.555
Aug 11 15:12:35.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e" satisfied condition "Succeeded or Failed"
Aug 11 15:12:35.557: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e container client-container: <nil>
STEP: delete the pod 08/11/23 15:12:35.564
Aug 11 15:12:35.574: INFO: Waiting for pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e to disappear
Aug 11 15:12:35.576: INFO: Pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:35.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4940" for this suite. 08/11/23 15:12:35.58
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:31.524
    Aug 11 15:12:31.524: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:12:31.525
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:31.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:31.538
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:12:31.54
    Aug 11 15:12:31.547: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e" in namespace "downward-api-4940" to be "Succeeded or Failed"
    Aug 11 15:12:31.551: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751073ms
    Aug 11 15:12:33.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007463461s
    Aug 11 15:12:35.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007331239s
    STEP: Saw pod success 08/11/23 15:12:35.555
    Aug 11 15:12:35.555: INFO: Pod "downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e" satisfied condition "Succeeded or Failed"
    Aug 11 15:12:35.557: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e container client-container: <nil>
    STEP: delete the pod 08/11/23 15:12:35.564
    Aug 11 15:12:35.574: INFO: Waiting for pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e to disappear
    Aug 11 15:12:35.576: INFO: Pod downwardapi-volume-8bb9dc20-efd6-461f-b32f-a69ecd726d9e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:35.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4940" for this suite. 08/11/23 15:12:35.58
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:35.584
Aug 11 15:12:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:12:35.585
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:35.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:35.598
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/11/23 15:12:35.6
Aug 11 15:12:35.605: INFO: Waiting up to 5m0s for pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873" in namespace "emptydir-5852" to be "Succeeded or Failed"
Aug 11 15:12:35.607: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081743ms
Aug 11 15:12:37.611: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005604912s
Aug 11 15:12:39.612: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006450739s
STEP: Saw pod success 08/11/23 15:12:39.612
Aug 11 15:12:39.612: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873" satisfied condition "Succeeded or Failed"
Aug 11 15:12:39.614: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 container test-container: <nil>
STEP: delete the pod 08/11/23 15:12:39.621
Aug 11 15:12:39.632: INFO: Waiting for pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 to disappear
Aug 11 15:12:39.635: INFO: Pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5852" for this suite. 08/11/23 15:12:39.638
------------------------------
• [4.058 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:35.584
    Aug 11 15:12:35.584: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:12:35.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:35.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:35.598
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/11/23 15:12:35.6
    Aug 11 15:12:35.605: INFO: Waiting up to 5m0s for pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873" in namespace "emptydir-5852" to be "Succeeded or Failed"
    Aug 11 15:12:35.607: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081743ms
    Aug 11 15:12:37.611: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005604912s
    Aug 11 15:12:39.612: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006450739s
    STEP: Saw pod success 08/11/23 15:12:39.612
    Aug 11 15:12:39.612: INFO: Pod "pod-c849894a-f8a6-4580-b287-8e57b61d7873" satisfied condition "Succeeded or Failed"
    Aug 11 15:12:39.614: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:12:39.621
    Aug 11 15:12:39.632: INFO: Waiting for pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 to disappear
    Aug 11 15:12:39.635: INFO: Pod pod-c849894a-f8a6-4580-b287-8e57b61d7873 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5852" for this suite. 08/11/23 15:12:39.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:39.643
Aug 11 15:12:39.643: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename daemonsets 08/11/23 15:12:39.644
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:39.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:39.656
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 15:12:39.67
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 15:12:39.674
Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:39.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:12:39.680: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:40.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:12:40.686: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 15:12:41.687: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/11/23 15:12:41.689
Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:41.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:12:41.702: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:42.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:12:42.709: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:43.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:12:43.711: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:44.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:44.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:44.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:44.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:12:44.709: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:12:45.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 15:12:45.708: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/11/23 15:12:45.71
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8503, will wait for the garbage collector to delete the pods 08/11/23 15:12:45.711
Aug 11 15:12:45.767: INFO: Deleting DaemonSet.extensions daemon-set took: 4.588645ms
Aug 11 15:12:45.868: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.001608ms
Aug 11 15:12:48.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:12:48.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 15:12:48.481: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48411"},"items":null}

Aug 11 15:12:48.484: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48411"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:48.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8503" for this suite. 08/11/23 15:12:48.495
------------------------------
• [SLOW TEST] [8.858 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:39.643
    Aug 11 15:12:39.643: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename daemonsets 08/11/23 15:12:39.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:39.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:39.656
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 15:12:39.67
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 15:12:39.674
    Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:39.677: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:39.680: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:12:39.680: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:40.683: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:40.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:12:40.686: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.684: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 15:12:41.687: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/11/23 15:12:41.689
    Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.700: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:41.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:12:41.702: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:42.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:42.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:12:42.709: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:43.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:43.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:12:43.711: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:44.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:44.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:44.707: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:44.709: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:12:44.709: INFO: Node constell-1cf5d931-worker-6381a7ba-mt98 is running 0 daemon pod, expected 1
    Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-clkl with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-gwlg with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:45.706: INFO: DaemonSet pods can't tolerate node constell-1cf5d931-control-plane-bb71bd41-kmfn with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:12:45.708: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 15:12:45.708: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 15:12:45.71
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8503, will wait for the garbage collector to delete the pods 08/11/23 15:12:45.711
    Aug 11 15:12:45.767: INFO: Deleting DaemonSet.extensions daemon-set took: 4.588645ms
    Aug 11 15:12:45.868: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.001608ms
    Aug 11 15:12:48.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:12:48.475: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 15:12:48.481: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48411"},"items":null}

    Aug 11 15:12:48.484: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48411"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:48.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8503" for this suite. 08/11/23 15:12:48.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:48.522
Aug 11 15:12:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:12:48.523
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:48.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:48.536
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 11 15:12:48.538: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:12:50.555
Aug 11 15:12:50.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 create -f -'
Aug 11 15:12:51.110: INFO: stderr: ""
Aug 11 15:12:51.110: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 11 15:12:51.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 delete e2e-test-crd-publish-openapi-8617-crds test-cr'
Aug 11 15:12:51.169: INFO: stderr: ""
Aug 11 15:12:51.169: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 11 15:12:51.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 apply -f -'
Aug 11 15:12:51.670: INFO: stderr: ""
Aug 11 15:12:51.670: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 11 15:12:51.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 delete e2e-test-crd-publish-openapi-8617-crds test-cr'
Aug 11 15:12:51.729: INFO: stderr: ""
Aug 11 15:12:51.729: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/11/23 15:12:51.729
Aug 11 15:12:51.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 explain e2e-test-crd-publish-openapi-8617-crds'
Aug 11 15:12:51.919: INFO: stderr: ""
Aug 11 15:12:51.919: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8617-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:53.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1202" for this suite. 08/11/23 15:12:53.982
------------------------------
• [SLOW TEST] [5.465 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:48.522
    Aug 11 15:12:48.522: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:12:48.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:48.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:48.536
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 11 15:12:48.538: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:12:50.555
    Aug 11 15:12:50.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 create -f -'
    Aug 11 15:12:51.110: INFO: stderr: ""
    Aug 11 15:12:51.110: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 11 15:12:51.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 delete e2e-test-crd-publish-openapi-8617-crds test-cr'
    Aug 11 15:12:51.169: INFO: stderr: ""
    Aug 11 15:12:51.169: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 11 15:12:51.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 apply -f -'
    Aug 11 15:12:51.670: INFO: stderr: ""
    Aug 11 15:12:51.670: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 11 15:12:51.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 --namespace=crd-publish-openapi-1202 delete e2e-test-crd-publish-openapi-8617-crds test-cr'
    Aug 11 15:12:51.729: INFO: stderr: ""
    Aug 11 15:12:51.729: INFO: stdout: "e2e-test-crd-publish-openapi-8617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/11/23 15:12:51.729
    Aug 11 15:12:51.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-1202 explain e2e-test-crd-publish-openapi-8617-crds'
    Aug 11 15:12:51.919: INFO: stderr: ""
    Aug 11 15:12:51.919: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8617-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:53.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1202" for this suite. 08/11/23 15:12:53.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:53.988
Aug 11 15:12:53.989: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:12:53.989
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:54.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:54.003
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-3884/configmap-test-b38f37b9-d670-43e7-81d2-5723b0948d51 08/11/23 15:12:54.005
STEP: Creating a pod to test consume configMaps 08/11/23 15:12:54.009
Aug 11 15:12:54.015: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984" in namespace "configmap-3884" to be "Succeeded or Failed"
Aug 11 15:12:54.017: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380222ms
Aug 11 15:12:56.020: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005216451s
Aug 11 15:12:58.023: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007868169s
STEP: Saw pod success 08/11/23 15:12:58.023
Aug 11 15:12:58.023: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984" satisfied condition "Succeeded or Failed"
Aug 11 15:12:58.025: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 container env-test: <nil>
STEP: delete the pod 08/11/23 15:12:58.033
Aug 11 15:12:58.042: INFO: Waiting for pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 to disappear
Aug 11 15:12:58.045: INFO: Pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:12:58.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3884" for this suite. 08/11/23 15:12:58.048
------------------------------
• [4.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:53.988
    Aug 11 15:12:53.989: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:12:53.989
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:54.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:54.003
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-3884/configmap-test-b38f37b9-d670-43e7-81d2-5723b0948d51 08/11/23 15:12:54.005
    STEP: Creating a pod to test consume configMaps 08/11/23 15:12:54.009
    Aug 11 15:12:54.015: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984" in namespace "configmap-3884" to be "Succeeded or Failed"
    Aug 11 15:12:54.017: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.380222ms
    Aug 11 15:12:56.020: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005216451s
    Aug 11 15:12:58.023: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007868169s
    STEP: Saw pod success 08/11/23 15:12:58.023
    Aug 11 15:12:58.023: INFO: Pod "pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984" satisfied condition "Succeeded or Failed"
    Aug 11 15:12:58.025: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 container env-test: <nil>
    STEP: delete the pod 08/11/23 15:12:58.033
    Aug 11 15:12:58.042: INFO: Waiting for pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 to disappear
    Aug 11 15:12:58.045: INFO: Pod pod-configmaps-7d2fc635-5d11-47bc-b152-6fcc48c31984 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:12:58.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3884" for this suite. 08/11/23 15:12:58.048
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:12:58.055
Aug 11 15:12:58.055: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 15:12:58.056
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:58.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:58.069
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/11/23 15:12:58.074
STEP: Verify that the required pods have come up. 08/11/23 15:12:58.078
Aug 11 15:12:58.080: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 15:13:03.086: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 15:13:03.087
STEP: Getting /status 08/11/23 15:13:03.087
Aug 11 15:13:03.090: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/11/23 15:13:03.09
Aug 11 15:13:03.098: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/11/23 15:13:03.098
Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: ADDED
Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.100: INFO: Found replicaset test-rs in namespace replicaset-7315 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 15:13:03.100: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/11/23 15:13:03.1
Aug 11 15:13:03.101: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 15:13:03.106: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/11/23 15:13:03.106
Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: ADDED
Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.108: INFO: Observed replicaset test-rs in namespace replicaset-7315 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 15:13:03.108: INFO: Found replicaset test-rs in namespace replicaset-7315 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 11 15:13:03.108: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:03.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7315" for this suite. 08/11/23 15:13:03.111
------------------------------
• [SLOW TEST] [5.061 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:12:58.055
    Aug 11 15:12:58.055: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:12:58.056
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:12:58.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:12:58.069
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/11/23 15:12:58.074
    STEP: Verify that the required pods have come up. 08/11/23 15:12:58.078
    Aug 11 15:12:58.080: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 15:13:03.086: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 15:13:03.087
    STEP: Getting /status 08/11/23 15:13:03.087
    Aug 11 15:13:03.090: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/11/23 15:13:03.09
    Aug 11 15:13:03.098: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/11/23 15:13:03.098
    Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: ADDED
    Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.100: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.100: INFO: Found replicaset test-rs in namespace replicaset-7315 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 15:13:03.100: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/11/23 15:13:03.1
    Aug 11 15:13:03.101: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 15:13:03.106: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/11/23 15:13:03.106
    Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: ADDED
    Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.108: INFO: Observed replicaset test-rs in namespace replicaset-7315 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 15:13:03.108: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 15:13:03.108: INFO: Found replicaset test-rs in namespace replicaset-7315 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 11 15:13:03.108: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:03.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7315" for this suite. 08/11/23 15:13:03.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:03.117
Aug 11 15:13:03.117: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 15:13:03.118
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:03.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:03.132
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 15:13:03.146
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:13:03.387
STEP: Deploying the webhook pod 08/11/23 15:13:03.393
STEP: Wait for the deployment to be ready 08/11/23 15:13:03.405
Aug 11 15:13:03.411: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:13:05.419
STEP: Verifying the service has paired with the endpoint 08/11/23 15:13:05.431
Aug 11 15:13:06.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/11/23 15:13:06.435
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.459
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/11/23 15:13:06.474
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.483
STEP: Patching a validating webhook configuration's rules to include the create operation 08/11/23 15:13:06.492
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.498
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2522" for this suite. 08/11/23 15:13:06.556
STEP: Destroying namespace "webhook-2522-markers" for this suite. 08/11/23 15:13:06.564
------------------------------
• [3.455 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:03.117
    Aug 11 15:13:03.117: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 15:13:03.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:03.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:03.132
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 15:13:03.146
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:13:03.387
    STEP: Deploying the webhook pod 08/11/23 15:13:03.393
    STEP: Wait for the deployment to be ready 08/11/23 15:13:03.405
    Aug 11 15:13:03.411: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:13:05.419
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:13:05.431
    Aug 11 15:13:06.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/11/23 15:13:06.435
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.459
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/11/23 15:13:06.474
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.483
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/11/23 15:13:06.492
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:13:06.498
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2522" for this suite. 08/11/23 15:13:06.556
    STEP: Destroying namespace "webhook-2522-markers" for this suite. 08/11/23 15:13:06.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:06.573
Aug 11 15:13:06.573: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:13:06.573
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:06.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:06.586
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-097b5720-afc6-4624-8e55-3e54fc83766a 08/11/23 15:13:06.589
STEP: Creating a pod to test consume configMaps 08/11/23 15:13:06.592
Aug 11 15:13:06.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809" in namespace "configmap-717" to be "Succeeded or Failed"
Aug 11 15:13:06.601: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570682ms
Aug 11 15:13:08.605: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006749192s
Aug 11 15:13:10.605: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006234088s
Aug 11 15:13:12.606: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007182045s
STEP: Saw pod success 08/11/23 15:13:12.606
Aug 11 15:13:12.606: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809" satisfied condition "Succeeded or Failed"
Aug 11 15:13:12.608: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 15:13:12.616
Aug 11 15:13:12.629: INFO: Waiting for pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 to disappear
Aug 11 15:13:12.632: INFO: Pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:12.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-717" for this suite. 08/11/23 15:13:12.635
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:06.573
    Aug 11 15:13:06.573: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:13:06.573
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:06.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:06.586
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-097b5720-afc6-4624-8e55-3e54fc83766a 08/11/23 15:13:06.589
    STEP: Creating a pod to test consume configMaps 08/11/23 15:13:06.592
    Aug 11 15:13:06.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809" in namespace "configmap-717" to be "Succeeded or Failed"
    Aug 11 15:13:06.601: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.570682ms
    Aug 11 15:13:08.605: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006749192s
    Aug 11 15:13:10.605: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006234088s
    Aug 11 15:13:12.606: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007182045s
    STEP: Saw pod success 08/11/23 15:13:12.606
    Aug 11 15:13:12.606: INFO: Pod "pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809" satisfied condition "Succeeded or Failed"
    Aug 11 15:13:12.608: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 15:13:12.616
    Aug 11 15:13:12.629: INFO: Waiting for pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 to disappear
    Aug 11 15:13:12.632: INFO: Pod pod-configmaps-42561b70-2f2f-4c7a-add6-d4fd82389809 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:12.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-717" for this suite. 08/11/23 15:13:12.635
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:12.641
Aug 11 15:13:12.641: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 15:13:12.641
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:12.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:12.658
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/11/23 15:13:12.66
Aug 11 15:13:12.667: INFO: Waiting up to 5m0s for pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809" in namespace "downward-api-9874" to be "running and ready"
Aug 11 15:13:12.670: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.354533ms
Aug 11 15:13:12.670: INFO: The phase of Pod annotationupdate2852711a-7dd0-402e-a433-db18ab851809 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:13:14.673: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809": Phase="Running", Reason="", readiness=true. Elapsed: 2.005591453s
Aug 11 15:13:14.673: INFO: The phase of Pod annotationupdate2852711a-7dd0-402e-a433-db18ab851809 is Running (Ready = true)
Aug 11 15:13:14.673: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809" satisfied condition "running and ready"
Aug 11 15:13:15.192: INFO: Successfully updated pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:17.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9874" for this suite. 08/11/23 15:13:17.211
------------------------------
• [4.575 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:12.641
    Aug 11 15:13:12.641: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:13:12.641
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:12.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:12.658
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/11/23 15:13:12.66
    Aug 11 15:13:12.667: INFO: Waiting up to 5m0s for pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809" in namespace "downward-api-9874" to be "running and ready"
    Aug 11 15:13:12.670: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.354533ms
    Aug 11 15:13:12.670: INFO: The phase of Pod annotationupdate2852711a-7dd0-402e-a433-db18ab851809 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:13:14.673: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809": Phase="Running", Reason="", readiness=true. Elapsed: 2.005591453s
    Aug 11 15:13:14.673: INFO: The phase of Pod annotationupdate2852711a-7dd0-402e-a433-db18ab851809 is Running (Ready = true)
    Aug 11 15:13:14.673: INFO: Pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809" satisfied condition "running and ready"
    Aug 11 15:13:15.192: INFO: Successfully updated pod "annotationupdate2852711a-7dd0-402e-a433-db18ab851809"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:17.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9874" for this suite. 08/11/23 15:13:17.211
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:17.216
Aug 11 15:13:17.216: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:13:17.217
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:17.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:17.231
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 11 15:13:17.234: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:13:19.207
Aug 11 15:13:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 create -f -'
Aug 11 15:13:19.777: INFO: stderr: ""
Aug 11 15:13:19.777: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 11 15:13:19.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 delete e2e-test-crd-publish-openapi-4388-crds test-cr'
Aug 11 15:13:19.830: INFO: stderr: ""
Aug 11 15:13:19.830: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 11 15:13:19.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 apply -f -'
Aug 11 15:13:20.329: INFO: stderr: ""
Aug 11 15:13:20.329: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 11 15:13:20.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 delete e2e-test-crd-publish-openapi-4388-crds test-cr'
Aug 11 15:13:20.385: INFO: stderr: ""
Aug 11 15:13:20.385: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/11/23 15:13:20.385
Aug 11 15:13:20.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 explain e2e-test-crd-publish-openapi-4388-crds'
Aug 11 15:13:20.865: INFO: stderr: ""
Aug 11 15:13:20.865: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4388-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:22.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7701" for this suite. 08/11/23 15:13:22.824
------------------------------
• [SLOW TEST] [5.616 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:17.216
    Aug 11 15:13:17.216: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:13:17.217
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:17.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:17.231
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 11 15:13:17.234: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:13:19.207
    Aug 11 15:13:19.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 create -f -'
    Aug 11 15:13:19.777: INFO: stderr: ""
    Aug 11 15:13:19.777: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 11 15:13:19.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 delete e2e-test-crd-publish-openapi-4388-crds test-cr'
    Aug 11 15:13:19.830: INFO: stderr: ""
    Aug 11 15:13:19.830: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 11 15:13:19.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 apply -f -'
    Aug 11 15:13:20.329: INFO: stderr: ""
    Aug 11 15:13:20.329: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 11 15:13:20.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 --namespace=crd-publish-openapi-7701 delete e2e-test-crd-publish-openapi-4388-crds test-cr'
    Aug 11 15:13:20.385: INFO: stderr: ""
    Aug 11 15:13:20.385: INFO: stdout: "e2e-test-crd-publish-openapi-4388-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/11/23 15:13:20.385
    Aug 11 15:13:20.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-7701 explain e2e-test-crd-publish-openapi-4388-crds'
    Aug 11 15:13:20.865: INFO: stderr: ""
    Aug 11 15:13:20.865: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4388-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:22.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7701" for this suite. 08/11/23 15:13:22.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:22.833
Aug 11 15:13:22.833: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:13:22.834
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:22.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:22.851
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 15:13:22.853
Aug 11 15:13:22.861: INFO: Waiting up to 5m0s for pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc" in namespace "emptydir-8432" to be "Succeeded or Failed"
Aug 11 15:13:22.865: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.835734ms
Aug 11 15:13:24.870: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008438075s
Aug 11 15:13:26.869: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008077779s
STEP: Saw pod success 08/11/23 15:13:26.869
Aug 11 15:13:26.869: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc" satisfied condition "Succeeded or Failed"
Aug 11 15:13:26.872: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc container test-container: <nil>
STEP: delete the pod 08/11/23 15:13:26.892
Aug 11 15:13:26.908: INFO: Waiting for pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc to disappear
Aug 11 15:13:26.910: INFO: Pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:26.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8432" for this suite. 08/11/23 15:13:26.914
------------------------------
• [4.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:22.833
    Aug 11 15:13:22.833: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:13:22.834
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:22.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:22.851
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 15:13:22.853
    Aug 11 15:13:22.861: INFO: Waiting up to 5m0s for pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc" in namespace "emptydir-8432" to be "Succeeded or Failed"
    Aug 11 15:13:22.865: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.835734ms
    Aug 11 15:13:24.870: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008438075s
    Aug 11 15:13:26.869: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008077779s
    STEP: Saw pod success 08/11/23 15:13:26.869
    Aug 11 15:13:26.869: INFO: Pod "pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc" satisfied condition "Succeeded or Failed"
    Aug 11 15:13:26.872: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc container test-container: <nil>
    STEP: delete the pod 08/11/23 15:13:26.892
    Aug 11 15:13:26.908: INFO: Waiting for pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc to disappear
    Aug 11 15:13:26.910: INFO: Pod pod-15405d53-9e3a-4a02-92fb-4bba7e43f9bc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:26.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8432" for this suite. 08/11/23 15:13:26.914
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:26.921
Aug 11 15:13:26.921: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 15:13:26.921
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:26.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:26.937
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:13:26.94
Aug 11 15:13:26.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834" in namespace "projected-7762" to be "Succeeded or Failed"
Aug 11 15:13:26.952: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Pending", Reason="", readiness=false. Elapsed: 3.786244ms
Aug 11 15:13:28.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008259434s
Aug 11 15:13:30.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00799192s
STEP: Saw pod success 08/11/23 15:13:30.957
Aug 11 15:13:30.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834" satisfied condition "Succeeded or Failed"
Aug 11 15:13:30.960: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 container client-container: <nil>
STEP: delete the pod 08/11/23 15:13:30.968
Aug 11 15:13:30.979: INFO: Waiting for pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 to disappear
Aug 11 15:13:30.982: INFO: Pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:30.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7762" for this suite. 08/11/23 15:13:30.986
------------------------------
• [4.072 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:26.921
    Aug 11 15:13:26.921: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 15:13:26.921
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:26.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:26.937
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:13:26.94
    Aug 11 15:13:26.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834" in namespace "projected-7762" to be "Succeeded or Failed"
    Aug 11 15:13:26.952: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Pending", Reason="", readiness=false. Elapsed: 3.786244ms
    Aug 11 15:13:28.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008259434s
    Aug 11 15:13:30.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00799192s
    STEP: Saw pod success 08/11/23 15:13:30.957
    Aug 11 15:13:30.957: INFO: Pod "downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834" satisfied condition "Succeeded or Failed"
    Aug 11 15:13:30.960: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:13:30.968
    Aug 11 15:13:30.979: INFO: Waiting for pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 to disappear
    Aug 11 15:13:30.982: INFO: Pod downwardapi-volume-06201e2f-bcb6-4d36-853f-c9ff3c1fc834 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:30.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7762" for this suite. 08/11/23 15:13:30.986
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:30.992
Aug 11 15:13:30.992: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 15:13:30.993
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:31.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:31.011
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/11/23 15:13:31.018
STEP: delete the rc 08/11/23 15:13:36.028
STEP: wait for the rc to be deleted 08/11/23 15:13:36.038
Aug 11 15:13:37.051: INFO: 88 pods remaining
Aug 11 15:13:37.051: INFO: 80 pods has nil DeletionTimestamp
Aug 11 15:13:37.051: INFO: 
Aug 11 15:13:38.052: INFO: 84 pods remaining
Aug 11 15:13:38.052: INFO: 71 pods has nil DeletionTimestamp
Aug 11 15:13:38.052: INFO: 
Aug 11 15:13:39.051: INFO: 79 pods remaining
Aug 11 15:13:39.051: INFO: 60 pods has nil DeletionTimestamp
Aug 11 15:13:39.051: INFO: 
Aug 11 15:13:40.052: INFO: 66 pods remaining
Aug 11 15:13:40.052: INFO: 40 pods has nil DeletionTimestamp
Aug 11 15:13:40.052: INFO: 
Aug 11 15:13:41.056: INFO: 58 pods remaining
Aug 11 15:13:41.056: INFO: 30 pods has nil DeletionTimestamp
Aug 11 15:13:41.056: INFO: 
Aug 11 15:13:42.052: INFO: 53 pods remaining
Aug 11 15:13:42.052: INFO: 20 pods has nil DeletionTimestamp
Aug 11 15:13:42.052: INFO: 
Aug 11 15:13:43.047: INFO: 38 pods remaining
Aug 11 15:13:43.047: INFO: 0 pods has nil DeletionTimestamp
Aug 11 15:13:43.047: INFO: 
Aug 11 15:13:44.051: INFO: 34 pods remaining
Aug 11 15:13:44.051: INFO: 0 pods has nil DeletionTimestamp
Aug 11 15:13:44.051: INFO: 
Aug 11 15:13:45.047: INFO: 22 pods remaining
Aug 11 15:13:45.047: INFO: 0 pods has nil DeletionTimestamp
Aug 11 15:13:45.047: INFO: 
Aug 11 15:13:46.046: INFO: 12 pods remaining
Aug 11 15:13:46.046: INFO: 0 pods has nil DeletionTimestamp
Aug 11 15:13:46.046: INFO: 
Aug 11 15:13:47.045: INFO: 6 pods remaining
Aug 11 15:13:47.045: INFO: 0 pods has nil DeletionTimestamp
Aug 11 15:13:47.045: INFO: 
STEP: Gathering metrics 08/11/23 15:13:48.045
Aug 11 15:13:48.082: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 15:13:48.085: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.012543ms
Aug 11 15:13:48.085: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 15:13:48.085: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 15:13:48.171: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2897" for this suite. 08/11/23 15:13:48.177
------------------------------
• [SLOW TEST] [17.193 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:30.992
    Aug 11 15:13:30.992: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 15:13:30.993
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:31.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:31.011
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/11/23 15:13:31.018
    STEP: delete the rc 08/11/23 15:13:36.028
    STEP: wait for the rc to be deleted 08/11/23 15:13:36.038
    Aug 11 15:13:37.051: INFO: 88 pods remaining
    Aug 11 15:13:37.051: INFO: 80 pods has nil DeletionTimestamp
    Aug 11 15:13:37.051: INFO: 
    Aug 11 15:13:38.052: INFO: 84 pods remaining
    Aug 11 15:13:38.052: INFO: 71 pods has nil DeletionTimestamp
    Aug 11 15:13:38.052: INFO: 
    Aug 11 15:13:39.051: INFO: 79 pods remaining
    Aug 11 15:13:39.051: INFO: 60 pods has nil DeletionTimestamp
    Aug 11 15:13:39.051: INFO: 
    Aug 11 15:13:40.052: INFO: 66 pods remaining
    Aug 11 15:13:40.052: INFO: 40 pods has nil DeletionTimestamp
    Aug 11 15:13:40.052: INFO: 
    Aug 11 15:13:41.056: INFO: 58 pods remaining
    Aug 11 15:13:41.056: INFO: 30 pods has nil DeletionTimestamp
    Aug 11 15:13:41.056: INFO: 
    Aug 11 15:13:42.052: INFO: 53 pods remaining
    Aug 11 15:13:42.052: INFO: 20 pods has nil DeletionTimestamp
    Aug 11 15:13:42.052: INFO: 
    Aug 11 15:13:43.047: INFO: 38 pods remaining
    Aug 11 15:13:43.047: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 15:13:43.047: INFO: 
    Aug 11 15:13:44.051: INFO: 34 pods remaining
    Aug 11 15:13:44.051: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 15:13:44.051: INFO: 
    Aug 11 15:13:45.047: INFO: 22 pods remaining
    Aug 11 15:13:45.047: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 15:13:45.047: INFO: 
    Aug 11 15:13:46.046: INFO: 12 pods remaining
    Aug 11 15:13:46.046: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 15:13:46.046: INFO: 
    Aug 11 15:13:47.045: INFO: 6 pods remaining
    Aug 11 15:13:47.045: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 15:13:47.045: INFO: 
    STEP: Gathering metrics 08/11/23 15:13:48.045
    Aug 11 15:13:48.082: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 15:13:48.085: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.012543ms
    Aug 11 15:13:48.085: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 15:13:48.085: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 15:13:48.171: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:48.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2897" for this suite. 08/11/23 15:13:48.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:48.188
Aug 11 15:13:48.188: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 15:13:48.188
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:48.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:48.208
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/11/23 15:13:48.211
STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:13:48.217
STEP: delete the deployment 08/11/23 15:13:48.724
STEP: wait for all rs to be garbage collected 08/11/23 15:13:48.73
STEP: expected 0 rs, got 1 rs 08/11/23 15:13:48.735
STEP: expected 0 pods, got 2 pods 08/11/23 15:13:48.738
STEP: Gathering metrics 08/11/23 15:13:49.247
Aug 11 15:13:49.269: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 15:13:49.272: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.940422ms
Aug 11 15:13:49.272: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 15:13:49.272: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 15:13:49.332: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:49.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3892" for this suite. 08/11/23 15:13:49.336
------------------------------
• [1.156 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:48.188
    Aug 11 15:13:48.188: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 15:13:48.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:48.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:48.208
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/11/23 15:13:48.211
    STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:13:48.217
    STEP: delete the deployment 08/11/23 15:13:48.724
    STEP: wait for all rs to be garbage collected 08/11/23 15:13:48.73
    STEP: expected 0 rs, got 1 rs 08/11/23 15:13:48.735
    STEP: expected 0 pods, got 2 pods 08/11/23 15:13:48.738
    STEP: Gathering metrics 08/11/23 15:13:49.247
    Aug 11 15:13:49.269: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 15:13:49.272: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.940422ms
    Aug 11 15:13:49.272: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 15:13:49.272: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 15:13:49.332: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:49.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3892" for this suite. 08/11/23 15:13:49.336
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:49.344
Aug 11 15:13:49.344: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 15:13:49.344
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:49.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:49.363
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-9431c451-94a7-420b-a1da-260208fbf4ce 08/11/23 15:13:49.365
STEP: Creating secret with name secret-projected-all-test-volume-0845e062-88ba-4aa4-b111-557331239d8e 08/11/23 15:13:49.37
STEP: Creating a pod to test Check all projections for projected volume plugin 08/11/23 15:13:49.374
Aug 11 15:13:49.385: INFO: Waiting up to 5m0s for pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7" in namespace "projected-2670" to be "Succeeded or Failed"
Aug 11 15:13:49.389: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069864ms
Aug 11 15:13:51.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008364123s
Aug 11 15:13:53.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007985381s
Aug 11 15:13:55.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008370348s
Aug 11 15:13:57.394: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008950655s
STEP: Saw pod success 08/11/23 15:13:57.394
Aug 11 15:13:57.394: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7" satisfied condition "Succeeded or Failed"
Aug 11 15:13:57.398: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 container projected-all-volume-test: <nil>
STEP: delete the pod 08/11/23 15:13:57.411
Aug 11 15:13:57.426: INFO: Waiting for pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 to disappear
Aug 11 15:13:57.428: INFO: Pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 11 15:13:57.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2670" for this suite. 08/11/23 15:13:57.432
------------------------------
• [SLOW TEST] [8.095 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:49.344
    Aug 11 15:13:49.344: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 15:13:49.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:49.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:49.363
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-9431c451-94a7-420b-a1da-260208fbf4ce 08/11/23 15:13:49.365
    STEP: Creating secret with name secret-projected-all-test-volume-0845e062-88ba-4aa4-b111-557331239d8e 08/11/23 15:13:49.37
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/11/23 15:13:49.374
    Aug 11 15:13:49.385: INFO: Waiting up to 5m0s for pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7" in namespace "projected-2670" to be "Succeeded or Failed"
    Aug 11 15:13:49.389: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069864ms
    Aug 11 15:13:51.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008364123s
    Aug 11 15:13:53.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007985381s
    Aug 11 15:13:55.393: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008370348s
    Aug 11 15:13:57.394: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.008950655s
    STEP: Saw pod success 08/11/23 15:13:57.394
    Aug 11 15:13:57.394: INFO: Pod "projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7" satisfied condition "Succeeded or Failed"
    Aug 11 15:13:57.398: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 container projected-all-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:13:57.411
    Aug 11 15:13:57.426: INFO: Waiting for pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 to disappear
    Aug 11 15:13:57.428: INFO: Pod projected-volume-b6c0f139-5baf-498f-9d48-be4394f3a9c7 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:13:57.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2670" for this suite. 08/11/23 15:13:57.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:13:57.44
Aug 11 15:13:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replicaset 08/11/23 15:13:57.44
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:57.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:57.458
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/11/23 15:13:57.46
Aug 11 15:13:57.469: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1910" to be "running and ready"
Aug 11 15:13:57.475: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.763406ms
Aug 11 15:13:57.475: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:13:59.479: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010475136s
Aug 11 15:13:59.479: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:14:01.479: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.010609723s
Aug 11 15:14:01.479: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 11 15:14:01.479: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/11/23 15:14:01.483
STEP: Then the orphan pod is adopted 08/11/23 15:14:01.489
STEP: When the matched label of one of its pods change 08/11/23 15:14:02.497
Aug 11 15:14:02.499: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/11/23 15:14:02.51
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:03.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1910" for this suite. 08/11/23 15:14:03.521
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:13:57.44
    Aug 11 15:13:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:13:57.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:13:57.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:13:57.458
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/11/23 15:13:57.46
    Aug 11 15:13:57.469: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1910" to be "running and ready"
    Aug 11 15:13:57.475: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.763406ms
    Aug 11 15:13:57.475: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:13:59.479: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010475136s
    Aug 11 15:13:59.479: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:14:01.479: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.010609723s
    Aug 11 15:14:01.479: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 11 15:14:01.479: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/11/23 15:14:01.483
    STEP: Then the orphan pod is adopted 08/11/23 15:14:01.489
    STEP: When the matched label of one of its pods change 08/11/23 15:14:02.497
    Aug 11 15:14:02.499: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/11/23 15:14:02.51
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:03.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1910" for this suite. 08/11/23 15:14:03.521
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:03.528
Aug 11 15:14:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename subpath 08/11/23 15:14:03.529
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:03.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:03.547
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 15:14:03.549
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-75sx 08/11/23 15:14:03.558
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:14:03.558
Aug 11 15:14:03.566: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-75sx" in namespace "subpath-8303" to be "Succeeded or Failed"
Aug 11 15:14:03.571: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.237235ms
Aug 11 15:14:05.575: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009235165s
Aug 11 15:14:07.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 4.009848963s
Aug 11 15:14:09.577: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 6.010593411s
Aug 11 15:14:11.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 8.010063528s
Aug 11 15:14:13.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009365493s
Aug 11 15:14:15.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 12.00982841s
Aug 11 15:14:17.575: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 14.009147445s
Aug 11 15:14:19.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 16.009701662s
Aug 11 15:14:21.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 18.009469469s
Aug 11 15:14:23.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 20.010053445s
Aug 11 15:14:25.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=false. Elapsed: 22.010236652s
Aug 11 15:14:27.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009315298s
STEP: Saw pod success 08/11/23 15:14:27.576
Aug 11 15:14:27.576: INFO: Pod "pod-subpath-test-projected-75sx" satisfied condition "Succeeded or Failed"
Aug 11 15:14:27.579: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-projected-75sx container test-container-subpath-projected-75sx: <nil>
STEP: delete the pod 08/11/23 15:14:27.588
Aug 11 15:14:27.602: INFO: Waiting for pod pod-subpath-test-projected-75sx to disappear
Aug 11 15:14:27.605: INFO: Pod pod-subpath-test-projected-75sx no longer exists
STEP: Deleting pod pod-subpath-test-projected-75sx 08/11/23 15:14:27.605
Aug 11 15:14:27.605: INFO: Deleting pod "pod-subpath-test-projected-75sx" in namespace "subpath-8303"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:27.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8303" for this suite. 08/11/23 15:14:27.611
------------------------------
• [SLOW TEST] [24.089 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:03.528
    Aug 11 15:14:03.528: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename subpath 08/11/23 15:14:03.529
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:03.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:03.547
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 15:14:03.549
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-75sx 08/11/23 15:14:03.558
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:14:03.558
    Aug 11 15:14:03.566: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-75sx" in namespace "subpath-8303" to be "Succeeded or Failed"
    Aug 11 15:14:03.571: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.237235ms
    Aug 11 15:14:05.575: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009235165s
    Aug 11 15:14:07.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 4.009848963s
    Aug 11 15:14:09.577: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 6.010593411s
    Aug 11 15:14:11.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 8.010063528s
    Aug 11 15:14:13.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009365493s
    Aug 11 15:14:15.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 12.00982841s
    Aug 11 15:14:17.575: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 14.009147445s
    Aug 11 15:14:19.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 16.009701662s
    Aug 11 15:14:21.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 18.009469469s
    Aug 11 15:14:23.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=true. Elapsed: 20.010053445s
    Aug 11 15:14:25.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Running", Reason="", readiness=false. Elapsed: 22.010236652s
    Aug 11 15:14:27.576: INFO: Pod "pod-subpath-test-projected-75sx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009315298s
    STEP: Saw pod success 08/11/23 15:14:27.576
    Aug 11 15:14:27.576: INFO: Pod "pod-subpath-test-projected-75sx" satisfied condition "Succeeded or Failed"
    Aug 11 15:14:27.579: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-projected-75sx container test-container-subpath-projected-75sx: <nil>
    STEP: delete the pod 08/11/23 15:14:27.588
    Aug 11 15:14:27.602: INFO: Waiting for pod pod-subpath-test-projected-75sx to disappear
    Aug 11 15:14:27.605: INFO: Pod pod-subpath-test-projected-75sx no longer exists
    STEP: Deleting pod pod-subpath-test-projected-75sx 08/11/23 15:14:27.605
    Aug 11 15:14:27.605: INFO: Deleting pod "pod-subpath-test-projected-75sx" in namespace "subpath-8303"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:27.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8303" for this suite. 08/11/23 15:14:27.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:27.619
Aug 11 15:14:27.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-runtime 08/11/23 15:14:27.62
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:27.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:27.636
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/11/23 15:14:27.638
STEP: wait for the container to reach Failed 08/11/23 15:14:27.646
STEP: get the container status 08/11/23 15:14:30.663
STEP: the container should be terminated 08/11/23 15:14:30.666
STEP: the termination message should be set 08/11/23 15:14:30.666
Aug 11 15:14:30.666: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/11/23 15:14:30.666
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8153" for this suite. 08/11/23 15:14:30.687
------------------------------
• [3.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:27.619
    Aug 11 15:14:27.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-runtime 08/11/23 15:14:27.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:27.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:27.636
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/11/23 15:14:27.638
    STEP: wait for the container to reach Failed 08/11/23 15:14:27.646
    STEP: get the container status 08/11/23 15:14:30.663
    STEP: the container should be terminated 08/11/23 15:14:30.666
    STEP: the termination message should be set 08/11/23 15:14:30.666
    Aug 11 15:14:30.666: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/11/23 15:14:30.666
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:30.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8153" for this suite. 08/11/23 15:14:30.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:30.694
Aug 11 15:14:30.694: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 15:14:30.694
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:30.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:30.713
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 11 15:14:30.715: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:37.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5554" for this suite. 08/11/23 15:14:37.015
------------------------------
• [SLOW TEST] [6.329 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:30.694
    Aug 11 15:14:30.694: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 15:14:30.694
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:30.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:30.713
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 11 15:14:30.715: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:37.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5554" for this suite. 08/11/23 15:14:37.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:37.023
Aug 11 15:14:37.023: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename job 08/11/23 15:14:37.024
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:37.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:37.043
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/11/23 15:14:37.048
STEP: Patching the Job 08/11/23 15:14:37.054
STEP: Watching for Job to be patched 08/11/23 15:14:37.071
Aug 11 15:14:37.072: INFO: Event ADDED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 11 15:14:37.072: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 11 15:14:37.072: INFO: Event MODIFIED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/11/23 15:14:37.072
STEP: Watching for Job to be updated 08/11/23 15:14:37.081
Aug 11 15:14:37.083: INFO: Event MODIFIED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:37.083: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/11/23 15:14:37.083
Aug 11 15:14:37.088: INFO: Job: e2e-rf944 as labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched]
STEP: Waiting for job to complete 08/11/23 15:14:37.088
STEP: Delete a job collection with a labelselector 08/11/23 15:14:47.092
STEP: Watching for Job to be deleted 08/11/23 15:14:47.101
Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.103: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.103: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 15:14:47.103: INFO: Event DELETED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/11/23 15:14:47.103
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:47.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1705" for this suite. 08/11/23 15:14:47.111
------------------------------
• [SLOW TEST] [10.104 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:37.023
    Aug 11 15:14:37.023: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename job 08/11/23 15:14:37.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:37.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:37.043
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/11/23 15:14:37.048
    STEP: Patching the Job 08/11/23 15:14:37.054
    STEP: Watching for Job to be patched 08/11/23 15:14:37.071
    Aug 11 15:14:37.072: INFO: Event ADDED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 11 15:14:37.072: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 11 15:14:37.072: INFO: Event MODIFIED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/11/23 15:14:37.072
    STEP: Watching for Job to be updated 08/11/23 15:14:37.081
    Aug 11 15:14:37.083: INFO: Event MODIFIED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:37.083: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/11/23 15:14:37.083
    Aug 11 15:14:37.088: INFO: Job: e2e-rf944 as labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched]
    STEP: Waiting for job to complete 08/11/23 15:14:37.088
    STEP: Delete a job collection with a labelselector 08/11/23 15:14:47.092
    STEP: Watching for Job to be deleted 08/11/23 15:14:47.101
    Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.102: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.103: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.103: INFO: Event MODIFIED observed for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 15:14:47.103: INFO: Event DELETED found for Job e2e-rf944 in namespace job-1705 with labels: map[e2e-job-label:e2e-rf944 e2e-rf944:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/11/23 15:14:47.103
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:47.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1705" for this suite. 08/11/23 15:14:47.111
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:47.127
Aug 11 15:14:47.127: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 15:14:47.128
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:47.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:47.144
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-b301b4c4-db2b-4c4e-b10e-a95bc96b09a0 08/11/23 15:14:47.146
STEP: Creating a pod to test consume secrets 08/11/23 15:14:47.151
Aug 11 15:14:47.158: INFO: Waiting up to 5m0s for pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af" in namespace "secrets-3080" to be "Succeeded or Failed"
Aug 11 15:14:47.161: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870192ms
Aug 11 15:14:49.165: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007267821s
Aug 11 15:14:51.166: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007822779s
STEP: Saw pod success 08/11/23 15:14:51.166
Aug 11 15:14:51.166: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af" satisfied condition "Succeeded or Failed"
Aug 11 15:14:51.169: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af container secret-env-test: <nil>
STEP: delete the pod 08/11/23 15:14:51.179
Aug 11 15:14:51.191: INFO: Waiting for pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af to disappear
Aug 11 15:14:51.194: INFO: Pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:51.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3080" for this suite. 08/11/23 15:14:51.198
------------------------------
• [4.077 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:47.127
    Aug 11 15:14:47.127: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 15:14:47.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:47.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:47.144
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-b301b4c4-db2b-4c4e-b10e-a95bc96b09a0 08/11/23 15:14:47.146
    STEP: Creating a pod to test consume secrets 08/11/23 15:14:47.151
    Aug 11 15:14:47.158: INFO: Waiting up to 5m0s for pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af" in namespace "secrets-3080" to be "Succeeded or Failed"
    Aug 11 15:14:47.161: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870192ms
    Aug 11 15:14:49.165: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007267821s
    Aug 11 15:14:51.166: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007822779s
    STEP: Saw pod success 08/11/23 15:14:51.166
    Aug 11 15:14:51.166: INFO: Pod "pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af" satisfied condition "Succeeded or Failed"
    Aug 11 15:14:51.169: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af container secret-env-test: <nil>
    STEP: delete the pod 08/11/23 15:14:51.179
    Aug 11 15:14:51.191: INFO: Waiting for pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af to disappear
    Aug 11 15:14:51.194: INFO: Pod pod-secrets-a6845058-ba22-4ae4-921c-947b6c1e67af no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:51.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3080" for this suite. 08/11/23 15:14:51.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:51.205
Aug 11 15:14:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 15:14:51.206
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:51.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:51.224
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7640 08/11/23 15:14:51.227
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 15:14:51.245
STEP: creating service externalsvc in namespace services-7640 08/11/23 15:14:51.245
STEP: creating replication controller externalsvc in namespace services-7640 08/11/23 15:14:51.262
I0811 15:14:51.274942      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7640, replica count: 2
I0811 15:14:54.325599      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/11/23 15:14:54.329
Aug 11 15:14:54.353: INFO: Creating new exec pod
Aug 11 15:14:54.362: INFO: Waiting up to 5m0s for pod "execpodtzgb4" in namespace "services-7640" to be "running"
Aug 11 15:14:54.365: INFO: Pod "execpodtzgb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638232ms
Aug 11 15:14:56.369: INFO: Pod "execpodtzgb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006590122s
Aug 11 15:14:56.369: INFO: Pod "execpodtzgb4" satisfied condition "running"
Aug 11 15:14:56.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7640 exec execpodtzgb4 -- /bin/sh -x -c nslookup nodeport-service.services-7640.svc.cluster.local'
Aug 11 15:14:56.526: INFO: stderr: "+ nslookup nodeport-service.services-7640.svc.cluster.local\n"
Aug 11 15:14:56.526: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7640.svc.cluster.local\tcanonical name = externalsvc.services-7640.svc.cluster.local.\nName:\texternalsvc.services-7640.svc.cluster.local\nAddress: 10.101.205.134\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7640, will wait for the garbage collector to delete the pods 08/11/23 15:14:56.527
Aug 11 15:14:56.587: INFO: Deleting ReplicationController externalsvc took: 6.695527ms
Aug 11 15:14:56.688: INFO: Terminating ReplicationController externalsvc pods took: 100.678368ms
Aug 11 15:14:58.712: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 15:14:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7640" for this suite. 08/11/23 15:14:58.73
------------------------------
• [SLOW TEST] [7.531 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:51.205
    Aug 11 15:14:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 15:14:51.206
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:51.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:51.224
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7640 08/11/23 15:14:51.227
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 15:14:51.245
    STEP: creating service externalsvc in namespace services-7640 08/11/23 15:14:51.245
    STEP: creating replication controller externalsvc in namespace services-7640 08/11/23 15:14:51.262
    I0811 15:14:51.274942      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7640, replica count: 2
    I0811 15:14:54.325599      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/11/23 15:14:54.329
    Aug 11 15:14:54.353: INFO: Creating new exec pod
    Aug 11 15:14:54.362: INFO: Waiting up to 5m0s for pod "execpodtzgb4" in namespace "services-7640" to be "running"
    Aug 11 15:14:54.365: INFO: Pod "execpodtzgb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.638232ms
    Aug 11 15:14:56.369: INFO: Pod "execpodtzgb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006590122s
    Aug 11 15:14:56.369: INFO: Pod "execpodtzgb4" satisfied condition "running"
    Aug 11 15:14:56.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-7640 exec execpodtzgb4 -- /bin/sh -x -c nslookup nodeport-service.services-7640.svc.cluster.local'
    Aug 11 15:14:56.526: INFO: stderr: "+ nslookup nodeport-service.services-7640.svc.cluster.local\n"
    Aug 11 15:14:56.526: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7640.svc.cluster.local\tcanonical name = externalsvc.services-7640.svc.cluster.local.\nName:\texternalsvc.services-7640.svc.cluster.local\nAddress: 10.101.205.134\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7640, will wait for the garbage collector to delete the pods 08/11/23 15:14:56.527
    Aug 11 15:14:56.587: INFO: Deleting ReplicationController externalsvc took: 6.695527ms
    Aug 11 15:14:56.688: INFO: Terminating ReplicationController externalsvc pods took: 100.678368ms
    Aug 11 15:14:58.712: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:14:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7640" for this suite. 08/11/23 15:14:58.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:14:58.737
Aug 11 15:14:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 15:14:58.738
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:58.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:58.755
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/11/23 15:14:58.757
Aug 11 15:14:58.764: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3278" to be "running and ready"
Aug 11 15:14:58.769: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042725ms
Aug 11 15:14:58.769: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:15:00.774: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202056s
Aug 11 15:15:00.774: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 11 15:15:00.774: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/11/23 15:15:00.777
STEP: Then the orphan pod is adopted 08/11/23 15:15:00.784
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 15:15:01.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3278" for this suite. 08/11/23 15:15:01.796
------------------------------
• [3.066 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:14:58.737
    Aug 11 15:14:58.737: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 15:14:58.738
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:14:58.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:14:58.755
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/11/23 15:14:58.757
    Aug 11 15:14:58.764: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3278" to be "running and ready"
    Aug 11 15:14:58.769: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.042725ms
    Aug 11 15:14:58.769: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:15:00.774: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010202056s
    Aug 11 15:15:00.774: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 11 15:15:00.774: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/11/23 15:15:00.777
    STEP: Then the orphan pod is adopted 08/11/23 15:15:00.784
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:15:01.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3278" for this suite. 08/11/23 15:15:01.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:15:01.804
Aug 11 15:15:01.804: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 15:15:01.805
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:01.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:01.825
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5635" for this suite. 08/11/23 15:16:01.846
------------------------------
• [SLOW TEST] [60.050 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:15:01.804
    Aug 11 15:15:01.804: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:15:01.805
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:01.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:01.825
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5635" for this suite. 08/11/23 15:16:01.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:01.854
Aug 11 15:16:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 15:16:01.855
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:01.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:01.877
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/11/23 15:16:01.879
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local;sleep 1; done
 08/11/23 15:16:01.883
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local;sleep 1; done
 08/11/23 15:16:01.884
STEP: creating a pod to probe DNS 08/11/23 15:16:01.884
STEP: submitting the pod to kubernetes 08/11/23 15:16:01.884
Aug 11 15:16:01.896: INFO: Waiting up to 15m0s for pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa" in namespace "dns-4508" to be "running"
Aug 11 15:16:01.903: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.955927ms
Aug 11 15:16:03.909: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012391409s
Aug 11 15:16:03.909: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:16:03.909
STEP: looking for the results for each expected name from probers 08/11/23 15:16:03.914
Aug 11 15:16:03.927: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.933: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.939: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.947: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.953: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.959: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.972: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:03.972: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:08.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:08.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:08.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:08.998: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:09.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:09.010: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:09.015: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:09.021: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:09.021: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:13.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:13.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:13.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:13.999: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:14.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:14.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:14.017: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:14.023: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:14.023: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:18.979: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:18.984: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:18.990: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:18.996: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:19.003: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:19.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:19.014: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:19.020: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:19.020: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:23.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:23.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:23.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:23.998: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:24.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:24.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:24.017: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:24.023: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:24.023: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:28.979: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:28.985: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:28.991: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:28.997: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:29.003: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:29.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:29.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:29.022: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
Aug 11 15:16:29.022: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

Aug 11 15:16:34.020: INFO: DNS probes using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa succeeded

STEP: deleting the pod 08/11/23 15:16:34.02
STEP: deleting the test headless service 08/11/23 15:16:34.036
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:34.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4508" for this suite. 08/11/23 15:16:34.056
------------------------------
• [SLOW TEST] [32.209 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:01.854
    Aug 11 15:16:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 15:16:01.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:01.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:01.877
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/11/23 15:16:01.879
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local;sleep 1; done
     08/11/23 15:16:01.883
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4508.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local;sleep 1; done
     08/11/23 15:16:01.884
    STEP: creating a pod to probe DNS 08/11/23 15:16:01.884
    STEP: submitting the pod to kubernetes 08/11/23 15:16:01.884
    Aug 11 15:16:01.896: INFO: Waiting up to 15m0s for pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa" in namespace "dns-4508" to be "running"
    Aug 11 15:16:01.903: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.955927ms
    Aug 11 15:16:03.909: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012391409s
    Aug 11 15:16:03.909: INFO: Pod "dns-test-9b894df6-287b-4bc9-945c-7155639a61fa" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:16:03.909
    STEP: looking for the results for each expected name from probers 08/11/23 15:16:03.914
    Aug 11 15:16:03.927: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.933: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.939: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.947: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.953: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.959: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.972: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:03.972: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:08.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:08.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:08.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:08.998: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:09.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:09.010: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:09.015: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:09.021: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:09.021: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:13.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:13.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:13.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:13.999: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:14.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:14.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:14.017: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:14.023: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:14.023: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:18.979: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:18.984: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:18.990: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:18.996: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:19.003: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:19.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:19.014: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:19.020: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:19.020: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:23.981: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:23.987: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:23.993: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:23.998: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:24.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:24.011: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:24.017: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:24.023: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:24.023: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:28.979: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:28.985: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:28.991: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:28.997: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:29.003: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:29.009: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:29.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:29.022: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local from pod dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa: the server could not find the requested resource (get pods dns-test-9b894df6-287b-4bc9-945c-7155639a61fa)
    Aug 11 15:16:29.022: INFO: Lookups using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4508.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4508.svc.cluster.local jessie_udp@dns-test-service-2.dns-4508.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4508.svc.cluster.local]

    Aug 11 15:16:34.020: INFO: DNS probes using dns-4508/dns-test-9b894df6-287b-4bc9-945c-7155639a61fa succeeded

    STEP: deleting the pod 08/11/23 15:16:34.02
    STEP: deleting the test headless service 08/11/23 15:16:34.036
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:34.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4508" for this suite. 08/11/23 15:16:34.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:34.065
Aug 11 15:16:34.065: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename events 08/11/23 15:16:34.065
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:34.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:34.084
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/11/23 15:16:34.086
STEP: listing all events in all namespaces 08/11/23 15:16:34.093
STEP: patching the test event 08/11/23 15:16:34.097
STEP: fetching the test event 08/11/23 15:16:34.104
STEP: updating the test event 08/11/23 15:16:34.109
STEP: getting the test event 08/11/23 15:16:34.118
STEP: deleting the test event 08/11/23 15:16:34.121
STEP: listing all events in all namespaces 08/11/23 15:16:34.128
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6482" for this suite. 08/11/23 15:16:34.136
------------------------------
• [0.078 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:34.065
    Aug 11 15:16:34.065: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename events 08/11/23 15:16:34.065
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:34.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:34.084
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/11/23 15:16:34.086
    STEP: listing all events in all namespaces 08/11/23 15:16:34.093
    STEP: patching the test event 08/11/23 15:16:34.097
    STEP: fetching the test event 08/11/23 15:16:34.104
    STEP: updating the test event 08/11/23 15:16:34.109
    STEP: getting the test event 08/11/23 15:16:34.118
    STEP: deleting the test event 08/11/23 15:16:34.121
    STEP: listing all events in all namespaces 08/11/23 15:16:34.128
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6482" for this suite. 08/11/23 15:16:34.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:34.142
Aug 11 15:16:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 15:16:34.143
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:34.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:34.16
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 15:16:34.175
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:16:34.45
STEP: Deploying the webhook pod 08/11/23 15:16:34.459
STEP: Wait for the deployment to be ready 08/11/23 15:16:34.472
Aug 11 15:16:34.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:16:36.489
STEP: Verifying the service has paired with the endpoint 08/11/23 15:16:36.505
Aug 11 15:16:37.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/11/23 15:16:37.509
STEP: Creating a custom resource definition that should be denied by the webhook 08/11/23 15:16:37.532
Aug 11 15:16:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:37.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5673" for this suite. 08/11/23 15:16:37.605
STEP: Destroying namespace "webhook-5673-markers" for this suite. 08/11/23 15:16:37.616
------------------------------
• [3.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:34.142
    Aug 11 15:16:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 15:16:34.143
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:34.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:34.16
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 15:16:34.175
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:16:34.45
    STEP: Deploying the webhook pod 08/11/23 15:16:34.459
    STEP: Wait for the deployment to be ready 08/11/23 15:16:34.472
    Aug 11 15:16:34.479: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:16:36.489
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:16:36.505
    Aug 11 15:16:37.505: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/11/23 15:16:37.509
    STEP: Creating a custom resource definition that should be denied by the webhook 08/11/23 15:16:37.532
    Aug 11 15:16:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:37.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5673" for this suite. 08/11/23 15:16:37.605
    STEP: Destroying namespace "webhook-5673-markers" for this suite. 08/11/23 15:16:37.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:37.623
Aug 11 15:16:37.623: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 15:16:37.624
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:37.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:37.643
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 11 15:16:37.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 create -f -'
Aug 11 15:16:38.434: INFO: stderr: ""
Aug 11 15:16:38.434: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 11 15:16:38.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 create -f -'
Aug 11 15:16:39.232: INFO: stderr: ""
Aug 11 15:16:39.232: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 15:16:39.232
Aug 11 15:16:40.236: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:16:40.236: INFO: Found 1 / 1
Aug 11 15:16:40.236: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 11 15:16:40.239: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:16:40.239: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 15:16:40.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe pod agnhost-primary-p6pgv'
Aug 11 15:16:40.299: INFO: stderr: ""
Aug 11 15:16:40.299: INFO: stdout: "Name:             agnhost-primary-p6pgv\nNamespace:        kubectl-9840\nPriority:         0\nService Account:  default\nNode:             constell-1cf5d931-worker-6381a7ba-nd80/192.168.178.3\nStart Time:       Fri, 11 Aug 2023 15:16:38 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.1.94\nIPs:\n  IP:           10.10.1.94\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d423628440d2fe2f2b489e4c80a34a030ddcd1e1e9465c7d5d49af944e4a198f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Aug 2023 15:16:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d4qcb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d4qcb:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9840/agnhost-primary-p6pgv to constell-1cf5d931-worker-6381a7ba-nd80\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Aug 11 15:16:40.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe rc agnhost-primary'
Aug 11 15:16:40.358: INFO: stderr: ""
Aug 11 15:16:40.358: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9840\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-p6pgv\n"
Aug 11 15:16:40.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe service agnhost-primary'
Aug 11 15:16:40.412: INFO: stderr: ""
Aug 11 15:16:40.412: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9840\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.225.198\nIPs:               10.106.225.198\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.1.94:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 11 15:16:40.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe node constell-1cf5d931-control-plane-bb71bd41-clkl'
Aug 11 15:16:40.489: INFO: stderr: ""
Aug 11 15:16:40.489: INFO: stdout: "Name:               constell-1cf5d931-control-plane-bb71bd41-clkl\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n2d-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west3\n                    failure-domain.beta.kubernetes.io/zone=europe-west3-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=constell-1cf5d931-control-plane-bb71bd41-clkl\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=n2d-standard-4\n                    topology.gke.io/zone=europe-west3-b\n                    topology.kubernetes.io/region=europe-west3\n                    topology.kubernetes.io/zone=europe-west3-b\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.178.6\n                    constellation.edgeless.systems/kubernetes-components: k8s-components-sha256-405fe26a24a309707216ff06cefa149bea4c56aa2737f0dd8d5f8ce1aebaa333\n                    constellation.edgeless.systems/node-image: projects/constellation-images/global/images/v2-9-1-gcp-sev-es-stable\n                    constellation.edgeless.systems/scaling-group-id:\n                      projects/constellation-331613/zones/europe-west3-b/instanceGroupManagers/constell-1cf5d931-control-plane-bb71bd41\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"gcp.csi.confidential.cloud\":\"projects/constellation-331613/zones/europe-west3-b/instances/constell-1cf5d931-control-plane-bb71bd41-clkl\"...\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 11 Aug 2023 13:56:34 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  constell-1cf5d931-control-plane-bb71bd41-clkl\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Aug 2023 15:16:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 11 Aug 2023 13:57:00 +0000   Fri, 11 Aug 2023 13:57:00 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.178.6\n  Hostname:    constell-1cf5d931-control-plane-bb71bd41-clkl\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30467368Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15365128Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  28078726303\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15262728Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c3cb456a32826dff3710349f4837d6ee\n  System UUID:                c3cb456a-3282-6dff-3710-349f4837d6ee\n  Boot ID:                    3ce98dc3-8cb4-4015-8519-97853cf6d1a0\n  Kernel Version:             6.3.12-200.fc38.x86_64\n  OS Image:                   Fedora Linux 38 (Thirty Eight)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      10.10.4.0/24\nPodCIDRs:                     10.10.4.0/24\nProviderID:                   gce://constellation-331613/europe-west3-b/constell-1cf5d931-control-plane-bb71bd41-clkl\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                     ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-7fd6h                                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         80m\n  kube-system                 cloud-controller-manager-wfjdt                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 coredns-9ff5c7c6f-jls5f                                                  100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     61m\n  kube-system                 csi-gce-pd-node-mvmcb                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 etcd-constell-1cf5d931-control-plane-bb71bd41-clkl                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         80m\n  kube-system                 gcp-guest-agent-wt289                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 join-service-v5896                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 key-service-w5fjd                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 konnectivity-agent-wxlg6                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 konnectivity-server-constell-1cf5d931-control-plane-bb71bd41-clkl        0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-apiserver-constell-1cf5d931-control-plane-bb71bd41-clkl             250m (6%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-clkl    200m (5%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-proxy-gxq8z                                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 kube-scheduler-constell-1cf5d931-control-plane-bb71bd41-clkl             100m (2%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 verification-service-f85qv                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-195978b949114b12-xs6sr                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (21%)  0 (0%)\n  memory             270Mi (1%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Aug 11 15:16:40.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe namespace kubectl-9840'
Aug 11 15:16:40.545: INFO: stderr: ""
Aug 11 15:16:40.545: INFO: stdout: "Name:         kubectl-9840\nLabels:       e2e-framework=kubectl\n              e2e-run=e9629551-98de-4270-b96a-6c3e0c707863\n              kubernetes.io/metadata.name=kubectl-9840\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:40.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9840" for this suite. 08/11/23 15:16:40.55
------------------------------
• [2.933 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:37.623
    Aug 11 15:16:37.623: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:16:37.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:37.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:37.643
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 11 15:16:37.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 create -f -'
    Aug 11 15:16:38.434: INFO: stderr: ""
    Aug 11 15:16:38.434: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 11 15:16:38.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 create -f -'
    Aug 11 15:16:39.232: INFO: stderr: ""
    Aug 11 15:16:39.232: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 15:16:39.232
    Aug 11 15:16:40.236: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:16:40.236: INFO: Found 1 / 1
    Aug 11 15:16:40.236: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 11 15:16:40.239: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:16:40.239: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 15:16:40.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe pod agnhost-primary-p6pgv'
    Aug 11 15:16:40.299: INFO: stderr: ""
    Aug 11 15:16:40.299: INFO: stdout: "Name:             agnhost-primary-p6pgv\nNamespace:        kubectl-9840\nPriority:         0\nService Account:  default\nNode:             constell-1cf5d931-worker-6381a7ba-nd80/192.168.178.3\nStart Time:       Fri, 11 Aug 2023 15:16:38 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.1.94\nIPs:\n  IP:           10.10.1.94\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d423628440d2fe2f2b489e4c80a34a030ddcd1e1e9465c7d5d49af944e4a198f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Aug 2023 15:16:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d4qcb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d4qcb:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9840/agnhost-primary-p6pgv to constell-1cf5d931-worker-6381a7ba-nd80\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Aug 11 15:16:40.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe rc agnhost-primary'
    Aug 11 15:16:40.358: INFO: stderr: ""
    Aug 11 15:16:40.358: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9840\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-p6pgv\n"
    Aug 11 15:16:40.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe service agnhost-primary'
    Aug 11 15:16:40.412: INFO: stderr: ""
    Aug 11 15:16:40.412: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9840\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.106.225.198\nIPs:               10.106.225.198\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.1.94:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 11 15:16:40.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe node constell-1cf5d931-control-plane-bb71bd41-clkl'
    Aug 11 15:16:40.489: INFO: stderr: ""
    Aug 11 15:16:40.489: INFO: stdout: "Name:               constell-1cf5d931-control-plane-bb71bd41-clkl\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n2d-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west3\n                    failure-domain.beta.kubernetes.io/zone=europe-west3-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=constell-1cf5d931-control-plane-bb71bd41-clkl\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=n2d-standard-4\n                    topology.gke.io/zone=europe-west3-b\n                    topology.kubernetes.io/region=europe-west3\n                    topology.kubernetes.io/zone=europe-west3-b\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.178.6\n                    constellation.edgeless.systems/kubernetes-components: k8s-components-sha256-405fe26a24a309707216ff06cefa149bea4c56aa2737f0dd8d5f8ce1aebaa333\n                    constellation.edgeless.systems/node-image: projects/constellation-images/global/images/v2-9-1-gcp-sev-es-stable\n                    constellation.edgeless.systems/scaling-group-id:\n                      projects/constellation-331613/zones/europe-west3-b/instanceGroupManagers/constell-1cf5d931-control-plane-bb71bd41\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"gcp.csi.confidential.cloud\":\"projects/constellation-331613/zones/europe-west3-b/instances/constell-1cf5d931-control-plane-bb71bd41-clkl\"...\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 11 Aug 2023 13:56:34 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  constell-1cf5d931-control-plane-bb71bd41-clkl\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Aug 2023 15:16:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 11 Aug 2023 13:57:00 +0000   Fri, 11 Aug 2023 13:57:00 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Aug 2023 15:13:49 +0000   Fri, 11 Aug 2023 13:56:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.178.6\n  Hostname:    constell-1cf5d931-control-plane-bb71bd41-clkl\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30467368Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15365128Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  28078726303\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15262728Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c3cb456a32826dff3710349f4837d6ee\n  System UUID:                c3cb456a-3282-6dff-3710-349f4837d6ee\n  Boot ID:                    3ce98dc3-8cb4-4015-8519-97853cf6d1a0\n  Kernel Version:             6.3.12-200.fc38.x86_64\n  OS Image:                   Fedora Linux 38 (Thirty Eight)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      10.10.4.0/24\nPodCIDRs:                     10.10.4.0/24\nProviderID:                   gce://constellation-331613/europe-west3-b/constell-1cf5d931-control-plane-bb71bd41-clkl\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                     ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-7fd6h                                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         80m\n  kube-system                 cloud-controller-manager-wfjdt                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 coredns-9ff5c7c6f-jls5f                                                  100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     61m\n  kube-system                 csi-gce-pd-node-mvmcb                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 etcd-constell-1cf5d931-control-plane-bb71bd41-clkl                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         80m\n  kube-system                 gcp-guest-agent-wt289                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 join-service-v5896                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 key-service-w5fjd                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 konnectivity-agent-wxlg6                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 konnectivity-server-constell-1cf5d931-control-plane-bb71bd41-clkl        0 (0%)        0 (0%)      0 (0%)           0 (0%)         78m\n  kube-system                 kube-apiserver-constell-1cf5d931-control-plane-bb71bd41-clkl             250m (6%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-clkl    200m (5%)     0 (0%)      0 (0%)           0 (0%)         79m\n  kube-system                 kube-proxy-gxq8z                                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 kube-scheduler-constell-1cf5d931-control-plane-bb71bd41-clkl             100m (2%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                 verification-service-f85qv                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-195978b949114b12-xs6sr                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (21%)  0 (0%)\n  memory             270Mi (1%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
    Aug 11 15:16:40.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-9840 describe namespace kubectl-9840'
    Aug 11 15:16:40.545: INFO: stderr: ""
    Aug 11 15:16:40.545: INFO: stdout: "Name:         kubectl-9840\nLabels:       e2e-framework=kubectl\n              e2e-run=e9629551-98de-4270-b96a-6c3e0c707863\n              kubernetes.io/metadata.name=kubectl-9840\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:40.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9840" for this suite. 08/11/23 15:16:40.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:40.557
Aug 11 15:16:40.557: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 15:16:40.558
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:40.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:40.578
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/11/23 15:16:40.58
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/11/23 15:16:40.58
STEP: creating a pod to probe DNS 08/11/23 15:16:40.58
STEP: submitting the pod to kubernetes 08/11/23 15:16:40.58
Aug 11 15:16:40.589: INFO: Waiting up to 15m0s for pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b" in namespace "dns-8153" to be "running"
Aug 11 15:16:40.594: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.134145ms
Aug 11 15:16:42.599: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009970356s
Aug 11 15:16:42.599: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:16:42.599
STEP: looking for the results for each expected name from probers 08/11/23 15:16:42.602
Aug 11 15:16:42.628: INFO: DNS probes using dns-8153/dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b succeeded

STEP: deleting the pod 08/11/23 15:16:42.628
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:42.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8153" for this suite. 08/11/23 15:16:42.648
------------------------------
• [2.098 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:40.557
    Aug 11 15:16:40.557: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 15:16:40.558
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:40.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:40.578
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/11/23 15:16:40.58
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/11/23 15:16:40.58
    STEP: creating a pod to probe DNS 08/11/23 15:16:40.58
    STEP: submitting the pod to kubernetes 08/11/23 15:16:40.58
    Aug 11 15:16:40.589: INFO: Waiting up to 15m0s for pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b" in namespace "dns-8153" to be "running"
    Aug 11 15:16:40.594: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.134145ms
    Aug 11 15:16:42.599: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009970356s
    Aug 11 15:16:42.599: INFO: Pod "dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:16:42.599
    STEP: looking for the results for each expected name from probers 08/11/23 15:16:42.602
    Aug 11 15:16:42.628: INFO: DNS probes using dns-8153/dns-test-eb4ad60f-cca2-479a-a9b4-f52c6635945b succeeded

    STEP: deleting the pod 08/11/23 15:16:42.628
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:42.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8153" for this suite. 08/11/23 15:16:42.648
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:42.656
Aug 11 15:16:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 15:16:42.657
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:42.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:42.68
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/11/23 15:16:42.683
Aug 11 15:16:42.695: INFO: Waiting up to 5m0s for pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc" in namespace "var-expansion-8483" to be "Succeeded or Failed"
Aug 11 15:16:42.708: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.345732ms
Aug 11 15:16:44.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015953219s
Aug 11 15:16:46.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015907297s
STEP: Saw pod success 08/11/23 15:16:46.711
Aug 11 15:16:46.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc" satisfied condition "Succeeded or Failed"
Aug 11 15:16:46.715: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc container dapi-container: <nil>
STEP: delete the pod 08/11/23 15:16:46.736
Aug 11 15:16:46.747: INFO: Waiting for pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc to disappear
Aug 11 15:16:46.750: INFO: Pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:46.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8483" for this suite. 08/11/23 15:16:46.754
------------------------------
• [4.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:42.656
    Aug 11 15:16:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 15:16:42.657
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:42.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:42.68
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/11/23 15:16:42.683
    Aug 11 15:16:42.695: INFO: Waiting up to 5m0s for pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc" in namespace "var-expansion-8483" to be "Succeeded or Failed"
    Aug 11 15:16:42.708: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.345732ms
    Aug 11 15:16:44.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015953219s
    Aug 11 15:16:46.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015907297s
    STEP: Saw pod success 08/11/23 15:16:46.711
    Aug 11 15:16:46.711: INFO: Pod "var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc" satisfied condition "Succeeded or Failed"
    Aug 11 15:16:46.715: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc container dapi-container: <nil>
    STEP: delete the pod 08/11/23 15:16:46.736
    Aug 11 15:16:46.747: INFO: Waiting for pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc to disappear
    Aug 11 15:16:46.750: INFO: Pod var-expansion-95b9fa0c-739e-4b10-8c3c-57c4882d75bc no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:46.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8483" for this suite. 08/11/23 15:16:46.754
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:46.762
Aug 11 15:16:46.762: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 15:16:46.763
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:46.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:46.782
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/11/23 15:16:46.784
Aug 11 15:16:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 create -f -'
Aug 11 15:16:46.968: INFO: stderr: ""
Aug 11 15:16:46.968: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/11/23 15:16:46.968
Aug 11 15:16:46.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 diff -f -'
Aug 11 15:16:47.176: INFO: rc: 1
Aug 11 15:16:47.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 delete -f -'
Aug 11 15:16:47.226: INFO: stderr: ""
Aug 11 15:16:47.226: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3405" for this suite. 08/11/23 15:16:47.232
------------------------------
• [0.480 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:46.762
    Aug 11 15:16:46.762: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:16:46.763
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:46.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:46.782
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/11/23 15:16:46.784
    Aug 11 15:16:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 create -f -'
    Aug 11 15:16:46.968: INFO: stderr: ""
    Aug 11 15:16:46.968: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/11/23 15:16:46.968
    Aug 11 15:16:46.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 diff -f -'
    Aug 11 15:16:47.176: INFO: rc: 1
    Aug 11 15:16:47.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-3405 delete -f -'
    Aug 11 15:16:47.226: INFO: stderr: ""
    Aug 11 15:16:47.226: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3405" for this suite. 08/11/23 15:16:47.232
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:47.242
Aug 11 15:16:47.242: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 15:16:47.243
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:47.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:47.26
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-a4974afa-f786-4456-9fd6-11a5ee4410ec 08/11/23 15:16:47.279
STEP: Creating a pod to test consume secrets 08/11/23 15:16:47.284
Aug 11 15:16:47.295: INFO: Waiting up to 5m0s for pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d" in namespace "secrets-828" to be "Succeeded or Failed"
Aug 11 15:16:47.298: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798524ms
Aug 11 15:16:49.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369065s
Aug 11 15:16:51.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007374371s
STEP: Saw pod success 08/11/23 15:16:51.303
Aug 11 15:16:51.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d" satisfied condition "Succeeded or Failed"
Aug 11 15:16:51.306: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 15:16:51.315
Aug 11 15:16:51.332: INFO: Waiting for pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d to disappear
Aug 11 15:16:51.335: INFO: Pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-828" for this suite. 08/11/23 15:16:51.338
STEP: Destroying namespace "secret-namespace-3816" for this suite. 08/11/23 15:16:51.344
------------------------------
• [4.108 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:47.242
    Aug 11 15:16:47.242: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 15:16:47.243
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:47.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:47.26
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-a4974afa-f786-4456-9fd6-11a5ee4410ec 08/11/23 15:16:47.279
    STEP: Creating a pod to test consume secrets 08/11/23 15:16:47.284
    Aug 11 15:16:47.295: INFO: Waiting up to 5m0s for pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d" in namespace "secrets-828" to be "Succeeded or Failed"
    Aug 11 15:16:47.298: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798524ms
    Aug 11 15:16:49.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369065s
    Aug 11 15:16:51.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007374371s
    STEP: Saw pod success 08/11/23 15:16:51.303
    Aug 11 15:16:51.303: INFO: Pod "pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d" satisfied condition "Succeeded or Failed"
    Aug 11 15:16:51.306: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:16:51.315
    Aug 11 15:16:51.332: INFO: Waiting for pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d to disappear
    Aug 11 15:16:51.335: INFO: Pod pod-secrets-9256e68d-e2b6-408f-9414-af7655a3303d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-828" for this suite. 08/11/23 15:16:51.338
    STEP: Destroying namespace "secret-namespace-3816" for this suite. 08/11/23 15:16:51.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:51.351
Aug 11 15:16:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename proxy 08/11/23 15:16:51.352
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:51.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:51.367
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/11/23 15:16:51.382
STEP: creating replication controller proxy-service-74sh7 in namespace proxy-148 08/11/23 15:16:51.382
I0811 15:16:51.391846      21 runners.go:193] Created replication controller with name: proxy-service-74sh7, namespace: proxy-148, replica count: 1
I0811 15:16:52.442948      21 runners.go:193] proxy-service-74sh7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0811 15:16:53.443726      21 runners.go:193] proxy-service-74sh7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:16:53.447: INFO: setup took 2.077338549s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/11/23 15:16:53.447
Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 18.625379ms)
Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 18.662599ms)
Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 19.131409ms)
Aug 11 15:16:53.467: INFO: (0) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 20.414211ms)
Aug 11 15:16:53.469: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 21.965942ms)
Aug 11 15:16:53.469: INFO: (0) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 22.097672ms)
Aug 11 15:16:53.470: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 23.195253ms)
Aug 11 15:16:53.470: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 23.345484ms)
Aug 11 15:16:53.471: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 24.081754ms)
Aug 11 15:16:53.471: INFO: (0) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 23.932934ms)
Aug 11 15:16:53.472: INFO: (0) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 24.640515ms)
Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 28.659779ms)
Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 28.589729ms)
Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 29.37181ms)
Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 29.39907ms)
Aug 11 15:16:53.477: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 29.62344ms)
Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.079009ms)
Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.169399ms)
Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.14054ms)
Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 9.56592ms)
Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 9.51863ms)
Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.485051ms)
Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.609411ms)
Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.621071ms)
Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.707201ms)
Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.331602ms)
Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.553382ms)
Aug 11 15:16:53.489: INFO: (1) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.327083ms)
Aug 11 15:16:53.489: INFO: (1) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 12.605343ms)
Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.158285ms)
Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.070085ms)
Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.466155ms)
Aug 11 15:16:53.501: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.15421ms)
Aug 11 15:16:53.501: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.24148ms)
Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.021601ms)
Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.425321ms)
Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.413271ms)
Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.839882ms)
Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.192922ms)
Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.265072ms)
Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.483452ms)
Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.554622ms)
Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.509472ms)
Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 12.230863ms)
Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.296553ms)
Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 12.631463ms)
Aug 11 15:16:53.505: INFO: (2) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.635624ms)
Aug 11 15:16:53.506: INFO: (2) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.226615ms)
Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.87419ms)
Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.308781ms)
Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.486501ms)
Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.488341ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.642681ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.961491ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 10.943701ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.135881ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.175941ms)
Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.982681ms)
Aug 11 15:16:53.518: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.806922ms)
Aug 11 15:16:53.518: INFO: (3) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.907712ms)
Aug 11 15:16:53.519: INFO: (3) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.838053ms)
Aug 11 15:16:53.520: INFO: (3) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 13.928324ms)
Aug 11 15:16:53.520: INFO: (3) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.192824ms)
Aug 11 15:16:53.521: INFO: (3) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.907475ms)
Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.74493ms)
Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.12903ms)
Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.1777ms)
Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.805291ms)
Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 10.928131ms)
Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.802021ms)
Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.909471ms)
Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.258061ms)
Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.721032ms)
Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.224812ms)
Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.261902ms)
Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.290802ms)
Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.088664ms)
Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.422105ms)
Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.502835ms)
Aug 11 15:16:53.536: INFO: (4) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.290305ms)
Aug 11 15:16:53.543: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 6.789737ms)
Aug 11 15:16:53.544: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 7.398407ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.02384ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 9.980639ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.094629ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.03573ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.468221ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.68928ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.57124ms)
Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.78841ms)
Aug 11 15:16:53.550: INFO: (5) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.319643ms)
Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.963133ms)
Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.144784ms)
Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.654004ms)
Aug 11 15:16:53.552: INFO: (5) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 14.985464ms)
Aug 11 15:16:53.552: INFO: (5) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.993454ms)
Aug 11 15:16:53.559: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 6.893407ms)
Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 8.713648ms)
Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 8.807118ms)
Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 8.956619ms)
Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 9.419409ms)
Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.537399ms)
Aug 11 15:16:53.562: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.875399ms)
Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.115181ms)
Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.75ms)
Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.9505ms)
Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.04823ms)
Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.137421ms)
Aug 11 15:16:53.565: INFO: (6) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 13.326673ms)
Aug 11 15:16:53.565: INFO: (6) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 13.298752ms)
Aug 11 15:16:53.566: INFO: (6) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.403014ms)
Aug 11 15:16:53.567: INFO: (6) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.657134ms)
Aug 11 15:16:53.575: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 8.038408ms)
Aug 11 15:16:53.577: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.87771ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.695662ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 12.072642ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.140872ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.104582ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.532122ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.374652ms)
Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.504973ms)
Aug 11 15:16:53.580: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 12.552513ms)
Aug 11 15:16:53.580: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.668253ms)
Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.160845ms)
Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.226445ms)
Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.536405ms)
Aug 11 15:16:53.583: INFO: (7) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 15.700615ms)
Aug 11 15:16:53.583: INFO: (7) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.212916ms)
Aug 11 15:16:53.593: INFO: (8) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 10.1023ms)
Aug 11 15:16:53.593: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.08368ms)
Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.814351ms)
Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.014661ms)
Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.191371ms)
Aug 11 15:16:53.595: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.423682ms)
Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.547522ms)
Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 13.014402ms)
Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 13.158373ms)
Aug 11 15:16:53.597: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 13.203093ms)
Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.371064ms)
Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 14.269994ms)
Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 14.390424ms)
Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.627934ms)
Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.943915ms)
Aug 11 15:16:53.602: INFO: (8) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 18.412887ms)
Aug 11 15:16:53.611: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.038969ms)
Aug 11 15:16:53.612: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.17973ms)
Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.48879ms)
Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.760811ms)
Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.01746ms)
Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.004271ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.678502ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.594561ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.652391ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.950992ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.277752ms)
Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.510493ms)
Aug 11 15:16:53.615: INFO: (9) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.821702ms)
Aug 11 15:16:53.616: INFO: (9) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 13.806124ms)
Aug 11 15:16:53.616: INFO: (9) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.387254ms)
Aug 11 15:16:53.618: INFO: (9) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.920396ms)
Aug 11 15:16:53.626: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 7.666048ms)
Aug 11 15:16:53.628: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.18877ms)
Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.43547ms)
Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 10.698521ms)
Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.071131ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.613662ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.838362ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.704282ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.680151ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.728782ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.889331ms)
Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 12.055492ms)
Aug 11 15:16:53.633: INFO: (10) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.843315ms)
Aug 11 15:16:53.633: INFO: (10) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.275856ms)
Aug 11 15:16:53.634: INFO: (10) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.642406ms)
Aug 11 15:16:53.634: INFO: (10) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.880576ms)
Aug 11 15:16:53.643: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 8.490018ms)
Aug 11 15:16:53.644: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.95245ms)
Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.58287ms)
Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.601821ms)
Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.994171ms)
Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.164121ms)
Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.305981ms)
Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.926902ms)
Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.933062ms)
Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.515103ms)
Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 12.716712ms)
Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.791412ms)
Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.257054ms)
Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.497894ms)
Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.874365ms)
Aug 11 15:16:53.650: INFO: (11) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.223905ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.839161ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.112361ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.250961ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.162921ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.354561ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.721202ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.586162ms)
Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.671742ms)
Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.455192ms)
Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.400823ms)
Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 12.591973ms)
Aug 11 15:16:53.663: INFO: (12) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.961473ms)
Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.447215ms)
Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.557225ms)
Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.794505ms)
Aug 11 15:16:53.665: INFO: (12) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.738575ms)
Aug 11 15:16:53.676: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.90591ms)
Aug 11 15:16:53.676: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.80694ms)
Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.057981ms)
Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.221201ms)
Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.974541ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 13.106933ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 13.411733ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 13.598203ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 13.429753ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.413923ms)
Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 13.420443ms)
Aug 11 15:16:53.681: INFO: (13) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 15.073124ms)
Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.058146ms)
Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.349566ms)
Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 16.681686ms)
Aug 11 15:16:53.684: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 18.324458ms)
Aug 11 15:16:53.694: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.31209ms)
Aug 11 15:16:53.694: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.36917ms)
Aug 11 15:16:53.695: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.595601ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.573742ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.743011ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.582821ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.581162ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.727821ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.865691ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.638021ms)
Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.824802ms)
Aug 11 15:16:53.697: INFO: (14) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.120063ms)
Aug 11 15:16:53.698: INFO: (14) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.880584ms)
Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.819174ms)
Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.034945ms)
Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.151955ms)
Aug 11 15:16:53.709: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.10974ms)
Aug 11 15:16:53.710: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.27933ms)
Aug 11 15:16:53.710: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.175091ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.031441ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.152261ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.502111ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.816071ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.882971ms)
Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.133582ms)
Aug 11 15:16:53.712: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.375422ms)
Aug 11 15:16:53.714: INFO: (15) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 14.217434ms)
Aug 11 15:16:53.714: INFO: (15) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 14.868344ms)
Aug 11 15:16:53.715: INFO: (15) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.363265ms)
Aug 11 15:16:53.715: INFO: (15) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.413755ms)
Aug 11 15:16:53.716: INFO: (15) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.152146ms)
Aug 11 15:16:53.716: INFO: (15) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.089576ms)
Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.6756ms)
Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.67404ms)
Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.71553ms)
Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.148421ms)
Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.267851ms)
Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.776391ms)
Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.166242ms)
Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.442492ms)
Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.524802ms)
Aug 11 15:16:53.729: INFO: (16) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.157873ms)
Aug 11 15:16:53.729: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 13.127563ms)
Aug 11 15:16:53.730: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 13.991984ms)
Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.018986ms)
Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 16.413646ms)
Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.621056ms)
Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 16.654506ms)
Aug 11 15:16:53.742: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 8.943389ms)
Aug 11 15:16:53.742: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.31414ms)
Aug 11 15:16:53.743: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.813701ms)
Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.055081ms)
Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.922581ms)
Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 11.138961ms)
Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.677002ms)
Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.942912ms)
Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.134743ms)
Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.570753ms)
Aug 11 15:16:53.746: INFO: (17) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.279924ms)
Aug 11 15:16:53.746: INFO: (17) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.553323ms)
Aug 11 15:16:53.747: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 14.017023ms)
Aug 11 15:16:53.747: INFO: (17) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 14.058513ms)
Aug 11 15:16:53.748: INFO: (17) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.985394ms)
Aug 11 15:16:53.748: INFO: (17) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.127325ms)
Aug 11 15:16:53.755: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 7.555458ms)
Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 7.725438ms)
Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 8.379509ms)
Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 8.425289ms)
Aug 11 15:16:53.757: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.154249ms)
Aug 11 15:16:53.757: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 9.63349ms)
Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 9.95919ms)
Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.28934ms)
Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.364949ms)
Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.071821ms)
Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.9986ms)
Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.03703ms)
Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 11.443321ms)
Aug 11 15:16:53.760: INFO: (18) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.083362ms)
Aug 11 15:16:53.762: INFO: (18) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.082614ms)
Aug 11 15:16:53.763: INFO: (18) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.651494ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.807421ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.865202ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.005902ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.985692ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.033662ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 12.365562ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.357132ms)
Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.352402ms)
Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.813142ms)
Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.016742ms)
Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 13.500403ms)
Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.713873ms)
Aug 11 15:16:53.779: INFO: (19) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.998256ms)
Aug 11 15:16:53.779: INFO: (19) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 16.224236ms)
Aug 11 15:16:53.780: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 16.982916ms)
Aug 11 15:16:53.781: INFO: (19) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 18.010338ms)
STEP: deleting ReplicationController proxy-service-74sh7 in namespace proxy-148, will wait for the garbage collector to delete the pods 08/11/23 15:16:53.781
Aug 11 15:16:53.842: INFO: Deleting ReplicationController proxy-service-74sh7 took: 7.039507ms
Aug 11 15:16:53.943: INFO: Terminating ReplicationController proxy-service-74sh7 pods took: 100.956429ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 11 15:16:56.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-148" for this suite. 08/11/23 15:16:56.049
------------------------------
• [4.704 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:51.351
    Aug 11 15:16:51.351: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename proxy 08/11/23 15:16:51.352
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:51.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:51.367
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/11/23 15:16:51.382
    STEP: creating replication controller proxy-service-74sh7 in namespace proxy-148 08/11/23 15:16:51.382
    I0811 15:16:51.391846      21 runners.go:193] Created replication controller with name: proxy-service-74sh7, namespace: proxy-148, replica count: 1
    I0811 15:16:52.442948      21 runners.go:193] proxy-service-74sh7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0811 15:16:53.443726      21 runners.go:193] proxy-service-74sh7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:16:53.447: INFO: setup took 2.077338549s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/11/23 15:16:53.447
    Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 18.625379ms)
    Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 18.662599ms)
    Aug 11 15:16:53.466: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 19.131409ms)
    Aug 11 15:16:53.467: INFO: (0) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 20.414211ms)
    Aug 11 15:16:53.469: INFO: (0) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 21.965942ms)
    Aug 11 15:16:53.469: INFO: (0) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 22.097672ms)
    Aug 11 15:16:53.470: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 23.195253ms)
    Aug 11 15:16:53.470: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 23.345484ms)
    Aug 11 15:16:53.471: INFO: (0) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 24.081754ms)
    Aug 11 15:16:53.471: INFO: (0) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 23.932934ms)
    Aug 11 15:16:53.472: INFO: (0) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 24.640515ms)
    Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 28.659779ms)
    Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 28.589729ms)
    Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 29.37181ms)
    Aug 11 15:16:53.476: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 29.39907ms)
    Aug 11 15:16:53.477: INFO: (0) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 29.62344ms)
    Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.079009ms)
    Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.169399ms)
    Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.14054ms)
    Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 9.56592ms)
    Aug 11 15:16:53.486: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 9.51863ms)
    Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.485051ms)
    Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.609411ms)
    Aug 11 15:16:53.487: INFO: (1) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.621071ms)
    Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.707201ms)
    Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.331602ms)
    Aug 11 15:16:53.488: INFO: (1) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.553382ms)
    Aug 11 15:16:53.489: INFO: (1) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.327083ms)
    Aug 11 15:16:53.489: INFO: (1) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 12.605343ms)
    Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.158285ms)
    Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.070085ms)
    Aug 11 15:16:53.491: INFO: (1) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.466155ms)
    Aug 11 15:16:53.501: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.15421ms)
    Aug 11 15:16:53.501: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.24148ms)
    Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.021601ms)
    Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.425321ms)
    Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.413271ms)
    Aug 11 15:16:53.502: INFO: (2) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.839882ms)
    Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.192922ms)
    Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.265072ms)
    Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.483452ms)
    Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.554622ms)
    Aug 11 15:16:53.503: INFO: (2) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.509472ms)
    Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 12.230863ms)
    Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.296553ms)
    Aug 11 15:16:53.504: INFO: (2) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 12.631463ms)
    Aug 11 15:16:53.505: INFO: (2) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.635624ms)
    Aug 11 15:16:53.506: INFO: (2) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.226615ms)
    Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.87419ms)
    Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.308781ms)
    Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.486501ms)
    Aug 11 15:16:53.516: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.488341ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.642681ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.961491ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 10.943701ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.135881ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.175941ms)
    Aug 11 15:16:53.517: INFO: (3) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.982681ms)
    Aug 11 15:16:53.518: INFO: (3) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.806922ms)
    Aug 11 15:16:53.518: INFO: (3) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.907712ms)
    Aug 11 15:16:53.519: INFO: (3) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.838053ms)
    Aug 11 15:16:53.520: INFO: (3) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 13.928324ms)
    Aug 11 15:16:53.520: INFO: (3) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.192824ms)
    Aug 11 15:16:53.521: INFO: (3) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.907475ms)
    Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.74493ms)
    Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.12903ms)
    Aug 11 15:16:53.531: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.1777ms)
    Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.805291ms)
    Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 10.928131ms)
    Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.802021ms)
    Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.909471ms)
    Aug 11 15:16:53.532: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.258061ms)
    Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.721032ms)
    Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.224812ms)
    Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.261902ms)
    Aug 11 15:16:53.533: INFO: (4) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.290802ms)
    Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.088664ms)
    Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.422105ms)
    Aug 11 15:16:53.535: INFO: (4) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.502835ms)
    Aug 11 15:16:53.536: INFO: (4) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.290305ms)
    Aug 11 15:16:53.543: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 6.789737ms)
    Aug 11 15:16:53.544: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 7.398407ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.02384ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 9.980639ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.094629ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.03573ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.468221ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.68928ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.57124ms)
    Aug 11 15:16:53.547: INFO: (5) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.78841ms)
    Aug 11 15:16:53.550: INFO: (5) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.319643ms)
    Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.963133ms)
    Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.144784ms)
    Aug 11 15:16:53.551: INFO: (5) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.654004ms)
    Aug 11 15:16:53.552: INFO: (5) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 14.985464ms)
    Aug 11 15:16:53.552: INFO: (5) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.993454ms)
    Aug 11 15:16:53.559: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 6.893407ms)
    Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 8.713648ms)
    Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 8.807118ms)
    Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 8.956619ms)
    Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 9.419409ms)
    Aug 11 15:16:53.561: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.537399ms)
    Aug 11 15:16:53.562: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 9.875399ms)
    Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.115181ms)
    Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.75ms)
    Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.9505ms)
    Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.04823ms)
    Aug 11 15:16:53.563: INFO: (6) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.137421ms)
    Aug 11 15:16:53.565: INFO: (6) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 13.326673ms)
    Aug 11 15:16:53.565: INFO: (6) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 13.298752ms)
    Aug 11 15:16:53.566: INFO: (6) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.403014ms)
    Aug 11 15:16:53.567: INFO: (6) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.657134ms)
    Aug 11 15:16:53.575: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 8.038408ms)
    Aug 11 15:16:53.577: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.87771ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.695662ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 12.072642ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.140872ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.104582ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.532122ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.374652ms)
    Aug 11 15:16:53.579: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.504973ms)
    Aug 11 15:16:53.580: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 12.552513ms)
    Aug 11 15:16:53.580: INFO: (7) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.668253ms)
    Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.160845ms)
    Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.226445ms)
    Aug 11 15:16:53.582: INFO: (7) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.536405ms)
    Aug 11 15:16:53.583: INFO: (7) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 15.700615ms)
    Aug 11 15:16:53.583: INFO: (7) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.212916ms)
    Aug 11 15:16:53.593: INFO: (8) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 10.1023ms)
    Aug 11 15:16:53.593: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.08368ms)
    Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.814351ms)
    Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.014661ms)
    Aug 11 15:16:53.594: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.191371ms)
    Aug 11 15:16:53.595: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.423682ms)
    Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.547522ms)
    Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 13.014402ms)
    Aug 11 15:16:53.596: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 13.158373ms)
    Aug 11 15:16:53.597: INFO: (8) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 13.203093ms)
    Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.371064ms)
    Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 14.269994ms)
    Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 14.390424ms)
    Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.627934ms)
    Aug 11 15:16:53.598: INFO: (8) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.943915ms)
    Aug 11 15:16:53.602: INFO: (8) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 18.412887ms)
    Aug 11 15:16:53.611: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 9.038969ms)
    Aug 11 15:16:53.612: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.17973ms)
    Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.48879ms)
    Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.760811ms)
    Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.01746ms)
    Aug 11 15:16:53.613: INFO: (9) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.004271ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.678502ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.594561ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.652391ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.950992ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.277752ms)
    Aug 11 15:16:53.614: INFO: (9) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.510493ms)
    Aug 11 15:16:53.615: INFO: (9) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.821702ms)
    Aug 11 15:16:53.616: INFO: (9) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 13.806124ms)
    Aug 11 15:16:53.616: INFO: (9) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.387254ms)
    Aug 11 15:16:53.618: INFO: (9) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.920396ms)
    Aug 11 15:16:53.626: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 7.666048ms)
    Aug 11 15:16:53.628: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.18877ms)
    Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.43547ms)
    Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 10.698521ms)
    Aug 11 15:16:53.629: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.071131ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.613662ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.838362ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.704282ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.680151ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.728782ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.889331ms)
    Aug 11 15:16:53.630: INFO: (10) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 12.055492ms)
    Aug 11 15:16:53.633: INFO: (10) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.843315ms)
    Aug 11 15:16:53.633: INFO: (10) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.275856ms)
    Aug 11 15:16:53.634: INFO: (10) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.642406ms)
    Aug 11 15:16:53.634: INFO: (10) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.880576ms)
    Aug 11 15:16:53.643: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 8.490018ms)
    Aug 11 15:16:53.644: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 9.95245ms)
    Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.58287ms)
    Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.601821ms)
    Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.994171ms)
    Aug 11 15:16:53.645: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.164121ms)
    Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.305981ms)
    Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.926902ms)
    Aug 11 15:16:53.646: INFO: (11) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.933062ms)
    Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.515103ms)
    Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 12.716712ms)
    Aug 11 15:16:53.647: INFO: (11) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.791412ms)
    Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.257054ms)
    Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.497894ms)
    Aug 11 15:16:53.649: INFO: (11) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.874365ms)
    Aug 11 15:16:53.650: INFO: (11) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.223905ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.839161ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.112361ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.250961ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.162921ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.354561ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.721202ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.586162ms)
    Aug 11 15:16:53.661: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.671742ms)
    Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.455192ms)
    Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.400823ms)
    Aug 11 15:16:53.662: INFO: (12) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 12.591973ms)
    Aug 11 15:16:53.663: INFO: (12) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 12.961473ms)
    Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 14.447215ms)
    Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.557225ms)
    Aug 11 15:16:53.664: INFO: (12) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.794505ms)
    Aug 11 15:16:53.665: INFO: (12) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 15.738575ms)
    Aug 11 15:16:53.676: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.90591ms)
    Aug 11 15:16:53.676: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.80694ms)
    Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.057981ms)
    Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.221201ms)
    Aug 11 15:16:53.677: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.974541ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 13.106933ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 13.411733ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 13.598203ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 13.429753ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.413923ms)
    Aug 11 15:16:53.679: INFO: (13) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 13.420443ms)
    Aug 11 15:16:53.681: INFO: (13) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 15.073124ms)
    Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.058146ms)
    Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.349566ms)
    Aug 11 15:16:53.682: INFO: (13) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 16.681686ms)
    Aug 11 15:16:53.684: INFO: (13) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 18.324458ms)
    Aug 11 15:16:53.694: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.31209ms)
    Aug 11 15:16:53.694: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 10.36917ms)
    Aug 11 15:16:53.695: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.595601ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.573742ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.743011ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 11.582821ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.581162ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.727821ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.865691ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.638021ms)
    Aug 11 15:16:53.696: INFO: (14) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.824802ms)
    Aug 11 15:16:53.697: INFO: (14) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.120063ms)
    Aug 11 15:16:53.698: INFO: (14) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.880584ms)
    Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.819174ms)
    Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.034945ms)
    Aug 11 15:16:53.699: INFO: (14) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.151955ms)
    Aug 11 15:16:53.709: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 10.10974ms)
    Aug 11 15:16:53.710: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.27933ms)
    Aug 11 15:16:53.710: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 11.175091ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.031441ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.152261ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.502111ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 11.816071ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.882971ms)
    Aug 11 15:16:53.711: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.133582ms)
    Aug 11 15:16:53.712: INFO: (15) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.375422ms)
    Aug 11 15:16:53.714: INFO: (15) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 14.217434ms)
    Aug 11 15:16:53.714: INFO: (15) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 14.868344ms)
    Aug 11 15:16:53.715: INFO: (15) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 15.363265ms)
    Aug 11 15:16:53.715: INFO: (15) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 15.413755ms)
    Aug 11 15:16:53.716: INFO: (15) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.152146ms)
    Aug 11 15:16:53.716: INFO: (15) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.089576ms)
    Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.6756ms)
    Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 10.67404ms)
    Aug 11 15:16:53.726: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.71553ms)
    Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.148421ms)
    Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.267851ms)
    Aug 11 15:16:53.727: INFO: (16) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.776391ms)
    Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.166242ms)
    Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.442492ms)
    Aug 11 15:16:53.728: INFO: (16) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.524802ms)
    Aug 11 15:16:53.729: INFO: (16) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.157873ms)
    Aug 11 15:16:53.729: INFO: (16) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 13.127563ms)
    Aug 11 15:16:53.730: INFO: (16) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 13.991984ms)
    Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 16.018986ms)
    Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 16.413646ms)
    Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 16.621056ms)
    Aug 11 15:16:53.732: INFO: (16) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 16.654506ms)
    Aug 11 15:16:53.742: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 8.943389ms)
    Aug 11 15:16:53.742: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.31414ms)
    Aug 11 15:16:53.743: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 10.813701ms)
    Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.055081ms)
    Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 10.922581ms)
    Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 11.138961ms)
    Aug 11 15:16:53.744: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 11.677002ms)
    Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 11.942912ms)
    Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.134743ms)
    Aug 11 15:16:53.745: INFO: (17) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 12.570753ms)
    Aug 11 15:16:53.746: INFO: (17) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 13.279924ms)
    Aug 11 15:16:53.746: INFO: (17) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.553323ms)
    Aug 11 15:16:53.747: INFO: (17) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 14.017023ms)
    Aug 11 15:16:53.747: INFO: (17) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 14.058513ms)
    Aug 11 15:16:53.748: INFO: (17) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 14.985394ms)
    Aug 11 15:16:53.748: INFO: (17) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.127325ms)
    Aug 11 15:16:53.755: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 7.555458ms)
    Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 7.725438ms)
    Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 8.379509ms)
    Aug 11 15:16:53.756: INFO: (18) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 8.425289ms)
    Aug 11 15:16:53.757: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 9.154249ms)
    Aug 11 15:16:53.757: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 9.63349ms)
    Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 9.95919ms)
    Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 10.28934ms)
    Aug 11 15:16:53.758: INFO: (18) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 10.364949ms)
    Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 11.071821ms)
    Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 10.9986ms)
    Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 11.03703ms)
    Aug 11 15:16:53.759: INFO: (18) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 11.443321ms)
    Aug 11 15:16:53.760: INFO: (18) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 12.083362ms)
    Aug 11 15:16:53.762: INFO: (18) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 14.082614ms)
    Aug 11 15:16:53.763: INFO: (18) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 14.651494ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:1080/proxy/rewriteme">t... (200; 11.807421ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:443/proxy/tlsrewriteme... (200; 11.865202ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:1080/proxy/rewriteme">test</... (200; 12.005902ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/: <a href="/api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g/proxy/rewriteme">test</a> (200; 11.985692ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.033662ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:462/proxy/: tls qux (200; 12.365562ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname2/proxy/: tls qux (200; 12.357132ms)
    Aug 11 15:16:53.775: INFO: (19) /api/v1/namespaces/proxy-148/pods/http:proxy-service-74sh7-ld59g:160/proxy/: foo (200; 12.352402ms)
    Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:162/proxy/: bar (200; 12.813142ms)
    Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname1/proxy/: foo (200; 13.016742ms)
    Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/pods/https:proxy-service-74sh7-ld59g:460/proxy/: tls baz (200; 13.500403ms)
    Aug 11 15:16:53.776: INFO: (19) /api/v1/namespaces/proxy-148/services/https:proxy-service-74sh7:tlsportname1/proxy/: tls baz (200; 13.713873ms)
    Aug 11 15:16:53.779: INFO: (19) /api/v1/namespaces/proxy-148/services/http:proxy-service-74sh7:portname2/proxy/: bar (200; 15.998256ms)
    Aug 11 15:16:53.779: INFO: (19) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname1/proxy/: foo (200; 16.224236ms)
    Aug 11 15:16:53.780: INFO: (19) /api/v1/namespaces/proxy-148/pods/proxy-service-74sh7-ld59g:160/proxy/: foo (200; 16.982916ms)
    Aug 11 15:16:53.781: INFO: (19) /api/v1/namespaces/proxy-148/services/proxy-service-74sh7:portname2/proxy/: bar (200; 18.010338ms)
    STEP: deleting ReplicationController proxy-service-74sh7 in namespace proxy-148, will wait for the garbage collector to delete the pods 08/11/23 15:16:53.781
    Aug 11 15:16:53.842: INFO: Deleting ReplicationController proxy-service-74sh7 took: 7.039507ms
    Aug 11 15:16:53.943: INFO: Terminating ReplicationController proxy-service-74sh7 pods took: 100.956429ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:16:56.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-148" for this suite. 08/11/23 15:16:56.049
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:16:56.056
Aug 11 15:16:56.056: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename init-container 08/11/23 15:16:56.057
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:56.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:56.074
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/11/23 15:16:56.076
Aug 11 15:16:56.076: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5493" for this suite. 08/11/23 15:17:00.96
------------------------------
• [4.911 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:16:56.056
    Aug 11 15:16:56.056: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename init-container 08/11/23 15:16:56.057
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:56.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:56.074
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/11/23 15:16:56.076
    Aug 11 15:16:56.076: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5493" for this suite. 08/11/23 15:17:00.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:00.97
Aug 11 15:17:00.970: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename endpointslicemirroring 08/11/23 15:17:00.971
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:00.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:00.987
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/11/23 15:17:01.003
Aug 11 15:17:01.010: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/11/23 15:17:03.014
STEP: mirroring deletion of a custom Endpoint 08/11/23 15:17:03.026
Aug 11 15:17:03.037: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7565" for this suite. 08/11/23 15:17:05.046
------------------------------
• [4.082 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:00.97
    Aug 11 15:17:00.970: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename endpointslicemirroring 08/11/23 15:17:00.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:00.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:00.987
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/11/23 15:17:01.003
    Aug 11 15:17:01.010: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/11/23 15:17:03.014
    STEP: mirroring deletion of a custom Endpoint 08/11/23 15:17:03.026
    Aug 11 15:17:03.037: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:05.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7565" for this suite. 08/11/23 15:17:05.046
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:05.052
Aug 11 15:17:05.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:17:05.053
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:05.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:05.069
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-h2s7c"  08/11/23 15:17:05.071
Aug 11 15:17:05.076: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-h2s7c"  08/11/23 15:17:05.076
Aug 11 15:17:05.083: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:05.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2173" for this suite. 08/11/23 15:17:05.087
------------------------------
• [0.041 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:05.052
    Aug 11 15:17:05.053: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:17:05.053
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:05.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:05.069
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-h2s7c"  08/11/23 15:17:05.071
    Aug 11 15:17:05.076: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-h2s7c"  08/11/23 15:17:05.076
    Aug 11 15:17:05.083: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:05.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2173" for this suite. 08/11/23 15:17:05.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:05.095
Aug 11 15:17:05.095: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 15:17:05.095
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:05.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:05.111
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 11 15:17:05.113: INFO: Creating deployment "webserver-deployment"
Aug 11 15:17:05.120: INFO: Waiting for observed generation 1
Aug 11 15:17:07.127: INFO: Waiting for all required pods to come up
Aug 11 15:17:07.131: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/11/23 15:17:07.131
Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-whcr7" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dfl5g" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dwtk4" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-528jb" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rqjsf" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7c66q" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-c99fr" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-67868" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mrnsn" in namespace "deployment-7406" to be "running"
Aug 11 15:17:07.135: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412773ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332984ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-528jb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420675ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.229923ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-67868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328244ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525605ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810235ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.736665ms
Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702105ms
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008497495s
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q": Phase="Running", Reason="", readiness=true. Elapsed: 2.008428405s
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q" satisfied condition "running"
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008685355s
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7" satisfied condition "running"
Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254045s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-528jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009494955s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-528jb" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009353595s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-67868": Phase="Running", Reason="", readiness=true. Elapsed: 2.009468465s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009728076s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-67868" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g": Phase="Running", Reason="", readiness=true. Elapsed: 2.009940056s
Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g" satisfied condition "running"
Aug 11 15:17:09.141: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 11 15:17:09.146: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 11 15:17:09.155: INFO: Updating deployment webserver-deployment
Aug 11 15:17:09.155: INFO: Waiting for observed generation 2
Aug 11 15:17:11.164: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 11 15:17:11.166: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 11 15:17:11.169: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:17:11.177: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 11 15:17:11.177: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 11 15:17:11.179: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:17:11.185: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 11 15:17:11.185: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 11 15:17:11.196: INFO: Updating deployment webserver-deployment
Aug 11 15:17:11.196: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:17:11.205: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 11 15:17:11.210: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:17:11.222: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7406  1bbbd0f3-12b9-447e-8a48-4303f48f74dd 52779 3 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-11 15:17:09 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:17:11 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 11 15:17:11.231: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7406  3be08529-ae26-42e7-8809-35a87e9da927 52777 3 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1bbbd0f3-12b9-447e-8a48-4303f48f74dd 0xc006e8d697 0xc006e8d698}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbbd0f3-12b9-447e-8a48-4303f48f74dd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:17:11.231: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 11 15:17:11.231: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7406  ff69a4d2-8029-4881-816e-4bb18f4a299d 52773 3 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1bbbd0f3-12b9-447e-8a48-4303f48f74dd 0xc006e8d5a7 0xc006e8d5a8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbbd0f3-12b9-447e-8a48-4303f48f74dd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-2ldgh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2ldgh webserver-deployment-7f5969cbc7- deployment-7406  0599350a-955b-46b2-8439-ed082140ba2a 52783 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8dc17 0xc006e8dc18}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lnk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lnk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-528jb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-528jb webserver-deployment-7f5969cbc7- deployment-7406  509e2dee-2703-495f-905a-8ae9078b4830 52642 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8dd70 0xc006e8dd71}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xb8vk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xb8vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.48,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7a061a0244ea46b034f0ccef1472d6c4b7701d79c7315cdb4b07d5a76239c30c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-5hhzt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5hhzt webserver-deployment-7f5969cbc7- deployment-7406  61450ad0-a76b-4517-96bb-da18d9476e5c 52785 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8df40 0xc006e8df41}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6z27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6z27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-67868" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67868 webserver-deployment-7f5969cbc7- deployment-7406  4add194b-d499-4123-8d6b-fdf3e60cfee5 52647 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70090 0xc005b70091}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqr9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqr9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.28,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45c1f086380fdd3169c18cc3b5e18923ea63235962d2700f99633f7cf070211e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7c66q webserver-deployment-7f5969cbc7- deployment-7406  c2acaade-189a-48cf-823a-4e42fd98d12a 52649 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70260 0xc005b70261}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnnxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnnxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.41,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://456ec5784903408621afac22755b6fe925e97be73808755c5bd7937e2721c71c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-8spnd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8spnd webserver-deployment-7f5969cbc7- deployment-7406  61eaf93d-78e9-455e-b800-061e37e0cd79 52790 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70440 0xc005b70441}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbfs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbfs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c99fr webserver-deployment-7f5969cbc7- deployment-7406  ad50bf88-b441-4036-9c8a-69350f7b103c 52641 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70577 0xc005b70578}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4x9zq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4x9zq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.49,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fab553f3a494c07bfe74850385b5320ff217b6ee3efa1ffccd0c51c20ece099e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-cllzh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cllzh webserver-deployment-7f5969cbc7- deployment-7406  0e4bb516-6274-4453-b40c-ddac194bbab0 52788 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70780 0xc005b70781}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58k2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58k2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dfl5g webserver-deployment-7f5969cbc7- deployment-7406  3d1df467-bae9-4df3-a487-7aaf696ae97f 52632 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b708b7 0xc005b708b8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj2qc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj2qc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.179,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://84c540921e2843eec11f92758de4e7e94a68221e3161a10f6097b380716b56cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwtk4 webserver-deployment-7f5969cbc7- deployment-7406  bfd0fbb5-e7d8-451b-82f2-d8ae9f0120b7 52638 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70a90 0xc005b70a91}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2w5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2w5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.80,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c59a07ca3dfd7f4fb159de014a8fa46b67128c37cdc21946e936c029c29005b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-rfgv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rfgv6 webserver-deployment-7f5969cbc7- deployment-7406  43c4ae7d-6f96-4b2c-a325-95e8f7ff4d3c 52794 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70c70 0xc005b70c71}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24mgr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24mgr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-rnn4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rnn4c webserver-deployment-7f5969cbc7- deployment-7406  03e3aa91-d47a-46c3-84bc-d12bcc7d8a75 52787 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70dc0 0xc005b70dc1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlq68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlq68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rqjsf webserver-deployment-7f5969cbc7- deployment-7406  97f84479-531b-4896-8a05-1d71a7d833dd 52655 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70ef7 0xc005b70ef8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q22vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q22vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.14,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d51b614b45b26e8d592fe90b37112de00397bc29f7d06b970aec44145bba9034,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whcr7 webserver-deployment-7f5969cbc7- deployment-7406  681ced26-4da4-4259-beea-ba078f2c2050 52635 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b710d0 0xc005b710d1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l79d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l79d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.100,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dab8477d6252d729e01b78bef5363b64f82ef041216fae2fcf25ba90fdc3c038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-zwhzp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zwhzp webserver-deployment-7f5969cbc7- deployment-7406  76047dbf-0c33-4cc4-a1aa-42386c4b60b4 52789 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b712a0 0xc005b712a1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8rc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8rc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-d9f79cb5-8dgtj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8dgtj webserver-deployment-d9f79cb5- deployment-7406  87c49836-d65c-4fe8-a466-fc78ecbb6f71 52770 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b713d7 0xc005b713d8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpv8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpv8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.83,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-d9f79cb5-9ssdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9ssdb webserver-deployment-d9f79cb5- deployment-7406  85cad105-e15e-4cee-8bbb-cec939390b9f 52792 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b715ef 0xc005b71600}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hvfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hvfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-b6kvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6kvf webserver-deployment-d9f79cb5- deployment-7406  683a13d6-c7b5-498e-b6c0-0b92d9fac153 52700 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71867 0xc005b71868}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8grsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8grsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-j62lv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j62lv webserver-deployment-d9f79cb5- deployment-7406  0062c338-2eab-4ea5-92f1-38bf7148e20b 52717 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71a57 0xc005b71a58}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6br5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6br5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-jnv9r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jnv9r webserver-deployment-d9f79cb5- deployment-7406  a6ea8663-29b4-45ac-bbe8-fabc43611c21 52753 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71c57 0xc005b71c58}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xmnrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xmnrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-m267d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m267d webserver-deployment-d9f79cb5- deployment-7406  a42ed1cb-01bd-47c6-b68d-d46cc8ec36b5 52767 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71e97 0xc005b71e98}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gnlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gnlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.50,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.250: INFO: Pod "webserver-deployment-d9f79cb5-rbnjl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rbnjl webserver-deployment-d9f79cb5- deployment-7406  d18e5067-d965-4ec3-9c36-b48614efcc53 52793 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc0050b608f 0xc0050b60a0}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btwk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btwk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:17:11.250: INFO: Pod "webserver-deployment-d9f79cb5-xfppw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xfppw webserver-deployment-d9f79cb5- deployment-7406  fa39465a-1791-47ff-aad4-c94cc9e5ad8e 52791 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc0050b61e7 0xc0050b61e8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdtgf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdtgf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:11.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7406" for this suite. 08/11/23 15:17:11.264
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:05.095
    Aug 11 15:17:05.095: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 15:17:05.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:05.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:05.111
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 11 15:17:05.113: INFO: Creating deployment "webserver-deployment"
    Aug 11 15:17:05.120: INFO: Waiting for observed generation 1
    Aug 11 15:17:07.127: INFO: Waiting for all required pods to come up
    Aug 11 15:17:07.131: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/11/23 15:17:07.131
    Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-whcr7" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dfl5g" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dwtk4" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-528jb" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rqjsf" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7c66q" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-c99fr" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-67868" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.131: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mrnsn" in namespace "deployment-7406" to be "running"
    Aug 11 15:17:07.135: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.412773ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332984ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-528jb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420675ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.229923ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-67868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328244ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525605ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810235ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.736665ms
    Aug 11 15:17:07.136: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702105ms
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008497495s
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q": Phase="Running", Reason="", readiness=true. Elapsed: 2.008428405s
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q" satisfied condition "running"
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008685355s
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7" satisfied condition "running"
    Aug 11 15:17:09.140: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009254045s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-528jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009494955s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-528jb" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009353595s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-mrnsn" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-67868": Phase="Running", Reason="", readiness=true. Elapsed: 2.009468465s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009728076s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-67868" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g": Phase="Running", Reason="", readiness=true. Elapsed: 2.009940056s
    Aug 11 15:17:09.141: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g" satisfied condition "running"
    Aug 11 15:17:09.141: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 11 15:17:09.146: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 11 15:17:09.155: INFO: Updating deployment webserver-deployment
    Aug 11 15:17:09.155: INFO: Waiting for observed generation 2
    Aug 11 15:17:11.164: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 11 15:17:11.166: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 11 15:17:11.169: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:17:11.177: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 11 15:17:11.177: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 11 15:17:11.179: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:17:11.185: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 11 15:17:11.185: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 11 15:17:11.196: INFO: Updating deployment webserver-deployment
    Aug 11 15:17:11.196: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:17:11.205: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 11 15:17:11.210: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:17:11.222: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-7406  1bbbd0f3-12b9-447e-8a48-4303f48f74dd 52779 3 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-11 15:17:09 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:17:11 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 11 15:17:11.231: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-7406  3be08529-ae26-42e7-8809-35a87e9da927 52777 3 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1bbbd0f3-12b9-447e-8a48-4303f48f74dd 0xc006e8d697 0xc006e8d698}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbbd0f3-12b9-447e-8a48-4303f48f74dd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:17:11.231: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 11 15:17:11.231: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-7406  ff69a4d2-8029-4881-816e-4bb18f4a299d 52773 3 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1bbbd0f3-12b9-447e-8a48-4303f48f74dd 0xc006e8d5a7 0xc006e8d5a8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1bbbd0f3-12b9-447e-8a48-4303f48f74dd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006e8d638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-2ldgh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2ldgh webserver-deployment-7f5969cbc7- deployment-7406  0599350a-955b-46b2-8439-ed082140ba2a 52783 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8dc17 0xc006e8dc18}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lnk6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lnk6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-528jb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-528jb webserver-deployment-7f5969cbc7- deployment-7406  509e2dee-2703-495f-905a-8ae9078b4830 52642 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8dd70 0xc006e8dd71}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xb8vk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xb8vk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.48,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7a061a0244ea46b034f0ccef1472d6c4b7701d79c7315cdb4b07d5a76239c30c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.48,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-5hhzt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5hhzt webserver-deployment-7f5969cbc7- deployment-7406  61450ad0-a76b-4517-96bb-da18d9476e5c 52785 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc006e8df40 0xc006e8df41}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6z27,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6z27,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.245: INFO: Pod "webserver-deployment-7f5969cbc7-67868" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67868 webserver-deployment-7f5969cbc7- deployment-7406  4add194b-d499-4123-8d6b-fdf3e60cfee5 52647 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70090 0xc005b70091}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqr9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqr9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.28,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45c1f086380fdd3169c18cc3b5e18923ea63235962d2700f99633f7cf070211e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-7c66q" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7c66q webserver-deployment-7f5969cbc7- deployment-7406  c2acaade-189a-48cf-823a-4e42fd98d12a 52649 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70260 0xc005b70261}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnnxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnnxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.41,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://456ec5784903408621afac22755b6fe925e97be73808755c5bd7937e2721c71c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-8spnd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8spnd webserver-deployment-7f5969cbc7- deployment-7406  61eaf93d-78e9-455e-b800-061e37e0cd79 52790 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70440 0xc005b70441}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbfs4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbfs4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.246: INFO: Pod "webserver-deployment-7f5969cbc7-c99fr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c99fr webserver-deployment-7f5969cbc7- deployment-7406  ad50bf88-b441-4036-9c8a-69350f7b103c 52641 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70577 0xc005b70578}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4x9zq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4x9zq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.49,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fab553f3a494c07bfe74850385b5320ff217b6ee3efa1ffccd0c51c20ece099e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-cllzh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cllzh webserver-deployment-7f5969cbc7- deployment-7406  0e4bb516-6274-4453-b40c-ddac194bbab0 52788 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70780 0xc005b70781}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58k2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58k2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-dfl5g" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dfl5g webserver-deployment-7f5969cbc7- deployment-7406  3d1df467-bae9-4df3-a487-7aaf696ae97f 52632 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b708b7 0xc005b708b8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj2qc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj2qc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.179,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://84c540921e2843eec11f92758de4e7e94a68221e3161a10f6097b380716b56cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-dwtk4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwtk4 webserver-deployment-7f5969cbc7- deployment-7406  bfd0fbb5-e7d8-451b-82f2-d8ae9f0120b7 52638 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70a90 0xc005b70a91}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2w5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2w5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.80,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c59a07ca3dfd7f4fb159de014a8fa46b67128c37cdc21946e936c029c29005b7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.247: INFO: Pod "webserver-deployment-7f5969cbc7-rfgv6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rfgv6 webserver-deployment-7f5969cbc7- deployment-7406  43c4ae7d-6f96-4b2c-a325-95e8f7ff4d3c 52794 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70c70 0xc005b70c71}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24mgr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24mgr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-rnn4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rnn4c webserver-deployment-7f5969cbc7- deployment-7406  03e3aa91-d47a-46c3-84bc-d12bcc7d8a75 52787 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70dc0 0xc005b70dc1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlq68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlq68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-rqjsf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rqjsf webserver-deployment-7f5969cbc7- deployment-7406  97f84479-531b-4896-8a05-1d71a7d833dd 52655 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b70ef7 0xc005b70ef8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q22vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q22vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.14,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d51b614b45b26e8d592fe90b37112de00397bc29f7d06b970aec44145bba9034,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-whcr7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whcr7 webserver-deployment-7f5969cbc7- deployment-7406  681ced26-4da4-4259-beea-ba078f2c2050 52635 0 2023-08-11 15:17:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b710d0 0xc005b710d1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l79d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l79d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.100,StartTime:2023-08-11 15:17:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:17:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dab8477d6252d729e01b78bef5363b64f82ef041216fae2fcf25ba90fdc3c038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-7f5969cbc7-zwhzp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zwhzp webserver-deployment-7f5969cbc7- deployment-7406  76047dbf-0c33-4cc4-a1aa-42386c4b60b4 52789 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff69a4d2-8029-4881-816e-4bb18f4a299d 0xc005b712a0 0xc005b712a1}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff69a4d2-8029-4881-816e-4bb18f4a299d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8rc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8rc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-d9f79cb5-8dgtj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8dgtj webserver-deployment-d9f79cb5- deployment-7406  87c49836-d65c-4fe8-a466-fc78ecbb6f71 52770 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b713d7 0xc005b713d8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tpv8w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tpv8w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.83,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.248: INFO: Pod "webserver-deployment-d9f79cb5-9ssdb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9ssdb webserver-deployment-d9f79cb5- deployment-7406  85cad105-e15e-4cee-8bbb-cec939390b9f 52792 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b715ef 0xc005b71600}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hvfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hvfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-b6kvf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6kvf webserver-deployment-d9f79cb5- deployment-7406  683a13d6-c7b5-498e-b6c0-0b92d9fac153 52700 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71867 0xc005b71868}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8grsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8grsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-j62lv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j62lv webserver-deployment-d9f79cb5- deployment-7406  0062c338-2eab-4ea5-92f1-38bf7148e20b 52717 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71a57 0xc005b71a58}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6br5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6br5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-jnv9r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jnv9r webserver-deployment-d9f79cb5- deployment-7406  a6ea8663-29b4-45ac-bbe8-fabc43611c21 52753 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71c57 0xc005b71c58}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xmnrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xmnrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.249: INFO: Pod "webserver-deployment-d9f79cb5-m267d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-m267d webserver-deployment-d9f79cb5- deployment-7406  a42ed1cb-01bd-47c6-b68d-d46cc8ec36b5 52767 0 2023-08-11 15:17:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc005b71e97 0xc005b71e98}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gnlh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gnlh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.50,StartTime:2023-08-11 15:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.250: INFO: Pod "webserver-deployment-d9f79cb5-rbnjl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rbnjl webserver-deployment-d9f79cb5- deployment-7406  d18e5067-d965-4ec3-9c36-b48614efcc53 52793 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc0050b608f 0xc0050b60a0}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btwk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btwk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:17:11.250: INFO: Pod "webserver-deployment-d9f79cb5-xfppw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xfppw webserver-deployment-d9f79cb5- deployment-7406  fa39465a-1791-47ff-aad4-c94cc9e5ad8e 52791 0 2023-08-11 15:17:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3be08529-ae26-42e7-8809-35a87e9da927 0xc0050b61e7 0xc0050b61e8}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3be08529-ae26-42e7-8809-35a87e9da927\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdtgf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdtgf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-mt98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:11.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7406" for this suite. 08/11/23 15:17:11.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:11.29
Aug 11 15:17:11.290: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename subpath 08/11/23 15:17:11.29
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:11.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:11.314
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 15:17:11.316
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-md9v 08/11/23 15:17:11.327
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:17:11.327
Aug 11 15:17:11.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-md9v" in namespace "subpath-2516" to be "Succeeded or Failed"
Aug 11 15:17:11.341: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059795ms
Aug 11 15:17:13.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008924105s
Aug 11 15:17:15.344: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008659062s
Aug 11 15:17:17.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011591001s
Aug 11 15:17:19.344: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008717094s
Aug 11 15:17:21.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009654442s
Aug 11 15:17:23.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.010302809s
Aug 11 15:17:25.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.009996173s
Aug 11 15:17:27.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.009431038s
Aug 11 15:17:29.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.011072106s
Aug 11 15:17:31.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.00902835s
Aug 11 15:17:33.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 22.010928668s
Aug 11 15:17:35.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=false. Elapsed: 24.010430104s
Aug 11 15:17:37.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010127281s
STEP: Saw pod success 08/11/23 15:17:37.346
Aug 11 15:17:37.346: INFO: Pod "pod-subpath-test-downwardapi-md9v" satisfied condition "Succeeded or Failed"
Aug 11 15:17:37.349: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-downwardapi-md9v container test-container-subpath-downwardapi-md9v: <nil>
STEP: delete the pod 08/11/23 15:17:37.362
Aug 11 15:17:37.377: INFO: Waiting for pod pod-subpath-test-downwardapi-md9v to disappear
Aug 11 15:17:37.379: INFO: Pod pod-subpath-test-downwardapi-md9v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-md9v 08/11/23 15:17:37.379
Aug 11 15:17:37.379: INFO: Deleting pod "pod-subpath-test-downwardapi-md9v" in namespace "subpath-2516"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:37.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2516" for this suite. 08/11/23 15:17:37.385
------------------------------
• [SLOW TEST] [26.104 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:11.29
    Aug 11 15:17:11.290: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename subpath 08/11/23 15:17:11.29
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:11.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:11.314
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 15:17:11.316
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-md9v 08/11/23 15:17:11.327
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:17:11.327
    Aug 11 15:17:11.336: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-md9v" in namespace "subpath-2516" to be "Succeeded or Failed"
    Aug 11 15:17:11.341: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059795ms
    Aug 11 15:17:13.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008924105s
    Aug 11 15:17:15.344: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008659062s
    Aug 11 15:17:17.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011591001s
    Aug 11 15:17:19.344: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008717094s
    Aug 11 15:17:21.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009654442s
    Aug 11 15:17:23.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.010302809s
    Aug 11 15:17:25.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.009996173s
    Aug 11 15:17:27.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.009431038s
    Aug 11 15:17:29.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.011072106s
    Aug 11 15:17:31.345: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.00902835s
    Aug 11 15:17:33.347: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=true. Elapsed: 22.010928668s
    Aug 11 15:17:35.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Running", Reason="", readiness=false. Elapsed: 24.010430104s
    Aug 11 15:17:37.346: INFO: Pod "pod-subpath-test-downwardapi-md9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010127281s
    STEP: Saw pod success 08/11/23 15:17:37.346
    Aug 11 15:17:37.346: INFO: Pod "pod-subpath-test-downwardapi-md9v" satisfied condition "Succeeded or Failed"
    Aug 11 15:17:37.349: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-downwardapi-md9v container test-container-subpath-downwardapi-md9v: <nil>
    STEP: delete the pod 08/11/23 15:17:37.362
    Aug 11 15:17:37.377: INFO: Waiting for pod pod-subpath-test-downwardapi-md9v to disappear
    Aug 11 15:17:37.379: INFO: Pod pod-subpath-test-downwardapi-md9v no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-md9v 08/11/23 15:17:37.379
    Aug 11 15:17:37.379: INFO: Deleting pod "pod-subpath-test-downwardapi-md9v" in namespace "subpath-2516"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:37.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2516" for this suite. 08/11/23 15:17:37.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:37.394
Aug 11 15:17:37.394: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename deployment 08/11/23 15:17:37.395
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:37.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:37.411
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 11 15:17:37.413: INFO: Creating deployment "test-recreate-deployment"
Aug 11 15:17:37.419: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 11 15:17:37.425: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 11 15:17:39.433: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 11 15:17:39.436: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 11 15:17:39.446: INFO: Updating deployment test-recreate-deployment
Aug 11 15:17:39.446: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:17:39.515: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5174  9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 53274 2 2023-08-11 15:17:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554a778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:17:39 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-11 15:17:39 +0000 UTC,LastTransitionTime:2023-08-11 15:17:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 11 15:17:39.518: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5174  6cca3642-4a1e-4a79-a142-e52c99f6a29c 53271 1 2023-08-11 15:17:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 0xc00554ac40 0xc00554ac41}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554acd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:17:39.518: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 11 15:17:39.518: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5174  a6d27b03-3615-49fc-94c8-7df540f4c473 53260 2 2023-08-11 15:17:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 0xc00554ab27 0xc00554ab28}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554abd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:17:39.521: INFO: Pod "test-recreate-deployment-cff6dc657-ljf95" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ljf95 test-recreate-deployment-cff6dc657- deployment-5174  c8b7294f-8f78-4f09-b6c1-12373b45c3d0 53275 0 2023-08-11 15:17:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6cca3642-4a1e-4a79-a142-e52c99f6a29c 0xc00554b150 0xc00554b151}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cca3642-4a1e-4a79-a142-e52c99f6a29c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kh45c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kh45c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 11 15:17:39.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5174" for this suite. 08/11/23 15:17:39.525
------------------------------
• [2.137 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:37.394
    Aug 11 15:17:37.394: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename deployment 08/11/23 15:17:37.395
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:37.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:37.411
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 11 15:17:37.413: INFO: Creating deployment "test-recreate-deployment"
    Aug 11 15:17:37.419: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 11 15:17:37.425: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 11 15:17:39.433: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 11 15:17:39.436: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 11 15:17:39.446: INFO: Updating deployment test-recreate-deployment
    Aug 11 15:17:39.446: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:17:39.515: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5174  9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 53274 2 2023-08-11 15:17:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554a778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:17:39 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-11 15:17:39 +0000 UTC,LastTransitionTime:2023-08-11 15:17:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 11 15:17:39.518: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5174  6cca3642-4a1e-4a79-a142-e52c99f6a29c 53271 1 2023-08-11 15:17:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 0xc00554ac40 0xc00554ac41}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554acd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:17:39.518: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 11 15:17:39.518: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5174  a6d27b03-3615-49fc-94c8-7df540f4c473 53260 2 2023-08-11 15:17:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c 0xc00554ab27 0xc00554ab28}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a25e5dc-816d-4fc1-83d8-3cdc15da9b6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00554abd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:17:39.521: INFO: Pod "test-recreate-deployment-cff6dc657-ljf95" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-ljf95 test-recreate-deployment-cff6dc657- deployment-5174  c8b7294f-8f78-4f09-b6c1-12373b45c3d0 53275 0 2023-08-11 15:17:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 6cca3642-4a1e-4a79-a142-e52c99f6a29c 0xc00554b150 0xc00554b151}] [] [{kube-controller-manager Update v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cca3642-4a1e-4a79-a142-e52c99f6a29c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:17:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kh45c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kh45c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-1cf5d931-worker-6381a7ba-nd80,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:17:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:17:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:17:39.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5174" for this suite. 08/11/23 15:17:39.525
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:17:39.531
Aug 11 15:17:39.531: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename aggregator 08/11/23 15:17:39.532
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:39.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:39.549
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 11 15:17:39.551: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/11/23 15:17:39.552
Aug 11 15:17:39.886: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 11 15:17:41.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:43.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:45.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:47.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:49.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:51.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:53.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:55.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:57.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:59.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:18:01.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:18:04.073: INFO: Waited 127.678224ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/11/23 15:18:04.131
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/11/23 15:18:04.135
STEP: List APIServices 08/11/23 15:18:04.142
Aug 11 15:18:04.149: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 11 15:18:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-7240" for this suite. 08/11/23 15:18:04.504
------------------------------
• [SLOW TEST] [25.027 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:17:39.531
    Aug 11 15:17:39.531: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename aggregator 08/11/23 15:17:39.532
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:39.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:39.549
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 11 15:17:39.551: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/11/23 15:17:39.552
    Aug 11 15:17:39.886: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 11 15:17:41.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:43.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:45.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:47.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:49.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:51.937: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:53.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:55.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:57.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:59.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:18:01.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 17, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:18:04.073: INFO: Waited 127.678224ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/11/23 15:18:04.131
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/11/23 15:18:04.135
    STEP: List APIServices 08/11/23 15:18:04.142
    Aug 11 15:18:04.149: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:18:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-7240" for this suite. 08/11/23 15:18:04.504
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:18:04.559
Aug 11 15:18:04.559: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 15:18:04.559
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:04.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:04.579
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470 08/11/23 15:18:04.582
Aug 11 15:18:04.591: INFO: Pod name my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Found 0 pods out of 1
Aug 11 15:18:09.598: INFO: Pod name my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Found 1 pods out of 1
Aug 11 15:18:09.598: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470" are running
Aug 11 15:18:09.598: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" in namespace "replication-controller-6037" to be "running"
Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx": Phase="Running", Reason="", readiness=true. Elapsed: 4.935455ms
Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" satisfied condition "running"
Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:04 +0000 UTC Reason: Message:}])
Aug 11 15:18:09.603: INFO: Trying to dial the pod
Aug 11 15:18:14.622: INFO: Controller my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Got expected result from replica 1 [my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx]: "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 15:18:14.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6037" for this suite. 08/11/23 15:18:14.626
------------------------------
• [SLOW TEST] [10.074 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:18:04.559
    Aug 11 15:18:04.559: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 15:18:04.559
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:04.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:04.579
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470 08/11/23 15:18:04.582
    Aug 11 15:18:04.591: INFO: Pod name my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Found 0 pods out of 1
    Aug 11 15:18:09.598: INFO: Pod name my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Found 1 pods out of 1
    Aug 11 15:18:09.598: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470" are running
    Aug 11 15:18:09.598: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" in namespace "replication-controller-6037" to be "running"
    Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx": Phase="Running", Reason="", readiness=true. Elapsed: 4.935455ms
    Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" satisfied condition "running"
    Aug 11 15:18:09.603: INFO: Pod "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 15:18:04 +0000 UTC Reason: Message:}])
    Aug 11 15:18:09.603: INFO: Trying to dial the pod
    Aug 11 15:18:14.622: INFO: Controller my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470: Got expected result from replica 1 [my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx]: "my-hostname-basic-1ef833fb-2c38-4af3-8bae-4dc588069470-m6kgx", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:18:14.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6037" for this suite. 08/11/23 15:18:14.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:18:14.634
Aug 11 15:18:14.634: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 15:18:14.634
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:14.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:14.652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/11/23 15:18:14.654
Aug 11 15:18:14.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-4984 create -f -'
Aug 11 15:18:15.429: INFO: stderr: ""
Aug 11 15:18:15.429: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 15:18:15.429
Aug 11 15:18:16.434: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:18:16.434: INFO: Found 0 / 1
Aug 11 15:18:17.433: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:18:17.433: INFO: Found 1 / 1
Aug 11 15:18:17.434: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/11/23 15:18:17.434
Aug 11 15:18:17.437: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:18:17.437: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 15:18:17.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-4984 patch pod agnhost-primary-8nqxr -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 11 15:18:17.493: INFO: stderr: ""
Aug 11 15:18:17.493: INFO: stdout: "pod/agnhost-primary-8nqxr patched\n"
STEP: checking annotations 08/11/23 15:18:17.493
Aug 11 15:18:17.497: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:18:17.497: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 15:18:17.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4984" for this suite. 08/11/23 15:18:17.502
------------------------------
• [2.876 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:18:14.634
    Aug 11 15:18:14.634: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:18:14.634
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:14.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:14.652
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/11/23 15:18:14.654
    Aug 11 15:18:14.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-4984 create -f -'
    Aug 11 15:18:15.429: INFO: stderr: ""
    Aug 11 15:18:15.429: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 15:18:15.429
    Aug 11 15:18:16.434: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:18:16.434: INFO: Found 0 / 1
    Aug 11 15:18:17.433: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:18:17.433: INFO: Found 1 / 1
    Aug 11 15:18:17.434: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/11/23 15:18:17.434
    Aug 11 15:18:17.437: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:18:17.437: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 15:18:17.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-4984 patch pod agnhost-primary-8nqxr -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 11 15:18:17.493: INFO: stderr: ""
    Aug 11 15:18:17.493: INFO: stdout: "pod/agnhost-primary-8nqxr patched\n"
    STEP: checking annotations 08/11/23 15:18:17.493
    Aug 11 15:18:17.497: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:18:17.497: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:18:17.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4984" for this suite. 08/11/23 15:18:17.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:18:17.511
Aug 11 15:18:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename resourcequota 08/11/23 15:18:17.512
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:17.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:17.528
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-c49fd" 08/11/23 15:18:17.533
Aug 11 15:18:17.543: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard cpu limit of 500m
Aug 11 15:18:17.543: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.543
STEP: Confirm /status for "e2e-rq-status-c49fd" resourceQuota via watch 08/11/23 15:18:17.551
Aug 11 15:18:17.553: INFO: observed resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList(nil)
Aug 11 15:18:17.553: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 11 15:18:17.553: INFO: ResourceQuota "e2e-rq-status-c49fd" /status was updated
STEP: Patching hard spec values for cpu & memory 08/11/23 15:18:17.557
Aug 11 15:18:17.562: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard cpu limit of 1
Aug 11 15:18:17.562: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.562
STEP: Confirm /status for "e2e-rq-status-c49fd" resourceQuota via watch 08/11/23 15:18:17.569
Aug 11 15:18:17.570: INFO: observed resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 11 15:18:17.571: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 11 15:18:17.571: INFO: ResourceQuota "e2e-rq-status-c49fd" /status was patched
STEP: Get "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.571
Aug 11 15:18:17.574: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard cpu of 1
Aug 11 15:18:17.574: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-c49fd" /status before checking Spec is unchanged 08/11/23 15:18:17.577
Aug 11 15:18:17.583: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard cpu of 2
Aug 11 15:18:17.583: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard memory of 2Gi
Aug 11 15:18:17.584: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 11 15:22:27.590: INFO: ResourceQuota "e2e-rq-status-c49fd" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 11 15:22:27.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2343" for this suite. 08/11/23 15:22:27.594
------------------------------
• [SLOW TEST] [250.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:18:17.511
    Aug 11 15:18:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename resourcequota 08/11/23 15:18:17.512
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:18:17.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:18:17.528
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-c49fd" 08/11/23 15:18:17.533
    Aug 11 15:18:17.543: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard cpu limit of 500m
    Aug 11 15:18:17.543: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.543
    STEP: Confirm /status for "e2e-rq-status-c49fd" resourceQuota via watch 08/11/23 15:18:17.551
    Aug 11 15:18:17.553: INFO: observed resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList(nil)
    Aug 11 15:18:17.553: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 11 15:18:17.553: INFO: ResourceQuota "e2e-rq-status-c49fd" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/11/23 15:18:17.557
    Aug 11 15:18:17.562: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard cpu limit of 1
    Aug 11 15:18:17.562: INFO: Resource quota "e2e-rq-status-c49fd" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.562
    STEP: Confirm /status for "e2e-rq-status-c49fd" resourceQuota via watch 08/11/23 15:18:17.569
    Aug 11 15:18:17.570: INFO: observed resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 11 15:18:17.571: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 11 15:18:17.571: INFO: ResourceQuota "e2e-rq-status-c49fd" /status was patched
    STEP: Get "e2e-rq-status-c49fd" /status 08/11/23 15:18:17.571
    Aug 11 15:18:17.574: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard cpu of 1
    Aug 11 15:18:17.574: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-c49fd" /status before checking Spec is unchanged 08/11/23 15:18:17.577
    Aug 11 15:18:17.583: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard cpu of 2
    Aug 11 15:18:17.583: INFO: Resourcequota "e2e-rq-status-c49fd" reports status: hard memory of 2Gi
    Aug 11 15:18:17.584: INFO: Found resourceQuota "e2e-rq-status-c49fd" in namespace "resourcequota-2343" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 11 15:22:27.590: INFO: ResourceQuota "e2e-rq-status-c49fd" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:22:27.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2343" for this suite. 08/11/23 15:22:27.594
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:22:27.601
Aug 11 15:22:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename replication-controller 08/11/23 15:22:27.602
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:27.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:27.62
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-5wdqk" 08/11/23 15:22:27.622
Aug 11 15:22:27.626: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
Aug 11 15:22:28.629: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
Aug 11 15:22:28.632: INFO: Found 1 replicas for "e2e-rc-5wdqk" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-5wdqk" 08/11/23 15:22:28.632
STEP: Updating a scale subresource 08/11/23 15:22:28.635
STEP: Verifying replicas where modified for replication controller "e2e-rc-5wdqk" 08/11/23 15:22:28.641
Aug 11 15:22:28.641: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
Aug 11 15:22:29.646: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
Aug 11 15:22:29.649: INFO: Found 2 replicas for "e2e-rc-5wdqk" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 11 15:22:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6042" for this suite. 08/11/23 15:22:29.654
------------------------------
• [2.060 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:22:27.601
    Aug 11 15:22:27.601: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename replication-controller 08/11/23 15:22:27.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:27.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:27.62
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-5wdqk" 08/11/23 15:22:27.622
    Aug 11 15:22:27.626: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
    Aug 11 15:22:28.629: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
    Aug 11 15:22:28.632: INFO: Found 1 replicas for "e2e-rc-5wdqk" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-5wdqk" 08/11/23 15:22:28.632
    STEP: Updating a scale subresource 08/11/23 15:22:28.635
    STEP: Verifying replicas where modified for replication controller "e2e-rc-5wdqk" 08/11/23 15:22:28.641
    Aug 11 15:22:28.641: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
    Aug 11 15:22:29.646: INFO: Get Replication Controller "e2e-rc-5wdqk" to confirm replicas
    Aug 11 15:22:29.649: INFO: Found 2 replicas for "e2e-rc-5wdqk" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:22:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6042" for this suite. 08/11/23 15:22:29.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:22:29.662
Aug 11 15:22:29.662: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 15:22:29.663
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:29.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:29.683
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-5d481af1-9658-4f25-aca4-1e8daa3fd9f5 08/11/23 15:22:29.685
STEP: Creating a pod to test consume secrets 08/11/23 15:22:29.689
Aug 11 15:22:29.698: INFO: Waiting up to 5m0s for pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3" in namespace "secrets-5960" to be "Succeeded or Failed"
Aug 11 15:22:29.702: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935213ms
Aug 11 15:22:31.706: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007940824s
Aug 11 15:22:33.708: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600431s
STEP: Saw pod success 08/11/23 15:22:33.708
Aug 11 15:22:33.708: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3" satisfied condition "Succeeded or Failed"
Aug 11 15:22:33.711: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 15:22:33.729
Aug 11 15:22:33.741: INFO: Waiting for pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 to disappear
Aug 11 15:22:33.743: INFO: Pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 15:22:33.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5960" for this suite. 08/11/23 15:22:33.747
------------------------------
• [4.091 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:22:29.662
    Aug 11 15:22:29.662: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 15:22:29.663
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:29.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:29.683
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-5d481af1-9658-4f25-aca4-1e8daa3fd9f5 08/11/23 15:22:29.685
    STEP: Creating a pod to test consume secrets 08/11/23 15:22:29.689
    Aug 11 15:22:29.698: INFO: Waiting up to 5m0s for pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3" in namespace "secrets-5960" to be "Succeeded or Failed"
    Aug 11 15:22:29.702: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935213ms
    Aug 11 15:22:31.706: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007940824s
    Aug 11 15:22:33.708: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600431s
    STEP: Saw pod success 08/11/23 15:22:33.708
    Aug 11 15:22:33.708: INFO: Pod "pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3" satisfied condition "Succeeded or Failed"
    Aug 11 15:22:33.711: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:22:33.729
    Aug 11 15:22:33.741: INFO: Waiting for pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 to disappear
    Aug 11 15:22:33.743: INFO: Pod pod-secrets-ed2d8871-7e79-4411-a9ec-20af9d4722a3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:22:33.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5960" for this suite. 08/11/23 15:22:33.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:22:33.753
Aug 11 15:22:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption 08/11/23 15:22:33.754
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:33.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:33.775
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/11/23 15:22:33.777
STEP: Waiting for the pdb to be processed 08/11/23 15:22:33.782
STEP: First trying to evict a pod which shouldn't be evictable 08/11/23 15:22:35.796
STEP: Waiting for all pods to be running 08/11/23 15:22:35.796
Aug 11 15:22:35.799: INFO: pods: 0 < 3
STEP: locating a running pod 08/11/23 15:22:37.804
STEP: Updating the pdb to allow a pod to be evicted 08/11/23 15:22:37.814
STEP: Waiting for the pdb to be processed 08/11/23 15:22:37.822
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 15:22:39.831
STEP: Waiting for all pods to be running 08/11/23 15:22:39.831
STEP: Waiting for the pdb to observed all healthy pods 08/11/23 15:22:39.834
STEP: Patching the pdb to disallow a pod to be evicted 08/11/23 15:22:39.86
STEP: Waiting for the pdb to be processed 08/11/23 15:22:39.883
STEP: Waiting for all pods to be running 08/11/23 15:22:41.894
STEP: locating a running pod 08/11/23 15:22:41.897
STEP: Deleting the pdb to allow a pod to be evicted 08/11/23 15:22:41.908
STEP: Waiting for the pdb to be deleted 08/11/23 15:22:41.914
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 15:22:41.916
STEP: Waiting for all pods to be running 08/11/23 15:22:41.916
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 11 15:22:41.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6282" for this suite. 08/11/23 15:22:41.941
------------------------------
• [SLOW TEST] [8.201 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:22:33.753
    Aug 11 15:22:33.753: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption 08/11/23 15:22:33.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:33.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:33.775
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/11/23 15:22:33.777
    STEP: Waiting for the pdb to be processed 08/11/23 15:22:33.782
    STEP: First trying to evict a pod which shouldn't be evictable 08/11/23 15:22:35.796
    STEP: Waiting for all pods to be running 08/11/23 15:22:35.796
    Aug 11 15:22:35.799: INFO: pods: 0 < 3
    STEP: locating a running pod 08/11/23 15:22:37.804
    STEP: Updating the pdb to allow a pod to be evicted 08/11/23 15:22:37.814
    STEP: Waiting for the pdb to be processed 08/11/23 15:22:37.822
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 15:22:39.831
    STEP: Waiting for all pods to be running 08/11/23 15:22:39.831
    STEP: Waiting for the pdb to observed all healthy pods 08/11/23 15:22:39.834
    STEP: Patching the pdb to disallow a pod to be evicted 08/11/23 15:22:39.86
    STEP: Waiting for the pdb to be processed 08/11/23 15:22:39.883
    STEP: Waiting for all pods to be running 08/11/23 15:22:41.894
    STEP: locating a running pod 08/11/23 15:22:41.897
    STEP: Deleting the pdb to allow a pod to be evicted 08/11/23 15:22:41.908
    STEP: Waiting for the pdb to be deleted 08/11/23 15:22:41.914
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 15:22:41.916
    STEP: Waiting for all pods to be running 08/11/23 15:22:41.916
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:22:41.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6282" for this suite. 08/11/23 15:22:41.941
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:22:41.954
Aug 11 15:22:41.954: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 15:22:41.955
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:41.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:41.977
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/11/23 15:22:41.98
STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:22:42.21
Aug 11 15:22:42.319: INFO: Pod name wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 15:22:42.319
Aug 11 15:22:42.319: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:22:42.359: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 39.799039ms
Aug 11 15:22:44.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044345729s
Aug 11 15:22:46.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044525836s
Aug 11 15:22:48.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044679763s
Aug 11 15:22:50.365: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04569408s
Aug 11 15:22:52.363: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043863514s
Aug 11 15:22:54.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048746735s
Aug 11 15:22:56.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Running", Reason="", readiness=true. Elapsed: 14.044936488s
Aug 11 15:22:56.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck" satisfied condition "running"
Aug 11 15:22:56.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:22:56.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv": Phase="Running", Reason="", readiness=true. Elapsed: 3.428283ms
Aug 11 15:22:56.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv" satisfied condition "running"
Aug 11 15:22:56.368: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:22:56.371: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v": Phase="Running", Reason="", readiness=true. Elapsed: 3.412294ms
Aug 11 15:22:56.371: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v" satisfied condition "running"
Aug 11 15:22:56.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:22:56.376: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn": Phase="Running", Reason="", readiness=true. Elapsed: 4.129064ms
Aug 11 15:22:56.376: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn" satisfied condition "running"
Aug 11 15:22:56.376: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:22:56.379: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4": Phase="Running", Reason="", readiness=true. Elapsed: 3.270703ms
Aug 11 15:22:56.379: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:22:56.379
Aug 11 15:22:56.440: INFO: Deleting ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b took: 7.139328ms
Aug 11 15:22:56.540: INFO: Terminating ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b pods took: 100.346548ms
STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:23:01.146
Aug 11 15:23:01.159: INFO: Pod name wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58: Found 0 pods out of 5
Aug 11 15:23:06.166: INFO: Pod name wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 15:23:06.166
Aug 11 15:23:06.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:06.170: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687033ms
Aug 11 15:23:08.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008292644s
Aug 11 15:23:10.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008605422s
Aug 11 15:23:12.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01135757s
Aug 11 15:23:14.176: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009740125s
Aug 11 15:23:16.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00866396s
Aug 11 15:23:18.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Running", Reason="", readiness=true. Elapsed: 12.008387906s
Aug 11 15:23:18.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n" satisfied condition "running"
Aug 11 15:23:18.175: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:18.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg": Phase="Running", Reason="", readiness=true. Elapsed: 3.643223ms
Aug 11 15:23:18.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg" satisfied condition "running"
Aug 11 15:23:18.178: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:18.182: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk": Phase="Running", Reason="", readiness=true. Elapsed: 3.313073ms
Aug 11 15:23:18.182: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk" satisfied condition "running"
Aug 11 15:23:18.182: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:18.188: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp": Phase="Running", Reason="", readiness=true. Elapsed: 6.351747ms
Aug 11 15:23:18.188: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp" satisfied condition "running"
Aug 11 15:23:18.188: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:18.192: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn": Phase="Running", Reason="", readiness=true. Elapsed: 3.619043ms
Aug 11 15:23:18.192: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:23:18.192
Aug 11 15:23:18.254: INFO: Deleting ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 took: 8.611528ms
Aug 11 15:23:18.355: INFO: Terminating ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 pods took: 100.617969ms
STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:23:21.56
Aug 11 15:23:21.575: INFO: Pod name wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33: Found 0 pods out of 5
Aug 11 15:23:26.581: INFO: Pod name wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 15:23:26.581
Aug 11 15:23:26.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:26.586: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720335ms
Aug 11 15:23:28.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009645456s
Aug 11 15:23:30.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009265422s
Aug 11 15:23:32.592: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010365139s
Aug 11 15:23:34.590: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009016553s
Aug 11 15:23:36.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.009990391s
Aug 11 15:23:36.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x" satisfied condition "running"
Aug 11 15:23:36.591: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:36.595: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2": Phase="Running", Reason="", readiness=true. Elapsed: 3.693933ms
Aug 11 15:23:36.595: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2" satisfied condition "running"
Aug 11 15:23:36.595: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:36.598: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq": Phase="Running", Reason="", readiness=true. Elapsed: 3.217693ms
Aug 11 15:23:36.598: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq" satisfied condition "running"
Aug 11 15:23:36.598: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:36.601: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.982643ms
Aug 11 15:23:36.601: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8" satisfied condition "running"
Aug 11 15:23:36.601: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5" in namespace "emptydir-wrapper-5722" to be "running"
Aug 11 15:23:36.604: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5": Phase="Running", Reason="", readiness=true. Elapsed: 2.844803ms
Aug 11 15:23:36.604: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:23:36.604
Aug 11 15:23:36.666: INFO: Deleting ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 took: 7.632848ms
Aug 11 15:23:36.767: INFO: Terminating ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 pods took: 100.770457ms
STEP: Cleaning up the configMaps 08/11/23 15:23:39.068
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:23:39.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5722" for this suite. 08/11/23 15:23:39.373
------------------------------
• [SLOW TEST] [57.425 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:22:41.954
    Aug 11 15:22:41.954: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 15:22:41.955
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:41.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:41.977
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/11/23 15:22:41.98
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:22:42.21
    Aug 11 15:22:42.319: INFO: Pod name wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 15:22:42.319
    Aug 11 15:22:42.319: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:22:42.359: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 39.799039ms
    Aug 11 15:22:44.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044345729s
    Aug 11 15:22:46.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044525836s
    Aug 11 15:22:48.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044679763s
    Aug 11 15:22:50.365: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04569408s
    Aug 11 15:22:52.363: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 10.043863514s
    Aug 11 15:22:54.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Pending", Reason="", readiness=false. Elapsed: 12.048746735s
    Aug 11 15:22:56.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck": Phase="Running", Reason="", readiness=true. Elapsed: 14.044936488s
    Aug 11 15:22:56.364: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-4gdck" satisfied condition "running"
    Aug 11 15:22:56.364: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:22:56.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv": Phase="Running", Reason="", readiness=true. Elapsed: 3.428283ms
    Aug 11 15:22:56.368: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-9zhrv" satisfied condition "running"
    Aug 11 15:22:56.368: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:22:56.371: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v": Phase="Running", Reason="", readiness=true. Elapsed: 3.412294ms
    Aug 11 15:22:56.371: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-cgj8v" satisfied condition "running"
    Aug 11 15:22:56.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:22:56.376: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn": Phase="Running", Reason="", readiness=true. Elapsed: 4.129064ms
    Aug 11 15:22:56.376: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-dmssn" satisfied condition "running"
    Aug 11 15:22:56.376: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:22:56.379: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4": Phase="Running", Reason="", readiness=true. Elapsed: 3.270703ms
    Aug 11 15:22:56.379: INFO: Pod "wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b-fcpv4" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:22:56.379
    Aug 11 15:22:56.440: INFO: Deleting ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b took: 7.139328ms
    Aug 11 15:22:56.540: INFO: Terminating ReplicationController wrapped-volume-race-88c87335-f9d1-4e63-8fba-a84e618b348b pods took: 100.346548ms
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:23:01.146
    Aug 11 15:23:01.159: INFO: Pod name wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58: Found 0 pods out of 5
    Aug 11 15:23:06.166: INFO: Pod name wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 15:23:06.166
    Aug 11 15:23:06.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:06.170: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687033ms
    Aug 11 15:23:08.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008292644s
    Aug 11 15:23:10.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008605422s
    Aug 11 15:23:12.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01135757s
    Aug 11 15:23:14.176: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009740125s
    Aug 11 15:23:16.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00866396s
    Aug 11 15:23:18.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n": Phase="Running", Reason="", readiness=true. Elapsed: 12.008387906s
    Aug 11 15:23:18.175: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-2lz9n" satisfied condition "running"
    Aug 11 15:23:18.175: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:18.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg": Phase="Running", Reason="", readiness=true. Elapsed: 3.643223ms
    Aug 11 15:23:18.178: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-bxfxg" satisfied condition "running"
    Aug 11 15:23:18.178: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:18.182: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk": Phase="Running", Reason="", readiness=true. Elapsed: 3.313073ms
    Aug 11 15:23:18.182: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-hmdtk" satisfied condition "running"
    Aug 11 15:23:18.182: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:18.188: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp": Phase="Running", Reason="", readiness=true. Elapsed: 6.351747ms
    Aug 11 15:23:18.188: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-j9djp" satisfied condition "running"
    Aug 11 15:23:18.188: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:18.192: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn": Phase="Running", Reason="", readiness=true. Elapsed: 3.619043ms
    Aug 11 15:23:18.192: INFO: Pod "wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58-x87fn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:23:18.192
    Aug 11 15:23:18.254: INFO: Deleting ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 took: 8.611528ms
    Aug 11 15:23:18.355: INFO: Terminating ReplicationController wrapped-volume-race-93738e3d-2d95-4502-bbb3-b227edd4be58 pods took: 100.617969ms
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 15:23:21.56
    Aug 11 15:23:21.575: INFO: Pod name wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33: Found 0 pods out of 5
    Aug 11 15:23:26.581: INFO: Pod name wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 15:23:26.581
    Aug 11 15:23:26.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:26.586: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720335ms
    Aug 11 15:23:28.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009645456s
    Aug 11 15:23:30.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009265422s
    Aug 11 15:23:32.592: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010365139s
    Aug 11 15:23:34.590: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009016553s
    Aug 11 15:23:36.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x": Phase="Running", Reason="", readiness=true. Elapsed: 10.009990391s
    Aug 11 15:23:36.591: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-2xg6x" satisfied condition "running"
    Aug 11 15:23:36.591: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:36.595: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2": Phase="Running", Reason="", readiness=true. Elapsed: 3.693933ms
    Aug 11 15:23:36.595: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-9kvj2" satisfied condition "running"
    Aug 11 15:23:36.595: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:36.598: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq": Phase="Running", Reason="", readiness=true. Elapsed: 3.217693ms
    Aug 11 15:23:36.598: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-kgftq" satisfied condition "running"
    Aug 11 15:23:36.598: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:36.601: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.982643ms
    Aug 11 15:23:36.601: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-lndm8" satisfied condition "running"
    Aug 11 15:23:36.601: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5" in namespace "emptydir-wrapper-5722" to be "running"
    Aug 11 15:23:36.604: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5": Phase="Running", Reason="", readiness=true. Elapsed: 2.844803ms
    Aug 11 15:23:36.604: INFO: Pod "wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33-qdht5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 in namespace emptydir-wrapper-5722, will wait for the garbage collector to delete the pods 08/11/23 15:23:36.604
    Aug 11 15:23:36.666: INFO: Deleting ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 took: 7.632848ms
    Aug 11 15:23:36.767: INFO: Terminating ReplicationController wrapped-volume-race-bfd36002-b157-440a-a21d-7c68423a3c33 pods took: 100.770457ms
    STEP: Cleaning up the configMaps 08/11/23 15:23:39.068
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:23:39.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5722" for this suite. 08/11/23 15:23:39.373
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:23:39.379
Aug 11 15:23:39.379: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:23:39.38
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:39.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:39.396
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-ed810c64-2b50-4ac0-97c1-de1af6db87b5 08/11/23 15:23:39.398
STEP: Creating a pod to test consume configMaps 08/11/23 15:23:39.404
Aug 11 15:23:39.411: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478" in namespace "configmap-2533" to be "Succeeded or Failed"
Aug 11 15:23:39.415: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838355ms
Aug 11 15:23:41.420: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009093854s
Aug 11 15:23:43.420: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009782822s
Aug 11 15:23:45.421: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010080378s
STEP: Saw pod success 08/11/23 15:23:45.421
Aug 11 15:23:45.421: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478" satisfied condition "Succeeded or Failed"
Aug 11 15:23:45.424: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 15:23:45.436
Aug 11 15:23:45.450: INFO: Waiting for pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 to disappear
Aug 11 15:23:45.453: INFO: Pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:23:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2533" for this suite. 08/11/23 15:23:45.459
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:23:39.379
    Aug 11 15:23:39.379: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:23:39.38
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:39.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:39.396
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-ed810c64-2b50-4ac0-97c1-de1af6db87b5 08/11/23 15:23:39.398
    STEP: Creating a pod to test consume configMaps 08/11/23 15:23:39.404
    Aug 11 15:23:39.411: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478" in namespace "configmap-2533" to be "Succeeded or Failed"
    Aug 11 15:23:39.415: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 4.838355ms
    Aug 11 15:23:41.420: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009093854s
    Aug 11 15:23:43.420: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009782822s
    Aug 11 15:23:45.421: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010080378s
    STEP: Saw pod success 08/11/23 15:23:45.421
    Aug 11 15:23:45.421: INFO: Pod "pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478" satisfied condition "Succeeded or Failed"
    Aug 11 15:23:45.424: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 15:23:45.436
    Aug 11 15:23:45.450: INFO: Waiting for pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 to disappear
    Aug 11 15:23:45.453: INFO: Pod pod-configmaps-2a3065c3-ca52-460e-8cbb-dc2723234478 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:23:45.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2533" for this suite. 08/11/23 15:23:45.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:23:45.467
Aug 11 15:23:45.467: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 15:23:45.467
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:45.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:45.482
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-396631ab-9966-4779-bdbb-0d313a439833 08/11/23 15:23:45.485
STEP: Creating a pod to test consume secrets 08/11/23 15:23:45.489
Aug 11 15:23:45.498: INFO: Waiting up to 5m0s for pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6" in namespace "secrets-1482" to be "Succeeded or Failed"
Aug 11 15:23:45.503: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198906ms
Aug 11 15:23:47.508: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010042127s
Aug 11 15:23:49.507: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509952s
STEP: Saw pod success 08/11/23 15:23:49.507
Aug 11 15:23:49.508: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6" satisfied condition "Succeeded or Failed"
Aug 11 15:23:49.511: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 15:23:49.519
Aug 11 15:23:49.534: INFO: Waiting for pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 to disappear
Aug 11 15:23:49.537: INFO: Pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 15:23:49.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1482" for this suite. 08/11/23 15:23:49.543
------------------------------
• [4.082 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:23:45.467
    Aug 11 15:23:45.467: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 15:23:45.467
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:45.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:45.482
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-396631ab-9966-4779-bdbb-0d313a439833 08/11/23 15:23:45.485
    STEP: Creating a pod to test consume secrets 08/11/23 15:23:45.489
    Aug 11 15:23:45.498: INFO: Waiting up to 5m0s for pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6" in namespace "secrets-1482" to be "Succeeded or Failed"
    Aug 11 15:23:45.503: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198906ms
    Aug 11 15:23:47.508: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010042127s
    Aug 11 15:23:49.507: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009509952s
    STEP: Saw pod success 08/11/23 15:23:49.507
    Aug 11 15:23:49.508: INFO: Pod "pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6" satisfied condition "Succeeded or Failed"
    Aug 11 15:23:49.511: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:23:49.519
    Aug 11 15:23:49.534: INFO: Waiting for pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 to disappear
    Aug 11 15:23:49.537: INFO: Pod pod-secrets-857a959a-7a50-4edd-a084-f25883d63fc6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:23:49.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1482" for this suite. 08/11/23 15:23:49.543
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:23:49.549
Aug 11 15:23:49.549: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 15:23:49.55
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:49.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:49.567
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/11/23 15:23:49.569
Aug 11 15:23:49.569: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1682 proxy --unix-socket=/tmp/kubectl-proxy-unix1080313837/test'
STEP: retrieving proxy /api/ output 08/11/23 15:23:49.606
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 15:23:49.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1682" for this suite. 08/11/23 15:23:49.611
------------------------------
• [0.069 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:23:49.549
    Aug 11 15:23:49.549: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:23:49.55
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:49.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:49.567
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/11/23 15:23:49.569
    Aug 11 15:23:49.569: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-1682 proxy --unix-socket=/tmp/kubectl-proxy-unix1080313837/test'
    STEP: retrieving proxy /api/ output 08/11/23 15:23:49.606
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:23:49.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1682" for this suite. 08/11/23 15:23:49.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:23:49.618
Aug 11 15:23:49.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:23:49.619
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:49.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:49.634
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 11 15:23:49.649: INFO: created pod
Aug 11 15:23:49.649: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5777" to be "Succeeded or Failed"
Aug 11 15:23:49.651: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489422ms
Aug 11 15:23:51.656: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006948643s
Aug 11 15:23:53.656: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006622008s
STEP: Saw pod success 08/11/23 15:23:53.656
Aug 11 15:23:53.656: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 11 15:24:23.659: INFO: polling logs
Aug 11 15:24:23.670: INFO: Pod logs: 
I0811 15:23:50.624910       1 log.go:198] OK: Got token
I0811 15:23:50.624943       1 log.go:198] validating with in-cluster discovery
I0811 15:23:50.625208       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0811 15:23:50.625234       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5777:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768029, NotBefore:1691767429, IssuedAt:1691767429, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5777", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"60f9adbc-6ba5-4f40-96cd-410800df885b"}}}
I0811 15:23:50.637672       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0811 15:23:50.643357       1 log.go:198] OK: Validated signature on JWT
I0811 15:23:50.643467       1 log.go:198] OK: Got valid claims from token!
I0811 15:23:50.643528       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5777:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768029, NotBefore:1691767429, IssuedAt:1691767429, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5777", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"60f9adbc-6ba5-4f40-96cd-410800df885b"}}}

Aug 11 15:24:23.670: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 11 15:24:23.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5777" for this suite. 08/11/23 15:24:23.681
------------------------------
• [SLOW TEST] [34.069 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:23:49.618
    Aug 11 15:23:49.619: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:23:49.619
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:49.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:49.634
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 11 15:23:49.649: INFO: created pod
    Aug 11 15:23:49.649: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5777" to be "Succeeded or Failed"
    Aug 11 15:23:49.651: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.489422ms
    Aug 11 15:23:51.656: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006948643s
    Aug 11 15:23:53.656: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006622008s
    STEP: Saw pod success 08/11/23 15:23:53.656
    Aug 11 15:23:53.656: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 11 15:24:23.659: INFO: polling logs
    Aug 11 15:24:23.670: INFO: Pod logs: 
    I0811 15:23:50.624910       1 log.go:198] OK: Got token
    I0811 15:23:50.624943       1 log.go:198] validating with in-cluster discovery
    I0811 15:23:50.625208       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0811 15:23:50.625234       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5777:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768029, NotBefore:1691767429, IssuedAt:1691767429, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5777", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"60f9adbc-6ba5-4f40-96cd-410800df885b"}}}
    I0811 15:23:50.637672       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0811 15:23:50.643357       1 log.go:198] OK: Validated signature on JWT
    I0811 15:23:50.643467       1 log.go:198] OK: Got valid claims from token!
    I0811 15:23:50.643528       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5777:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768029, NotBefore:1691767429, IssuedAt:1691767429, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5777", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"60f9adbc-6ba5-4f40-96cd-410800df885b"}}}

    Aug 11 15:24:23.670: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:24:23.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5777" for this suite. 08/11/23 15:24:23.681
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:24:23.688
Aug 11 15:24:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 15:24:23.689
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:23.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:23.706
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/11/23 15:24:23.708
STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:24:23.714
STEP: delete the deployment 08/11/23 15:24:24.226
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/11/23 15:24:24.232
STEP: Gathering metrics 08/11/23 15:24:24.753
Aug 11 15:24:24.792: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
Aug 11 15:24:24.796: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.800224ms
Aug 11 15:24:24.796: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
Aug 11 15:24:24.796: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
Aug 11 15:24:24.872: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 15:24:24.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3706" for this suite. 08/11/23 15:24:24.876
------------------------------
• [1.196 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:24:23.688
    Aug 11 15:24:23.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 15:24:23.689
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:23.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:23.706
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/11/23 15:24:23.708
    STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:24:23.714
    STEP: delete the deployment 08/11/23 15:24:24.226
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/11/23 15:24:24.232
    STEP: Gathering metrics 08/11/23 15:24:24.753
    Aug 11 15:24:24.792: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" in namespace "kube-system" to be "running and ready"
    Aug 11 15:24:24.796: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn": Phase="Running", Reason="", readiness=true. Elapsed: 3.800224ms
    Aug 11 15:24:24.796: INFO: The phase of Pod kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn is Running (Ready = true)
    Aug 11 15:24:24.796: INFO: Pod "kube-controller-manager-constell-1cf5d931-control-plane-bb71bd41-kmfn" satisfied condition "running and ready"
    Aug 11 15:24:24.872: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:24:24.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3706" for this suite. 08/11/23 15:24:24.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:24:24.886
Aug 11 15:24:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubectl 08/11/23 15:24:24.886
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:24.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:24.904
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/11/23 15:24:24.906
Aug 11 15:24:24.906: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 11 15:24:24.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:25.718: INFO: stderr: ""
Aug 11 15:24:25.718: INFO: stdout: "service/agnhost-replica created\n"
Aug 11 15:24:25.718: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 11 15:24:25.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:25.931: INFO: stderr: ""
Aug 11 15:24:25.931: INFO: stdout: "service/agnhost-primary created\n"
Aug 11 15:24:25.931: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 11 15:24:25.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:26.757: INFO: stderr: ""
Aug 11 15:24:26.757: INFO: stdout: "service/frontend created\n"
Aug 11 15:24:26.757: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 11 15:24:26.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:26.964: INFO: stderr: ""
Aug 11 15:24:26.964: INFO: stdout: "deployment.apps/frontend created\n"
Aug 11 15:24:26.964: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 11 15:24:26.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:27.152: INFO: stderr: ""
Aug 11 15:24:27.152: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 11 15:24:27.152: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 11 15:24:27.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
Aug 11 15:24:27.348: INFO: stderr: ""
Aug 11 15:24:27.348: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/11/23 15:24:27.348
Aug 11 15:24:27.348: INFO: Waiting for all frontend pods to be Running.
Aug 11 15:24:32.401: INFO: Waiting for frontend to serve content.
Aug 11 15:24:32.417: INFO: Trying to add a new entry to the guestbook.
Aug 11 15:24:32.430: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/11/23 15:24:32.448
Aug 11 15:24:32.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.527: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.527: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:24:32.527
Aug 11 15:24:32.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.606: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:24:32.606
Aug 11 15:24:32.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.683: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.683: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:24:32.683
Aug 11 15:24:32.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.742: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.742: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:24:32.742
Aug 11 15:24:32.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.813: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.813: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:24:32.813
Aug 11 15:24:32.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
Aug 11 15:24:32.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:32.879: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 11 15:24:32.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2948" for this suite. 08/11/23 15:24:32.891
------------------------------
• [SLOW TEST] [8.018 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:24:24.886
    Aug 11 15:24:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:24:24.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:24.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:24.904
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/11/23 15:24:24.906
    Aug 11 15:24:24.906: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 11 15:24:24.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:25.718: INFO: stderr: ""
    Aug 11 15:24:25.718: INFO: stdout: "service/agnhost-replica created\n"
    Aug 11 15:24:25.718: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 11 15:24:25.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:25.931: INFO: stderr: ""
    Aug 11 15:24:25.931: INFO: stdout: "service/agnhost-primary created\n"
    Aug 11 15:24:25.931: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 11 15:24:25.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:26.757: INFO: stderr: ""
    Aug 11 15:24:26.757: INFO: stdout: "service/frontend created\n"
    Aug 11 15:24:26.757: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 11 15:24:26.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:26.964: INFO: stderr: ""
    Aug 11 15:24:26.964: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 11 15:24:26.964: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 11 15:24:26.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:27.152: INFO: stderr: ""
    Aug 11 15:24:27.152: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 11 15:24:27.152: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 11 15:24:27.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 create -f -'
    Aug 11 15:24:27.348: INFO: stderr: ""
    Aug 11 15:24:27.348: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/11/23 15:24:27.348
    Aug 11 15:24:27.348: INFO: Waiting for all frontend pods to be Running.
    Aug 11 15:24:32.401: INFO: Waiting for frontend to serve content.
    Aug 11 15:24:32.417: INFO: Trying to add a new entry to the guestbook.
    Aug 11 15:24:32.430: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/11/23 15:24:32.448
    Aug 11 15:24:32.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.527: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.527: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:24:32.527
    Aug 11 15:24:32.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.606: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.606: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:24:32.606
    Aug 11 15:24:32.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.683: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.683: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:24:32.683
    Aug 11 15:24:32.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.742: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.742: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:24:32.742
    Aug 11 15:24:32.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.813: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.813: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:24:32.813
    Aug 11 15:24:32.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=kubectl-2948 delete --grace-period=0 --force -f -'
    Aug 11 15:24:32.879: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:32.879: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:24:32.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2948" for this suite. 08/11/23 15:24:32.891
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:24:32.904
Aug 11 15:24:32.904: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:24:32.905
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:32.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:32.922
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 11 15:24:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:24:34.979
Aug 11 15:24:34.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 create -f -'
Aug 11 15:24:35.532: INFO: stderr: ""
Aug 11 15:24:35.532: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 11 15:24:35.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 delete e2e-test-crd-publish-openapi-904-crds test-cr'
Aug 11 15:24:35.603: INFO: stderr: ""
Aug 11 15:24:35.603: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 11 15:24:35.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 apply -f -'
Aug 11 15:24:36.094: INFO: stderr: ""
Aug 11 15:24:36.094: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 11 15:24:36.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 delete e2e-test-crd-publish-openapi-904-crds test-cr'
Aug 11 15:24:36.149: INFO: stderr: ""
Aug 11 15:24:36.149: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/11/23 15:24:36.149
Aug 11 15:24:36.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 explain e2e-test-crd-publish-openapi-904-crds'
Aug 11 15:24:36.619: INFO: stderr: ""
Aug 11 15:24:36.619: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-904-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:24:39.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8382" for this suite. 08/11/23 15:24:39.103
------------------------------
• [SLOW TEST] [6.206 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:24:32.904
    Aug 11 15:24:32.904: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:24:32.905
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:32.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:32.922
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 11 15:24:32.925: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:24:34.979
    Aug 11 15:24:34.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 create -f -'
    Aug 11 15:24:35.532: INFO: stderr: ""
    Aug 11 15:24:35.532: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 11 15:24:35.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 delete e2e-test-crd-publish-openapi-904-crds test-cr'
    Aug 11 15:24:35.603: INFO: stderr: ""
    Aug 11 15:24:35.603: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 11 15:24:35.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 apply -f -'
    Aug 11 15:24:36.094: INFO: stderr: ""
    Aug 11 15:24:36.094: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 11 15:24:36.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 --namespace=crd-publish-openapi-8382 delete e2e-test-crd-publish-openapi-904-crds test-cr'
    Aug 11 15:24:36.149: INFO: stderr: ""
    Aug 11 15:24:36.149: INFO: stdout: "e2e-test-crd-publish-openapi-904-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/11/23 15:24:36.149
    Aug 11 15:24:36.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=crd-publish-openapi-8382 explain e2e-test-crd-publish-openapi-904-crds'
    Aug 11 15:24:36.619: INFO: stderr: ""
    Aug 11 15:24:36.619: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-904-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:24:39.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8382" for this suite. 08/11/23 15:24:39.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:24:39.11
Aug 11 15:24:39.111: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-probe 08/11/23 15:24:39.111
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:39.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:39.128
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-92402606-46cc-4698-a607-5b4d84859988 in namespace container-probe-8018 08/11/23 15:24:39.13
Aug 11 15:24:39.138: INFO: Waiting up to 5m0s for pod "liveness-92402606-46cc-4698-a607-5b4d84859988" in namespace "container-probe-8018" to be "not pending"
Aug 11 15:24:39.142: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987264ms
Aug 11 15:24:41.146: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988": Phase="Running", Reason="", readiness=true. Elapsed: 2.007715193s
Aug 11 15:24:41.146: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988" satisfied condition "not pending"
Aug 11 15:24:41.146: INFO: Started pod liveness-92402606-46cc-4698-a607-5b4d84859988 in namespace container-probe-8018
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:24:41.146
Aug 11 15:24:41.149: INFO: Initial restart count of pod liveness-92402606-46cc-4698-a607-5b4d84859988 is 0
STEP: deleting the pod 08/11/23 15:28:41.66
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 11 15:28:41.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8018" for this suite. 08/11/23 15:28:41.681
------------------------------
• [SLOW TEST] [242.576 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:24:39.11
    Aug 11 15:24:39.111: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:24:39.111
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:39.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:39.128
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-92402606-46cc-4698-a607-5b4d84859988 in namespace container-probe-8018 08/11/23 15:24:39.13
    Aug 11 15:24:39.138: INFO: Waiting up to 5m0s for pod "liveness-92402606-46cc-4698-a607-5b4d84859988" in namespace "container-probe-8018" to be "not pending"
    Aug 11 15:24:39.142: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987264ms
    Aug 11 15:24:41.146: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988": Phase="Running", Reason="", readiness=true. Elapsed: 2.007715193s
    Aug 11 15:24:41.146: INFO: Pod "liveness-92402606-46cc-4698-a607-5b4d84859988" satisfied condition "not pending"
    Aug 11 15:24:41.146: INFO: Started pod liveness-92402606-46cc-4698-a607-5b4d84859988 in namespace container-probe-8018
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:24:41.146
    Aug 11 15:24:41.149: INFO: Initial restart count of pod liveness-92402606-46cc-4698-a607-5b4d84859988 is 0
    STEP: deleting the pod 08/11/23 15:28:41.66
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:28:41.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8018" for this suite. 08/11/23 15:28:41.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:28:41.687
Aug 11 15:28:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:28:41.688
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:41.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:41.705
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:28:41.711
Aug 11 15:28:41.719: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4476" to be "running and ready"
Aug 11 15:28:41.724: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.920115ms
Aug 11 15:28:41.724: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:28:43.729: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009763536s
Aug 11 15:28:43.729: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 15:28:43.729: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/11/23 15:28:43.732
Aug 11 15:28:43.738: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4476" to be "running and ready"
Aug 11 15:28:43.740: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510733ms
Aug 11 15:28:43.740: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:28:45.745: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007087303s
Aug 11 15:28:45.745: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 11 15:28:45.745: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/11/23 15:28:45.747
STEP: delete the pod with lifecycle hook 08/11/23 15:28:45.768
Aug 11 15:28:45.776: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 15:28:45.779: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 11 15:28:47.780: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 15:28:47.783: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 11 15:28:49.780: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 15:28:49.784: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 11 15:28:49.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4476" for this suite. 08/11/23 15:28:49.788
------------------------------
• [SLOW TEST] [8.107 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:28:41.687
    Aug 11 15:28:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:28:41.688
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:41.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:41.705
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:28:41.711
    Aug 11 15:28:41.719: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4476" to be "running and ready"
    Aug 11 15:28:41.724: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.920115ms
    Aug 11 15:28:41.724: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:28:43.729: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009763536s
    Aug 11 15:28:43.729: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 15:28:43.729: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/11/23 15:28:43.732
    Aug 11 15:28:43.738: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4476" to be "running and ready"
    Aug 11 15:28:43.740: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510733ms
    Aug 11 15:28:43.740: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:28:45.745: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007087303s
    Aug 11 15:28:45.745: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 11 15:28:45.745: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/11/23 15:28:45.747
    STEP: delete the pod with lifecycle hook 08/11/23 15:28:45.768
    Aug 11 15:28:45.776: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 15:28:45.779: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 11 15:28:47.780: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 15:28:47.783: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 11 15:28:49.780: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 15:28:49.784: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:28:49.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4476" for this suite. 08/11/23 15:28:49.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:28:49.795
Aug 11 15:28:49.795: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename prestop 08/11/23 15:28:49.796
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:49.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:49.817
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5366 08/11/23 15:28:49.82
STEP: Waiting for pods to come up. 08/11/23 15:28:49.828
Aug 11 15:28:49.828: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5366" to be "running"
Aug 11 15:28:49.833: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729564ms
Aug 11 15:28:51.836: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008294024s
Aug 11 15:28:51.836: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5366 08/11/23 15:28:51.839
Aug 11 15:28:51.846: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5366" to be "running"
Aug 11 15:28:51.849: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877263ms
Aug 11 15:28:53.853: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007054802s
Aug 11 15:28:53.853: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/11/23 15:28:53.853
Aug 11 15:28:58.881: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/11/23 15:28:58.881
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 11 15:28:58.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5366" for this suite. 08/11/23 15:28:58.897
------------------------------
• [SLOW TEST] [9.108 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:28:49.795
    Aug 11 15:28:49.795: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename prestop 08/11/23 15:28:49.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:49.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:49.817
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5366 08/11/23 15:28:49.82
    STEP: Waiting for pods to come up. 08/11/23 15:28:49.828
    Aug 11 15:28:49.828: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5366" to be "running"
    Aug 11 15:28:49.833: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729564ms
    Aug 11 15:28:51.836: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008294024s
    Aug 11 15:28:51.836: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5366 08/11/23 15:28:51.839
    Aug 11 15:28:51.846: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5366" to be "running"
    Aug 11 15:28:51.849: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877263ms
    Aug 11 15:28:53.853: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007054802s
    Aug 11 15:28:53.853: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/11/23 15:28:53.853
    Aug 11 15:28:58.881: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/11/23 15:28:58.881
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:28:58.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5366" for this suite. 08/11/23 15:28:58.897
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:28:58.903
Aug 11 15:28:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename sched-pred 08/11/23 15:28:58.904
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:58.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:58.922
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 11 15:28:58.925: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 15:28:58.934: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 15:28:58.937: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
Aug 11 15:28:58.945: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:28:58.945: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container coredns ready: true, restart count 0
Aug 11 15:28:58.945: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:28:58.945: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:28:58.945: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:28:58.945: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:28:58.945: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:28:58.945: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:28:58.945: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 15:28:58.945: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container e2e ready: true, restart count 0
Aug 11 15:28:58.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:28:58.945: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 15:28:58.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:28:58.945: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 15:28:58.945: INFO: 
Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
Aug 11 15:28:58.953: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 15:28:58.953: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:28:58.953: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:28:58.953: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:28:58.953: INFO: gcp-guest-agent-rscx6 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:28:58.953: INFO: konnectivity-agent-5vq55 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:28:58.953: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:28:58.953: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:28:58.953: INFO: tester from prestop-5366 started at 2023-08-11 15:28:52 +0000 UTC (1 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container tester ready: true, restart count 0
Aug 11 15:28:58.953: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
Aug 11 15:28:58.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:28:58.953: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 15:28:58.953
Aug 11 15:28:58.961: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7632" to be "running"
Aug 11 15:28:58.963: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339533ms
Aug 11 15:29:00.967: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006304003s
Aug 11 15:29:00.967: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 15:29:00.97
STEP: Trying to apply a random label on the found node. 08/11/23 15:29:00.98
STEP: verifying the node has the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 42 08/11/23 15:29:00.992
STEP: Trying to relaunch the pod, now with labels. 08/11/23 15:29:00.995
Aug 11 15:29:01.002: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7632" to be "not pending"
Aug 11 15:29:01.007: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024765ms
Aug 11 15:29:03.011: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008711166s
Aug 11 15:29:03.011: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 15:29:03.014
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 08/11/23 15:29:03.026
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:03.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7632" for this suite. 08/11/23 15:29:03.033
------------------------------
• [4.138 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:28:58.903
    Aug 11 15:28:58.903: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename sched-pred 08/11/23 15:28:58.904
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:58.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:58.922
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 11 15:28:58.925: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 15:28:58.934: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 15:28:58.937: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-mt98 before test
    Aug 11 15:28:58.945: INFO: cilium-k88wg from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:28:58.945: INFO: coredns-9ff5c7c6f-qtwv7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: csi-gce-pd-node-57ng9 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (2 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: gcp-guest-agent-tncp7 from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: konnectivity-agent-g8nvt from kube-system started at 2023-08-11 13:55:27 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: kube-proxy-kkq86 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: verification-service-vwjx2 from kube-system started at 2023-08-11 13:55:10 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:02:02 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: sonobuoy-e2e-job-550936f75fcb40a8 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-4gx9l from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 15:28:58.945: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 15:28:58.945: INFO: 
    Logging pods the apiserver thinks is on node constell-1cf5d931-worker-6381a7ba-nd80 before test
    Aug 11 15:28:58.953: INFO: cilium-operator-86847fc955-vpgcn from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: cilium-rq95f from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:28:58.953: INFO: csi-gce-pd-node-9mzs6 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (2 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: gcp-guest-agent-rscx6 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: konnectivity-agent-5vq55 from kube-system started at 2023-08-11 14:41:53 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: kube-proxy-mk54d from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: verification-service-swmj4 from kube-system started at 2023-08-11 13:55:06 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: tester from prestop-5366 started at 2023-08-11 15:28:52 +0000 UTC (1 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container tester ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: sonobuoy-systemd-logs-daemon-set-195978b949114b12-np772 from sonobuoy started at 2023-08-11 14:02:06 +0000 UTC (2 container statuses recorded)
    Aug 11 15:28:58.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:28:58.953: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 15:28:58.953
    Aug 11 15:28:58.961: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7632" to be "running"
    Aug 11 15:28:58.963: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339533ms
    Aug 11 15:29:00.967: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006304003s
    Aug 11 15:29:00.967: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 15:29:00.97
    STEP: Trying to apply a random label on the found node. 08/11/23 15:29:00.98
    STEP: verifying the node has the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 42 08/11/23 15:29:00.992
    STEP: Trying to relaunch the pod, now with labels. 08/11/23 15:29:00.995
    Aug 11 15:29:01.002: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7632" to be "not pending"
    Aug 11 15:29:01.007: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024765ms
    Aug 11 15:29:03.011: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.008711166s
    Aug 11 15:29:03.011: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 off the node constell-1cf5d931-worker-6381a7ba-nd80 08/11/23 15:29:03.014
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-7330c6cf-59d9-4802-8ff2-3701393d2f73 08/11/23 15:29:03.026
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:03.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7632" for this suite. 08/11/23 15:29:03.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:03.042
Aug 11 15:29:03.042: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:29:03.043
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:03.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:03.06
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:29:03.065
Aug 11 15:29:03.074: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8443" to be "running and ready"
Aug 11 15:29:03.076: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.703003ms
Aug 11 15:29:03.076: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:29:05.081: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007407974s
Aug 11 15:29:05.081: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 15:29:05.081: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/11/23 15:29:05.084
Aug 11 15:29:05.089: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8443" to be "running and ready"
Aug 11 15:29:05.092: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422892ms
Aug 11 15:29:05.092: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:29:07.097: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149814s
Aug 11 15:29:07.097: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 11 15:29:07.097: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/11/23 15:29:07.103
Aug 11 15:29:07.115: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 11 15:29:07.119: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 11 15:29:09.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 11 15:29:09.124: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 11 15:29:11.119: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 11 15:29:11.123: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/11/23 15:29:11.123
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:11.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8443" for this suite. 08/11/23 15:29:11.136
------------------------------
• [SLOW TEST] [8.100 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:03.042
    Aug 11 15:29:03.042: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:29:03.043
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:03.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:03.06
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:29:03.065
    Aug 11 15:29:03.074: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8443" to be "running and ready"
    Aug 11 15:29:03.076: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.703003ms
    Aug 11 15:29:03.076: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:29:05.081: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007407974s
    Aug 11 15:29:05.081: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 15:29:05.081: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/11/23 15:29:05.084
    Aug 11 15:29:05.089: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8443" to be "running and ready"
    Aug 11 15:29:05.092: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422892ms
    Aug 11 15:29:05.092: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:29:07.097: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007149814s
    Aug 11 15:29:07.097: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 11 15:29:07.097: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/11/23 15:29:07.103
    Aug 11 15:29:07.115: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 11 15:29:07.119: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 11 15:29:09.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 11 15:29:09.124: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 11 15:29:11.119: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 11 15:29:11.123: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/11/23 15:29:11.123
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:11.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8443" for this suite. 08/11/23 15:29:11.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:11.143
Aug 11 15:29:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 15:29:11.144
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:11.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:11.161
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 11 15:29:11.172: INFO: Waiting up to 2m0s for pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" in namespace "var-expansion-4214" to be "container 0 failed with reason CreateContainerConfigError"
Aug 11 15:29:11.174: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833613ms
Aug 11 15:29:13.178: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006375353s
Aug 11 15:29:13.178: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 11 15:29:13.178: INFO: Deleting pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" in namespace "var-expansion-4214"
Aug 11 15:29:13.185: INFO: Wait up to 5m0s for pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4214" for this suite. 08/11/23 15:29:15.196
------------------------------
• [4.059 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:11.143
    Aug 11 15:29:11.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 15:29:11.144
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:11.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:11.161
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 11 15:29:11.172: INFO: Waiting up to 2m0s for pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" in namespace "var-expansion-4214" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 11 15:29:11.174: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833613ms
    Aug 11 15:29:13.178: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006375353s
    Aug 11 15:29:13.178: INFO: Pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 11 15:29:13.178: INFO: Deleting pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" in namespace "var-expansion-4214"
    Aug 11 15:29:13.185: INFO: Wait up to 5m0s for pod "var-expansion-fe29cbc9-79df-4e96-a4d3-7660efc05f99" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4214" for this suite. 08/11/23 15:29:15.196
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:15.203
Aug 11 15:29:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename podtemplate 08/11/23 15:29:15.204
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:15.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:15.223
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:15.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-131" for this suite. 08/11/23 15:29:15.254
------------------------------
• [0.057 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:15.203
    Aug 11 15:29:15.203: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename podtemplate 08/11/23 15:29:15.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:15.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:15.223
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:15.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-131" for this suite. 08/11/23 15:29:15.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:15.261
Aug 11 15:29:15.261: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:29:15.261
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:15.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:15.28
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-4961/configmap-test-3489a610-50b5-440f-b6f0-26deadb9ac14 08/11/23 15:29:15.282
STEP: Creating a pod to test consume configMaps 08/11/23 15:29:15.287
Aug 11 15:29:15.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe" in namespace "configmap-4961" to be "Succeeded or Failed"
Aug 11 15:29:15.297: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403992ms
Aug 11 15:29:17.300: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006068053s
Aug 11 15:29:19.300: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006334147s
STEP: Saw pod success 08/11/23 15:29:19.301
Aug 11 15:29:19.301: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe" satisfied condition "Succeeded or Failed"
Aug 11 15:29:19.303: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe container env-test: <nil>
STEP: delete the pod 08/11/23 15:29:19.325
Aug 11 15:29:19.340: INFO: Waiting for pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe to disappear
Aug 11 15:29:19.343: INFO: Pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:19.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4961" for this suite. 08/11/23 15:29:19.347
------------------------------
• [4.094 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:15.261
    Aug 11 15:29:15.261: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:29:15.261
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:15.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:15.28
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-4961/configmap-test-3489a610-50b5-440f-b6f0-26deadb9ac14 08/11/23 15:29:15.282
    STEP: Creating a pod to test consume configMaps 08/11/23 15:29:15.287
    Aug 11 15:29:15.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe" in namespace "configmap-4961" to be "Succeeded or Failed"
    Aug 11 15:29:15.297: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.403992ms
    Aug 11 15:29:17.300: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006068053s
    Aug 11 15:29:19.300: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006334147s
    STEP: Saw pod success 08/11/23 15:29:19.301
    Aug 11 15:29:19.301: INFO: Pod "pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe" satisfied condition "Succeeded or Failed"
    Aug 11 15:29:19.303: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe container env-test: <nil>
    STEP: delete the pod 08/11/23 15:29:19.325
    Aug 11 15:29:19.340: INFO: Waiting for pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe to disappear
    Aug 11 15:29:19.343: INFO: Pod pod-configmaps-dfeef4be-67fc-4809-beac-d491bdb67bbe no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:19.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4961" for this suite. 08/11/23 15:29:19.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:19.355
Aug 11 15:29:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 15:29:19.356
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:19.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:19.375
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 15:29:19.39
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:29:19.54
STEP: Deploying the webhook pod 08/11/23 15:29:19.548
STEP: Wait for the deployment to be ready 08/11/23 15:29:19.559
Aug 11 15:29:19.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:29:21.576
STEP: Verifying the service has paired with the endpoint 08/11/23 15:29:21.59
Aug 11 15:29:22.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 11 15:29:22.594: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/11/23 15:29:23.104
STEP: Creating a custom resource that should be denied by the webhook 08/11/23 15:29:23.13
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/11/23 15:29:25.165
STEP: Updating the custom resource with disallowed data should be denied 08/11/23 15:29:25.174
STEP: Deleting the custom resource should be denied 08/11/23 15:29:25.185
STEP: Remove the offending key and value from the custom resource data 08/11/23 15:29:25.193
STEP: Deleting the updated custom resource should be successful 08/11/23 15:29:25.206
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:25.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6193" for this suite. 08/11/23 15:29:25.787
STEP: Destroying namespace "webhook-6193-markers" for this suite. 08/11/23 15:29:25.797
------------------------------
• [SLOW TEST] [6.456 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:19.355
    Aug 11 15:29:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 15:29:19.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:19.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:19.375
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 15:29:19.39
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:29:19.54
    STEP: Deploying the webhook pod 08/11/23 15:29:19.548
    STEP: Wait for the deployment to be ready 08/11/23 15:29:19.559
    Aug 11 15:29:19.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:29:21.576
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:29:21.59
    Aug 11 15:29:22.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 11 15:29:22.594: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/11/23 15:29:23.104
    STEP: Creating a custom resource that should be denied by the webhook 08/11/23 15:29:23.13
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/11/23 15:29:25.165
    STEP: Updating the custom resource with disallowed data should be denied 08/11/23 15:29:25.174
    STEP: Deleting the custom resource should be denied 08/11/23 15:29:25.185
    STEP: Remove the offending key and value from the custom resource data 08/11/23 15:29:25.193
    STEP: Deleting the updated custom resource should be successful 08/11/23 15:29:25.206
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:25.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6193" for this suite. 08/11/23 15:29:25.787
    STEP: Destroying namespace "webhook-6193-markers" for this suite. 08/11/23 15:29:25.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:25.812
Aug 11 15:29:25.812: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 15:29:25.813
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:25.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:25.83
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/11/23 15:29:25.833
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_tcp@PTR;sleep 1; done
 08/11/23 15:29:25.855
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_tcp@PTR;sleep 1; done
 08/11/23 15:29:25.855
STEP: creating a pod to probe DNS 08/11/23 15:29:25.855
STEP: submitting the pod to kubernetes 08/11/23 15:29:25.855
Aug 11 15:29:25.866: INFO: Waiting up to 15m0s for pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4" in namespace "dns-7276" to be "running"
Aug 11 15:29:25.873: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711128ms
Aug 11 15:29:27.877: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010525197s
Aug 11 15:29:27.877: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:29:27.877
STEP: looking for the results for each expected name from probers 08/11/23 15:29:27.879
Aug 11 15:29:27.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.902: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.908: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.937: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.943: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:27.974: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:32.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:32.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:32.994: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:32.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:33.027: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:33.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:33.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:33.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:33.067: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:37.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:37.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:37.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:37.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:38.025: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:38.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:38.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:38.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:38.063: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:42.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:42.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:42.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:42.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:43.025: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:43.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:43.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:43.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:43.062: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:47.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:47.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:47.995: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.000: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.028: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:48.069: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:52.983: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:52.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:52.994: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:52.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:53.027: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:53.032: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:53.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:53.044: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
Aug 11 15:29:53.067: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

Aug 11 15:29:58.066: INFO: DNS probes using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 succeeded

STEP: deleting the pod 08/11/23 15:29:58.066
STEP: deleting the test service 08/11/23 15:29:58.078
STEP: deleting the test headless service 08/11/23 15:29:58.116
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 15:29:58.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7276" for this suite. 08/11/23 15:29:58.131
------------------------------
• [SLOW TEST] [32.330 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:25.812
    Aug 11 15:29:25.812: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 15:29:25.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:25.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:25.83
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/11/23 15:29:25.833
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_tcp@PTR;sleep 1; done
     08/11/23 15:29:25.855
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7276.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7276.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7276.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_udp@PTR;check="$$(dig +tcp +noall +answer +search 85.253.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.253.85_tcp@PTR;sleep 1; done
     08/11/23 15:29:25.855
    STEP: creating a pod to probe DNS 08/11/23 15:29:25.855
    STEP: submitting the pod to kubernetes 08/11/23 15:29:25.855
    Aug 11 15:29:25.866: INFO: Waiting up to 15m0s for pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4" in namespace "dns-7276" to be "running"
    Aug 11 15:29:25.873: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711128ms
    Aug 11 15:29:27.877: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010525197s
    Aug 11 15:29:27.877: INFO: Pod "dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:29:27.877
    STEP: looking for the results for each expected name from probers 08/11/23 15:29:27.879
    Aug 11 15:29:27.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.902: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.908: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.937: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.943: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.953: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:27.974: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:32.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:32.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:32.994: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:32.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:33.027: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:33.033: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:33.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:33.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:33.067: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:37.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:37.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:37.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:37.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:38.025: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:38.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:38.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:38.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:38.063: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:42.981: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:42.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:42.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:42.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:43.025: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:43.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:43.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:43.041: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:43.062: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:47.982: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:47.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:47.995: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.000: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.028: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:48.069: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:52.983: INFO: Unable to read wheezy_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:52.988: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:52.994: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:52.999: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:53.027: INFO: Unable to read jessie_udp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:53.032: INFO: Unable to read jessie_tcp@dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:53.038: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:53.044: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local from pod dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4: the server could not find the requested resource (get pods dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4)
    Aug 11 15:29:53.067: INFO: Lookups using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 failed for: [wheezy_udp@dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@dns-test-service.dns-7276.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_udp@dns-test-service.dns-7276.svc.cluster.local jessie_tcp@dns-test-service.dns-7276.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7276.svc.cluster.local]

    Aug 11 15:29:58.066: INFO: DNS probes using dns-7276/dns-test-e1cccfe4-df6a-4369-a113-81d8dc421ec4 succeeded

    STEP: deleting the pod 08/11/23 15:29:58.066
    STEP: deleting the test service 08/11/23 15:29:58.078
    STEP: deleting the test headless service 08/11/23 15:29:58.116
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:29:58.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7276" for this suite. 08/11/23 15:29:58.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:29:58.143
Aug 11 15:29:58.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pods 08/11/23 15:29:58.143
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:58.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:58.159
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 11 15:29:58.170: INFO: Waiting up to 5m0s for pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675" in namespace "pods-5615" to be "running and ready"
Aug 11 15:29:58.173: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.812002ms
Aug 11 15:29:58.173: INFO: The phase of Pod server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:30:00.177: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675": Phase="Running", Reason="", readiness=true. Elapsed: 2.007393013s
Aug 11 15:30:00.177: INFO: The phase of Pod server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675 is Running (Ready = true)
Aug 11 15:30:00.177: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675" satisfied condition "running and ready"
Aug 11 15:30:00.206: INFO: Waiting up to 5m0s for pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68" in namespace "pods-5615" to be "Succeeded or Failed"
Aug 11 15:30:00.209: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074413ms
Aug 11 15:30:02.214: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007591003s
Aug 11 15:30:04.215: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008558391s
STEP: Saw pod success 08/11/23 15:30:04.215
Aug 11 15:30:04.215: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68" satisfied condition "Succeeded or Failed"
Aug 11 15:30:04.218: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 container env3cont: <nil>
STEP: delete the pod 08/11/23 15:30:04.229
Aug 11 15:30:04.239: INFO: Waiting for pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 to disappear
Aug 11 15:30:04.242: INFO: Pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 11 15:30:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5615" for this suite. 08/11/23 15:30:04.245
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:29:58.143
    Aug 11 15:29:58.143: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pods 08/11/23 15:29:58.143
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:58.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:58.159
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 11 15:29:58.170: INFO: Waiting up to 5m0s for pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675" in namespace "pods-5615" to be "running and ready"
    Aug 11 15:29:58.173: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.812002ms
    Aug 11 15:29:58.173: INFO: The phase of Pod server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:30:00.177: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675": Phase="Running", Reason="", readiness=true. Elapsed: 2.007393013s
    Aug 11 15:30:00.177: INFO: The phase of Pod server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675 is Running (Ready = true)
    Aug 11 15:30:00.177: INFO: Pod "server-envvars-72a4bd1a-7318-434b-a8db-0147a9752675" satisfied condition "running and ready"
    Aug 11 15:30:00.206: INFO: Waiting up to 5m0s for pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68" in namespace "pods-5615" to be "Succeeded or Failed"
    Aug 11 15:30:00.209: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074413ms
    Aug 11 15:30:02.214: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007591003s
    Aug 11 15:30:04.215: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008558391s
    STEP: Saw pod success 08/11/23 15:30:04.215
    Aug 11 15:30:04.215: INFO: Pod "client-envvars-7c848d12-ced0-4e03-988e-41632628ec68" satisfied condition "Succeeded or Failed"
    Aug 11 15:30:04.218: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 container env3cont: <nil>
    STEP: delete the pod 08/11/23 15:30:04.229
    Aug 11 15:30:04.239: INFO: Waiting for pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 to disappear
    Aug 11 15:30:04.242: INFO: Pod client-envvars-7c848d12-ced0-4e03-988e-41632628ec68 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:30:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5615" for this suite. 08/11/23 15:30:04.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:30:04.255
Aug 11 15:30:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename statefulset 08/11/23 15:30:04.256
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:04.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:04.271
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1761 08/11/23 15:30:04.273
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/11/23 15:30:04.278
STEP: Creating pod with conflicting port in namespace statefulset-1761 08/11/23 15:30:04.283
STEP: Waiting until pod test-pod will start running in namespace statefulset-1761 08/11/23 15:30:04.291
Aug 11 15:30:04.291: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1761" to be "running"
Aug 11 15:30:04.293: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317322ms
Aug 11 15:30:06.297: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006300601s
Aug 11 15:30:06.297: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-1761 08/11/23 15:30:06.297
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1761 08/11/23 15:30:06.303
Aug 11 15:30:06.316: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Pending. Waiting for statefulset controller to delete.
Aug 11 15:30:06.332: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Failed. Waiting for statefulset controller to delete.
Aug 11 15:30:06.341: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Failed. Waiting for statefulset controller to delete.
Aug 11 15:30:06.344: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1761
STEP: Removing pod with conflicting port in namespace statefulset-1761 08/11/23 15:30:06.344
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1761 and will be in running state 08/11/23 15:30:06.366
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 11 15:30:08.374: INFO: Deleting all statefulset in ns statefulset-1761
Aug 11 15:30:08.376: INFO: Scaling statefulset ss to 0
Aug 11 15:30:18.395: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:30:18.398: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 11 15:30:18.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1761" for this suite. 08/11/23 15:30:18.416
------------------------------
• [SLOW TEST] [14.166 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:30:04.255
    Aug 11 15:30:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:30:04.256
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:04.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:04.271
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1761 08/11/23 15:30:04.273
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/11/23 15:30:04.278
    STEP: Creating pod with conflicting port in namespace statefulset-1761 08/11/23 15:30:04.283
    STEP: Waiting until pod test-pod will start running in namespace statefulset-1761 08/11/23 15:30:04.291
    Aug 11 15:30:04.291: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1761" to be "running"
    Aug 11 15:30:04.293: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317322ms
    Aug 11 15:30:06.297: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006300601s
    Aug 11 15:30:06.297: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-1761 08/11/23 15:30:06.297
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1761 08/11/23 15:30:06.303
    Aug 11 15:30:06.316: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 11 15:30:06.332: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 11 15:30:06.341: INFO: Observed stateful pod in namespace: statefulset-1761, name: ss-0, uid: 3dedf6a8-1e46-4250-8abd-43c8e1087d84, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 11 15:30:06.344: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1761
    STEP: Removing pod with conflicting port in namespace statefulset-1761 08/11/23 15:30:06.344
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1761 and will be in running state 08/11/23 15:30:06.366
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 11 15:30:08.374: INFO: Deleting all statefulset in ns statefulset-1761
    Aug 11 15:30:08.376: INFO: Scaling statefulset ss to 0
    Aug 11 15:30:18.395: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:30:18.398: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:30:18.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1761" for this suite. 08/11/23 15:30:18.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:30:18.423
Aug 11 15:30:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename container-runtime 08/11/23 15:30:18.424
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:18.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:18.442
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/11/23 15:30:18.444
STEP: wait for the container to reach Succeeded 08/11/23 15:30:18.452
STEP: get the container status 08/11/23 15:30:22.469
STEP: the container should be terminated 08/11/23 15:30:22.472
STEP: the termination message should be set 08/11/23 15:30:22.472
Aug 11 15:30:22.472: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/11/23 15:30:22.472
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 11 15:30:22.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1998" for this suite. 08/11/23 15:30:22.491
------------------------------
• [4.075 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:30:18.423
    Aug 11 15:30:18.423: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename container-runtime 08/11/23 15:30:18.424
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:18.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:18.442
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/11/23 15:30:18.444
    STEP: wait for the container to reach Succeeded 08/11/23 15:30:18.452
    STEP: get the container status 08/11/23 15:30:22.469
    STEP: the container should be terminated 08/11/23 15:30:22.472
    STEP: the termination message should be set 08/11/23 15:30:22.472
    Aug 11 15:30:22.472: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/11/23 15:30:22.472
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:30:22.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1998" for this suite. 08/11/23 15:30:22.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:30:22.499
Aug 11 15:30:22.499: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 15:30:22.5
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:22.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:22.517
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/11/23 15:30:22.519
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:22.524
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:22.524
STEP: creating a pod to probe DNS 08/11/23 15:30:22.524
STEP: submitting the pod to kubernetes 08/11/23 15:30:22.524
Aug 11 15:30:22.531: INFO: Waiting up to 15m0s for pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b" in namespace "dns-6784" to be "running"
Aug 11 15:30:22.534: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831503ms
Aug 11 15:30:24.538: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006982063s
Aug 11 15:30:24.538: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:30:24.538
STEP: looking for the results for each expected name from probers 08/11/23 15:30:24.541
Aug 11 15:30:24.557: INFO: DNS probes using dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b succeeded

STEP: deleting the pod 08/11/23 15:30:24.557
STEP: changing the externalName to bar.example.com 08/11/23 15:30:24.572
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:24.581
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:24.581
STEP: creating a second pod to probe DNS 08/11/23 15:30:24.581
STEP: submitting the pod to kubernetes 08/11/23 15:30:24.581
Aug 11 15:30:24.590: INFO: Waiting up to 15m0s for pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629" in namespace "dns-6784" to be "running"
Aug 11 15:30:24.596: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194094ms
Aug 11 15:30:26.600: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629": Phase="Running", Reason="", readiness=true. Elapsed: 2.009362622s
Aug 11 15:30:26.600: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:30:26.6
STEP: looking for the results for each expected name from probers 08/11/23 15:30:26.603
Aug 11 15:30:26.614: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:26.619: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:26.619: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:31.628: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:31.633: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:31.633: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:36.627: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:36.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:36.632: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:41.626: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:41.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:41.633: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:46.626: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:46.635: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:46.635: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:51.627: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:51.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:30:51.632: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

Aug 11 15:30:56.632: INFO: DNS probes using dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 succeeded

STEP: deleting the pod 08/11/23 15:30:56.632
STEP: changing the service to type=ClusterIP 08/11/23 15:30:56.65
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:56.671
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
 08/11/23 15:30:56.671
STEP: creating a third pod to probe DNS 08/11/23 15:30:56.671
STEP: submitting the pod to kubernetes 08/11/23 15:30:56.676
Aug 11 15:30:56.687: INFO: Waiting up to 15m0s for pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89" in namespace "dns-6784" to be "running"
Aug 11 15:30:56.692: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901806ms
Aug 11 15:30:58.695: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89": Phase="Running", Reason="", readiness=true. Elapsed: 2.008756345s
Aug 11 15:30:58.695: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:30:58.695
STEP: looking for the results for each expected name from probers 08/11/23 15:30:58.699
Aug 11 15:30:58.715: INFO: DNS probes using dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89 succeeded

STEP: deleting the pod 08/11/23 15:30:58.715
STEP: deleting the test externalName service 08/11/23 15:30:58.725
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 15:30:58.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6784" for this suite. 08/11/23 15:30:58.749
------------------------------
• [SLOW TEST] [36.256 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:30:22.499
    Aug 11 15:30:22.499: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 15:30:22.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:22.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:22.517
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/11/23 15:30:22.519
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:22.524
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:22.524
    STEP: creating a pod to probe DNS 08/11/23 15:30:22.524
    STEP: submitting the pod to kubernetes 08/11/23 15:30:22.524
    Aug 11 15:30:22.531: INFO: Waiting up to 15m0s for pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b" in namespace "dns-6784" to be "running"
    Aug 11 15:30:22.534: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831503ms
    Aug 11 15:30:24.538: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006982063s
    Aug 11 15:30:24.538: INFO: Pod "dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:30:24.538
    STEP: looking for the results for each expected name from probers 08/11/23 15:30:24.541
    Aug 11 15:30:24.557: INFO: DNS probes using dns-test-91842814-9d6e-4c69-9909-4a0acb359c9b succeeded

    STEP: deleting the pod 08/11/23 15:30:24.557
    STEP: changing the externalName to bar.example.com 08/11/23 15:30:24.572
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:24.581
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:24.581
    STEP: creating a second pod to probe DNS 08/11/23 15:30:24.581
    STEP: submitting the pod to kubernetes 08/11/23 15:30:24.581
    Aug 11 15:30:24.590: INFO: Waiting up to 15m0s for pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629" in namespace "dns-6784" to be "running"
    Aug 11 15:30:24.596: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629": Phase="Pending", Reason="", readiness=false. Elapsed: 5.194094ms
    Aug 11 15:30:26.600: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629": Phase="Running", Reason="", readiness=true. Elapsed: 2.009362622s
    Aug 11 15:30:26.600: INFO: Pod "dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:30:26.6
    STEP: looking for the results for each expected name from probers 08/11/23 15:30:26.603
    Aug 11 15:30:26.614: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:26.619: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:26.619: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:31.628: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:31.633: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:31.633: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:36.627: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:36.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:36.632: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:41.626: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:41.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:41.633: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:46.626: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:46.635: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:46.635: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:51.627: INFO: File wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:51.632: INFO: File jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local from pod  dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:30:51.632: INFO: Lookups using dns-6784/dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 failed for: [wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local]

    Aug 11 15:30:56.632: INFO: DNS probes using dns-test-aa43fdd0-0c61-4fdb-b187-d4150709e629 succeeded

    STEP: deleting the pod 08/11/23 15:30:56.632
    STEP: changing the service to type=ClusterIP 08/11/23 15:30:56.65
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:56.671
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6784.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6784.svc.cluster.local; sleep 1; done
     08/11/23 15:30:56.671
    STEP: creating a third pod to probe DNS 08/11/23 15:30:56.671
    STEP: submitting the pod to kubernetes 08/11/23 15:30:56.676
    Aug 11 15:30:56.687: INFO: Waiting up to 15m0s for pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89" in namespace "dns-6784" to be "running"
    Aug 11 15:30:56.692: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901806ms
    Aug 11 15:30:58.695: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89": Phase="Running", Reason="", readiness=true. Elapsed: 2.008756345s
    Aug 11 15:30:58.695: INFO: Pod "dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:30:58.695
    STEP: looking for the results for each expected name from probers 08/11/23 15:30:58.699
    Aug 11 15:30:58.715: INFO: DNS probes using dns-test-c6bd0aa5-1501-43c4-826c-bc7285818c89 succeeded

    STEP: deleting the pod 08/11/23 15:30:58.715
    STEP: deleting the test externalName service 08/11/23 15:30:58.725
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:30:58.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6784" for this suite. 08/11/23 15:30:58.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:30:58.757
Aug 11 15:30:58.757: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename projected 08/11/23 15:30:58.758
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:58.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:58.774
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/11/23 15:30:58.776
Aug 11 15:30:58.785: INFO: Waiting up to 5m0s for pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70" in namespace "projected-1792" to be "running and ready"
Aug 11 15:30:58.792: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411837ms
Aug 11 15:30:58.792: INFO: The phase of Pod labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:31:00.796: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.011236015s
Aug 11 15:31:00.796: INFO: The phase of Pod labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70 is Running (Ready = true)
Aug 11 15:31:00.796: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70" satisfied condition "running and ready"
Aug 11 15:31:01.325: INFO: Successfully updated pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:05.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1792" for this suite. 08/11/23 15:31:05.359
------------------------------
• [SLOW TEST] [6.607 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:30:58.757
    Aug 11 15:30:58.757: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename projected 08/11/23 15:30:58.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:58.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:58.774
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/11/23 15:30:58.776
    Aug 11 15:30:58.785: INFO: Waiting up to 5m0s for pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70" in namespace "projected-1792" to be "running and ready"
    Aug 11 15:30:58.792: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411837ms
    Aug 11 15:30:58.792: INFO: The phase of Pod labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:31:00.796: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70": Phase="Running", Reason="", readiness=true. Elapsed: 2.011236015s
    Aug 11 15:31:00.796: INFO: The phase of Pod labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70 is Running (Ready = true)
    Aug 11 15:31:00.796: INFO: Pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70" satisfied condition "running and ready"
    Aug 11 15:31:01.325: INFO: Successfully updated pod "labelsupdate23e85c66-9671-444e-ad83-68cca6d57d70"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:05.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1792" for this suite. 08/11/23 15:31:05.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:05.366
Aug 11 15:31:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:31:05.367
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:05.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:05.383
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7978 08/11/23 15:31:05.385
STEP: creating a selector 08/11/23 15:31:05.386
STEP: Creating the service pods in kubernetes 08/11/23 15:31:05.386
Aug 11 15:31:05.386: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 15:31:05.405: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7978" to be "running and ready"
Aug 11 15:31:05.410: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019125ms
Aug 11 15:31:05.410: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:31:07.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009724495s
Aug 11 15:31:07.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:09.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00867515s
Aug 11 15:31:09.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:11.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009787368s
Aug 11 15:31:11.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:13.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009303191s
Aug 11 15:31:13.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:15.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009024738s
Aug 11 15:31:15.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:17.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010246715s
Aug 11 15:31:17.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:19.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009478759s
Aug 11 15:31:19.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:21.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008330873s
Aug 11 15:31:21.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:23.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008907529s
Aug 11 15:31:23.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:25.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010480797s
Aug 11 15:31:25.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:31:27.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009458071s
Aug 11 15:31:27.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 15:31:27.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 15:31:27.417: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7978" to be "running and ready"
Aug 11 15:31:27.420: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.574873ms
Aug 11 15:31:27.420: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 15:31:27.420: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 15:31:27.422
Aug 11 15:31:27.430: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7978" to be "running"
Aug 11 15:31:27.435: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.369895ms
Aug 11 15:31:29.442: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012070417s
Aug 11 15:31:29.442: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 15:31:29.445: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 15:31:29.445: INFO: Breadth first check of 10.10.0.140 on host 192.168.178.2...
Aug 11 15:31:29.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.150:9080/dial?request=hostname&protocol=http&host=10.10.0.140&port=8083&tries=1'] Namespace:pod-network-test-7978 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:31:29.447: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:31:29.448: INFO: ExecWithOptions: Clientset creation
Aug 11 15:31:29.448: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7978/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.150%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.140%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 15:31:29.528: INFO: Waiting for responses: map[]
Aug 11 15:31:29.528: INFO: reached 10.10.0.140 after 0/1 tries
Aug 11 15:31:29.528: INFO: Breadth first check of 10.10.1.41 on host 192.168.178.3...
Aug 11 15:31:29.531: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.150:9080/dial?request=hostname&protocol=http&host=10.10.1.41&port=8083&tries=1'] Namespace:pod-network-test-7978 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:31:29.531: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:31:29.532: INFO: ExecWithOptions: Clientset creation
Aug 11 15:31:29.532: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7978/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.150%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 15:31:29.609: INFO: Waiting for responses: map[]
Aug 11 15:31:29.609: INFO: reached 10.10.1.41 after 0/1 tries
Aug 11 15:31:29.609: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:29.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7978" for this suite. 08/11/23 15:31:29.613
------------------------------
• [SLOW TEST] [24.252 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:05.366
    Aug 11 15:31:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:31:05.367
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:05.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:05.383
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7978 08/11/23 15:31:05.385
    STEP: creating a selector 08/11/23 15:31:05.386
    STEP: Creating the service pods in kubernetes 08/11/23 15:31:05.386
    Aug 11 15:31:05.386: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 15:31:05.405: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7978" to be "running and ready"
    Aug 11 15:31:05.410: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019125ms
    Aug 11 15:31:05.410: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:31:07.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009724495s
    Aug 11 15:31:07.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:09.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00867515s
    Aug 11 15:31:09.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:11.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009787368s
    Aug 11 15:31:11.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:13.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009303191s
    Aug 11 15:31:13.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:15.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009024738s
    Aug 11 15:31:15.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:17.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010246715s
    Aug 11 15:31:17.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:19.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009478759s
    Aug 11 15:31:19.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:21.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008330873s
    Aug 11 15:31:21.413: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:23.413: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.008907529s
    Aug 11 15:31:23.414: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:25.415: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010480797s
    Aug 11 15:31:25.415: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:31:27.414: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009458071s
    Aug 11 15:31:27.414: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 15:31:27.414: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 15:31:27.417: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7978" to be "running and ready"
    Aug 11 15:31:27.420: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.574873ms
    Aug 11 15:31:27.420: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 15:31:27.420: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 15:31:27.422
    Aug 11 15:31:27.430: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7978" to be "running"
    Aug 11 15:31:27.435: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.369895ms
    Aug 11 15:31:29.442: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012070417s
    Aug 11 15:31:29.442: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 15:31:29.445: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 15:31:29.445: INFO: Breadth first check of 10.10.0.140 on host 192.168.178.2...
    Aug 11 15:31:29.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.150:9080/dial?request=hostname&protocol=http&host=10.10.0.140&port=8083&tries=1'] Namespace:pod-network-test-7978 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:31:29.447: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:31:29.448: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:31:29.448: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7978/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.150%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.140%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 15:31:29.528: INFO: Waiting for responses: map[]
    Aug 11 15:31:29.528: INFO: reached 10.10.0.140 after 0/1 tries
    Aug 11 15:31:29.528: INFO: Breadth first check of 10.10.1.41 on host 192.168.178.3...
    Aug 11 15:31:29.531: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.1.150:9080/dial?request=hostname&protocol=http&host=10.10.1.41&port=8083&tries=1'] Namespace:pod-network-test-7978 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:31:29.531: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:31:29.532: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:31:29.532: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7978/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.1.150%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 15:31:29.609: INFO: Waiting for responses: map[]
    Aug 11 15:31:29.609: INFO: reached 10.10.1.41 after 0/1 tries
    Aug 11 15:31:29.609: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:29.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7978" for this suite. 08/11/23 15:31:29.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:29.621
Aug 11 15:31:29.621: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename services 08/11/23 15:31:29.621
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:29.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:29.638
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-6372 08/11/23 15:31:29.64
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[] 08/11/23 15:31:29.653
Aug 11 15:31:29.656: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 11 15:31:30.663: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6372 08/11/23 15:31:30.663
Aug 11 15:31:30.671: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6372" to be "running and ready"
Aug 11 15:31:30.675: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723883ms
Aug 11 15:31:30.675: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:31:32.680: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008315463s
Aug 11 15:31:32.680: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 15:31:32.680: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod1:[80]] 08/11/23 15:31:32.683
Aug 11 15:31:32.693: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/11/23 15:31:32.693
Aug 11 15:31:32.693: INFO: Creating new exec pod
Aug 11 15:31:32.698: INFO: Waiting up to 5m0s for pod "execpodcct9h" in namespace "services-6372" to be "running"
Aug 11 15:31:32.701: INFO: Pod "execpodcct9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552923ms
Aug 11 15:31:34.706: INFO: Pod "execpodcct9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.007357953s
Aug 11 15:31:34.706: INFO: Pod "execpodcct9h" satisfied condition "running"
Aug 11 15:31:35.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 11 15:31:35.820: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:35.820: INFO: stdout: ""
Aug 11 15:31:35.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
Aug 11 15:31:35.947: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:35.947: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-6372 08/11/23 15:31:35.947
Aug 11 15:31:35.955: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6372" to be "running and ready"
Aug 11 15:31:35.962: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.550797ms
Aug 11 15:31:35.962: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:31:37.967: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012509888s
Aug 11 15:31:37.967: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 15:31:37.967: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod1:[80] pod2:[80]] 08/11/23 15:31:37.971
Aug 11 15:31:37.982: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/11/23 15:31:37.982
Aug 11 15:31:38.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 11 15:31:39.109: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:39.109: INFO: stdout: ""
Aug 11 15:31:39.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
Aug 11 15:31:39.227: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:39.227: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-6372 08/11/23 15:31:39.227
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod2:[80]] 08/11/23 15:31:39.24
Aug 11 15:31:39.253: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/11/23 15:31:39.253
Aug 11 15:31:40.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 11 15:31:40.378: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:40.378: INFO: stdout: ""
Aug 11 15:31:40.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
Aug 11 15:31:40.503: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
Aug 11 15:31:40.503: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-6372 08/11/23 15:31:40.503
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[] 08/11/23 15:31:40.515
Aug 11 15:31:40.525: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:40.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6372" for this suite. 08/11/23 15:31:40.555
------------------------------
• [SLOW TEST] [10.942 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:29.621
    Aug 11 15:31:29.621: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename services 08/11/23 15:31:29.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:29.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:29.638
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-6372 08/11/23 15:31:29.64
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[] 08/11/23 15:31:29.653
    Aug 11 15:31:29.656: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 11 15:31:30.663: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6372 08/11/23 15:31:30.663
    Aug 11 15:31:30.671: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6372" to be "running and ready"
    Aug 11 15:31:30.675: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723883ms
    Aug 11 15:31:30.675: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:31:32.680: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008315463s
    Aug 11 15:31:32.680: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 15:31:32.680: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod1:[80]] 08/11/23 15:31:32.683
    Aug 11 15:31:32.693: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/11/23 15:31:32.693
    Aug 11 15:31:32.693: INFO: Creating new exec pod
    Aug 11 15:31:32.698: INFO: Waiting up to 5m0s for pod "execpodcct9h" in namespace "services-6372" to be "running"
    Aug 11 15:31:32.701: INFO: Pod "execpodcct9h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552923ms
    Aug 11 15:31:34.706: INFO: Pod "execpodcct9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.007357953s
    Aug 11 15:31:34.706: INFO: Pod "execpodcct9h" satisfied condition "running"
    Aug 11 15:31:35.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 11 15:31:35.820: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:35.820: INFO: stdout: ""
    Aug 11 15:31:35.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
    Aug 11 15:31:35.947: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:35.947: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-6372 08/11/23 15:31:35.947
    Aug 11 15:31:35.955: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6372" to be "running and ready"
    Aug 11 15:31:35.962: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.550797ms
    Aug 11 15:31:35.962: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:31:37.967: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.012509888s
    Aug 11 15:31:37.967: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 15:31:37.967: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod1:[80] pod2:[80]] 08/11/23 15:31:37.971
    Aug 11 15:31:37.982: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/11/23 15:31:37.982
    Aug 11 15:31:38.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 11 15:31:39.109: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:39.109: INFO: stdout: ""
    Aug 11 15:31:39.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
    Aug 11 15:31:39.227: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:39.227: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-6372 08/11/23 15:31:39.227
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[pod2:[80]] 08/11/23 15:31:39.24
    Aug 11 15:31:39.253: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/11/23 15:31:39.253
    Aug 11 15:31:40.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 11 15:31:40.378: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:40.378: INFO: stdout: ""
    Aug 11 15:31:40.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-591790350 --namespace=services-6372 exec execpodcct9h -- /bin/sh -x -c nc -v -z -w 2 10.105.6.173 80'
    Aug 11 15:31:40.503: INFO: stderr: "+ nc -v -z -w 2 10.105.6.173 80\nConnection to 10.105.6.173 80 port [tcp/http] succeeded!\n"
    Aug 11 15:31:40.503: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-6372 08/11/23 15:31:40.503
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6372 to expose endpoints map[] 08/11/23 15:31:40.515
    Aug 11 15:31:40.525: INFO: successfully validated that service endpoint-test2 in namespace services-6372 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:40.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6372" for this suite. 08/11/23 15:31:40.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:40.563
Aug 11 15:31:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename gc 08/11/23 15:31:40.564
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:40.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:40.581
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 11 15:31:40.619: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"882296ba-b1b7-4497-94ce-1768e47ee879", Controller:(*bool)(0xc004e502c6), BlockOwnerDeletion:(*bool)(0xc004e502c7)}}
Aug 11 15:31:40.626: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"25af9ece-78c9-450e-88b5-1d498d9cce85", Controller:(*bool)(0xc004804686), BlockOwnerDeletion:(*bool)(0xc004804687)}}
Aug 11 15:31:40.633: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"95d36cec-1087-4a1f-a05c-2577164d1a97", Controller:(*bool)(0xc0048048ae), BlockOwnerDeletion:(*bool)(0xc0048048af)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:45.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2920" for this suite. 08/11/23 15:31:45.656
------------------------------
• [SLOW TEST] [5.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:40.563
    Aug 11 15:31:40.563: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename gc 08/11/23 15:31:40.564
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:40.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:40.581
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 11 15:31:40.619: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"882296ba-b1b7-4497-94ce-1768e47ee879", Controller:(*bool)(0xc004e502c6), BlockOwnerDeletion:(*bool)(0xc004e502c7)}}
    Aug 11 15:31:40.626: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"25af9ece-78c9-450e-88b5-1d498d9cce85", Controller:(*bool)(0xc004804686), BlockOwnerDeletion:(*bool)(0xc004804687)}}
    Aug 11 15:31:40.633: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"95d36cec-1087-4a1f-a05c-2577164d1a97", Controller:(*bool)(0xc0048048ae), BlockOwnerDeletion:(*bool)(0xc0048048af)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:45.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2920" for this suite. 08/11/23 15:31:45.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:45.664
Aug 11 15:31:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename webhook 08/11/23 15:31:45.665
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:45.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:45.684
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/11/23 15:31:45.698
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:31:46.042
STEP: Deploying the webhook pod 08/11/23 15:31:46.05
STEP: Wait for the deployment to be ready 08/11/23 15:31:46.063
Aug 11 15:31:46.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:31:48.079
STEP: Verifying the service has paired with the endpoint 08/11/23 15:31:48.103
Aug 11 15:31:49.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 11 15:31:49.107: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7963-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 15:31:49.615
STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 15:31:49.639
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:52.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2018" for this suite. 08/11/23 15:31:52.246
STEP: Destroying namespace "webhook-2018-markers" for this suite. 08/11/23 15:31:52.254
------------------------------
• [SLOW TEST] [6.597 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:45.664
    Aug 11 15:31:45.664: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename webhook 08/11/23 15:31:45.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:45.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:45.684
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/11/23 15:31:45.698
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:31:46.042
    STEP: Deploying the webhook pod 08/11/23 15:31:46.05
    STEP: Wait for the deployment to be ready 08/11/23 15:31:46.063
    Aug 11 15:31:46.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:31:48.079
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:31:48.103
    Aug 11 15:31:49.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 11 15:31:49.107: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7963-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 15:31:49.615
    STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 15:31:49.639
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:52.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2018" for this suite. 08/11/23 15:31:52.246
    STEP: Destroying namespace "webhook-2018-markers" for this suite. 08/11/23 15:31:52.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:52.262
Aug 11 15:31:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:31:52.263
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:52.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:52.28
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 11 15:31:56.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4096" for this suite. 08/11/23 15:31:56.31
------------------------------
• [4.055 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:52.262
    Aug 11 15:31:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:31:52.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:52.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:52.28
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:31:56.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4096" for this suite. 08/11/23 15:31:56.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:31:56.317
Aug 11 15:31:56.317: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename var-expansion 08/11/23 15:31:56.318
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:56.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:56.336
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/11/23 15:31:56.338
Aug 11 15:31:56.346: INFO: Waiting up to 2m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512" to be "running"
Aug 11 15:31:56.351: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008384ms
Aug 11 15:31:58.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007469343s
Aug 11 15:32:00.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007927929s
Aug 11 15:32:02.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008585913s
Aug 11 15:32:04.358: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011092315s
Aug 11 15:32:06.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008101768s
Aug 11 15:32:08.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009042635s
Aug 11 15:32:10.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.0092692s
Aug 11 15:32:12.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008743885s
Aug 11 15:32:14.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009433593s
Aug 11 15:32:16.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008009178s
Aug 11 15:32:18.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009611435s
Aug 11 15:32:20.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00810456s
Aug 11 15:32:22.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008509475s
Aug 11 15:32:24.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00726754s
Aug 11 15:32:26.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007849695s
Aug 11 15:32:28.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008230001s
Aug 11 15:32:30.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008838188s
Aug 11 15:32:32.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008966134s
Aug 11 15:32:34.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00923058s
Aug 11 15:32:36.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007776785s
Aug 11 15:32:38.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009474963s
Aug 11 15:32:40.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007593797s
Aug 11 15:32:42.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009529744s
Aug 11 15:32:44.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008455769s
Aug 11 15:32:46.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007842725s
Aug 11 15:32:48.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009123342s
Aug 11 15:32:50.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009937449s
Aug 11 15:32:52.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008444861s
Aug 11 15:32:54.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008821056s
Aug 11 15:32:56.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008408502s
Aug 11 15:32:58.359: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012085924s
Aug 11 15:33:00.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008005716s
Aug 11 15:33:02.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008226222s
Aug 11 15:33:04.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008880889s
Aug 11 15:33:06.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008123114s
Aug 11 15:33:08.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008488659s
Aug 11 15:33:10.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008886856s
Aug 11 15:33:12.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008816089s
Aug 11 15:33:14.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008236814s
Aug 11 15:33:16.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008172913s
Aug 11 15:33:18.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008905699s
Aug 11 15:33:20.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009346645s
Aug 11 15:33:22.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009073162s
Aug 11 15:33:24.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008348947s
Aug 11 15:33:26.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008598363s
Aug 11 15:33:28.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008111738s
Aug 11 15:33:30.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008958465s
Aug 11 15:33:32.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009362242s
Aug 11 15:33:34.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009042285s
Aug 11 15:33:36.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007491472s
Aug 11 15:33:38.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009587577s
Aug 11 15:33:40.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009598655s
Aug 11 15:33:42.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008789337s
Aug 11 15:33:44.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009408345s
Aug 11 15:33:46.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008029941s
Aug 11 15:33:48.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009634087s
Aug 11 15:33:50.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008562193s
Aug 11 15:33:52.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008727409s
Aug 11 15:33:54.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009407495s
Aug 11 15:33:56.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00795534s
Aug 11 15:33:56.357: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010870872s
STEP: updating the pod 08/11/23 15:33:56.357
Aug 11 15:33:56.871: INFO: Successfully updated pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e"
STEP: waiting for pod running 08/11/23 15:33:56.871
Aug 11 15:33:56.871: INFO: Waiting up to 2m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512" to be "running"
Aug 11 15:33:56.876: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.066784ms
Aug 11 15:33:58.880: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008588534s
Aug 11 15:33:58.880: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 15:33:58.88
Aug 11 15:33:58.880: INFO: Deleting pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512"
Aug 11 15:33:58.887: INFO: Wait up to 5m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 11 15:34:30.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5512" for this suite. 08/11/23 15:34:30.897
------------------------------
• [SLOW TEST] [154.589 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:31:56.317
    Aug 11 15:31:56.317: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename var-expansion 08/11/23 15:31:56.318
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:56.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:56.336
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/11/23 15:31:56.338
    Aug 11 15:31:56.346: INFO: Waiting up to 2m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512" to be "running"
    Aug 11 15:31:56.351: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008384ms
    Aug 11 15:31:58.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007469343s
    Aug 11 15:32:00.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007927929s
    Aug 11 15:32:02.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008585913s
    Aug 11 15:32:04.358: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011092315s
    Aug 11 15:32:06.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008101768s
    Aug 11 15:32:08.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009042635s
    Aug 11 15:32:10.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.0092692s
    Aug 11 15:32:12.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008743885s
    Aug 11 15:32:14.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009433593s
    Aug 11 15:32:16.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008009178s
    Aug 11 15:32:18.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009611435s
    Aug 11 15:32:20.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00810456s
    Aug 11 15:32:22.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008509475s
    Aug 11 15:32:24.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.00726754s
    Aug 11 15:32:26.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007849695s
    Aug 11 15:32:28.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008230001s
    Aug 11 15:32:30.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008838188s
    Aug 11 15:32:32.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008966134s
    Aug 11 15:32:34.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00923058s
    Aug 11 15:32:36.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007776785s
    Aug 11 15:32:38.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009474963s
    Aug 11 15:32:40.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007593797s
    Aug 11 15:32:42.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009529744s
    Aug 11 15:32:44.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008455769s
    Aug 11 15:32:46.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007842725s
    Aug 11 15:32:48.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009123342s
    Aug 11 15:32:50.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009937449s
    Aug 11 15:32:52.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008444861s
    Aug 11 15:32:54.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008821056s
    Aug 11 15:32:56.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008408502s
    Aug 11 15:32:58.359: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012085924s
    Aug 11 15:33:00.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008005716s
    Aug 11 15:33:02.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008226222s
    Aug 11 15:33:04.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008880889s
    Aug 11 15:33:06.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008123114s
    Aug 11 15:33:08.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008488659s
    Aug 11 15:33:10.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008886856s
    Aug 11 15:33:12.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008816089s
    Aug 11 15:33:14.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008236814s
    Aug 11 15:33:16.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008172913s
    Aug 11 15:33:18.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008905699s
    Aug 11 15:33:20.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009346645s
    Aug 11 15:33:22.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009073162s
    Aug 11 15:33:24.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008348947s
    Aug 11 15:33:26.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008598363s
    Aug 11 15:33:28.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008111738s
    Aug 11 15:33:30.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008958465s
    Aug 11 15:33:32.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009362242s
    Aug 11 15:33:34.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009042285s
    Aug 11 15:33:36.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007491472s
    Aug 11 15:33:38.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.009587577s
    Aug 11 15:33:40.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009598655s
    Aug 11 15:33:42.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008789337s
    Aug 11 15:33:44.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009408345s
    Aug 11 15:33:46.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008029941s
    Aug 11 15:33:48.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009634087s
    Aug 11 15:33:50.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008562193s
    Aug 11 15:33:52.355: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008727409s
    Aug 11 15:33:54.356: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009407495s
    Aug 11 15:33:56.354: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00795534s
    Aug 11 15:33:56.357: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010870872s
    STEP: updating the pod 08/11/23 15:33:56.357
    Aug 11 15:33:56.871: INFO: Successfully updated pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e"
    STEP: waiting for pod running 08/11/23 15:33:56.871
    Aug 11 15:33:56.871: INFO: Waiting up to 2m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512" to be "running"
    Aug 11 15:33:56.876: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.066784ms
    Aug 11 15:33:58.880: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008588534s
    Aug 11 15:33:58.880: INFO: Pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 15:33:58.88
    Aug 11 15:33:58.880: INFO: Deleting pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" in namespace "var-expansion-5512"
    Aug 11 15:33:58.887: INFO: Wait up to 5m0s for pod "var-expansion-d2fbfd3e-a6bd-4a29-a245-ef7b5d29a26e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:34:30.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5512" for this suite. 08/11/23 15:34:30.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:34:30.907
Aug 11 15:34:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:34:30.908
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:30.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:30.927
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 15:34:30.929
Aug 11 15:34:30.939: INFO: Waiting up to 5m0s for pod "pod-c902effb-25ef-4b80-8043-2dff80182e33" in namespace "emptydir-6177" to be "Succeeded or Failed"
Aug 11 15:34:30.943: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911503ms
Aug 11 15:34:32.947: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007449581s
Aug 11 15:34:34.949: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009512048s
STEP: Saw pod success 08/11/23 15:34:34.949
Aug 11 15:34:34.949: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33" satisfied condition "Succeeded or Failed"
Aug 11 15:34:34.952: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c902effb-25ef-4b80-8043-2dff80182e33 container test-container: <nil>
STEP: delete the pod 08/11/23 15:34:34.974
Aug 11 15:34:34.985: INFO: Waiting for pod pod-c902effb-25ef-4b80-8043-2dff80182e33 to disappear
Aug 11 15:34:34.988: INFO: Pod pod-c902effb-25ef-4b80-8043-2dff80182e33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:34:34.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6177" for this suite. 08/11/23 15:34:34.992
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:34:30.907
    Aug 11 15:34:30.907: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:34:30.908
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:30.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:30.927
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 15:34:30.929
    Aug 11 15:34:30.939: INFO: Waiting up to 5m0s for pod "pod-c902effb-25ef-4b80-8043-2dff80182e33" in namespace "emptydir-6177" to be "Succeeded or Failed"
    Aug 11 15:34:30.943: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911503ms
    Aug 11 15:34:32.947: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007449581s
    Aug 11 15:34:34.949: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009512048s
    STEP: Saw pod success 08/11/23 15:34:34.949
    Aug 11 15:34:34.949: INFO: Pod "pod-c902effb-25ef-4b80-8043-2dff80182e33" satisfied condition "Succeeded or Failed"
    Aug 11 15:34:34.952: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-c902effb-25ef-4b80-8043-2dff80182e33 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:34:34.974
    Aug 11 15:34:34.985: INFO: Waiting for pod pod-c902effb-25ef-4b80-8043-2dff80182e33 to disappear
    Aug 11 15:34:34.988: INFO: Pod pod-c902effb-25ef-4b80-8043-2dff80182e33 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:34:34.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6177" for this suite. 08/11/23 15:34:34.992
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:34:34.999
Aug 11 15:34:34.999: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:34:35
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:35.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:35.018
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4458 08/11/23 15:34:35.02
STEP: creating a selector 08/11/23 15:34:35.02
STEP: Creating the service pods in kubernetes 08/11/23 15:34:35.02
Aug 11 15:34:35.020: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 15:34:35.045: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4458" to be "running and ready"
Aug 11 15:34:35.050: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858995ms
Aug 11 15:34:35.050: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:34:37.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009075025s
Aug 11 15:34:37.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:34:39.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009275951s
Aug 11 15:34:39.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:34:41.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009058177s
Aug 11 15:34:41.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:34:43.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009001713s
Aug 11 15:34:43.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:34:45.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009786537s
Aug 11 15:34:45.055: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:34:47.056: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011009677s
Aug 11 15:34:47.056: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 15:34:47.056: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 15:34:47.059: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4458" to be "running and ready"
Aug 11 15:34:47.062: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.756902ms
Aug 11 15:34:47.062: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Aug 11 15:34:49.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005969121s
Aug 11 15:34:49.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Aug 11 15:34:51.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.006472497s
Aug 11 15:34:51.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Aug 11 15:34:53.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.006372563s
Aug 11 15:34:53.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Aug 11 15:34:55.066: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.00707676s
Aug 11 15:34:55.066: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Aug 11 15:34:57.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.006235595s
Aug 11 15:34:57.065: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 15:34:57.065: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 15:34:57.068
Aug 11 15:34:57.082: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4458" to be "running"
Aug 11 15:34:57.089: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.267206ms
Aug 11 15:34:59.093: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010960846s
Aug 11 15:34:59.093: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 15:34:59.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4458" to be "running"
Aug 11 15:34:59.099: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.746242ms
Aug 11 15:34:59.099: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 11 15:34:59.104: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 15:34:59.104: INFO: Going to poll 10.10.0.221 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:34:59.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:34:59.106: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:34:59.107: INFO: ExecWithOptions: Clientset creation
Aug 11 15:34:59.107: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4458/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:34:59.190: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 11 15:34:59.190: INFO: Going to poll 10.10.1.164 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:34:59.193: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:34:59.193: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:34:59.194: INFO: ExecWithOptions: Clientset creation
Aug 11 15:34:59.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4458/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:34:59.281: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 11 15:34:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4458" for this suite. 08/11/23 15:34:59.285
------------------------------
• [SLOW TEST] [24.291 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:34:34.999
    Aug 11 15:34:34.999: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:34:35
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:35.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:35.018
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4458 08/11/23 15:34:35.02
    STEP: creating a selector 08/11/23 15:34:35.02
    STEP: Creating the service pods in kubernetes 08/11/23 15:34:35.02
    Aug 11 15:34:35.020: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 15:34:35.045: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4458" to be "running and ready"
    Aug 11 15:34:35.050: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858995ms
    Aug 11 15:34:35.050: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:34:37.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009075025s
    Aug 11 15:34:37.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:34:39.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009275951s
    Aug 11 15:34:39.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:34:41.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009058177s
    Aug 11 15:34:41.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:34:43.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009001713s
    Aug 11 15:34:43.054: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:34:45.054: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009786537s
    Aug 11 15:34:45.055: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:34:47.056: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011009677s
    Aug 11 15:34:47.056: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 15:34:47.056: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 15:34:47.059: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4458" to be "running and ready"
    Aug 11 15:34:47.062: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.756902ms
    Aug 11 15:34:47.062: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Aug 11 15:34:49.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005969121s
    Aug 11 15:34:49.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Aug 11 15:34:51.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.006472497s
    Aug 11 15:34:51.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Aug 11 15:34:53.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.006372563s
    Aug 11 15:34:53.065: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Aug 11 15:34:55.066: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.00707676s
    Aug 11 15:34:55.066: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Aug 11 15:34:57.065: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.006235595s
    Aug 11 15:34:57.065: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 15:34:57.065: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 15:34:57.068
    Aug 11 15:34:57.082: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4458" to be "running"
    Aug 11 15:34:57.089: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.267206ms
    Aug 11 15:34:59.093: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010960846s
    Aug 11 15:34:59.093: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 15:34:59.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4458" to be "running"
    Aug 11 15:34:59.099: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.746242ms
    Aug 11 15:34:59.099: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 11 15:34:59.104: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 15:34:59.104: INFO: Going to poll 10.10.0.221 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:34:59.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:34:59.106: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:34:59.107: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:34:59.107: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4458/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:34:59.190: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 11 15:34:59.190: INFO: Going to poll 10.10.1.164 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:34:59.193: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4458 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:34:59.193: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:34:59.194: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:34:59.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4458/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:34:59.281: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:34:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4458" for this suite. 08/11/23 15:34:59.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:34:59.291
Aug 11 15:34:59.291: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename ephemeral-containers-test 08/11/23 15:34:59.292
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:59.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:59.312
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/11/23 15:34:59.314
Aug 11 15:34:59.322: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8166" to be "running and ready"
Aug 11 15:34:59.327: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764355ms
Aug 11 15:34:59.327: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:35:01.332: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009954945s
Aug 11 15:35:01.332: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 11 15:35:01.332: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/11/23 15:35:01.334
Aug 11 15:35:01.345: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8166" to be "container debugger running"
Aug 11 15:35:01.347: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.401723ms
Aug 11 15:35:03.351: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005863291s
Aug 11 15:35:03.351: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/11/23 15:35:03.351
Aug 11 15:35:03.351: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8166 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:35:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:35:03.351: INFO: ExecWithOptions: Clientset creation
Aug 11 15:35:03.351: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-8166/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 11 15:35:03.421: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 11 15:35:03.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-8166" for this suite. 08/11/23 15:35:03.437
------------------------------
• [4.153 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:34:59.291
    Aug 11 15:34:59.291: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/11/23 15:34:59.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:59.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:59.312
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/11/23 15:34:59.314
    Aug 11 15:34:59.322: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8166" to be "running and ready"
    Aug 11 15:34:59.327: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764355ms
    Aug 11 15:34:59.327: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:35:01.332: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009954945s
    Aug 11 15:35:01.332: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 11 15:35:01.332: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/11/23 15:35:01.334
    Aug 11 15:35:01.345: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8166" to be "container debugger running"
    Aug 11 15:35:01.347: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.401723ms
    Aug 11 15:35:03.351: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005863291s
    Aug 11 15:35:03.351: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/11/23 15:35:03.351
    Aug 11 15:35:03.351: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8166 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:35:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:35:03.351: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:35:03.351: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-8166/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 11 15:35:03.421: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:35:03.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-8166" for this suite. 08/11/23 15:35:03.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:35:03.444
Aug 11 15:35:03.444: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename subpath 08/11/23 15:35:03.445
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:03.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:03.46
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 15:35:03.463
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-ck5r 08/11/23 15:35:03.472
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:35:03.472
Aug 11 15:35:03.480: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ck5r" in namespace "subpath-6699" to be "Succeeded or Failed"
Aug 11 15:35:03.483: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669413ms
Aug 11 15:35:05.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 2.007528624s
Aug 11 15:35:07.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.006928889s
Aug 11 15:35:09.486: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 6.006155305s
Aug 11 15:35:11.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 8.006931221s
Aug 11 15:35:13.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 10.006461267s
Aug 11 15:35:15.486: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 12.005914871s
Aug 11 15:35:17.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 14.00753244s
Aug 11 15:35:19.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 16.007052886s
Aug 11 15:35:21.490: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 18.009417324s
Aug 11 15:35:23.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 20.007940648s
Aug 11 15:35:25.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=false. Elapsed: 22.006966613s
Aug 11 15:35:27.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007195989s
STEP: Saw pod success 08/11/23 15:35:27.487
Aug 11 15:35:27.487: INFO: Pod "pod-subpath-test-secret-ck5r" satisfied condition "Succeeded or Failed"
Aug 11 15:35:27.490: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-secret-ck5r container test-container-subpath-secret-ck5r: <nil>
STEP: delete the pod 08/11/23 15:35:27.5
Aug 11 15:35:27.514: INFO: Waiting for pod pod-subpath-test-secret-ck5r to disappear
Aug 11 15:35:27.516: INFO: Pod pod-subpath-test-secret-ck5r no longer exists
STEP: Deleting pod pod-subpath-test-secret-ck5r 08/11/23 15:35:27.516
Aug 11 15:35:27.516: INFO: Deleting pod "pod-subpath-test-secret-ck5r" in namespace "subpath-6699"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 11 15:35:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6699" for this suite. 08/11/23 15:35:27.522
------------------------------
• [SLOW TEST] [24.084 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:35:03.444
    Aug 11 15:35:03.444: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename subpath 08/11/23 15:35:03.445
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:03.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:03.46
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 15:35:03.463
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-ck5r 08/11/23 15:35:03.472
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:35:03.472
    Aug 11 15:35:03.480: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ck5r" in namespace "subpath-6699" to be "Succeeded or Failed"
    Aug 11 15:35:03.483: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.669413ms
    Aug 11 15:35:05.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 2.007528624s
    Aug 11 15:35:07.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 4.006928889s
    Aug 11 15:35:09.486: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 6.006155305s
    Aug 11 15:35:11.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 8.006931221s
    Aug 11 15:35:13.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 10.006461267s
    Aug 11 15:35:15.486: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 12.005914871s
    Aug 11 15:35:17.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 14.00753244s
    Aug 11 15:35:19.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 16.007052886s
    Aug 11 15:35:21.490: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 18.009417324s
    Aug 11 15:35:23.488: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=true. Elapsed: 20.007940648s
    Aug 11 15:35:25.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Running", Reason="", readiness=false. Elapsed: 22.006966613s
    Aug 11 15:35:27.487: INFO: Pod "pod-subpath-test-secret-ck5r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007195989s
    STEP: Saw pod success 08/11/23 15:35:27.487
    Aug 11 15:35:27.487: INFO: Pod "pod-subpath-test-secret-ck5r" satisfied condition "Succeeded or Failed"
    Aug 11 15:35:27.490: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-subpath-test-secret-ck5r container test-container-subpath-secret-ck5r: <nil>
    STEP: delete the pod 08/11/23 15:35:27.5
    Aug 11 15:35:27.514: INFO: Waiting for pod pod-subpath-test-secret-ck5r to disappear
    Aug 11 15:35:27.516: INFO: Pod pod-subpath-test-secret-ck5r no longer exists
    STEP: Deleting pod pod-subpath-test-secret-ck5r 08/11/23 15:35:27.516
    Aug 11 15:35:27.516: INFO: Deleting pod "pod-subpath-test-secret-ck5r" in namespace "subpath-6699"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:35:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6699" for this suite. 08/11/23 15:35:27.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:35:27.528
Aug 11 15:35:27.528: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename downward-api 08/11/23 15:35:27.529
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:27.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:27.546
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:35:27.548
Aug 11 15:35:27.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3" in namespace "downward-api-6533" to be "Succeeded or Failed"
Aug 11 15:35:27.561: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109055ms
Aug 11 15:35:29.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009449465s
Aug 11 15:35:31.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009161612s
STEP: Saw pod success 08/11/23 15:35:31.565
Aug 11 15:35:31.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3" satisfied condition "Succeeded or Failed"
Aug 11 15:35:31.568: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 container client-container: <nil>
STEP: delete the pod 08/11/23 15:35:31.576
Aug 11 15:35:31.587: INFO: Waiting for pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 to disappear
Aug 11 15:35:31.590: INFO: Pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 11 15:35:31.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6533" for this suite. 08/11/23 15:35:31.593
------------------------------
• [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:35:27.528
    Aug 11 15:35:27.528: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:35:27.529
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:27.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:27.546
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:35:27.548
    Aug 11 15:35:27.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3" in namespace "downward-api-6533" to be "Succeeded or Failed"
    Aug 11 15:35:27.561: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109055ms
    Aug 11 15:35:29.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009449465s
    Aug 11 15:35:31.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009161612s
    STEP: Saw pod success 08/11/23 15:35:31.565
    Aug 11 15:35:31.565: INFO: Pod "downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3" satisfied condition "Succeeded or Failed"
    Aug 11 15:35:31.568: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:35:31.576
    Aug 11 15:35:31.587: INFO: Waiting for pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 to disappear
    Aug 11 15:35:31.590: INFO: Pod downwardapi-volume-f4512018-d337-4467-8529-a160c6d988b3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:35:31.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6533" for this suite. 08/11/23 15:35:31.593
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:35:31.599
Aug 11 15:35:31.599: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename secrets 08/11/23 15:35:31.599
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:31.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:31.617
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/11/23 15:35:31.619
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/11/23 15:35:31.624
STEP: patching the secret 08/11/23 15:35:31.629
STEP: deleting the secret using a LabelSelector 08/11/23 15:35:31.636
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/11/23 15:35:31.643
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 11 15:35:31.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2556" for this suite. 08/11/23 15:35:31.651
------------------------------
• [0.058 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:35:31.599
    Aug 11 15:35:31.599: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename secrets 08/11/23 15:35:31.599
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:31.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:31.617
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/11/23 15:35:31.619
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/11/23 15:35:31.624
    STEP: patching the secret 08/11/23 15:35:31.629
    STEP: deleting the secret using a LabelSelector 08/11/23 15:35:31.636
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/11/23 15:35:31.643
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:35:31.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2556" for this suite. 08/11/23 15:35:31.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:35:31.657
Aug 11 15:35:31.657: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:35:31.658
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:31.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:31.674
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 15:35:31.676
Aug 11 15:35:31.683: INFO: Waiting up to 5m0s for pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2" in namespace "emptydir-881" to be "Succeeded or Failed"
Aug 11 15:35:31.688: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872005ms
Aug 11 15:35:33.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008663864s
Aug 11 15:35:35.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009082072s
STEP: Saw pod success 08/11/23 15:35:35.692
Aug 11 15:35:35.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2" satisfied condition "Succeeded or Failed"
Aug 11 15:35:35.695: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 container test-container: <nil>
STEP: delete the pod 08/11/23 15:35:35.703
Aug 11 15:35:35.714: INFO: Waiting for pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 to disappear
Aug 11 15:35:35.717: INFO: Pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:35:35.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-881" for this suite. 08/11/23 15:35:35.72
------------------------------
• [4.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:35:31.657
    Aug 11 15:35:31.657: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:35:31.658
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:31.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:31.674
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 15:35:31.676
    Aug 11 15:35:31.683: INFO: Waiting up to 5m0s for pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2" in namespace "emptydir-881" to be "Succeeded or Failed"
    Aug 11 15:35:31.688: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872005ms
    Aug 11 15:35:33.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008663864s
    Aug 11 15:35:35.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009082072s
    STEP: Saw pod success 08/11/23 15:35:35.692
    Aug 11 15:35:35.692: INFO: Pod "pod-bf232a7b-9867-4590-984c-faa5d2c4aff2" satisfied condition "Succeeded or Failed"
    Aug 11 15:35:35.695: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:35:35.703
    Aug 11 15:35:35.714: INFO: Waiting for pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 to disappear
    Aug 11 15:35:35.717: INFO: Pod pod-bf232a7b-9867-4590-984c-faa5d2c4aff2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:35:35.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-881" for this suite. 08/11/23 15:35:35.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:35:35.726
Aug 11 15:35:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename dns 08/11/23 15:35:35.727
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:35.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:35.742
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/11/23 15:35:35.745
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9105;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9105;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +notcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_tcp@PTR;sleep 1; done
 08/11/23 15:35:35.767
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9105;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9105;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +notcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_tcp@PTR;sleep 1; done
 08/11/23 15:35:35.767
STEP: creating a pod to probe DNS 08/11/23 15:35:35.767
STEP: submitting the pod to kubernetes 08/11/23 15:35:35.767
Aug 11 15:35:35.778: INFO: Waiting up to 15m0s for pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6" in namespace "dns-9105" to be "running"
Aug 11 15:35:35.783: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184395ms
Aug 11 15:35:37.788: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010234746s
Aug 11 15:35:37.788: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:35:37.788
STEP: looking for the results for each expected name from probers 08/11/23 15:35:37.791
Aug 11 15:35:37.803: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.808: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.814: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.836: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.841: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.870: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.875: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.880: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.887: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.892: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.904: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:37.934: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:35:42.941: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.947: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.952: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.962: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.967: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.973: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:42.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.005: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.010: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.015: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.020: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.025: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:43.066: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:35:47.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.963: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.968: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.973: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:47.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.015: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.020: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.031: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:48.069: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:35:52.943: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.967: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.978: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:52.984: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.013: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.019: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.025: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.037: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.054: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:53.078: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:35:57.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.947: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.969: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:57.980: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.015: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.020: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.031: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:35:58.068: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:36:02.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:02.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.013: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.018: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.024: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.029: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
Aug 11 15:36:03.068: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

Aug 11 15:36:08.066: INFO: DNS probes using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 succeeded

STEP: deleting the pod 08/11/23 15:36:08.066
STEP: deleting the test service 08/11/23 15:36:08.086
STEP: deleting the test headless service 08/11/23 15:36:08.12
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:08.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9105" for this suite. 08/11/23 15:36:08.139
------------------------------
• [SLOW TEST] [32.420 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:35:35.726
    Aug 11 15:35:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename dns 08/11/23 15:35:35.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:35.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:35.742
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/11/23 15:35:35.745
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9105;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9105;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +notcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_tcp@PTR;sleep 1; done
     08/11/23 15:35:35.767
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9105;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9105;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9105.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9105.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9105.svc;check="$$(dig +notcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_udp@PTR;check="$$(dig +tcp +noall +answer +search 171.216.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.216.171_tcp@PTR;sleep 1; done
     08/11/23 15:35:35.767
    STEP: creating a pod to probe DNS 08/11/23 15:35:35.767
    STEP: submitting the pod to kubernetes 08/11/23 15:35:35.767
    Aug 11 15:35:35.778: INFO: Waiting up to 15m0s for pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6" in namespace "dns-9105" to be "running"
    Aug 11 15:35:35.783: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.184395ms
    Aug 11 15:35:37.788: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.010234746s
    Aug 11 15:35:37.788: INFO: Pod "dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:35:37.788
    STEP: looking for the results for each expected name from probers 08/11/23 15:35:37.791
    Aug 11 15:35:37.803: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.808: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.814: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.819: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.825: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.836: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.841: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.870: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.875: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.880: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.887: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.892: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.899: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.904: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:37.934: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:35:42.941: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.947: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.952: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.957: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.962: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.967: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.973: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:42.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.005: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.010: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.015: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.020: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.025: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.039: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:43.066: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:35:47.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.963: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.968: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.973: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:47.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.015: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.020: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.031: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:48.069: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:35:52.943: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.957: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.967: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.973: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.978: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:52.984: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.013: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.019: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.025: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.031: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.037: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.049: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.054: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:53.078: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:35:57.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.947: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.969: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:57.980: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.015: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.020: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.026: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.031: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.037: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.042: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.047: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:35:58.068: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:36:02.942: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.948: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.953: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.964: INFO: Unable to read wheezy_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.970: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.975: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:02.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.008: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.013: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.018: INFO: Unable to read jessie_udp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.024: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105 from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.029: INFO: Unable to read jessie_udp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.034: INFO: Unable to read jessie_tcp@dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.040: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc from pod dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6: the server could not find the requested resource (get pods dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6)
    Aug 11 15:36:03.068: INFO: Lookups using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9105 wheezy_tcp@dns-test-service.dns-9105 wheezy_udp@dns-test-service.dns-9105.svc wheezy_tcp@dns-test-service.dns-9105.svc wheezy_udp@_http._tcp.dns-test-service.dns-9105.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9105.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9105 jessie_tcp@dns-test-service.dns-9105 jessie_udp@dns-test-service.dns-9105.svc jessie_tcp@dns-test-service.dns-9105.svc jessie_udp@_http._tcp.dns-test-service.dns-9105.svc jessie_tcp@_http._tcp.dns-test-service.dns-9105.svc]

    Aug 11 15:36:08.066: INFO: DNS probes using dns-9105/dns-test-8c97263e-12c9-4a56-a76a-02b3443626f6 succeeded

    STEP: deleting the pod 08/11/23 15:36:08.066
    STEP: deleting the test service 08/11/23 15:36:08.086
    STEP: deleting the test headless service 08/11/23 15:36:08.12
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:08.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9105" for this suite. 08/11/23 15:36:08.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:08.15
Aug 11 15:36:08.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:36:08.151
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:08.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:08.167
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7381" for this suite. 08/11/23 15:36:08.211
------------------------------
• [0.066 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:08.15
    Aug 11 15:36:08.150: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:36:08.151
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:08.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:08.167
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:08.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7381" for this suite. 08/11/23 15:36:08.211
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:08.216
Aug 11 15:36:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:36:08.217
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:08.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:08.234
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 15:36:08.236
Aug 11 15:36:08.244: INFO: Waiting up to 5m0s for pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778" in namespace "emptydir-4061" to be "Succeeded or Failed"
Aug 11 15:36:08.249: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357656ms
Aug 11 15:36:10.252: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008692404s
Aug 11 15:36:12.255: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011167312s
STEP: Saw pod success 08/11/23 15:36:12.255
Aug 11 15:36:12.255: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778" satisfied condition "Succeeded or Failed"
Aug 11 15:36:12.257: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 container test-container: <nil>
STEP: delete the pod 08/11/23 15:36:12.266
Aug 11 15:36:12.276: INFO: Waiting for pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 to disappear
Aug 11 15:36:12.278: INFO: Pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:12.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4061" for this suite. 08/11/23 15:36:12.282
------------------------------
• [4.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:08.216
    Aug 11 15:36:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:36:08.217
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:08.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:08.234
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 15:36:08.236
    Aug 11 15:36:08.244: INFO: Waiting up to 5m0s for pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778" in namespace "emptydir-4061" to be "Succeeded or Failed"
    Aug 11 15:36:08.249: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357656ms
    Aug 11 15:36:10.252: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008692404s
    Aug 11 15:36:12.255: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011167312s
    STEP: Saw pod success 08/11/23 15:36:12.255
    Aug 11 15:36:12.255: INFO: Pod "pod-42b8752f-1834-4051-8e39-f4c296f5b778" satisfied condition "Succeeded or Failed"
    Aug 11 15:36:12.257: INFO: Trying to get logs from node constell-1cf5d931-worker-6381a7ba-nd80 pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:36:12.266
    Aug 11 15:36:12.276: INFO: Waiting for pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 to disappear
    Aug 11 15:36:12.278: INFO: Pod pod-42b8752f-1834-4051-8e39-f4c296f5b778 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:12.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4061" for this suite. 08/11/23 15:36:12.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:12.288
Aug 11 15:36:12.288: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename configmap 08/11/23 15:36:12.289
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:12.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:12.304
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-b6c32e86-97c5-41cc-8ead-82f39d02838a 08/11/23 15:36:12.31
STEP: Creating configMap with name cm-test-opt-upd-20e6ffc3-b600-4e7b-840e-c8164a6a6b64 08/11/23 15:36:12.314
STEP: Creating the pod 08/11/23 15:36:12.319
Aug 11 15:36:12.327: INFO: Waiting up to 5m0s for pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513" in namespace "configmap-6361" to be "running and ready"
Aug 11 15:36:12.332: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859525ms
Aug 11 15:36:12.332: INFO: The phase of Pod pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:36:14.336: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513": Phase="Running", Reason="", readiness=true. Elapsed: 2.008681094s
Aug 11 15:36:14.336: INFO: The phase of Pod pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513 is Running (Ready = true)
Aug 11 15:36:14.336: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b6c32e86-97c5-41cc-8ead-82f39d02838a 08/11/23 15:36:14.361
STEP: Updating configmap cm-test-opt-upd-20e6ffc3-b600-4e7b-840e-c8164a6a6b64 08/11/23 15:36:14.369
STEP: Creating configMap with name cm-test-opt-create-27f25f5d-4ea5-4b30-8cde-9548e88b4de0 08/11/23 15:36:14.373
STEP: waiting to observe update in volume 08/11/23 15:36:14.378
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:16.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6361" for this suite. 08/11/23 15:36:16.413
------------------------------
• [4.131 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:12.288
    Aug 11 15:36:12.288: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename configmap 08/11/23 15:36:12.289
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:12.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:12.304
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-b6c32e86-97c5-41cc-8ead-82f39d02838a 08/11/23 15:36:12.31
    STEP: Creating configMap with name cm-test-opt-upd-20e6ffc3-b600-4e7b-840e-c8164a6a6b64 08/11/23 15:36:12.314
    STEP: Creating the pod 08/11/23 15:36:12.319
    Aug 11 15:36:12.327: INFO: Waiting up to 5m0s for pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513" in namespace "configmap-6361" to be "running and ready"
    Aug 11 15:36:12.332: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859525ms
    Aug 11 15:36:12.332: INFO: The phase of Pod pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:36:14.336: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513": Phase="Running", Reason="", readiness=true. Elapsed: 2.008681094s
    Aug 11 15:36:14.336: INFO: The phase of Pod pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513 is Running (Ready = true)
    Aug 11 15:36:14.336: INFO: Pod "pod-configmaps-159394a2-005c-44ff-85cc-d51fc3a87513" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b6c32e86-97c5-41cc-8ead-82f39d02838a 08/11/23 15:36:14.361
    STEP: Updating configmap cm-test-opt-upd-20e6ffc3-b600-4e7b-840e-c8164a6a6b64 08/11/23 15:36:14.369
    STEP: Creating configMap with name cm-test-opt-create-27f25f5d-4ea5-4b30-8cde-9548e88b4de0 08/11/23 15:36:14.373
    STEP: waiting to observe update in volume 08/11/23 15:36:14.378
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:16.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6361" for this suite. 08/11/23 15:36:16.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:16.42
Aug 11 15:36:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption 08/11/23 15:36:16.421
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:16.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:16.436
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:16.438
Aug 11 15:36:16.438: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename disruption-2 08/11/23 15:36:16.439
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:16.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:16.459
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/11/23 15:36:16.465
STEP: Waiting for the pdb to be processed 08/11/23 15:36:18.476
STEP: Waiting for the pdb to be processed 08/11/23 15:36:20.488
STEP: listing a collection of PDBs across all namespaces 08/11/23 15:36:22.496
STEP: listing a collection of PDBs in namespace disruption-8811 08/11/23 15:36:22.499
STEP: deleting a collection of PDBs 08/11/23 15:36:22.502
STEP: Waiting for the PDB collection to be deleted 08/11/23 15:36:22.514
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:22.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:22.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4296" for this suite. 08/11/23 15:36:22.524
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8811" for this suite. 08/11/23 15:36:22.529
------------------------------
• [SLOW TEST] [6.116 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:16.42
    Aug 11 15:36:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption 08/11/23 15:36:16.421
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:16.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:16.436
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:16.438
    Aug 11 15:36:16.438: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename disruption-2 08/11/23 15:36:16.439
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:16.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:16.459
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/11/23 15:36:16.465
    STEP: Waiting for the pdb to be processed 08/11/23 15:36:18.476
    STEP: Waiting for the pdb to be processed 08/11/23 15:36:20.488
    STEP: listing a collection of PDBs across all namespaces 08/11/23 15:36:22.496
    STEP: listing a collection of PDBs in namespace disruption-8811 08/11/23 15:36:22.499
    STEP: deleting a collection of PDBs 08/11/23 15:36:22.502
    STEP: Waiting for the PDB collection to be deleted 08/11/23 15:36:22.514
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:22.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:22.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4296" for this suite. 08/11/23 15:36:22.524
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8811" for this suite. 08/11/23 15:36:22.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/11/23 15:36:22.536
Aug 11 15:36:22.536: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
STEP: Building a namespace api object, basename emptydir 08/11/23 15:36:22.537
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:22.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:22.552
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/11/23 15:36:22.555
Aug 11 15:36:22.564: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb" in namespace "emptydir-9634" to be "running"
Aug 11 15:36:22.567: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407504ms
Aug 11 15:36:24.572: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb": Phase="Running", Reason="", readiness=false. Elapsed: 2.008417324s
Aug 11 15:36:24.572: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/11/23 15:36:24.572
Aug 11 15:36:24.573: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9634 PodName:pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:36:24.573: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
Aug 11 15:36:24.573: INFO: ExecWithOptions: Clientset creation
Aug 11 15:36:24.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9634/pods/pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 11 15:36:24.646: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 11 15:36:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9634" for this suite. 08/11/23 15:36:24.65
------------------------------
• [2.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/11/23 15:36:22.536
    Aug 11 15:36:22.536: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:36:22.537
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:36:22.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:36:22.552
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/11/23 15:36:22.555
    Aug 11 15:36:22.564: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb" in namespace "emptydir-9634" to be "running"
    Aug 11 15:36:22.567: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.407504ms
    Aug 11 15:36:24.572: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb": Phase="Running", Reason="", readiness=false. Elapsed: 2.008417324s
    Aug 11 15:36:24.572: INFO: Pod "pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/11/23 15:36:24.572
    Aug 11 15:36:24.573: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9634 PodName:pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:36:24.573: INFO: >>> kubeConfig: /tmp/kubeconfig-591790350
    Aug 11 15:36:24.573: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:36:24.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9634/pods/pod-sharedvolume-fd2e58d1-f6f5-4dbb-9663-10bfc85c00cb/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 11 15:36:24.646: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 11 15:36:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9634" for this suite. 08/11/23 15:36:24.65
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 11 15:36:24.658: INFO: Running AfterSuite actions on node 1
Aug 11 15:36:24.658: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 11 15:36:24.658: INFO: Running AfterSuite actions on node 1
    Aug 11 15:36:24.658: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.061 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5643.959 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h34m4.210115246s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


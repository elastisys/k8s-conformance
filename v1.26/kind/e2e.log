I0907 05:05:16.911024      29 e2e.go:126] Starting e2e run "b2ea5160-bc07-49e5-8e7a-ce95b6c6d343" on Ginkgo node 1
Sep  7 05:05:16.920: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1694063116 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  7 05:05:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:05:16.981: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0907 05:05:16.981817      29 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Sep  7 05:05:16.990: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  7 05:05:17.000: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  7 05:05:17.000: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Sep  7 05:05:17.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  7 05:05:17.002: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Sep  7 05:05:17.002: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  7 05:05:17.002: INFO: e2e test version: v1.26.6
Sep  7 05:05:17.003: INFO: kube-apiserver version: v1.26.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  7 05:05:17.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:05:17.006: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.025 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  7 05:05:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:05:16.981: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0907 05:05:16.981817      29 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Sep  7 05:05:16.990: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  7 05:05:17.000: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  7 05:05:17.000: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Sep  7 05:05:17.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  7 05:05:17.002: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
    Sep  7 05:05:17.002: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  7 05:05:17.002: INFO: e2e test version: v1.26.6
    Sep  7 05:05:17.003: INFO: kube-apiserver version: v1.26.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  7 05:05:17.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:05:17.006: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:05:17.022
Sep  7 05:05:17.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename endpointslice 09/07/23 05:05:17.023
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:17.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:17.033
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/07/23 05:05:17.034
STEP: getting /apis/discovery.k8s.io 09/07/23 05:05:17.036
STEP: getting /apis/discovery.k8s.iov1 09/07/23 05:05:17.037
STEP: creating 09/07/23 05:05:17.037
STEP: getting 09/07/23 05:05:17.046
STEP: listing 09/07/23 05:05:17.048
STEP: watching 09/07/23 05:05:17.049
Sep  7 05:05:17.049: INFO: starting watch
STEP: cluster-wide listing 09/07/23 05:05:17.05
STEP: cluster-wide watching 09/07/23 05:05:17.051
Sep  7 05:05:17.051: INFO: starting watch
STEP: patching 09/07/23 05:05:17.052
STEP: updating 09/07/23 05:05:17.057
Sep  7 05:05:17.063: INFO: waiting for watch events with expected annotations
Sep  7 05:05:17.063: INFO: saw patched and updated annotations
STEP: deleting 09/07/23 05:05:17.063
STEP: deleting a collection 09/07/23 05:05:17.073
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  7 05:05:17.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2043" for this suite. 09/07/23 05:05:17.082
------------------------------
â€¢ [0.063 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:05:17.022
    Sep  7 05:05:17.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename endpointslice 09/07/23 05:05:17.023
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:17.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:17.033
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/07/23 05:05:17.034
    STEP: getting /apis/discovery.k8s.io 09/07/23 05:05:17.036
    STEP: getting /apis/discovery.k8s.iov1 09/07/23 05:05:17.037
    STEP: creating 09/07/23 05:05:17.037
    STEP: getting 09/07/23 05:05:17.046
    STEP: listing 09/07/23 05:05:17.048
    STEP: watching 09/07/23 05:05:17.049
    Sep  7 05:05:17.049: INFO: starting watch
    STEP: cluster-wide listing 09/07/23 05:05:17.05
    STEP: cluster-wide watching 09/07/23 05:05:17.051
    Sep  7 05:05:17.051: INFO: starting watch
    STEP: patching 09/07/23 05:05:17.052
    STEP: updating 09/07/23 05:05:17.057
    Sep  7 05:05:17.063: INFO: waiting for watch events with expected annotations
    Sep  7 05:05:17.063: INFO: saw patched and updated annotations
    STEP: deleting 09/07/23 05:05:17.063
    STEP: deleting a collection 09/07/23 05:05:17.073
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:05:17.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2043" for this suite. 09/07/23 05:05:17.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSE0907 05:05:17.086491      29 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:05:17.086
Sep  7 05:05:17.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 05:05:17.087
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:17.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:17.096
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  7 05:05:17.112: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c79065bf-fbe2-4121-868b-c9bc92ee15ff", Controller:(*bool)(0xc00057e6e6), BlockOwnerDeletion:(*bool)(0xc00057e6e7)}}
Sep  7 05:05:17.116: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"42a5e9da-3198-40d2-8dcd-7cf1f86ee23a", Controller:(*bool)(0xc00057e90e), BlockOwnerDeletion:(*bool)(0xc00057e90f)}}
Sep  7 05:05:17.122: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79196a13-f1ee-4a27-a115-27e4a766958d", Controller:(*bool)(0xc00057eb36), BlockOwnerDeletion:(*bool)(0xc00057eb37)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 05:05:22.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9426" for this suite. 09/07/23 05:05:22.13
------------------------------
â€¢ [SLOW TEST] [5.047 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:05:17.086
    Sep  7 05:05:17.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 05:05:17.087
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:17.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:17.096
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  7 05:05:17.112: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c79065bf-fbe2-4121-868b-c9bc92ee15ff", Controller:(*bool)(0xc00057e6e6), BlockOwnerDeletion:(*bool)(0xc00057e6e7)}}
    Sep  7 05:05:17.116: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"42a5e9da-3198-40d2-8dcd-7cf1f86ee23a", Controller:(*bool)(0xc00057e90e), BlockOwnerDeletion:(*bool)(0xc00057e90f)}}
    Sep  7 05:05:17.122: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"79196a13-f1ee-4a27-a115-27e4a766958d", Controller:(*bool)(0xc00057eb36), BlockOwnerDeletion:(*bool)(0xc00057eb37)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:05:22.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9426" for this suite. 09/07/23 05:05:22.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:05:22.134
Sep  7 05:05:22.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename tables 09/07/23 05:05:22.135
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:22.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:22.147
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  7 05:05:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-453" for this suite. 09/07/23 05:05:22.152
------------------------------
â€¢ [0.020 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:05:22.134
    Sep  7 05:05:22.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename tables 09/07/23 05:05:22.135
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:22.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:22.147
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:05:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-453" for this suite. 09/07/23 05:05:22.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:05:22.156
Sep  7 05:05:22.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename taint-single-pod 09/07/23 05:05:22.156
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:22.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:22.166
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  7 05:05:22.168: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 05:06:22.179: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  7 05:06:22.181: INFO: Starting informer...
STEP: Starting pod... 09/07/23 05:06:22.181
Sep  7 05:06:22.391: INFO: Pod is running on kind-worker2. Tainting Node
STEP: Trying to apply a taint on the Node 09/07/23 05:06:22.391
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 05:06:22.399
STEP: Waiting short time to make sure Pod is queued for deletion 09/07/23 05:06:22.401
Sep  7 05:06:22.401: INFO: Pod wasn't evicted. Proceeding
Sep  7 05:06:22.401: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 05:06:22.408
STEP: Waiting some time to make sure that toleration time passed. 09/07/23 05:06:22.409
Sep  7 05:07:37.409: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:07:37.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-7013" for this suite. 09/07/23 05:07:37.412
------------------------------
â€¢ [SLOW TEST] [135.261 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:05:22.156
    Sep  7 05:05:22.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename taint-single-pod 09/07/23 05:05:22.156
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:05:22.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:05:22.166
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  7 05:05:22.168: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 05:06:22.179: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  7 05:06:22.181: INFO: Starting informer...
    STEP: Starting pod... 09/07/23 05:06:22.181
    Sep  7 05:06:22.391: INFO: Pod is running on kind-worker2. Tainting Node
    STEP: Trying to apply a taint on the Node 09/07/23 05:06:22.391
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 05:06:22.399
    STEP: Waiting short time to make sure Pod is queued for deletion 09/07/23 05:06:22.401
    Sep  7 05:06:22.401: INFO: Pod wasn't evicted. Proceeding
    Sep  7 05:06:22.401: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 05:06:22.408
    STEP: Waiting some time to make sure that toleration time passed. 09/07/23 05:06:22.409
    Sep  7 05:07:37.409: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:07:37.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-7013" for this suite. 09/07/23 05:07:37.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:07:37.417
Sep  7 05:07:37.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-pred 09/07/23 05:07:37.418
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:07:37.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:07:37.43
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  7 05:07:37.431: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  7 05:07:37.435: INFO: Waiting for terminating namespaces to be deleted...
Sep  7 05:07:37.436: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  7 05:07:37.439: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.439: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 05:07:37.439: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.439: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 05:07:37.439: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.439: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  7 05:07:37.439: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:07:37.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:07:37.439: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  7 05:07:37.439: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  7 05:07:37.442: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.442: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 05:07:37.442: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.442: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 05:07:37.442: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:07:37.442: INFO: 	Container e2e ready: true, restart count 0
Sep  7 05:07:37.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:07:37.442: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:07:37.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:07:37.442: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  7 05:07:37.442: INFO: taint-eviction-4 from taint-single-pod-7013 started at 2023-09-07 05:06:22 +0000 UTC (1 container statuses recorded)
Sep  7 05:07:37.442: INFO: 	Container pause ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 05:07:37.442
Sep  7 05:07:37.446: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5691" to be "running"
Sep  7 05:07:37.448: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258891ms
Sep  7 05:07:39.451: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004786563s
Sep  7 05:07:39.451: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 05:07:39.453
STEP: Trying to apply a random label on the found node. 09/07/23 05:07:39.465
STEP: verifying the node has the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 95 09/07/23 05:07:39.471
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/07/23 05:07:39.473
Sep  7 05:07:39.478: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5691" to be "not pending"
Sep  7 05:07:39.480: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561942ms
Sep  7 05:07:41.483: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005239199s
Sep  7 05:07:43.483: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.004660248s
Sep  7 05:07:43.483: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.8.3 on the node which pod4 resides and expect not scheduled 09/07/23 05:07:43.483
Sep  7 05:07:43.487: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5691" to be "not pending"
Sep  7 05:07:43.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.678921ms
Sep  7 05:07:45.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004752598s
Sep  7 05:07:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004241778s
Sep  7 05:07:49.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005742474s
Sep  7 05:07:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004877874s
Sep  7 05:07:53.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004518799s
Sep  7 05:07:55.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00461079s
Sep  7 05:07:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004484186s
Sep  7 05:07:59.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005767488s
Sep  7 05:08:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005096783s
Sep  7 05:08:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005661245s
Sep  7 05:08:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004827621s
Sep  7 05:08:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004365293s
Sep  7 05:08:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005505951s
Sep  7 05:08:11.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005720794s
Sep  7 05:08:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005195062s
Sep  7 05:08:15.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005569635s
Sep  7 05:08:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004391382s
Sep  7 05:08:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005634567s
Sep  7 05:08:21.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004600926s
Sep  7 05:08:23.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005792021s
Sep  7 05:08:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005466481s
Sep  7 05:08:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004911486s
Sep  7 05:08:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004766917s
Sep  7 05:08:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004951784s
Sep  7 05:08:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005567267s
Sep  7 05:08:35.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005723734s
Sep  7 05:08:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004692937s
Sep  7 05:08:39.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005768866s
Sep  7 05:08:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00486266s
Sep  7 05:08:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00481924s
Sep  7 05:08:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004593145s
Sep  7 05:08:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004017346s
Sep  7 05:08:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005233013s
Sep  7 05:08:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005318955s
Sep  7 05:08:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004762113s
Sep  7 05:08:55.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005986298s
Sep  7 05:08:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004574077s
Sep  7 05:08:59.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004628921s
Sep  7 05:09:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004984043s
Sep  7 05:09:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004699179s
Sep  7 05:09:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004898781s
Sep  7 05:09:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00462349s
Sep  7 05:09:09.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005982584s
Sep  7 05:09:11.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004974424s
Sep  7 05:09:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00485287s
Sep  7 05:09:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004658041s
Sep  7 05:09:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004340268s
Sep  7 05:09:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005650712s
Sep  7 05:09:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006075641s
Sep  7 05:09:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.003915606s
Sep  7 05:09:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005256467s
Sep  7 05:09:27.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004604264s
Sep  7 05:09:29.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005749718s
Sep  7 05:09:31.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005912327s
Sep  7 05:09:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004888111s
Sep  7 05:09:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005085622s
Sep  7 05:09:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004630999s
Sep  7 05:09:39.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005986493s
Sep  7 05:09:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005157211s
Sep  7 05:09:43.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005791756s
Sep  7 05:09:45.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004783977s
Sep  7 05:09:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004367264s
Sep  7 05:09:49.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004697568s
Sep  7 05:09:51.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005736039s
Sep  7 05:09:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004986874s
Sep  7 05:09:55.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005373536s
Sep  7 05:09:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005284634s
Sep  7 05:09:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004939608s
Sep  7 05:10:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005036729s
Sep  7 05:10:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005426216s
Sep  7 05:10:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005774429s
Sep  7 05:10:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004147627s
Sep  7 05:10:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005663774s
Sep  7 05:10:11.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006002806s
Sep  7 05:10:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004824883s
Sep  7 05:10:15.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006339759s
Sep  7 05:10:17.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005060989s
Sep  7 05:10:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004670246s
Sep  7 05:10:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005770059s
Sep  7 05:10:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004237068s
Sep  7 05:10:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005673056s
Sep  7 05:10:27.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004613208s
Sep  7 05:10:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005547607s
Sep  7 05:10:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005036722s
Sep  7 05:10:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004714274s
Sep  7 05:10:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004608502s
Sep  7 05:10:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004509727s
Sep  7 05:10:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005653609s
Sep  7 05:10:41.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004663556s
Sep  7 05:10:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00511263s
Sep  7 05:10:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004670871s
Sep  7 05:10:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004087318s
Sep  7 05:10:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005386013s
Sep  7 05:10:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004681543s
Sep  7 05:10:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004939899s
Sep  7 05:10:55.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005165973s
Sep  7 05:10:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004848783s
Sep  7 05:10:59.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.00469942s
Sep  7 05:11:01.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005925344s
Sep  7 05:11:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004760814s
Sep  7 05:11:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005900561s
Sep  7 05:11:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004534914s
Sep  7 05:11:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004871774s
Sep  7 05:11:11.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004471551s
Sep  7 05:11:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004629504s
Sep  7 05:11:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004692874s
Sep  7 05:11:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004141063s
Sep  7 05:11:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004629617s
Sep  7 05:11:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005806859s
Sep  7 05:11:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004015016s
Sep  7 05:11:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005125481s
Sep  7 05:11:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004950112s
Sep  7 05:11:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004855799s
Sep  7 05:11:31.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006778504s
Sep  7 05:11:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005630374s
Sep  7 05:11:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004671931s
Sep  7 05:11:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004401736s
Sep  7 05:11:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004712377s
Sep  7 05:11:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005139944s
Sep  7 05:11:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005033279s
Sep  7 05:11:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00464955s
Sep  7 05:11:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004537787s
Sep  7 05:11:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004847557s
Sep  7 05:11:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00453311s
Sep  7 05:11:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004897216s
Sep  7 05:11:55.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006778412s
Sep  7 05:11:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004588835s
Sep  7 05:11:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005634869s
Sep  7 05:12:01.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.004540598s
Sep  7 05:12:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004504992s
Sep  7 05:12:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006047225s
Sep  7 05:12:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004288015s
Sep  7 05:12:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00464299s
Sep  7 05:12:11.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005412334s
Sep  7 05:12:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.004695755s
Sep  7 05:12:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004578872s
Sep  7 05:12:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004278777s
Sep  7 05:12:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004882779s
Sep  7 05:12:21.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005293208s
Sep  7 05:12:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.003963054s
Sep  7 05:12:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005304138s
Sep  7 05:12:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005363358s
Sep  7 05:12:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004886236s
Sep  7 05:12:31.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00425845s
Sep  7 05:12:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005000982s
Sep  7 05:12:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005049991s
Sep  7 05:12:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005101238s
Sep  7 05:12:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005219421s
Sep  7 05:12:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004755452s
Sep  7 05:12:43.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00468751s
Sep  7 05:12:43.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00618863s
STEP: removing the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 off the node kind-worker 09/07/23 05:12:43.493
STEP: verifying the node doesn't have the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 09/07/23 05:12:43.501
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:12:43.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5691" for this suite. 09/07/23 05:12:43.505
------------------------------
â€¢ [SLOW TEST] [306.092 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:07:37.417
    Sep  7 05:07:37.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-pred 09/07/23 05:07:37.418
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:07:37.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:07:37.43
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  7 05:07:37.431: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  7 05:07:37.435: INFO: Waiting for terminating namespaces to be deleted...
    Sep  7 05:07:37.436: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  7 05:07:37.439: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.439: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 05:07:37.439: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.439: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 05:07:37.439: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.439: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  7 05:07:37.439: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:07:37.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:07:37.439: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  7 05:07:37.439: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  7 05:07:37.442: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.442: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.442: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:07:37.442: INFO: 	Container e2e ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:07:37.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  7 05:07:37.442: INFO: taint-eviction-4 from taint-single-pod-7013 started at 2023-09-07 05:06:22 +0000 UTC (1 container statuses recorded)
    Sep  7 05:07:37.442: INFO: 	Container pause ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 05:07:37.442
    Sep  7 05:07:37.446: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5691" to be "running"
    Sep  7 05:07:37.448: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.258891ms
    Sep  7 05:07:39.451: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004786563s
    Sep  7 05:07:39.451: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 05:07:39.453
    STEP: Trying to apply a random label on the found node. 09/07/23 05:07:39.465
    STEP: verifying the node has the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 95 09/07/23 05:07:39.471
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/07/23 05:07:39.473
    Sep  7 05:07:39.478: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5691" to be "not pending"
    Sep  7 05:07:39.480: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561942ms
    Sep  7 05:07:41.483: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005239199s
    Sep  7 05:07:43.483: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.004660248s
    Sep  7 05:07:43.483: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.8.3 on the node which pod4 resides and expect not scheduled 09/07/23 05:07:43.483
    Sep  7 05:07:43.487: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5691" to be "not pending"
    Sep  7 05:07:43.488: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.678921ms
    Sep  7 05:07:45.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004752598s
    Sep  7 05:07:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004241778s
    Sep  7 05:07:49.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005742474s
    Sep  7 05:07:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004877874s
    Sep  7 05:07:53.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004518799s
    Sep  7 05:07:55.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00461079s
    Sep  7 05:07:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004484186s
    Sep  7 05:07:59.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005767488s
    Sep  7 05:08:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005096783s
    Sep  7 05:08:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005661245s
    Sep  7 05:08:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004827621s
    Sep  7 05:08:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004365293s
    Sep  7 05:08:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005505951s
    Sep  7 05:08:11.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005720794s
    Sep  7 05:08:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005195062s
    Sep  7 05:08:15.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005569635s
    Sep  7 05:08:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004391382s
    Sep  7 05:08:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005634567s
    Sep  7 05:08:21.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004600926s
    Sep  7 05:08:23.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005792021s
    Sep  7 05:08:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005466481s
    Sep  7 05:08:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004911486s
    Sep  7 05:08:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004766917s
    Sep  7 05:08:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004951784s
    Sep  7 05:08:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005567267s
    Sep  7 05:08:35.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005723734s
    Sep  7 05:08:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004692937s
    Sep  7 05:08:39.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005768866s
    Sep  7 05:08:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00486266s
    Sep  7 05:08:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00481924s
    Sep  7 05:08:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004593145s
    Sep  7 05:08:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004017346s
    Sep  7 05:08:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005233013s
    Sep  7 05:08:51.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005318955s
    Sep  7 05:08:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004762113s
    Sep  7 05:08:55.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005986298s
    Sep  7 05:08:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004574077s
    Sep  7 05:08:59.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004628921s
    Sep  7 05:09:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004984043s
    Sep  7 05:09:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004699179s
    Sep  7 05:09:05.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004898781s
    Sep  7 05:09:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.00462349s
    Sep  7 05:09:09.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005982584s
    Sep  7 05:09:11.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004974424s
    Sep  7 05:09:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.00485287s
    Sep  7 05:09:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004658041s
    Sep  7 05:09:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004340268s
    Sep  7 05:09:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005650712s
    Sep  7 05:09:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006075641s
    Sep  7 05:09:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.003915606s
    Sep  7 05:09:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005256467s
    Sep  7 05:09:27.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004604264s
    Sep  7 05:09:29.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005749718s
    Sep  7 05:09:31.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005912327s
    Sep  7 05:09:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004888111s
    Sep  7 05:09:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005085622s
    Sep  7 05:09:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004630999s
    Sep  7 05:09:39.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005986493s
    Sep  7 05:09:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005157211s
    Sep  7 05:09:43.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005791756s
    Sep  7 05:09:45.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.004783977s
    Sep  7 05:09:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.004367264s
    Sep  7 05:09:49.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004697568s
    Sep  7 05:09:51.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005736039s
    Sep  7 05:09:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004986874s
    Sep  7 05:09:55.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005373536s
    Sep  7 05:09:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005284634s
    Sep  7 05:09:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004939608s
    Sep  7 05:10:01.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.005036729s
    Sep  7 05:10:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005426216s
    Sep  7 05:10:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005774429s
    Sep  7 05:10:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004147627s
    Sep  7 05:10:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005663774s
    Sep  7 05:10:11.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006002806s
    Sep  7 05:10:13.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.004824883s
    Sep  7 05:10:15.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006339759s
    Sep  7 05:10:17.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005060989s
    Sep  7 05:10:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004670246s
    Sep  7 05:10:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.005770059s
    Sep  7 05:10:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.004237068s
    Sep  7 05:10:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005673056s
    Sep  7 05:10:27.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.004613208s
    Sep  7 05:10:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005547607s
    Sep  7 05:10:31.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005036722s
    Sep  7 05:10:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004714274s
    Sep  7 05:10:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004608502s
    Sep  7 05:10:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.004509727s
    Sep  7 05:10:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.005653609s
    Sep  7 05:10:41.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004663556s
    Sep  7 05:10:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.00511263s
    Sep  7 05:10:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004670871s
    Sep  7 05:10:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.004087318s
    Sep  7 05:10:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005386013s
    Sep  7 05:10:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004681543s
    Sep  7 05:10:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.004939899s
    Sep  7 05:10:55.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005165973s
    Sep  7 05:10:57.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.004848783s
    Sep  7 05:10:59.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.00469942s
    Sep  7 05:11:01.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005925344s
    Sep  7 05:11:03.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004760814s
    Sep  7 05:11:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005900561s
    Sep  7 05:11:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.004534914s
    Sep  7 05:11:09.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004871774s
    Sep  7 05:11:11.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004471551s
    Sep  7 05:11:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.004629504s
    Sep  7 05:11:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004692874s
    Sep  7 05:11:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.004141063s
    Sep  7 05:11:19.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004629617s
    Sep  7 05:11:21.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005806859s
    Sep  7 05:11:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004015016s
    Sep  7 05:11:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005125481s
    Sep  7 05:11:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004950112s
    Sep  7 05:11:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004855799s
    Sep  7 05:11:31.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006778504s
    Sep  7 05:11:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005630374s
    Sep  7 05:11:35.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004671931s
    Sep  7 05:11:37.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004401736s
    Sep  7 05:11:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004712377s
    Sep  7 05:11:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.005139944s
    Sep  7 05:11:43.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005033279s
    Sep  7 05:11:45.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.00464955s
    Sep  7 05:11:47.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004537787s
    Sep  7 05:11:49.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004847557s
    Sep  7 05:11:51.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00453311s
    Sep  7 05:11:53.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004897216s
    Sep  7 05:11:55.494: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006778412s
    Sep  7 05:11:57.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004588835s
    Sep  7 05:11:59.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005634869s
    Sep  7 05:12:01.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.004540598s
    Sep  7 05:12:03.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004504992s
    Sep  7 05:12:05.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.006047225s
    Sep  7 05:12:07.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004288015s
    Sep  7 05:12:09.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.00464299s
    Sep  7 05:12:11.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005412334s
    Sep  7 05:12:13.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.004695755s
    Sep  7 05:12:15.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004578872s
    Sep  7 05:12:17.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004278777s
    Sep  7 05:12:19.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004882779s
    Sep  7 05:12:21.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.005293208s
    Sep  7 05:12:23.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.003963054s
    Sep  7 05:12:25.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005304138s
    Sep  7 05:12:27.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.005363358s
    Sep  7 05:12:29.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004886236s
    Sep  7 05:12:31.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.00425845s
    Sep  7 05:12:33.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005000982s
    Sep  7 05:12:35.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005049991s
    Sep  7 05:12:37.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005101238s
    Sep  7 05:12:39.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005219421s
    Sep  7 05:12:41.492: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004755452s
    Sep  7 05:12:43.491: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00468751s
    Sep  7 05:12:43.493: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00618863s
    STEP: removing the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 off the node kind-worker 09/07/23 05:12:43.493
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-698c983d-32bb-46fc-8884-09061a88c448 09/07/23 05:12:43.501
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:12:43.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5691" for this suite. 09/07/23 05:12:43.505
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:12:43.51
Sep  7 05:12:43.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:12:43.51
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:43.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:43.52
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-1ac9a0c5-f9a5-43fc-bab5-efc427339ad6 09/07/23 05:12:43.528
STEP: Creating configMap with name cm-test-opt-upd-d128dcc3-8edb-4fde-87c1-5a6b240caafe 09/07/23 05:12:43.531
STEP: Creating the pod 09/07/23 05:12:43.534
Sep  7 05:12:43.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164" in namespace "configmap-626" to be "running and ready"
Sep  7 05:12:43.543: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.762091ms
Sep  7 05:12:43.543: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:12:45.545: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004181686s
Sep  7 05:12:45.545: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:12:47.545: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Running", Reason="", readiness=true. Elapsed: 4.004701279s
Sep  7 05:12:47.546: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Running (Ready = true)
Sep  7 05:12:47.546: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-1ac9a0c5-f9a5-43fc-bab5-efc427339ad6 09/07/23 05:12:47.563
STEP: Updating configmap cm-test-opt-upd-d128dcc3-8edb-4fde-87c1-5a6b240caafe 09/07/23 05:12:47.566
STEP: Creating configMap with name cm-test-opt-create-383c1213-6429-4b5b-a9d0-7f20120a7fe3 09/07/23 05:12:47.569
STEP: waiting to observe update in volume 09/07/23 05:12:47.572
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:12:51.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-626" for this suite. 09/07/23 05:12:51.598
------------------------------
â€¢ [SLOW TEST] [8.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:12:43.51
    Sep  7 05:12:43.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:12:43.51
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:43.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:43.52
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-1ac9a0c5-f9a5-43fc-bab5-efc427339ad6 09/07/23 05:12:43.528
    STEP: Creating configMap with name cm-test-opt-upd-d128dcc3-8edb-4fde-87c1-5a6b240caafe 09/07/23 05:12:43.531
    STEP: Creating the pod 09/07/23 05:12:43.534
    Sep  7 05:12:43.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164" in namespace "configmap-626" to be "running and ready"
    Sep  7 05:12:43.543: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.762091ms
    Sep  7 05:12:43.543: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:12:45.545: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004181686s
    Sep  7 05:12:45.545: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:12:47.545: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164": Phase="Running", Reason="", readiness=true. Elapsed: 4.004701279s
    Sep  7 05:12:47.546: INFO: The phase of Pod pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164 is Running (Ready = true)
    Sep  7 05:12:47.546: INFO: Pod "pod-configmaps-569bcd94-de6e-4238-8ca5-00e8af228164" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-1ac9a0c5-f9a5-43fc-bab5-efc427339ad6 09/07/23 05:12:47.563
    STEP: Updating configmap cm-test-opt-upd-d128dcc3-8edb-4fde-87c1-5a6b240caafe 09/07/23 05:12:47.566
    STEP: Creating configMap with name cm-test-opt-create-383c1213-6429-4b5b-a9d0-7f20120a7fe3 09/07/23 05:12:47.569
    STEP: waiting to observe update in volume 09/07/23 05:12:47.572
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:12:51.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-626" for this suite. 09/07/23 05:12:51.598
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:12:51.602
Sep  7 05:12:51.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:12:51.602
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:51.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:51.611
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-8d68624c-2bec-4b57-9104-cbbacde04df8 09/07/23 05:12:51.612
STEP: Creating a pod to test consume configMaps 09/07/23 05:12:51.615
Sep  7 05:12:51.620: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756" in namespace "projected-4511" to be "Succeeded or Failed"
Sep  7 05:12:51.621: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30741ms
Sep  7 05:12:53.624: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004167766s
Sep  7 05:12:55.625: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005464958s
STEP: Saw pod success 09/07/23 05:12:55.625
Sep  7 05:12:55.625: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756" satisfied condition "Succeeded or Failed"
Sep  7 05:12:55.627: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:12:55.636
Sep  7 05:12:55.645: INFO: Waiting for pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 to disappear
Sep  7 05:12:55.646: INFO: Pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:12:55.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4511" for this suite. 09/07/23 05:12:55.648
------------------------------
â€¢ [4.050 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:12:51.602
    Sep  7 05:12:51.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:12:51.602
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:51.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:51.611
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-8d68624c-2bec-4b57-9104-cbbacde04df8 09/07/23 05:12:51.612
    STEP: Creating a pod to test consume configMaps 09/07/23 05:12:51.615
    Sep  7 05:12:51.620: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756" in namespace "projected-4511" to be "Succeeded or Failed"
    Sep  7 05:12:51.621: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30741ms
    Sep  7 05:12:53.624: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004167766s
    Sep  7 05:12:55.625: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005464958s
    STEP: Saw pod success 09/07/23 05:12:55.625
    Sep  7 05:12:55.625: INFO: Pod "pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756" satisfied condition "Succeeded or Failed"
    Sep  7 05:12:55.627: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:12:55.636
    Sep  7 05:12:55.645: INFO: Waiting for pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 to disappear
    Sep  7 05:12:55.646: INFO: Pod pod-projected-configmaps-9ec09be8-a9e3-4752-938a-bfadeea8a756 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:12:55.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4511" for this suite. 09/07/23 05:12:55.648
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:12:55.651
Sep  7 05:12:55.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename aggregator 09/07/23 05:12:55.652
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:55.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:55.663
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  7 05:12:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/07/23 05:12:55.665
Sep  7 05:12:56.041: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  7 05:12:58.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:00.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:02.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:04.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:06.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:08.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:10.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:12.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:14.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:16.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:18.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:13:20.188: INFO: Waited 112.253111ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/07/23 05:13:20.218
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/07/23 05:13:20.22
STEP: List APIServices 09/07/23 05:13:20.225
Sep  7 05:13:20.229: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  7 05:13:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9861" for this suite. 09/07/23 05:13:20.332
------------------------------
â€¢ [SLOW TEST] [24.770 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:12:55.651
    Sep  7 05:12:55.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename aggregator 09/07/23 05:12:55.652
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:12:55.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:12:55.663
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  7 05:12:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/07/23 05:12:55.665
    Sep  7 05:12:56.041: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  7 05:12:58.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:00.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:02.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:04.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:06.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:08.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:10.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:12.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:14.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:16.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:18.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 12, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:13:20.188: INFO: Waited 112.253111ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/07/23 05:13:20.218
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/07/23 05:13:20.22
    STEP: List APIServices 09/07/23 05:13:20.225
    Sep  7 05:13:20.229: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:13:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9861" for this suite. 09/07/23 05:13:20.332
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:13:20.422
Sep  7 05:13:20.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 05:13:20.423
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:20.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:20.513
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  7 05:13:20.537: INFO: Waiting up to 5m0s for pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542" in namespace "container-probe-5739" to be "running and ready"
Sep  7 05:13:20.538: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57869ms
Sep  7 05:13:20.538: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:13:22.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 2.004370797s
Sep  7 05:13:22.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:24.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 4.004271741s
Sep  7 05:13:24.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:26.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 6.005426362s
Sep  7 05:13:26.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:28.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 8.00497237s
Sep  7 05:13:28.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:30.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 10.005032776s
Sep  7 05:13:30.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:32.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 12.00440186s
Sep  7 05:13:32.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:34.543: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 14.006138581s
Sep  7 05:13:34.543: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:36.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 16.00546248s
Sep  7 05:13:36.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:38.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 18.005390316s
Sep  7 05:13:38.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:40.543: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 20.00592965s
Sep  7 05:13:40.543: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
Sep  7 05:13:42.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=true. Elapsed: 22.00424179s
Sep  7 05:13:42.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = true)
Sep  7 05:13:42.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542" satisfied condition "running and ready"
Sep  7 05:13:42.543: INFO: Container started at 2023-09-07 05:13:21 +0000 UTC, pod became ready at 2023-09-07 05:13:40 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 05:13:42.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5739" for this suite. 09/07/23 05:13:42.545
------------------------------
â€¢ [SLOW TEST] [22.126 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:13:20.422
    Sep  7 05:13:20.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 05:13:20.423
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:20.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:20.513
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  7 05:13:20.537: INFO: Waiting up to 5m0s for pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542" in namespace "container-probe-5739" to be "running and ready"
    Sep  7 05:13:20.538: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57869ms
    Sep  7 05:13:20.538: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:13:22.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 2.004370797s
    Sep  7 05:13:22.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:24.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 4.004271741s
    Sep  7 05:13:24.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:26.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 6.005426362s
    Sep  7 05:13:26.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:28.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 8.00497237s
    Sep  7 05:13:28.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:30.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 10.005032776s
    Sep  7 05:13:30.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:32.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 12.00440186s
    Sep  7 05:13:32.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:34.543: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 14.006138581s
    Sep  7 05:13:34.543: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:36.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 16.00546248s
    Sep  7 05:13:36.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:38.542: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 18.005390316s
    Sep  7 05:13:38.542: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:40.543: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=false. Elapsed: 20.00592965s
    Sep  7 05:13:40.543: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = false)
    Sep  7 05:13:42.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542": Phase="Running", Reason="", readiness=true. Elapsed: 22.00424179s
    Sep  7 05:13:42.541: INFO: The phase of Pod test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542 is Running (Ready = true)
    Sep  7 05:13:42.541: INFO: Pod "test-webserver-ab0cc63e-821b-448c-9cee-206eb4347542" satisfied condition "running and ready"
    Sep  7 05:13:42.543: INFO: Container started at 2023-09-07 05:13:21 +0000 UTC, pod became ready at 2023-09-07 05:13:40 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:13:42.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5739" for this suite. 09/07/23 05:13:42.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:13:42.55
Sep  7 05:13:42.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:13:42.55
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:42.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:42.559
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:13:42.561
Sep  7 05:13:42.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d" in namespace "downward-api-6048" to be "Succeeded or Failed"
Sep  7 05:13:42.567: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37842ms
Sep  7 05:13:44.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00458786s
Sep  7 05:13:46.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004696477s
STEP: Saw pod success 09/07/23 05:13:46.571
Sep  7 05:13:46.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d" satisfied condition "Succeeded or Failed"
Sep  7 05:13:46.573: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d container client-container: <nil>
STEP: delete the pod 09/07/23 05:13:46.576
Sep  7 05:13:46.583: INFO: Waiting for pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d to disappear
Sep  7 05:13:46.584: INFO: Pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 05:13:46.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6048" for this suite. 09/07/23 05:13:46.586
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:13:42.55
    Sep  7 05:13:42.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:13:42.55
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:42.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:42.559
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:13:42.561
    Sep  7 05:13:42.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d" in namespace "downward-api-6048" to be "Succeeded or Failed"
    Sep  7 05:13:42.567: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37842ms
    Sep  7 05:13:44.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00458786s
    Sep  7 05:13:46.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004696477s
    STEP: Saw pod success 09/07/23 05:13:46.571
    Sep  7 05:13:46.571: INFO: Pod "downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d" satisfied condition "Succeeded or Failed"
    Sep  7 05:13:46.573: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d container client-container: <nil>
    STEP: delete the pod 09/07/23 05:13:46.576
    Sep  7 05:13:46.583: INFO: Waiting for pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d to disappear
    Sep  7 05:13:46.584: INFO: Pod downwardapi-volume-d24bdaef-a5e6-4451-aad2-c348bf29a12d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:13:46.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6048" for this suite. 09/07/23 05:13:46.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:13:46.59
Sep  7 05:13:46.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 05:13:46.59
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:46.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:46.598
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/07/23 05:13:46.6
STEP: submitting the pod to kubernetes 09/07/23 05:13:46.6
Sep  7 05:13:46.604: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" in namespace "pods-4331" to be "running and ready"
Sep  7 05:13:46.605: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472521ms
Sep  7 05:13:46.605: INFO: The phase of Pod pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:13:48.609: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 2.004900015s
Sep  7 05:13:48.609: INFO: The phase of Pod pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407 is Running (Ready = true)
Sep  7 05:13:48.609: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/07/23 05:13:48.61
STEP: updating the pod 09/07/23 05:13:48.612
Sep  7 05:13:49.121: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407"
Sep  7 05:13:49.121: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" in namespace "pods-4331" to be "terminated with reason DeadlineExceeded"
Sep  7 05:13:49.123: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 1.558121ms
Sep  7 05:13:51.125: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 2.004051282s
Sep  7 05:13:53.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=false. Elapsed: 4.004845641s
Sep  7 05:13:55.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005121667s
Sep  7 05:13:55.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 05:13:55.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4331" for this suite. 09/07/23 05:13:55.128
------------------------------
â€¢ [SLOW TEST] [8.544 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:13:46.59
    Sep  7 05:13:46.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 05:13:46.59
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:46.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:46.598
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/07/23 05:13:46.6
    STEP: submitting the pod to kubernetes 09/07/23 05:13:46.6
    Sep  7 05:13:46.604: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" in namespace "pods-4331" to be "running and ready"
    Sep  7 05:13:46.605: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472521ms
    Sep  7 05:13:46.605: INFO: The phase of Pod pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:13:48.609: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 2.004900015s
    Sep  7 05:13:48.609: INFO: The phase of Pod pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407 is Running (Ready = true)
    Sep  7 05:13:48.609: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/07/23 05:13:48.61
    STEP: updating the pod 09/07/23 05:13:48.612
    Sep  7 05:13:49.121: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407"
    Sep  7 05:13:49.121: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" in namespace "pods-4331" to be "terminated with reason DeadlineExceeded"
    Sep  7 05:13:49.123: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 1.558121ms
    Sep  7 05:13:51.125: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=true. Elapsed: 2.004051282s
    Sep  7 05:13:53.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Running", Reason="", readiness=false. Elapsed: 4.004845641s
    Sep  7 05:13:55.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.005121667s
    Sep  7 05:13:55.126: INFO: Pod "pod-update-activedeadlineseconds-1e7b4566-ea3d-402d-b84c-61d888543407" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:13:55.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4331" for this suite. 09/07/23 05:13:55.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:13:55.136
Sep  7 05:13:55.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:13:55.137
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:55.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:55.146
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:13:55.148
Sep  7 05:13:55.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977" in namespace "downward-api-377" to be "Succeeded or Failed"
Sep  7 05:13:55.155: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409121ms
Sep  7 05:13:57.158: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004487665s
Sep  7 05:13:59.159: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005437286s
STEP: Saw pod success 09/07/23 05:13:59.159
Sep  7 05:13:59.159: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977" satisfied condition "Succeeded or Failed"
Sep  7 05:13:59.161: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 container client-container: <nil>
STEP: delete the pod 09/07/23 05:13:59.164
Sep  7 05:13:59.171: INFO: Waiting for pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 to disappear
Sep  7 05:13:59.173: INFO: Pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 05:13:59.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-377" for this suite. 09/07/23 05:13:59.177
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:13:55.136
    Sep  7 05:13:55.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:13:55.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:55.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:55.146
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:13:55.148
    Sep  7 05:13:55.154: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977" in namespace "downward-api-377" to be "Succeeded or Failed"
    Sep  7 05:13:55.155: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Pending", Reason="", readiness=false. Elapsed: 1.409121ms
    Sep  7 05:13:57.158: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004487665s
    Sep  7 05:13:59.159: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005437286s
    STEP: Saw pod success 09/07/23 05:13:59.159
    Sep  7 05:13:59.159: INFO: Pod "downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977" satisfied condition "Succeeded or Failed"
    Sep  7 05:13:59.161: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 container client-container: <nil>
    STEP: delete the pod 09/07/23 05:13:59.164
    Sep  7 05:13:59.171: INFO: Waiting for pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 to disappear
    Sep  7 05:13:59.173: INFO: Pod downwardapi-volume-e55d2603-6b8b-493d-8f26-6fad0dfeb977 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:13:59.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-377" for this suite. 09/07/23 05:13:59.177
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:13:59.18
Sep  7 05:13:59.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:13:59.181
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:59.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:59.189
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-c0506982-16a4-49b2-b072-8c52e28493b0 09/07/23 05:13:59.191
STEP: Creating a pod to test consume secrets 09/07/23 05:13:59.193
Sep  7 05:13:59.197: INFO: Waiting up to 5m0s for pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4" in namespace "secrets-6782" to be "Succeeded or Failed"
Sep  7 05:13:59.198: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.171341ms
Sep  7 05:14:01.200: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00315105s
Sep  7 05:14:03.202: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004374237s
STEP: Saw pod success 09/07/23 05:14:03.202
Sep  7 05:14:03.202: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4" satisfied condition "Succeeded or Failed"
Sep  7 05:14:03.203: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:14:03.206
Sep  7 05:14:03.215: INFO: Waiting for pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 to disappear
Sep  7 05:14:03.216: INFO: Pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6782" for this suite. 09/07/23 05:14:03.218
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:13:59.18
    Sep  7 05:13:59.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:13:59.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:13:59.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:13:59.189
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-c0506982-16a4-49b2-b072-8c52e28493b0 09/07/23 05:13:59.191
    STEP: Creating a pod to test consume secrets 09/07/23 05:13:59.193
    Sep  7 05:13:59.197: INFO: Waiting up to 5m0s for pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4" in namespace "secrets-6782" to be "Succeeded or Failed"
    Sep  7 05:13:59.198: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.171341ms
    Sep  7 05:14:01.200: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00315105s
    Sep  7 05:14:03.202: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004374237s
    STEP: Saw pod success 09/07/23 05:14:03.202
    Sep  7 05:14:03.202: INFO: Pod "pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4" satisfied condition "Succeeded or Failed"
    Sep  7 05:14:03.203: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:14:03.206
    Sep  7 05:14:03.215: INFO: Waiting for pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 to disappear
    Sep  7 05:14:03.216: INFO: Pod pod-secrets-d9b1f199-5792-4f7f-9ab2-23553faef1a4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6782" for this suite. 09/07/23 05:14:03.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:03.222
Sep  7 05:14:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:14:03.223
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:03.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:03.232
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/07/23 05:14:03.234
Sep  7 05:14:03.239: INFO: Waiting up to 5m0s for pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587" in namespace "emptydir-3350" to be "Succeeded or Failed"
Sep  7 05:14:03.240: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469011ms
Sep  7 05:14:05.242: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003586416s
Sep  7 05:14:07.243: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004691788s
STEP: Saw pod success 09/07/23 05:14:07.243
Sep  7 05:14:07.243: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587" satisfied condition "Succeeded or Failed"
Sep  7 05:14:07.245: INFO: Trying to get logs from node kind-worker2 pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 container test-container: <nil>
STEP: delete the pod 09/07/23 05:14:07.248
Sep  7 05:14:07.257: INFO: Waiting for pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 to disappear
Sep  7 05:14:07.258: INFO: Pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:07.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3350" for this suite. 09/07/23 05:14:07.261
------------------------------
â€¢ [4.042 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:03.222
    Sep  7 05:14:03.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:14:03.223
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:03.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:03.232
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/07/23 05:14:03.234
    Sep  7 05:14:03.239: INFO: Waiting up to 5m0s for pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587" in namespace "emptydir-3350" to be "Succeeded or Failed"
    Sep  7 05:14:03.240: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469011ms
    Sep  7 05:14:05.242: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003586416s
    Sep  7 05:14:07.243: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004691788s
    STEP: Saw pod success 09/07/23 05:14:07.243
    Sep  7 05:14:07.243: INFO: Pod "pod-1f719f79-171b-46d0-b6f1-e16e012d1587" satisfied condition "Succeeded or Failed"
    Sep  7 05:14:07.245: INFO: Trying to get logs from node kind-worker2 pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:14:07.248
    Sep  7 05:14:07.257: INFO: Waiting for pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 to disappear
    Sep  7 05:14:07.258: INFO: Pod pod-1f719f79-171b-46d0-b6f1-e16e012d1587 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:07.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3350" for this suite. 09/07/23 05:14:07.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:07.265
Sep  7 05:14:07.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context-test 09/07/23 05:14:07.265
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:07.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:07.273
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  7 05:14:07.279: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd" in namespace "security-context-test-8417" to be "Succeeded or Failed"
Sep  7 05:14:07.280: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.18989ms
Sep  7 05:14:09.282: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00339105s
Sep  7 05:14:11.283: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004653398s
Sep  7 05:14:11.283: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8417" for this suite. 09/07/23 05:14:11.285
------------------------------
â€¢ [4.024 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:07.265
    Sep  7 05:14:07.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context-test 09/07/23 05:14:07.265
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:07.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:07.273
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  7 05:14:07.279: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd" in namespace "security-context-test-8417" to be "Succeeded or Failed"
    Sep  7 05:14:07.280: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.18989ms
    Sep  7 05:14:09.282: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00339105s
    Sep  7 05:14:11.283: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004653398s
    Sep  7 05:14:11.283: INFO: Pod "busybox-readonly-false-7b2b213d-8e54-4328-bc8f-a325ba03f9cd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:11.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8417" for this suite. 09/07/23 05:14:11.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:11.289
Sep  7 05:14:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:14:11.29
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:11.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:11.298
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  7 05:14:11.309: INFO: created pod pod-service-account-defaultsa
Sep  7 05:14:11.309: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  7 05:14:11.313: INFO: created pod pod-service-account-mountsa
Sep  7 05:14:11.313: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  7 05:14:11.316: INFO: created pod pod-service-account-nomountsa
Sep  7 05:14:11.316: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  7 05:14:11.323: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  7 05:14:11.323: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  7 05:14:11.328: INFO: created pod pod-service-account-mountsa-mountspec
Sep  7 05:14:11.328: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  7 05:14:11.333: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  7 05:14:11.333: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  7 05:14:11.338: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  7 05:14:11.338: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  7 05:14:11.341: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  7 05:14:11.341: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  7 05:14:11.347: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  7 05:14:11.347: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-202" for this suite. 09/07/23 05:14:11.349
------------------------------
â€¢ [0.065 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:11.289
    Sep  7 05:14:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:14:11.29
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:11.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:11.298
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  7 05:14:11.309: INFO: created pod pod-service-account-defaultsa
    Sep  7 05:14:11.309: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  7 05:14:11.313: INFO: created pod pod-service-account-mountsa
    Sep  7 05:14:11.313: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  7 05:14:11.316: INFO: created pod pod-service-account-nomountsa
    Sep  7 05:14:11.316: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  7 05:14:11.323: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  7 05:14:11.323: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  7 05:14:11.328: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  7 05:14:11.328: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  7 05:14:11.333: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  7 05:14:11.333: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  7 05:14:11.338: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  7 05:14:11.338: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  7 05:14:11.341: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  7 05:14:11.341: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  7 05:14:11.347: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  7 05:14:11.347: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:11.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-202" for this suite. 09/07/23 05:14:11.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:11.357
Sep  7 05:14:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-runtime 09/07/23 05:14:11.358
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:11.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:11.37
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/07/23 05:14:11.372
STEP: wait for the container to reach Succeeded 09/07/23 05:14:11.376
STEP: get the container status 09/07/23 05:14:16.389
STEP: the container should be terminated 09/07/23 05:14:16.393
STEP: the termination message should be set 09/07/23 05:14:16.393
Sep  7 05:14:16.393: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/07/23 05:14:16.393
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6380" for this suite. 09/07/23 05:14:16.414
------------------------------
â€¢ [SLOW TEST] [5.061 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:11.357
    Sep  7 05:14:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-runtime 09/07/23 05:14:11.358
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:11.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:11.37
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/07/23 05:14:11.372
    STEP: wait for the container to reach Succeeded 09/07/23 05:14:11.376
    STEP: get the container status 09/07/23 05:14:16.389
    STEP: the container should be terminated 09/07/23 05:14:16.393
    STEP: the termination message should be set 09/07/23 05:14:16.393
    Sep  7 05:14:16.393: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/07/23 05:14:16.393
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:16.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6380" for this suite. 09/07/23 05:14:16.414
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:16.418
Sep  7 05:14:16.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 05:14:16.419
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:16.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:16.432
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  7 05:14:16.435: INFO: Creating ReplicaSet my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090
Sep  7 05:14:16.439: INFO: Pod name my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Found 0 pods out of 1
Sep  7 05:14:21.442: INFO: Pod name my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Found 1 pods out of 1
Sep  7 05:14:21.442: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090" is running
Sep  7 05:14:21.442: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" in namespace "replicaset-5056" to be "running"
Sep  7 05:14:21.445: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8": Phase="Running", Reason="", readiness=true. Elapsed: 3.033151ms
Sep  7 05:14:21.446: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" satisfied condition "running"
Sep  7 05:14:21.446: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:16 +0000 UTC Reason: Message:}])
Sep  7 05:14:21.446: INFO: Trying to dial the pod
Sep  7 05:14:26.452: INFO: Controller my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Got expected result from replica 1 [my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8]: "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:26.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5056" for this suite. 09/07/23 05:14:26.453
------------------------------
â€¢ [SLOW TEST] [10.039 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:16.418
    Sep  7 05:14:16.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 05:14:16.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:16.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:16.432
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  7 05:14:16.435: INFO: Creating ReplicaSet my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090
    Sep  7 05:14:16.439: INFO: Pod name my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Found 0 pods out of 1
    Sep  7 05:14:21.442: INFO: Pod name my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Found 1 pods out of 1
    Sep  7 05:14:21.442: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090" is running
    Sep  7 05:14:21.442: INFO: Waiting up to 5m0s for pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" in namespace "replicaset-5056" to be "running"
    Sep  7 05:14:21.445: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8": Phase="Running", Reason="", readiness=true. Elapsed: 3.033151ms
    Sep  7 05:14:21.446: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" satisfied condition "running"
    Sep  7 05:14:21.446: INFO: Pod "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 05:14:16 +0000 UTC Reason: Message:}])
    Sep  7 05:14:21.446: INFO: Trying to dial the pod
    Sep  7 05:14:26.452: INFO: Controller my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090: Got expected result from replica 1 [my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8]: "my-hostname-basic-2dc3d607-85a7-4169-919b-c37855823090-75jk8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:26.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5056" for this suite. 09/07/23 05:14:26.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:26.458
Sep  7 05:14:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 05:14:26.458
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:26.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:26.469
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 09/07/23 05:14:26.479
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:14:26.483
Sep  7 05:14:26.485: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:14:26.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:14:26.486: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:14:27.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:14:27.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:14:27.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:14:28.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:14:28.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:14:28.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:14:29.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:14:29.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:14:29.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:14:30.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:14:30.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:14:30.491: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/07/23 05:14:30.493
STEP: DeleteCollection of the DaemonSets 09/07/23 05:14:30.494
STEP: Verify that ReplicaSets have been deleted 09/07/23 05:14:30.498
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Sep  7 05:14:30.504: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2826"},"items":null}

Sep  7 05:14:30.506: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2826"},"items":[{"metadata":{"name":"daemon-set-hmk2z","generateName":"daemon-set-","namespace":"daemonsets-6647","uid":"8403e0bf-ccde-4d26-bfa7-bfe0eeeccee1","resourceVersion":"2822","creationTimestamp":"2023-09-07T05:14:26Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b66058b6-fa30-4ff4-8672-b4e6a952b7ad","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b66058b6-fa30-4ff4-8672-b4e6a952b7ad\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zmg7s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zmg7s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"}],"hostIP":"192.168.8.6","podIP":"10.244.2.12","podIPs":[{"ip":"10.244.2.12"}],"startTime":"2023-09-07T05:14:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-07T05:14:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://644a2a9e345a915284ea20c8754b9b4478a897b915d340dca2b2635b58c19cee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q7l2v","generateName":"daemon-set-","namespace":"daemonsets-6647","uid":"915062c6-ff4a-4fae-a585-2ae10a1a27b6","resourceVersion":"2824","creationTimestamp":"2023-09-07T05:14:26Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b66058b6-fa30-4ff4-8672-b4e6a952b7ad","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b66058b6-fa30-4ff4-8672-b4e6a952b7ad\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-j4ckk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-j4ckk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"}],"hostIP":"192.168.8.3","podIP":"10.244.1.16","podIPs":[{"ip":"10.244.1.16"}],"startTime":"2023-09-07T05:14:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-07T05:14:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f729237f4191fb88114d70a86bd28a9d51f0ad183bdb7ca6f217923fa7c02cc6","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:30.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6647" for this suite. 09/07/23 05:14:30.517
------------------------------
â€¢ [4.063 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:26.458
    Sep  7 05:14:26.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 05:14:26.458
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:26.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:26.469
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 09/07/23 05:14:26.479
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:14:26.483
    Sep  7 05:14:26.485: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:14:26.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:14:26.486: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:14:27.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:14:27.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:14:27.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:14:28.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:14:28.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:14:28.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:14:29.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:14:29.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:14:29.491: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:14:30.489: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:14:30.491: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:14:30.491: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/07/23 05:14:30.493
    STEP: DeleteCollection of the DaemonSets 09/07/23 05:14:30.494
    STEP: Verify that ReplicaSets have been deleted 09/07/23 05:14:30.498
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Sep  7 05:14:30.504: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2826"},"items":null}

    Sep  7 05:14:30.506: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2826"},"items":[{"metadata":{"name":"daemon-set-hmk2z","generateName":"daemon-set-","namespace":"daemonsets-6647","uid":"8403e0bf-ccde-4d26-bfa7-bfe0eeeccee1","resourceVersion":"2822","creationTimestamp":"2023-09-07T05:14:26Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b66058b6-fa30-4ff4-8672-b4e6a952b7ad","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b66058b6-fa30-4ff4-8672-b4e6a952b7ad\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zmg7s","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zmg7s","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"}],"hostIP":"192.168.8.6","podIP":"10.244.2.12","podIPs":[{"ip":"10.244.2.12"}],"startTime":"2023-09-07T05:14:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-07T05:14:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://644a2a9e345a915284ea20c8754b9b4478a897b915d340dca2b2635b58c19cee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-q7l2v","generateName":"daemon-set-","namespace":"daemonsets-6647","uid":"915062c6-ff4a-4fae-a585-2ae10a1a27b6","resourceVersion":"2824","creationTimestamp":"2023-09-07T05:14:26Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"b66058b6-fa30-4ff4-8672-b4e6a952b7ad","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b66058b6-fa30-4ff4-8672-b4e6a952b7ad\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-07T05:14:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-j4ckk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-j4ckk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kind-worker","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kind-worker"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-07T05:14:26Z"}],"hostIP":"192.168.8.3","podIP":"10.244.1.16","podIPs":[{"ip":"10.244.1.16"}],"startTime":"2023-09-07T05:14:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-07T05:14:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f729237f4191fb88114d70a86bd28a9d51f0ad183bdb7ca6f217923fa7c02cc6","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:30.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6647" for this suite. 09/07/23 05:14:30.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:30.522
Sep  7 05:14:30.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename subpath 09/07/23 05:14:30.522
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:30.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:30.532
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/07/23 05:14:30.533
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-nnfr 09/07/23 05:14:30.538
STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:14:30.538
Sep  7 05:14:30.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nnfr" in namespace "subpath-5518" to be "Succeeded or Failed"
Sep  7 05:14:30.546: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.43145ms
Sep  7 05:14:32.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294204s
Sep  7 05:14:34.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 4.005449205s
Sep  7 05:14:36.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 6.003833002s
Sep  7 05:14:38.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 8.004501519s
Sep  7 05:14:40.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 10.003910813s
Sep  7 05:14:42.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.004182484s
Sep  7 05:14:44.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 14.005061444s
Sep  7 05:14:46.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 16.004894552s
Sep  7 05:14:48.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 18.004941817s
Sep  7 05:14:50.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 20.00523699s
Sep  7 05:14:52.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=false. Elapsed: 22.004808381s
Sep  7 05:14:54.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005036209s
STEP: Saw pod success 09/07/23 05:14:54.55
Sep  7 05:14:54.550: INFO: Pod "pod-subpath-test-secret-nnfr" satisfied condition "Succeeded or Failed"
Sep  7 05:14:54.552: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-secret-nnfr container test-container-subpath-secret-nnfr: <nil>
STEP: delete the pod 09/07/23 05:14:54.556
Sep  7 05:14:54.565: INFO: Waiting for pod pod-subpath-test-secret-nnfr to disappear
Sep  7 05:14:54.566: INFO: Pod pod-subpath-test-secret-nnfr no longer exists
STEP: Deleting pod pod-subpath-test-secret-nnfr 09/07/23 05:14:54.566
Sep  7 05:14:54.567: INFO: Deleting pod "pod-subpath-test-secret-nnfr" in namespace "subpath-5518"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:54.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5518" for this suite. 09/07/23 05:14:54.57
------------------------------
â€¢ [SLOW TEST] [24.052 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:30.522
    Sep  7 05:14:30.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename subpath 09/07/23 05:14:30.522
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:30.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:30.532
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/07/23 05:14:30.533
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-nnfr 09/07/23 05:14:30.538
    STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:14:30.538
    Sep  7 05:14:30.545: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nnfr" in namespace "subpath-5518" to be "Succeeded or Failed"
    Sep  7 05:14:30.546: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.43145ms
    Sep  7 05:14:32.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294204s
    Sep  7 05:14:34.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 4.005449205s
    Sep  7 05:14:36.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 6.003833002s
    Sep  7 05:14:38.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 8.004501519s
    Sep  7 05:14:40.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 10.003910813s
    Sep  7 05:14:42.549: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.004182484s
    Sep  7 05:14:44.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 14.005061444s
    Sep  7 05:14:46.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 16.004894552s
    Sep  7 05:14:48.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 18.004941817s
    Sep  7 05:14:50.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=true. Elapsed: 20.00523699s
    Sep  7 05:14:52.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Running", Reason="", readiness=false. Elapsed: 22.004808381s
    Sep  7 05:14:54.550: INFO: Pod "pod-subpath-test-secret-nnfr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005036209s
    STEP: Saw pod success 09/07/23 05:14:54.55
    Sep  7 05:14:54.550: INFO: Pod "pod-subpath-test-secret-nnfr" satisfied condition "Succeeded or Failed"
    Sep  7 05:14:54.552: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-secret-nnfr container test-container-subpath-secret-nnfr: <nil>
    STEP: delete the pod 09/07/23 05:14:54.556
    Sep  7 05:14:54.565: INFO: Waiting for pod pod-subpath-test-secret-nnfr to disappear
    Sep  7 05:14:54.566: INFO: Pod pod-subpath-test-secret-nnfr no longer exists
    STEP: Deleting pod pod-subpath-test-secret-nnfr 09/07/23 05:14:54.566
    Sep  7 05:14:54.567: INFO: Deleting pod "pod-subpath-test-secret-nnfr" in namespace "subpath-5518"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:54.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5518" for this suite. 09/07/23 05:14:54.57
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:54.574
Sep  7 05:14:54.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:14:54.574
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:54.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:54.583
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-7ee87c25-902c-48a4-9eae-82b4093cffba 09/07/23 05:14:54.587
STEP: Creating secret with name s-test-opt-upd-887bc459-e2b0-4800-a6e7-9f1129bcaa9f 09/07/23 05:14:54.59
STEP: Creating the pod 09/07/23 05:14:54.593
Sep  7 05:14:54.599: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477" in namespace "projected-4916" to be "running and ready"
Sep  7 05:14:54.600: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451731ms
Sep  7 05:14:54.600: INFO: The phase of Pod pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:14:56.604: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477": Phase="Running", Reason="", readiness=true. Elapsed: 2.005005348s
Sep  7 05:14:56.604: INFO: The phase of Pod pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477 is Running (Ready = true)
Sep  7 05:14:56.604: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-7ee87c25-902c-48a4-9eae-82b4093cffba 09/07/23 05:14:56.615
STEP: Updating secret s-test-opt-upd-887bc459-e2b0-4800-a6e7-9f1129bcaa9f 09/07/23 05:14:56.619
STEP: Creating secret with name s-test-opt-create-c6702b0d-153e-4374-b4a7-93873b6643a4 09/07/23 05:14:56.624
STEP: waiting to observe update in volume 09/07/23 05:14:56.627
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:14:58.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4916" for this suite. 09/07/23 05:14:58.644
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:54.574
    Sep  7 05:14:54.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:14:54.574
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:54.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:54.583
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-7ee87c25-902c-48a4-9eae-82b4093cffba 09/07/23 05:14:54.587
    STEP: Creating secret with name s-test-opt-upd-887bc459-e2b0-4800-a6e7-9f1129bcaa9f 09/07/23 05:14:54.59
    STEP: Creating the pod 09/07/23 05:14:54.593
    Sep  7 05:14:54.599: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477" in namespace "projected-4916" to be "running and ready"
    Sep  7 05:14:54.600: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477": Phase="Pending", Reason="", readiness=false. Elapsed: 1.451731ms
    Sep  7 05:14:54.600: INFO: The phase of Pod pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:14:56.604: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477": Phase="Running", Reason="", readiness=true. Elapsed: 2.005005348s
    Sep  7 05:14:56.604: INFO: The phase of Pod pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477 is Running (Ready = true)
    Sep  7 05:14:56.604: INFO: Pod "pod-projected-secrets-43a1f2a6-a3b0-4e1d-be9f-897f6b8dc477" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-7ee87c25-902c-48a4-9eae-82b4093cffba 09/07/23 05:14:56.615
    STEP: Updating secret s-test-opt-upd-887bc459-e2b0-4800-a6e7-9f1129bcaa9f 09/07/23 05:14:56.619
    STEP: Creating secret with name s-test-opt-create-c6702b0d-153e-4374-b4a7-93873b6643a4 09/07/23 05:14:56.624
    STEP: waiting to observe update in volume 09/07/23 05:14:56.627
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:14:58.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4916" for this suite. 09/07/23 05:14:58.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:14:58.647
Sep  7 05:14:58.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:14:58.648
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:58.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:58.659
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/07/23 05:14:58.661
Sep  7 05:14:58.667: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384" in namespace "emptydir-3908" to be "running"
Sep  7 05:14:58.669: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472791ms
Sep  7 05:15:00.671: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384": Phase="Running", Reason="", readiness=false. Elapsed: 2.003944093s
Sep  7 05:15:00.671: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/07/23 05:15:00.671
Sep  7 05:15:00.671: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3908 PodName:pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:15:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:15:00.672: INFO: ExecWithOptions: Clientset creation
Sep  7 05:15:00.672: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3908/pods/pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  7 05:15:00.764: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:00.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3908" for this suite. 09/07/23 05:15:00.766
------------------------------
â€¢ [2.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:14:58.647
    Sep  7 05:14:58.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:14:58.648
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:14:58.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:14:58.659
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/07/23 05:14:58.661
    Sep  7 05:14:58.667: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384" in namespace "emptydir-3908" to be "running"
    Sep  7 05:14:58.669: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472791ms
    Sep  7 05:15:00.671: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384": Phase="Running", Reason="", readiness=false. Elapsed: 2.003944093s
    Sep  7 05:15:00.671: INFO: Pod "pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/07/23 05:15:00.671
    Sep  7 05:15:00.671: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3908 PodName:pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:15:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:15:00.672: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:15:00.672: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3908/pods/pod-sharedvolume-1e2df861-7968-4e9f-a767-09f64f6af384/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  7 05:15:00.764: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:00.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3908" for this suite. 09/07/23 05:15:00.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:00.771
Sep  7 05:15:00.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:15:00.771
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:00.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:00.781
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  7 05:15:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:06.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9031" for this suite. 09/07/23 05:15:06.917
------------------------------
â€¢ [SLOW TEST] [6.150 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:00.771
    Sep  7 05:15:00.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:15:00.771
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:00.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:00.781
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  7 05:15:00.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:06.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9031" for this suite. 09/07/23 05:15:06.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:06.921
Sep  7 05:15:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 05:15:06.922
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:06.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:06.931
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2655 09/07/23 05:15:06.933
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/07/23 05:15:06.936
STEP: Creating pod with conflicting port in namespace statefulset-2655 09/07/23 05:15:06.938
STEP: Waiting until pod test-pod will start running in namespace statefulset-2655 09/07/23 05:15:06.943
Sep  7 05:15:06.943: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2655" to be "running"
Sep  7 05:15:06.944: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630881ms
Sep  7 05:15:08.947: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004671405s
Sep  7 05:15:08.947: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2655 09/07/23 05:15:08.947
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2655 09/07/23 05:15:08.951
Sep  7 05:15:08.967: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Pending. Waiting for statefulset controller to delete.
Sep  7 05:15:08.973: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Failed. Waiting for statefulset controller to delete.
Sep  7 05:15:08.979: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Failed. Waiting for statefulset controller to delete.
Sep  7 05:15:08.981: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2655
STEP: Removing pod with conflicting port in namespace statefulset-2655 09/07/23 05:15:08.981
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2655 and will be in running state 09/07/23 05:15:08.992
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 05:15:10.997: INFO: Deleting all statefulset in ns statefulset-2655
Sep  7 05:15:10.999: INFO: Scaling statefulset ss to 0
Sep  7 05:15:21.014: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 05:15:21.016: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2655" for this suite. 09/07/23 05:15:21.025
------------------------------
â€¢ [SLOW TEST] [14.107 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:06.921
    Sep  7 05:15:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 05:15:06.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:06.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:06.931
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2655 09/07/23 05:15:06.933
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/07/23 05:15:06.936
    STEP: Creating pod with conflicting port in namespace statefulset-2655 09/07/23 05:15:06.938
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2655 09/07/23 05:15:06.943
    Sep  7 05:15:06.943: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2655" to be "running"
    Sep  7 05:15:06.944: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.630881ms
    Sep  7 05:15:08.947: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004671405s
    Sep  7 05:15:08.947: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2655 09/07/23 05:15:08.947
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2655 09/07/23 05:15:08.951
    Sep  7 05:15:08.967: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  7 05:15:08.973: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  7 05:15:08.979: INFO: Observed stateful pod in namespace: statefulset-2655, name: ss-0, uid: fd835e8d-1623-4882-b17e-74f9cba62b99, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  7 05:15:08.981: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2655
    STEP: Removing pod with conflicting port in namespace statefulset-2655 09/07/23 05:15:08.981
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2655 and will be in running state 09/07/23 05:15:08.992
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 05:15:10.997: INFO: Deleting all statefulset in ns statefulset-2655
    Sep  7 05:15:10.999: INFO: Scaling statefulset ss to 0
    Sep  7 05:15:21.014: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 05:15:21.016: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:21.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2655" for this suite. 09/07/23 05:15:21.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:21.03
Sep  7 05:15:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:15:21.03
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:21.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:21.039
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/07/23 05:15:21.04
Sep  7 05:15:21.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:15:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2442" for this suite. 09/07/23 05:15:27.421
------------------------------
â€¢ [SLOW TEST] [6.396 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:21.03
    Sep  7 05:15:21.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:15:21.03
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:21.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:21.039
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/07/23 05:15:21.04
    Sep  7 05:15:21.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:15:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2442" for this suite. 09/07/23 05:15:27.421
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:27.425
Sep  7 05:15:27.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename csiinlinevolumes 09/07/23 05:15:27.426
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:27.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:27.435
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/07/23 05:15:27.437
STEP: getting 09/07/23 05:15:27.45
STEP: listing in namespace 09/07/23 05:15:27.452
STEP: patching 09/07/23 05:15:27.454
STEP: deleting 09/07/23 05:15:27.463
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:27.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2375" for this suite. 09/07/23 05:15:27.47
------------------------------
â€¢ [0.048 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:27.425
    Sep  7 05:15:27.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename csiinlinevolumes 09/07/23 05:15:27.426
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:27.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:27.435
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/07/23 05:15:27.437
    STEP: getting 09/07/23 05:15:27.45
    STEP: listing in namespace 09/07/23 05:15:27.452
    STEP: patching 09/07/23 05:15:27.454
    STEP: deleting 09/07/23 05:15:27.463
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:27.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2375" for this suite. 09/07/23 05:15:27.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:27.474
Sep  7 05:15:27.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename subpath 09/07/23 05:15:27.474
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:27.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:27.484
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/07/23 05:15:27.486
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-hl5z 09/07/23 05:15:27.491
STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:15:27.491
Sep  7 05:15:27.497: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hl5z" in namespace "subpath-574" to be "Succeeded or Failed"
Sep  7 05:15:27.498: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688431ms
Sep  7 05:15:29.501: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004610965s
Sep  7 05:15:31.501: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.004040355s
Sep  7 05:15:33.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.005492794s
Sep  7 05:15:35.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005653281s
Sep  7 05:15:37.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.005174216s
Sep  7 05:15:39.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.005394039s
Sep  7 05:15:41.503: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.00579426s
Sep  7 05:15:43.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.004842129s
Sep  7 05:15:45.503: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.006033266s
Sep  7 05:15:47.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.005406421s
Sep  7 05:15:49.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.005500734s
Sep  7 05:15:51.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005484625s
STEP: Saw pod success 09/07/23 05:15:51.502
Sep  7 05:15:51.502: INFO: Pod "pod-subpath-test-configmap-hl5z" satisfied condition "Succeeded or Failed"
Sep  7 05:15:51.504: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-hl5z container test-container-subpath-configmap-hl5z: <nil>
STEP: delete the pod 09/07/23 05:15:51.508
Sep  7 05:15:51.516: INFO: Waiting for pod pod-subpath-test-configmap-hl5z to disappear
Sep  7 05:15:51.517: INFO: Pod pod-subpath-test-configmap-hl5z no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hl5z 09/07/23 05:15:51.517
Sep  7 05:15:51.517: INFO: Deleting pod "pod-subpath-test-configmap-hl5z" in namespace "subpath-574"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-574" for this suite. 09/07/23 05:15:51.52
------------------------------
â€¢ [SLOW TEST] [24.050 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:27.474
    Sep  7 05:15:27.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename subpath 09/07/23 05:15:27.474
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:27.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:27.484
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/07/23 05:15:27.486
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-hl5z 09/07/23 05:15:27.491
    STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:15:27.491
    Sep  7 05:15:27.497: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hl5z" in namespace "subpath-574" to be "Succeeded or Failed"
    Sep  7 05:15:27.498: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688431ms
    Sep  7 05:15:29.501: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.004610965s
    Sep  7 05:15:31.501: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.004040355s
    Sep  7 05:15:33.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.005492794s
    Sep  7 05:15:35.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.005653281s
    Sep  7 05:15:37.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.005174216s
    Sep  7 05:15:39.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.005394039s
    Sep  7 05:15:41.503: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.00579426s
    Sep  7 05:15:43.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.004842129s
    Sep  7 05:15:45.503: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.006033266s
    Sep  7 05:15:47.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.005406421s
    Sep  7 05:15:49.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.005500734s
    Sep  7 05:15:51.502: INFO: Pod "pod-subpath-test-configmap-hl5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005484625s
    STEP: Saw pod success 09/07/23 05:15:51.502
    Sep  7 05:15:51.502: INFO: Pod "pod-subpath-test-configmap-hl5z" satisfied condition "Succeeded or Failed"
    Sep  7 05:15:51.504: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-hl5z container test-container-subpath-configmap-hl5z: <nil>
    STEP: delete the pod 09/07/23 05:15:51.508
    Sep  7 05:15:51.516: INFO: Waiting for pod pod-subpath-test-configmap-hl5z to disappear
    Sep  7 05:15:51.517: INFO: Pod pod-subpath-test-configmap-hl5z no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-hl5z 09/07/23 05:15:51.517
    Sep  7 05:15:51.517: INFO: Deleting pod "pod-subpath-test-configmap-hl5z" in namespace "subpath-574"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-574" for this suite. 09/07/23 05:15:51.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:51.523
Sep  7 05:15:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename containers 09/07/23 05:15:51.524
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:51.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:51.534
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/07/23 05:15:51.535
Sep  7 05:15:51.540: INFO: Waiting up to 5m0s for pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e" in namespace "containers-285" to be "Succeeded or Failed"
Sep  7 05:15:51.542: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.169951ms
Sep  7 05:15:53.545: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403977s
Sep  7 05:15:55.546: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005034338s
STEP: Saw pod success 09/07/23 05:15:55.546
Sep  7 05:15:55.546: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e" satisfied condition "Succeeded or Failed"
Sep  7 05:15:55.547: INFO: Trying to get logs from node kind-worker2 pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:15:55.556
Sep  7 05:15:55.565: INFO: Waiting for pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e to disappear
Sep  7 05:15:55.567: INFO: Pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:15:55.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-285" for this suite. 09/07/23 05:15:55.569
------------------------------
â€¢ [4.048 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:51.523
    Sep  7 05:15:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename containers 09/07/23 05:15:51.524
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:51.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:51.534
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/07/23 05:15:51.535
    Sep  7 05:15:51.540: INFO: Waiting up to 5m0s for pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e" in namespace "containers-285" to be "Succeeded or Failed"
    Sep  7 05:15:51.542: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.169951ms
    Sep  7 05:15:53.545: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403977s
    Sep  7 05:15:55.546: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005034338s
    STEP: Saw pod success 09/07/23 05:15:55.546
    Sep  7 05:15:55.546: INFO: Pod "client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e" satisfied condition "Succeeded or Failed"
    Sep  7 05:15:55.547: INFO: Trying to get logs from node kind-worker2 pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:15:55.556
    Sep  7 05:15:55.565: INFO: Waiting for pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e to disappear
    Sep  7 05:15:55.567: INFO: Pod client-containers-9adf6fc8-fdbc-429e-a31b-8d634376723e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:15:55.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-285" for this suite. 09/07/23 05:15:55.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:15:55.572
Sep  7 05:15:55.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 05:15:55.573
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:55.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:55.582
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 05:16:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9467" for this suite. 09/07/23 05:16:55.594
------------------------------
â€¢ [SLOW TEST] [60.025 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:15:55.572
    Sep  7 05:15:55.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 05:15:55.573
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:15:55.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:15:55.582
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:16:55.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9467" for this suite. 09/07/23 05:16:55.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:16:55.598
Sep  7 05:16:55.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 05:16:55.598
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:16:55.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:16:55.609
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/07/23 05:16:55.612
STEP: Verify that the required pods have come up. 09/07/23 05:16:55.614
Sep  7 05:16:55.616: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  7 05:17:00.621: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 05:17:00.621
STEP: Getting /status 09/07/23 05:17:00.621
Sep  7 05:17:00.624: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/07/23 05:17:00.624
Sep  7 05:17:00.632: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/07/23 05:17:00.632
Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: ADDED
Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.634: INFO: Found replicaset test-rs in namespace replicaset-8996 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  7 05:17:00.634: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/07/23 05:17:00.634
Sep  7 05:17:00.634: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  7 05:17:00.638: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/07/23 05:17:00.638
Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: ADDED
Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.639: INFO: Observed replicaset test-rs in namespace replicaset-8996 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
Sep  7 05:17:00.639: INFO: Found replicaset test-rs in namespace replicaset-8996 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  7 05:17:00.639: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:00.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8996" for this suite. 09/07/23 05:17:00.641
------------------------------
â€¢ [SLOW TEST] [5.047 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:16:55.598
    Sep  7 05:16:55.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 05:16:55.598
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:16:55.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:16:55.609
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/07/23 05:16:55.612
    STEP: Verify that the required pods have come up. 09/07/23 05:16:55.614
    Sep  7 05:16:55.616: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  7 05:17:00.621: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 05:17:00.621
    STEP: Getting /status 09/07/23 05:17:00.621
    Sep  7 05:17:00.624: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/07/23 05:17:00.624
    Sep  7 05:17:00.632: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/07/23 05:17:00.632
    Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: ADDED
    Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.634: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.634: INFO: Found replicaset test-rs in namespace replicaset-8996 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  7 05:17:00.634: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/07/23 05:17:00.634
    Sep  7 05:17:00.634: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  7 05:17:00.638: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/07/23 05:17:00.638
    Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: ADDED
    Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.639: INFO: Observed replicaset test-rs in namespace replicaset-8996 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  7 05:17:00.639: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  7 05:17:00.639: INFO: Found replicaset test-rs in namespace replicaset-8996 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  7 05:17:00.639: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:00.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8996" for this suite. 09/07/23 05:17:00.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:00.645
Sep  7 05:17:00.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename csiinlinevolumes 09/07/23 05:17:00.646
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:00.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:00.66
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/07/23 05:17:00.664
STEP: getting 09/07/23 05:17:00.678
STEP: listing 09/07/23 05:17:00.68
STEP: deleting 09/07/23 05:17:00.682
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:00.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1708" for this suite. 09/07/23 05:17:00.692
------------------------------
â€¢ [0.049 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:00.645
    Sep  7 05:17:00.645: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename csiinlinevolumes 09/07/23 05:17:00.646
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:00.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:00.66
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/07/23 05:17:00.664
    STEP: getting 09/07/23 05:17:00.678
    STEP: listing 09/07/23 05:17:00.68
    STEP: deleting 09/07/23 05:17:00.682
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:00.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1708" for this suite. 09/07/23 05:17:00.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:00.695
Sep  7 05:17:00.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 05:17:00.696
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:00.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:00.706
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/07/23 05:17:00.707
STEP: Verify that the required pods have come up 09/07/23 05:17:00.71
Sep  7 05:17:00.712: INFO: Pod name sample-pod: Found 0 pods out of 3
Sep  7 05:17:05.715: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/07/23 05:17:05.715
Sep  7 05:17:05.717: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/07/23 05:17:05.717
STEP: DeleteCollection of the ReplicaSets 09/07/23 05:17:05.719
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/07/23 05:17:05.722
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:05.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3187" for this suite. 09/07/23 05:17:05.726
------------------------------
â€¢ [SLOW TEST] [5.040 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:00.695
    Sep  7 05:17:00.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 05:17:00.696
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:00.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:00.706
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/07/23 05:17:00.707
    STEP: Verify that the required pods have come up 09/07/23 05:17:00.71
    Sep  7 05:17:00.712: INFO: Pod name sample-pod: Found 0 pods out of 3
    Sep  7 05:17:05.715: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/07/23 05:17:05.715
    Sep  7 05:17:05.717: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/07/23 05:17:05.717
    STEP: DeleteCollection of the ReplicaSets 09/07/23 05:17:05.719
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/07/23 05:17:05.722
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:05.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3187" for this suite. 09/07/23 05:17:05.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:05.737
Sep  7 05:17:05.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:17:05.738
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:05.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:05.746
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-ccf0abc5-ebe0-4575-9f4f-2ff885d21ee6 09/07/23 05:17:05.752
STEP: Creating configMap with name cm-test-opt-upd-577e5840-5aa2-4ffa-8077-afb5151138e6 09/07/23 05:17:05.755
STEP: Creating the pod 09/07/23 05:17:05.757
Sep  7 05:17:05.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273" in namespace "projected-6139" to be "running and ready"
Sep  7 05:17:05.764: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273": Phase="Pending", Reason="", readiness=false. Elapsed: 1.08544ms
Sep  7 05:17:05.765: INFO: The phase of Pod pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:07.768: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273": Phase="Running", Reason="", readiness=true. Elapsed: 2.004409705s
Sep  7 05:17:07.768: INFO: The phase of Pod pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273 is Running (Ready = true)
Sep  7 05:17:07.768: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ccf0abc5-ebe0-4575-9f4f-2ff885d21ee6 09/07/23 05:17:07.779
STEP: Updating configmap cm-test-opt-upd-577e5840-5aa2-4ffa-8077-afb5151138e6 09/07/23 05:17:07.782
STEP: Creating configMap with name cm-test-opt-create-77a69f0e-1ef2-4da0-9e2c-11adb022557a 09/07/23 05:17:07.787
STEP: waiting to observe update in volume 09/07/23 05:17:07.79
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6139" for this suite. 09/07/23 05:17:09.806
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:05.737
    Sep  7 05:17:05.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:17:05.738
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:05.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:05.746
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-ccf0abc5-ebe0-4575-9f4f-2ff885d21ee6 09/07/23 05:17:05.752
    STEP: Creating configMap with name cm-test-opt-upd-577e5840-5aa2-4ffa-8077-afb5151138e6 09/07/23 05:17:05.755
    STEP: Creating the pod 09/07/23 05:17:05.757
    Sep  7 05:17:05.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273" in namespace "projected-6139" to be "running and ready"
    Sep  7 05:17:05.764: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273": Phase="Pending", Reason="", readiness=false. Elapsed: 1.08544ms
    Sep  7 05:17:05.765: INFO: The phase of Pod pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:07.768: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273": Phase="Running", Reason="", readiness=true. Elapsed: 2.004409705s
    Sep  7 05:17:07.768: INFO: The phase of Pod pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273 is Running (Ready = true)
    Sep  7 05:17:07.768: INFO: Pod "pod-projected-configmaps-83a15c7e-3317-448d-b22f-a75fbb625273" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ccf0abc5-ebe0-4575-9f4f-2ff885d21ee6 09/07/23 05:17:07.779
    STEP: Updating configmap cm-test-opt-upd-577e5840-5aa2-4ffa-8077-afb5151138e6 09/07/23 05:17:07.782
    STEP: Creating configMap with name cm-test-opt-create-77a69f0e-1ef2-4da0-9e2c-11adb022557a 09/07/23 05:17:07.787
    STEP: waiting to observe update in volume 09/07/23 05:17:07.79
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:09.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6139" for this suite. 09/07/23 05:17:09.806
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:09.81
Sep  7 05:17:09.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename hostport 09/07/23 05:17:09.811
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:09.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:09.822
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/07/23 05:17:09.825
Sep  7 05:17:09.830: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5117" to be "running and ready"
Sep  7 05:17:09.832: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1799ms
Sep  7 05:17:09.832: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:11.835: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004526941s
Sep  7 05:17:11.835: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  7 05:17:11.835: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.8.6 on the node which pod1 resides and expect scheduled 09/07/23 05:17:11.835
Sep  7 05:17:11.839: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5117" to be "running and ready"
Sep  7 05:17:11.841: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63284ms
Sep  7 05:17:11.841: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:13.843: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004029439s
Sep  7 05:17:13.843: INFO: The phase of Pod pod2 is Running (Ready = false)
Sep  7 05:17:15.843: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.003956117s
Sep  7 05:17:15.843: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  7 05:17:15.843: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.8.6 but use UDP protocol on the node which pod2 resides 09/07/23 05:17:15.843
Sep  7 05:17:15.849: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5117" to be "running and ready"
Sep  7 05:17:15.850: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47556ms
Sep  7 05:17:15.850: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:17.853: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004506687s
Sep  7 05:17:17.853: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  7 05:17:17.853: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  7 05:17:17.857: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5117" to be "running and ready"
Sep  7 05:17:17.858: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.334491ms
Sep  7 05:17:17.858: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:19.861: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.003821585s
Sep  7 05:17:19.861: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  7 05:17:19.861: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/07/23 05:17:19.862
Sep  7 05:17:19.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.8.6 http://127.0.0.1:54323/hostname] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:17:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:17:19.863: INFO: ExecWithOptions: Clientset creation
Sep  7 05:17:19.863: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.8.6+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.8.6, port: 54323 09/07/23 05:17:19.976
Sep  7 05:17:19.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.8.6:54323/hostname] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:17:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:17:19.977: INFO: ExecWithOptions: Clientset creation
Sep  7 05:17:19.977: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.8.6%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.8.6, port: 54323 UDP 09/07/23 05:17:20.082
Sep  7 05:17:20.082: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.8.6 54323] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:17:20.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:17:20.083: INFO: ExecWithOptions: Clientset creation
Sep  7 05:17:20.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.8.6+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5117" for this suite. 09/07/23 05:17:25.12
------------------------------
â€¢ [SLOW TEST] [15.314 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:09.81
    Sep  7 05:17:09.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename hostport 09/07/23 05:17:09.811
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:09.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:09.822
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/07/23 05:17:09.825
    Sep  7 05:17:09.830: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5117" to be "running and ready"
    Sep  7 05:17:09.832: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1799ms
    Sep  7 05:17:09.832: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:11.835: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004526941s
    Sep  7 05:17:11.835: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  7 05:17:11.835: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.8.6 on the node which pod1 resides and expect scheduled 09/07/23 05:17:11.835
    Sep  7 05:17:11.839: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5117" to be "running and ready"
    Sep  7 05:17:11.841: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63284ms
    Sep  7 05:17:11.841: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:13.843: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004029439s
    Sep  7 05:17:13.843: INFO: The phase of Pod pod2 is Running (Ready = false)
    Sep  7 05:17:15.843: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.003956117s
    Sep  7 05:17:15.843: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  7 05:17:15.843: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.8.6 but use UDP protocol on the node which pod2 resides 09/07/23 05:17:15.843
    Sep  7 05:17:15.849: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5117" to be "running and ready"
    Sep  7 05:17:15.850: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47556ms
    Sep  7 05:17:15.850: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:17.853: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004506687s
    Sep  7 05:17:17.853: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  7 05:17:17.853: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  7 05:17:17.857: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5117" to be "running and ready"
    Sep  7 05:17:17.858: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.334491ms
    Sep  7 05:17:17.858: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:19.861: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.003821585s
    Sep  7 05:17:19.861: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  7 05:17:19.861: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/07/23 05:17:19.862
    Sep  7 05:17:19.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.8.6 http://127.0.0.1:54323/hostname] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:17:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:17:19.863: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:17:19.863: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.8.6+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.8.6, port: 54323 09/07/23 05:17:19.976
    Sep  7 05:17:19.977: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.8.6:54323/hostname] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:17:19.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:17:19.977: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:17:19.977: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.8.6%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.8.6, port: 54323 UDP 09/07/23 05:17:20.082
    Sep  7 05:17:20.082: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.8.6 54323] Namespace:hostport-5117 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:17:20.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:17:20.083: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:17:20.083: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5117/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.8.6+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:25.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5117" for this suite. 09/07/23 05:17:25.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:25.125
Sep  7 05:17:25.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:17:25.126
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:25.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:25.135
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/07/23 05:17:25.137
STEP: Creating a ResourceQuota 09/07/23 05:17:30.139
STEP: Ensuring resource quota status is calculated 09/07/23 05:17:30.145
STEP: Creating a Pod that fits quota 09/07/23 05:17:32.148
STEP: Ensuring ResourceQuota status captures the pod usage 09/07/23 05:17:32.157
STEP: Not allowing a pod to be created that exceeds remaining quota 09/07/23 05:17:34.16
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/07/23 05:17:34.162
STEP: Ensuring a pod cannot update its resource requirements 09/07/23 05:17:34.163
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/07/23 05:17:34.166
STEP: Deleting the pod 09/07/23 05:17:36.169
STEP: Ensuring resource quota status released the pod usage 09/07/23 05:17:36.177
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2676" for this suite. 09/07/23 05:17:38.182
------------------------------
â€¢ [SLOW TEST] [13.060 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:25.125
    Sep  7 05:17:25.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:17:25.126
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:25.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:25.135
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/07/23 05:17:25.137
    STEP: Creating a ResourceQuota 09/07/23 05:17:30.139
    STEP: Ensuring resource quota status is calculated 09/07/23 05:17:30.145
    STEP: Creating a Pod that fits quota 09/07/23 05:17:32.148
    STEP: Ensuring ResourceQuota status captures the pod usage 09/07/23 05:17:32.157
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/07/23 05:17:34.16
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/07/23 05:17:34.162
    STEP: Ensuring a pod cannot update its resource requirements 09/07/23 05:17:34.163
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/07/23 05:17:34.166
    STEP: Deleting the pod 09/07/23 05:17:36.169
    STEP: Ensuring resource quota status released the pod usage 09/07/23 05:17:36.177
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2676" for this suite. 09/07/23 05:17:38.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:38.186
Sep  7 05:17:38.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:17:38.186
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:38.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:38.195
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/07/23 05:17:38.196
Sep  7 05:17:38.201: INFO: Waiting up to 5m0s for pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c" in namespace "emptydir-588" to be "Succeeded or Failed"
Sep  7 05:17:38.202: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30695ms
Sep  7 05:17:40.206: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004598439s
Sep  7 05:17:42.205: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004114135s
STEP: Saw pod success 09/07/23 05:17:42.205
Sep  7 05:17:42.205: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c" satisfied condition "Succeeded or Failed"
Sep  7 05:17:42.207: INFO: Trying to get logs from node kind-worker pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c container test-container: <nil>
STEP: delete the pod 09/07/23 05:17:42.211
Sep  7 05:17:42.219: INFO: Waiting for pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c to disappear
Sep  7 05:17:42.221: INFO: Pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:42.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-588" for this suite. 09/07/23 05:17:42.223
------------------------------
â€¢ [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:38.186
    Sep  7 05:17:38.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:17:38.186
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:38.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:38.195
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/07/23 05:17:38.196
    Sep  7 05:17:38.201: INFO: Waiting up to 5m0s for pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c" in namespace "emptydir-588" to be "Succeeded or Failed"
    Sep  7 05:17:38.202: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30695ms
    Sep  7 05:17:40.206: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004598439s
    Sep  7 05:17:42.205: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004114135s
    STEP: Saw pod success 09/07/23 05:17:42.205
    Sep  7 05:17:42.205: INFO: Pod "pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c" satisfied condition "Succeeded or Failed"
    Sep  7 05:17:42.207: INFO: Trying to get logs from node kind-worker pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c container test-container: <nil>
    STEP: delete the pod 09/07/23 05:17:42.211
    Sep  7 05:17:42.219: INFO: Waiting for pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c to disappear
    Sep  7 05:17:42.221: INFO: Pod pod-5f40bbec-32a0-47b2-90d7-a8711e6a454c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:42.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-588" for this suite. 09/07/23 05:17:42.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:42.226
Sep  7 05:17:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:17:42.227
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:42.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:42.237
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  7 05:17:42.242: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090" in namespace "kubelet-test-7312" to be "running and ready"
Sep  7 05:17:42.244: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246651ms
Sep  7 05:17:42.244: INFO: The phase of Pod busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:17:44.247: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632706s
Sep  7 05:17:44.247: INFO: The phase of Pod busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090 is Running (Ready = true)
Sep  7 05:17:44.247: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:44.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7312" for this suite. 09/07/23 05:17:44.259
------------------------------
â€¢ [2.038 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:42.226
    Sep  7 05:17:42.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:17:42.227
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:42.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:42.237
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  7 05:17:42.242: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090" in namespace "kubelet-test-7312" to be "running and ready"
    Sep  7 05:17:42.244: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090": Phase="Pending", Reason="", readiness=false. Elapsed: 1.246651ms
    Sep  7 05:17:42.244: INFO: The phase of Pod busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:17:44.247: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632706s
    Sep  7 05:17:44.247: INFO: The phase of Pod busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090 is Running (Ready = true)
    Sep  7 05:17:44.247: INFO: Pod "busybox-readonly-fs706cc265-281e-43e8-a1a3-9a4bb5a07090" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:44.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7312" for this suite. 09/07/23 05:17:44.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:44.266
Sep  7 05:17:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:17:44.266
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:44.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:44.275
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/07/23 05:17:44.276
Sep  7 05:17:44.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6285 create -f -'
Sep  7 05:17:44.650: INFO: stderr: ""
Sep  7 05:17:44.650: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/07/23 05:17:44.65
Sep  7 05:17:45.653: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:17:45.653: INFO: Found 0 / 1
Sep  7 05:17:46.653: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:17:46.653: INFO: Found 1 / 1
Sep  7 05:17:46.653: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/07/23 05:17:46.653
Sep  7 05:17:46.655: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:17:46.655: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  7 05:17:46.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6285 patch pod agnhost-primary-2fcxm -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  7 05:17:46.714: INFO: stderr: ""
Sep  7 05:17:46.714: INFO: stdout: "pod/agnhost-primary-2fcxm patched\n"
STEP: checking annotations 09/07/23 05:17:46.714
Sep  7 05:17:46.716: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:17:46.716: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:46.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6285" for this suite. 09/07/23 05:17:46.718
------------------------------
â€¢ [2.456 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:44.266
    Sep  7 05:17:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:17:44.266
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:44.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:44.275
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/07/23 05:17:44.276
    Sep  7 05:17:44.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6285 create -f -'
    Sep  7 05:17:44.650: INFO: stderr: ""
    Sep  7 05:17:44.650: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/07/23 05:17:44.65
    Sep  7 05:17:45.653: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:17:45.653: INFO: Found 0 / 1
    Sep  7 05:17:46.653: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:17:46.653: INFO: Found 1 / 1
    Sep  7 05:17:46.653: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/07/23 05:17:46.653
    Sep  7 05:17:46.655: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:17:46.655: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  7 05:17:46.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6285 patch pod agnhost-primary-2fcxm -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  7 05:17:46.714: INFO: stderr: ""
    Sep  7 05:17:46.714: INFO: stdout: "pod/agnhost-primary-2fcxm patched\n"
    STEP: checking annotations 09/07/23 05:17:46.714
    Sep  7 05:17:46.716: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:17:46.716: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:46.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6285" for this suite. 09/07/23 05:17:46.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:46.722
Sep  7 05:17:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:17:46.723
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:46.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:46.735
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/07/23 05:17:46.737
Sep  7 05:17:46.741: INFO: Waiting up to 5m0s for pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566" in namespace "svcaccounts-8620" to be "Succeeded or Failed"
Sep  7 05:17:46.743: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457201ms
Sep  7 05:17:48.745: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003818822s
Sep  7 05:17:50.746: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004416712s
STEP: Saw pod success 09/07/23 05:17:50.746
Sep  7 05:17:50.746: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566" satisfied condition "Succeeded or Failed"
Sep  7 05:17:50.748: INFO: Trying to get logs from node kind-worker2 pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:17:50.752
Sep  7 05:17:50.759: INFO: Waiting for pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 to disappear
Sep  7 05:17:50.760: INFO: Pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:50.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8620" for this suite. 09/07/23 05:17:50.762
------------------------------
â€¢ [4.043 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:46.722
    Sep  7 05:17:46.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:17:46.723
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:46.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:46.735
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/07/23 05:17:46.737
    Sep  7 05:17:46.741: INFO: Waiting up to 5m0s for pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566" in namespace "svcaccounts-8620" to be "Succeeded or Failed"
    Sep  7 05:17:46.743: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457201ms
    Sep  7 05:17:48.745: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003818822s
    Sep  7 05:17:50.746: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004416712s
    STEP: Saw pod success 09/07/23 05:17:50.746
    Sep  7 05:17:50.746: INFO: Pod "test-pod-9e58a174-3030-45a6-97fe-ed07923ce566" satisfied condition "Succeeded or Failed"
    Sep  7 05:17:50.748: INFO: Trying to get logs from node kind-worker2 pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:17:50.752
    Sep  7 05:17:50.759: INFO: Waiting for pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 to disappear
    Sep  7 05:17:50.760: INFO: Pod test-pod-9e58a174-3030-45a6-97fe-ed07923ce566 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:50.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8620" for this suite. 09/07/23 05:17:50.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:50.766
Sep  7 05:17:50.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 05:17:50.767
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:50.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:50.778
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/07/23 05:17:50.78
STEP: patching the Namespace 09/07/23 05:17:50.786
STEP: get the Namespace and ensuring it has the label 09/07/23 05:17:50.789
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:50.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3531" for this suite. 09/07/23 05:17:50.794
STEP: Destroying namespace "nspatchtest-8fc19d20-aa9b-4b5e-9b0d-d15144cf9726-5052" for this suite. 09/07/23 05:17:50.798
------------------------------
â€¢ [0.034 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:50.766
    Sep  7 05:17:50.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 05:17:50.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:50.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:50.778
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/07/23 05:17:50.78
    STEP: patching the Namespace 09/07/23 05:17:50.786
    STEP: get the Namespace and ensuring it has the label 09/07/23 05:17:50.789
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:50.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3531" for this suite. 09/07/23 05:17:50.794
    STEP: Destroying namespace "nspatchtest-8fc19d20-aa9b-4b5e-9b0d-d15144cf9726-5052" for this suite. 09/07/23 05:17:50.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:50.801
Sep  7 05:17:50.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption 09/07/23 05:17:50.801
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:50.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:50.809
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/07/23 05:17:50.811
STEP: Waiting for the pdb to be processed 09/07/23 05:17:50.813
STEP: updating the pdb 09/07/23 05:17:52.818
STEP: Waiting for the pdb to be processed 09/07/23 05:17:52.823
STEP: patching the pdb 09/07/23 05:17:54.828
STEP: Waiting for the pdb to be processed 09/07/23 05:17:54.834
STEP: Waiting for the pdb to be deleted 09/07/23 05:17:56.841
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3903" for this suite. 09/07/23 05:17:56.844
------------------------------
â€¢ [SLOW TEST] [6.047 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:50.801
    Sep  7 05:17:50.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption 09/07/23 05:17:50.801
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:50.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:50.809
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/07/23 05:17:50.811
    STEP: Waiting for the pdb to be processed 09/07/23 05:17:50.813
    STEP: updating the pdb 09/07/23 05:17:52.818
    STEP: Waiting for the pdb to be processed 09/07/23 05:17:52.823
    STEP: patching the pdb 09/07/23 05:17:54.828
    STEP: Waiting for the pdb to be processed 09/07/23 05:17:54.834
    STEP: Waiting for the pdb to be deleted 09/07/23 05:17:56.841
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:56.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3903" for this suite. 09/07/23 05:17:56.844
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:56.848
Sep  7 05:17:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:17:56.849
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:56.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:56.86
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/07/23 05:17:56.862
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/07/23 05:17:56.863
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/07/23 05:17:56.863
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/07/23 05:17:56.863
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/07/23 05:17:56.864
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/07/23 05:17:56.864
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/07/23 05:17:56.864
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:17:56.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8025" for this suite. 09/07/23 05:17:56.866
------------------------------
â€¢ [0.021 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:56.848
    Sep  7 05:17:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:17:56.849
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:56.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:56.86
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/07/23 05:17:56.862
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/07/23 05:17:56.863
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/07/23 05:17:56.863
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/07/23 05:17:56.863
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/07/23 05:17:56.864
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/07/23 05:17:56.864
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/07/23 05:17:56.864
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:17:56.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8025" for this suite. 09/07/23 05:17:56.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:17:56.871
Sep  7 05:17:56.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 05:17:56.871
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:56.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:56.88
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/07/23 05:17:56.883
STEP: waiting for Deployment to be created 09/07/23 05:17:56.887
STEP: waiting for all Replicas to be Ready 09/07/23 05:17:56.888
Sep  7 05:17:56.889: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.889: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.898: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.898: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.908: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.908: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.920: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:56.920: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  7 05:17:58.149: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  7 05:17:58.149: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  7 05:17:58.155: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/07/23 05:17:58.155
W0907 05:17:58.163372      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  7 05:17:58.164: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/07/23 05:17:58.164
Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.172: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.172: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.183: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.183: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:58.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:58.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:58.200: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:58.200: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:17:59.161: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:59.161: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:17:59.175: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
STEP: listing Deployments 09/07/23 05:17:59.175
Sep  7 05:17:59.177: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/07/23 05:17:59.177
Sep  7 05:17:59.187: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/07/23 05:17:59.187
Sep  7 05:17:59.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:17:59.195: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:17:59.212: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:17:59.220: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:18:00.153: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:18:00.187: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:18:00.210: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:18:00.216: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  7 05:18:01.181: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/07/23 05:18:01.211
STEP: fetching the DeploymentStatus 09/07/23 05:18:01.215
Sep  7 05:18:01.218: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3
STEP: deleting the Deployment 09/07/23 05:18:01.219
Sep  7 05:18:01.224: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
Sep  7 05:18:01.225: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 05:18:01.226: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:01.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8543" for this suite. 09/07/23 05:18:01.231
------------------------------
â€¢ [4.368 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:17:56.871
    Sep  7 05:17:56.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 05:17:56.871
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:17:56.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:17:56.88
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/07/23 05:17:56.883
    STEP: waiting for Deployment to be created 09/07/23 05:17:56.887
    STEP: waiting for all Replicas to be Ready 09/07/23 05:17:56.888
    Sep  7 05:17:56.889: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.889: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.898: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.898: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.908: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.908: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.920: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:56.920: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  7 05:17:58.149: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  7 05:17:58.149: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  7 05:17:58.155: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/07/23 05:17:58.155
    W0907 05:17:58.163372      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  7 05:17:58.164: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/07/23 05:17:58.164
    Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.165: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 0
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.166: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.172: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.172: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.183: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.183: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:58.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:58.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:58.200: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:58.200: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:17:59.161: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:59.161: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:17:59.175: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    STEP: listing Deployments 09/07/23 05:17:59.175
    Sep  7 05:17:59.177: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/07/23 05:17:59.177
    Sep  7 05:17:59.187: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/07/23 05:17:59.187
    Sep  7 05:17:59.191: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:17:59.195: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:17:59.212: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:17:59.220: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:18:00.153: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:18:00.187: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:18:00.210: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:18:00.216: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  7 05:18:01.181: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/07/23 05:18:01.211
    STEP: fetching the DeploymentStatus 09/07/23 05:18:01.215
    Sep  7 05:18:01.218: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 1
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 2
    Sep  7 05:18:01.219: INFO: observed Deployment test-deployment in namespace deployment-8543 with ReadyReplicas 3
    STEP: deleting the Deployment 09/07/23 05:18:01.219
    Sep  7 05:18:01.224: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    Sep  7 05:18:01.225: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 05:18:01.226: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:01.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8543" for this suite. 09/07/23 05:18:01.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:01.239
Sep  7 05:18:01.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:18:01.24
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:01.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:01.252
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/07/23 05:18:01.254
STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:18:01.257
STEP: Creating a ResourceQuota with not terminating scope 09/07/23 05:18:03.26
STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:18:03.263
STEP: Creating a long running pod 09/07/23 05:18:05.266
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/07/23 05:18:05.28
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/07/23 05:18:07.283
STEP: Deleting the pod 09/07/23 05:18:09.287
STEP: Ensuring resource quota status released the pod usage 09/07/23 05:18:09.292
STEP: Creating a terminating pod 09/07/23 05:18:11.295
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/07/23 05:18:11.304
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/07/23 05:18:13.308
STEP: Deleting the pod 09/07/23 05:18:15.31
STEP: Ensuring resource quota status released the pod usage 09/07/23 05:18:15.32
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:17.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-579" for this suite. 09/07/23 05:18:17.324
------------------------------
â€¢ [SLOW TEST] [16.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:01.239
    Sep  7 05:18:01.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:18:01.24
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:01.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:01.252
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/07/23 05:18:01.254
    STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:18:01.257
    STEP: Creating a ResourceQuota with not terminating scope 09/07/23 05:18:03.26
    STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:18:03.263
    STEP: Creating a long running pod 09/07/23 05:18:05.266
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/07/23 05:18:05.28
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/07/23 05:18:07.283
    STEP: Deleting the pod 09/07/23 05:18:09.287
    STEP: Ensuring resource quota status released the pod usage 09/07/23 05:18:09.292
    STEP: Creating a terminating pod 09/07/23 05:18:11.295
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/07/23 05:18:11.304
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/07/23 05:18:13.308
    STEP: Deleting the pod 09/07/23 05:18:15.31
    STEP: Ensuring resource quota status released the pod usage 09/07/23 05:18:15.32
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:17.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-579" for this suite. 09/07/23 05:18:17.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:17.33
Sep  7 05:18:17.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:18:17.33
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:17.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:17.342
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:18:17.35
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:18:17.692
STEP: Deploying the webhook pod 09/07/23 05:18:17.698
STEP: Wait for the deployment to be ready 09/07/23 05:18:17.707
Sep  7 05:18:17.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:18:19.719
STEP: Verifying the service has paired with the endpoint 09/07/23 05:18:19.728
Sep  7 05:18:20.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  7 05:18:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7571-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 05:18:21.238
STEP: Creating a custom resource that should be mutated by the webhook 09/07/23 05:18:21.251
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:23.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9240" for this suite. 09/07/23 05:18:23.814
STEP: Destroying namespace "webhook-9240-markers" for this suite. 09/07/23 05:18:23.819
------------------------------
â€¢ [SLOW TEST] [6.493 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:17.33
    Sep  7 05:18:17.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:18:17.33
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:17.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:17.342
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:18:17.35
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:18:17.692
    STEP: Deploying the webhook pod 09/07/23 05:18:17.698
    STEP: Wait for the deployment to be ready 09/07/23 05:18:17.707
    Sep  7 05:18:17.713: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:18:19.719
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:18:19.728
    Sep  7 05:18:20.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  7 05:18:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7571-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 05:18:21.238
    STEP: Creating a custom resource that should be mutated by the webhook 09/07/23 05:18:21.251
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:23.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9240" for this suite. 09/07/23 05:18:23.814
    STEP: Destroying namespace "webhook-9240-markers" for this suite. 09/07/23 05:18:23.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:23.823
Sep  7 05:18:23.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:18:23.823
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:23.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:23.832
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/07/23 05:18:23.834
Sep  7 05:18:23.839: INFO: Waiting up to 5m0s for pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6" in namespace "downward-api-5293" to be "running and ready"
Sep  7 05:18:23.840: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52569ms
Sep  7 05:18:23.840: INFO: The phase of Pod annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:18:25.844: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005095765s
Sep  7 05:18:25.844: INFO: The phase of Pod annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6 is Running (Ready = true)
Sep  7 05:18:25.844: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6" satisfied condition "running and ready"
Sep  7 05:18:26.359: INFO: Successfully updated pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:30.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5293" for this suite. 09/07/23 05:18:30.375
------------------------------
â€¢ [SLOW TEST] [6.559 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:23.823
    Sep  7 05:18:23.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:18:23.823
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:23.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:23.832
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/07/23 05:18:23.834
    Sep  7 05:18:23.839: INFO: Waiting up to 5m0s for pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6" in namespace "downward-api-5293" to be "running and ready"
    Sep  7 05:18:23.840: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52569ms
    Sep  7 05:18:23.840: INFO: The phase of Pod annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:18:25.844: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005095765s
    Sep  7 05:18:25.844: INFO: The phase of Pod annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6 is Running (Ready = true)
    Sep  7 05:18:25.844: INFO: Pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6" satisfied condition "running and ready"
    Sep  7 05:18:26.359: INFO: Successfully updated pod "annotationupdate97cac820-7273-4ab1-997d-d0170fc4a4e6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:30.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5293" for this suite. 09/07/23 05:18:30.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:30.383
Sep  7 05:18:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 05:18:30.384
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:30.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:30.395
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Sep  7 05:18:30.407: INFO: Create a RollingUpdate DaemonSet
Sep  7 05:18:30.410: INFO: Check that daemon pods launch on every node of the cluster
Sep  7 05:18:30.412: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:18:30.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:18:30.414: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:18:31.417: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:18:31.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:18:31.419: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  7 05:18:31.419: INFO: Update the DaemonSet to trigger a rollout
Sep  7 05:18:31.424: INFO: Updating DaemonSet daemon-set
Sep  7 05:18:35.433: INFO: Roll back the DaemonSet before rollout is complete
Sep  7 05:18:35.443: INFO: Updating DaemonSet daemon-set
Sep  7 05:18:35.443: INFO: Make sure DaemonSet rollback is complete
Sep  7 05:18:35.444: INFO: Wrong image for pod: daemon-set-wmh77. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep  7 05:18:35.444: INFO: Pod daemon-set-wmh77 is not available
Sep  7 05:18:35.446: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:18:36.451: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:18:37.451: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:18:38.450: INFO: Pod daemon-set-5qgtq is not available
Sep  7 05:18:38.452: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:18:38.455
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5345, will wait for the garbage collector to delete the pods 09/07/23 05:18:38.455
Sep  7 05:18:38.511: INFO: Deleting DaemonSet.extensions daemon-set took: 3.83021ms
Sep  7 05:18:38.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.634791ms
Sep  7 05:18:41.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:18:41.315: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 05:18:41.316: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4451"},"items":null}

Sep  7 05:18:41.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4451"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:41.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5345" for this suite. 09/07/23 05:18:41.324
------------------------------
â€¢ [SLOW TEST] [10.949 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:30.383
    Sep  7 05:18:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 05:18:30.384
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:30.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:30.395
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Sep  7 05:18:30.407: INFO: Create a RollingUpdate DaemonSet
    Sep  7 05:18:30.410: INFO: Check that daemon pods launch on every node of the cluster
    Sep  7 05:18:30.412: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:18:30.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:18:30.414: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:18:31.417: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:18:31.419: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:18:31.419: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  7 05:18:31.419: INFO: Update the DaemonSet to trigger a rollout
    Sep  7 05:18:31.424: INFO: Updating DaemonSet daemon-set
    Sep  7 05:18:35.433: INFO: Roll back the DaemonSet before rollout is complete
    Sep  7 05:18:35.443: INFO: Updating DaemonSet daemon-set
    Sep  7 05:18:35.443: INFO: Make sure DaemonSet rollback is complete
    Sep  7 05:18:35.444: INFO: Wrong image for pod: daemon-set-wmh77. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep  7 05:18:35.444: INFO: Pod daemon-set-wmh77 is not available
    Sep  7 05:18:35.446: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:18:36.451: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:18:37.451: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:18:38.450: INFO: Pod daemon-set-5qgtq is not available
    Sep  7 05:18:38.452: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:18:38.455
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5345, will wait for the garbage collector to delete the pods 09/07/23 05:18:38.455
    Sep  7 05:18:38.511: INFO: Deleting DaemonSet.extensions daemon-set took: 3.83021ms
    Sep  7 05:18:38.612: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.634791ms
    Sep  7 05:18:41.315: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:18:41.315: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 05:18:41.316: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4451"},"items":null}

    Sep  7 05:18:41.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4451"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:41.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5345" for this suite. 09/07/23 05:18:41.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:41.333
Sep  7 05:18:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption 09/07/23 05:18:41.334
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:41.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:41.342
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/07/23 05:18:41.347
STEP: Waiting for all pods to be running 09/07/23 05:18:43.368
Sep  7 05:18:43.370: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:45.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5787" for this suite. 09/07/23 05:18:45.377
------------------------------
â€¢ [4.048 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:41.333
    Sep  7 05:18:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption 09/07/23 05:18:41.334
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:41.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:41.342
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/07/23 05:18:41.347
    STEP: Waiting for all pods to be running 09/07/23 05:18:43.368
    Sep  7 05:18:43.370: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:45.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5787" for this suite. 09/07/23 05:18:45.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:45.382
Sep  7 05:18:45.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename containers 09/07/23 05:18:45.382
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:45.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:45.391
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/07/23 05:18:45.392
Sep  7 05:18:45.397: INFO: Waiting up to 5m0s for pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35" in namespace "containers-9052" to be "Succeeded or Failed"
Sep  7 05:18:45.398: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38346ms
Sep  7 05:18:47.401: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00433066s
Sep  7 05:18:49.402: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005280019s
STEP: Saw pod success 09/07/23 05:18:49.402
Sep  7 05:18:49.402: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35" satisfied condition "Succeeded or Failed"
Sep  7 05:18:49.404: INFO: Trying to get logs from node kind-worker2 pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:18:49.407
Sep  7 05:18:49.414: INFO: Waiting for pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 to disappear
Sep  7 05:18:49.416: INFO: Pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9052" for this suite. 09/07/23 05:18:49.417
------------------------------
â€¢ [4.039 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:45.382
    Sep  7 05:18:45.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename containers 09/07/23 05:18:45.382
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:45.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:45.391
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/07/23 05:18:45.392
    Sep  7 05:18:45.397: INFO: Waiting up to 5m0s for pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35" in namespace "containers-9052" to be "Succeeded or Failed"
    Sep  7 05:18:45.398: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38346ms
    Sep  7 05:18:47.401: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00433066s
    Sep  7 05:18:49.402: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005280019s
    STEP: Saw pod success 09/07/23 05:18:49.402
    Sep  7 05:18:49.402: INFO: Pod "client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35" satisfied condition "Succeeded or Failed"
    Sep  7 05:18:49.404: INFO: Trying to get logs from node kind-worker2 pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:18:49.407
    Sep  7 05:18:49.414: INFO: Waiting for pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 to disappear
    Sep  7 05:18:49.416: INFO: Pod client-containers-2f34e600-b63c-4ecf-8dc2-8a8a6387cd35 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:49.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9052" for this suite. 09/07/23 05:18:49.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:49.422
Sep  7 05:18:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:18:49.422
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:49.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:49.431
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/07/23 05:18:49.432
Sep  7 05:18:49.436: INFO: Waiting up to 5m0s for pod "pod-206b6c57-02d9-42a2-9622-5621b570f146" in namespace "emptydir-6901" to be "Succeeded or Failed"
Sep  7 05:18:49.438: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34437ms
Sep  7 05:18:51.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004621137s
Sep  7 05:18:53.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004925802s
STEP: Saw pod success 09/07/23 05:18:53.441
Sep  7 05:18:53.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146" satisfied condition "Succeeded or Failed"
Sep  7 05:18:53.443: INFO: Trying to get logs from node kind-worker pod pod-206b6c57-02d9-42a2-9622-5621b570f146 container test-container: <nil>
STEP: delete the pod 09/07/23 05:18:53.447
Sep  7 05:18:53.456: INFO: Waiting for pod pod-206b6c57-02d9-42a2-9622-5621b570f146 to disappear
Sep  7 05:18:53.457: INFO: Pod pod-206b6c57-02d9-42a2-9622-5621b570f146 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:53.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6901" for this suite. 09/07/23 05:18:53.459
------------------------------
â€¢ [4.042 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:49.422
    Sep  7 05:18:49.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:18:49.422
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:49.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:49.431
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/07/23 05:18:49.432
    Sep  7 05:18:49.436: INFO: Waiting up to 5m0s for pod "pod-206b6c57-02d9-42a2-9622-5621b570f146" in namespace "emptydir-6901" to be "Succeeded or Failed"
    Sep  7 05:18:49.438: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34437ms
    Sep  7 05:18:51.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004621137s
    Sep  7 05:18:53.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004925802s
    STEP: Saw pod success 09/07/23 05:18:53.441
    Sep  7 05:18:53.441: INFO: Pod "pod-206b6c57-02d9-42a2-9622-5621b570f146" satisfied condition "Succeeded or Failed"
    Sep  7 05:18:53.443: INFO: Trying to get logs from node kind-worker pod pod-206b6c57-02d9-42a2-9622-5621b570f146 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:18:53.447
    Sep  7 05:18:53.456: INFO: Waiting for pod pod-206b6c57-02d9-42a2-9622-5621b570f146 to disappear
    Sep  7 05:18:53.457: INFO: Pod pod-206b6c57-02d9-42a2-9622-5621b570f146 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:53.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6901" for this suite. 09/07/23 05:18:53.459
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:53.463
Sep  7 05:18:53.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:18:53.464
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:53.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:53.472
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-e0a4490a-c1d2-46ab-ac78-82ec79c4b6f6 09/07/23 05:18:53.474
STEP: Creating a pod to test consume configMaps 09/07/23 05:18:53.477
Sep  7 05:18:53.482: INFO: Waiting up to 5m0s for pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0" in namespace "configmap-3618" to be "Succeeded or Failed"
Sep  7 05:18:53.483: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35292ms
Sep  7 05:18:55.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005058255s
Sep  7 05:18:57.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005159268s
STEP: Saw pod success 09/07/23 05:18:57.487
Sep  7 05:18:57.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0" satisfied condition "Succeeded or Failed"
Sep  7 05:18:57.489: INFO: Trying to get logs from node kind-worker pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:18:57.493
Sep  7 05:18:57.500: INFO: Waiting for pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 to disappear
Sep  7 05:18:57.501: INFO: Pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:18:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3618" for this suite. 09/07/23 05:18:57.503
------------------------------
â€¢ [4.043 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:53.463
    Sep  7 05:18:53.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:18:53.464
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:53.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:53.472
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-e0a4490a-c1d2-46ab-ac78-82ec79c4b6f6 09/07/23 05:18:53.474
    STEP: Creating a pod to test consume configMaps 09/07/23 05:18:53.477
    Sep  7 05:18:53.482: INFO: Waiting up to 5m0s for pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0" in namespace "configmap-3618" to be "Succeeded or Failed"
    Sep  7 05:18:53.483: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35292ms
    Sep  7 05:18:55.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005058255s
    Sep  7 05:18:57.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005159268s
    STEP: Saw pod success 09/07/23 05:18:57.487
    Sep  7 05:18:57.487: INFO: Pod "pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0" satisfied condition "Succeeded or Failed"
    Sep  7 05:18:57.489: INFO: Trying to get logs from node kind-worker pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:18:57.493
    Sep  7 05:18:57.500: INFO: Waiting for pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 to disappear
    Sep  7 05:18:57.501: INFO: Pod pod-configmaps-d232e64d-2bc4-4a9e-ad8f-f008b131c4f0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:18:57.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3618" for this suite. 09/07/23 05:18:57.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:18:57.507
Sep  7 05:18:57.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:18:57.508
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:57.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:57.518
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/07/23 05:18:57.52
Sep  7 05:18:57.524: INFO: Waiting up to 5m0s for pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b" in namespace "projected-3875" to be "running and ready"
Sep  7 05:18:57.525: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2431ms
Sep  7 05:18:57.525: INFO: The phase of Pod annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:18:59.529: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004503992s
Sep  7 05:18:59.529: INFO: The phase of Pod annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b is Running (Ready = true)
Sep  7 05:18:59.529: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b" satisfied condition "running and ready"
Sep  7 05:19:00.043: INFO: Successfully updated pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:04.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3875" for this suite. 09/07/23 05:19:04.057
------------------------------
â€¢ [SLOW TEST] [6.554 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:18:57.507
    Sep  7 05:18:57.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:18:57.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:18:57.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:18:57.518
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/07/23 05:18:57.52
    Sep  7 05:18:57.524: INFO: Waiting up to 5m0s for pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b" in namespace "projected-3875" to be "running and ready"
    Sep  7 05:18:57.525: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2431ms
    Sep  7 05:18:57.525: INFO: The phase of Pod annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:18:59.529: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004503992s
    Sep  7 05:18:59.529: INFO: The phase of Pod annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b is Running (Ready = true)
    Sep  7 05:18:59.529: INFO: Pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b" satisfied condition "running and ready"
    Sep  7 05:19:00.043: INFO: Successfully updated pod "annotationupdate2e2f20eb-5e62-4e74-9e28-94ff9c8af75b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:04.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3875" for this suite. 09/07/23 05:19:04.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:04.061
Sep  7 05:19:04.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:19:04.062
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.073
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  7 05:19:04.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:04.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7183" for this suite. 09/07/23 05:19:04.604
------------------------------
â€¢ [0.546 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:04.061
    Sep  7 05:19:04.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:19:04.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.073
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  7 05:19:04.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:04.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7183" for this suite. 09/07/23 05:19:04.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:04.608
Sep  7 05:19:04.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename events 09/07/23 05:19:04.609
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.619
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/07/23 05:19:04.62
Sep  7 05:19:04.623: INFO: created test-event-1
Sep  7 05:19:04.625: INFO: created test-event-2
Sep  7 05:19:04.629: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/07/23 05:19:04.629
STEP: delete collection of events 09/07/23 05:19:04.63
Sep  7 05:19:04.630: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/07/23 05:19:04.64
Sep  7 05:19:04.640: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:04.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4375" for this suite. 09/07/23 05:19:04.643
------------------------------
â€¢ [0.038 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:04.608
    Sep  7 05:19:04.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename events 09/07/23 05:19:04.609
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.619
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/07/23 05:19:04.62
    Sep  7 05:19:04.623: INFO: created test-event-1
    Sep  7 05:19:04.625: INFO: created test-event-2
    Sep  7 05:19:04.629: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/07/23 05:19:04.629
    STEP: delete collection of events 09/07/23 05:19:04.63
    Sep  7 05:19:04.630: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/07/23 05:19:04.64
    Sep  7 05:19:04.640: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:04.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4375" for this suite. 09/07/23 05:19:04.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:04.647
Sep  7 05:19:04.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:19:04.647
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.658
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  7 05:19:04.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 05:19:05.945
Sep  7 05:19:05.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 create -f -'
Sep  7 05:19:06.331: INFO: stderr: ""
Sep  7 05:19:06.331: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  7 05:19:06.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 delete e2e-test-crd-publish-openapi-9592-crds test-cr'
Sep  7 05:19:06.386: INFO: stderr: ""
Sep  7 05:19:06.386: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  7 05:19:06.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 apply -f -'
Sep  7 05:19:06.505: INFO: stderr: ""
Sep  7 05:19:06.505: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  7 05:19:06.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 delete e2e-test-crd-publish-openapi-9592-crds test-cr'
Sep  7 05:19:06.566: INFO: stderr: ""
Sep  7 05:19:06.566: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/07/23 05:19:06.566
Sep  7 05:19:06.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 explain e2e-test-crd-publish-openapi-9592-crds'
Sep  7 05:19:06.676: INFO: stderr: ""
Sep  7 05:19:06.676: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9592-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2938" for this suite. 09/07/23 05:19:07.974
------------------------------
â€¢ [3.331 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:04.647
    Sep  7 05:19:04.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:19:04.647
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:04.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:04.658
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  7 05:19:04.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 05:19:05.945
    Sep  7 05:19:05.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 create -f -'
    Sep  7 05:19:06.331: INFO: stderr: ""
    Sep  7 05:19:06.331: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  7 05:19:06.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 delete e2e-test-crd-publish-openapi-9592-crds test-cr'
    Sep  7 05:19:06.386: INFO: stderr: ""
    Sep  7 05:19:06.386: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  7 05:19:06.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 apply -f -'
    Sep  7 05:19:06.505: INFO: stderr: ""
    Sep  7 05:19:06.505: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  7 05:19:06.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 --namespace=crd-publish-openapi-2938 delete e2e-test-crd-publish-openapi-9592-crds test-cr'
    Sep  7 05:19:06.566: INFO: stderr: ""
    Sep  7 05:19:06.566: INFO: stdout: "e2e-test-crd-publish-openapi-9592-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/07/23 05:19:06.566
    Sep  7 05:19:06.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2938 explain e2e-test-crd-publish-openapi-9592-crds'
    Sep  7 05:19:06.676: INFO: stderr: ""
    Sep  7 05:19:06.676: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9592-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:07.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2938" for this suite. 09/07/23 05:19:07.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:07.978
Sep  7 05:19:07.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:19:07.979
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:07.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:07.988
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-9b2fb56f-c2d7-438b-89be-5826d99e9d96 09/07/23 05:19:07.99
STEP: Creating a pod to test consume secrets 09/07/23 05:19:07.996
Sep  7 05:19:08.002: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247" in namespace "projected-1903" to be "Succeeded or Failed"
Sep  7 05:19:08.003: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717991ms
Sep  7 05:19:10.007: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005052976s
Sep  7 05:19:12.006: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004696029s
STEP: Saw pod success 09/07/23 05:19:12.006
Sep  7 05:19:12.007: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247" satisfied condition "Succeeded or Failed"
Sep  7 05:19:12.008: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:19:12.012
Sep  7 05:19:12.020: INFO: Waiting for pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 to disappear
Sep  7 05:19:12.022: INFO: Pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:12.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1903" for this suite. 09/07/23 05:19:12.024
------------------------------
â€¢ [4.049 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:07.978
    Sep  7 05:19:07.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:19:07.979
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:07.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:07.988
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-9b2fb56f-c2d7-438b-89be-5826d99e9d96 09/07/23 05:19:07.99
    STEP: Creating a pod to test consume secrets 09/07/23 05:19:07.996
    Sep  7 05:19:08.002: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247" in namespace "projected-1903" to be "Succeeded or Failed"
    Sep  7 05:19:08.003: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717991ms
    Sep  7 05:19:10.007: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005052976s
    Sep  7 05:19:12.006: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004696029s
    STEP: Saw pod success 09/07/23 05:19:12.006
    Sep  7 05:19:12.007: INFO: Pod "pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247" satisfied condition "Succeeded or Failed"
    Sep  7 05:19:12.008: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:19:12.012
    Sep  7 05:19:12.020: INFO: Waiting for pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 to disappear
    Sep  7 05:19:12.022: INFO: Pod pod-projected-secrets-89d93d64-d950-4df7-ba60-99e7ffe2f247 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:12.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1903" for this suite. 09/07/23 05:19:12.024
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:12.028
Sep  7 05:19:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:19:12.028
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:12.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:12.036
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9227/secret-test-367f9e08-a41c-462f-9e78-e8241f6e6f03 09/07/23 05:19:12.038
STEP: Creating a pod to test consume secrets 09/07/23 05:19:12.041
Sep  7 05:19:12.045: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685" in namespace "secrets-9227" to be "Succeeded or Failed"
Sep  7 05:19:12.046: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445911ms
Sep  7 05:19:14.049: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004203353s
Sep  7 05:19:16.050: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004866834s
STEP: Saw pod success 09/07/23 05:19:16.05
Sep  7 05:19:16.050: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685" satisfied condition "Succeeded or Failed"
Sep  7 05:19:16.051: INFO: Trying to get logs from node kind-worker pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 container env-test: <nil>
STEP: delete the pod 09/07/23 05:19:16.056
Sep  7 05:19:16.064: INFO: Waiting for pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 to disappear
Sep  7 05:19:16.066: INFO: Pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:16.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9227" for this suite. 09/07/23 05:19:16.068
------------------------------
â€¢ [4.044 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:12.028
    Sep  7 05:19:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:19:12.028
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:12.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:12.036
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9227/secret-test-367f9e08-a41c-462f-9e78-e8241f6e6f03 09/07/23 05:19:12.038
    STEP: Creating a pod to test consume secrets 09/07/23 05:19:12.041
    Sep  7 05:19:12.045: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685" in namespace "secrets-9227" to be "Succeeded or Failed"
    Sep  7 05:19:12.046: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445911ms
    Sep  7 05:19:14.049: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004203353s
    Sep  7 05:19:16.050: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004866834s
    STEP: Saw pod success 09/07/23 05:19:16.05
    Sep  7 05:19:16.050: INFO: Pod "pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685" satisfied condition "Succeeded or Failed"
    Sep  7 05:19:16.051: INFO: Trying to get logs from node kind-worker pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 container env-test: <nil>
    STEP: delete the pod 09/07/23 05:19:16.056
    Sep  7 05:19:16.064: INFO: Waiting for pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 to disappear
    Sep  7 05:19:16.066: INFO: Pod pod-configmaps-b2bf11c4-6878-49a1-9f1e-df83b1993685 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:16.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9227" for this suite. 09/07/23 05:19:16.068
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:16.071
Sep  7 05:19:16.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:19:16.072
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:16.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:16.08
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:19:16.082
Sep  7 05:19:16.087: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2" in namespace "projected-5928" to be "Succeeded or Failed"
Sep  7 05:19:16.088: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47626ms
Sep  7 05:19:18.091: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00450136s
Sep  7 05:19:20.092: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005207258s
STEP: Saw pod success 09/07/23 05:19:20.092
Sep  7 05:19:20.092: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2" satisfied condition "Succeeded or Failed"
Sep  7 05:19:20.094: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 container client-container: <nil>
STEP: delete the pod 09/07/23 05:19:20.097
Sep  7 05:19:20.104: INFO: Waiting for pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 to disappear
Sep  7 05:19:20.106: INFO: Pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5928" for this suite. 09/07/23 05:19:20.108
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:16.071
    Sep  7 05:19:16.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:19:16.072
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:16.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:16.08
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:19:16.082
    Sep  7 05:19:16.087: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2" in namespace "projected-5928" to be "Succeeded or Failed"
    Sep  7 05:19:16.088: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47626ms
    Sep  7 05:19:18.091: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00450136s
    Sep  7 05:19:20.092: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005207258s
    STEP: Saw pod success 09/07/23 05:19:20.092
    Sep  7 05:19:20.092: INFO: Pod "downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2" satisfied condition "Succeeded or Failed"
    Sep  7 05:19:20.094: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 container client-container: <nil>
    STEP: delete the pod 09/07/23 05:19:20.097
    Sep  7 05:19:20.104: INFO: Waiting for pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 to disappear
    Sep  7 05:19:20.106: INFO: Pod downwardapi-volume-89305dcd-05fe-485a-9d3e-7b307f8147d2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5928" for this suite. 09/07/23 05:19:20.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:20.112
Sep  7 05:19:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename endpointslice 09/07/23 05:19:20.112
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:20.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:20.122
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:22.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6156" for this suite. 09/07/23 05:19:22.157
------------------------------
â€¢ [2.048 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:20.112
    Sep  7 05:19:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename endpointslice 09/07/23 05:19:20.112
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:20.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:20.122
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:22.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6156" for this suite. 09/07/23 05:19:22.157
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:22.16
Sep  7 05:19:22.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 05:19:22.161
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:22.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:22.169
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Sep  7 05:19:22.178: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:19:22.181
Sep  7 05:19:22.183: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:22.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:19:22.186: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:19:23.189: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:23.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:19:23.191: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:19:24.189: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:24.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:19:24.191: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/07/23 05:19:24.198
STEP: Check that daemon pods images are updated. 09/07/23 05:19:24.207
Sep  7 05:19:24.209: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:24.209: INFO: Wrong image for pod: daemon-set-hnsmp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:24.210: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:25.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:25.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:26.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:26.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:27.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:27.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:28.214: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:28.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:29.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  7 05:19:29.213: INFO: Pod daemon-set-n2prt is not available
Sep  7 05:19:29.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:30.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:31.215: INFO: Pod daemon-set-qrtvq is not available
Sep  7 05:19:31.217: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 09/07/23 05:19:31.217
Sep  7 05:19:31.220: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:31.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:19:31.222: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:19:32.224: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:19:32.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:19:32.226: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:19:32.234
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2667, will wait for the garbage collector to delete the pods 09/07/23 05:19:32.234
Sep  7 05:19:32.290: INFO: Deleting DaemonSet.extensions daemon-set took: 3.668391ms
Sep  7 05:19:32.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.761089ms
Sep  7 05:19:36.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:19:36.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 05:19:36.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4954"},"items":null}

Sep  7 05:19:36.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4954"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:36.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2667" for this suite. 09/07/23 05:19:36.403
------------------------------
â€¢ [SLOW TEST] [14.247 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:22.16
    Sep  7 05:19:22.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 05:19:22.161
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:22.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:22.169
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Sep  7 05:19:22.178: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:19:22.181
    Sep  7 05:19:22.183: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:22.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:19:22.186: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:19:23.189: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:23.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:19:23.191: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:19:24.189: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:24.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:19:24.191: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/07/23 05:19:24.198
    STEP: Check that daemon pods images are updated. 09/07/23 05:19:24.207
    Sep  7 05:19:24.209: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:24.209: INFO: Wrong image for pod: daemon-set-hnsmp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:24.210: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:25.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:25.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:26.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:26.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:27.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:27.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:28.214: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:28.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:29.213: INFO: Wrong image for pod: daemon-set-59dqg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  7 05:19:29.213: INFO: Pod daemon-set-n2prt is not available
    Sep  7 05:19:29.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:30.215: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:31.215: INFO: Pod daemon-set-qrtvq is not available
    Sep  7 05:19:31.217: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 09/07/23 05:19:31.217
    Sep  7 05:19:31.220: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:31.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:19:31.222: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:19:32.224: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:19:32.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:19:32.226: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:19:32.234
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2667, will wait for the garbage collector to delete the pods 09/07/23 05:19:32.234
    Sep  7 05:19:32.290: INFO: Deleting DaemonSet.extensions daemon-set took: 3.668391ms
    Sep  7 05:19:32.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.761089ms
    Sep  7 05:19:36.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:19:36.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 05:19:36.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4954"},"items":null}

    Sep  7 05:19:36.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4954"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:36.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2667" for this suite. 09/07/23 05:19:36.403
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:36.407
Sep  7 05:19:36.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename events 09/07/23 05:19:36.408
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:36.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:36.418
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/07/23 05:19:36.42
STEP: listing events in all namespaces 09/07/23 05:19:36.424
STEP: listing events in test namespace 09/07/23 05:19:36.428
STEP: listing events with field selection filtering on source 09/07/23 05:19:36.43
STEP: listing events with field selection filtering on reportingController 09/07/23 05:19:36.431
STEP: getting the test event 09/07/23 05:19:36.432
STEP: patching the test event 09/07/23 05:19:36.434
STEP: getting the test event 09/07/23 05:19:36.439
STEP: updating the test event 09/07/23 05:19:36.44
STEP: getting the test event 09/07/23 05:19:36.445
STEP: deleting the test event 09/07/23 05:19:36.446
STEP: listing events in all namespaces 09/07/23 05:19:36.449
STEP: listing events in test namespace 09/07/23 05:19:36.452
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1653" for this suite. 09/07/23 05:19:36.455
------------------------------
â€¢ [0.051 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:36.407
    Sep  7 05:19:36.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename events 09/07/23 05:19:36.408
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:36.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:36.418
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/07/23 05:19:36.42
    STEP: listing events in all namespaces 09/07/23 05:19:36.424
    STEP: listing events in test namespace 09/07/23 05:19:36.428
    STEP: listing events with field selection filtering on source 09/07/23 05:19:36.43
    STEP: listing events with field selection filtering on reportingController 09/07/23 05:19:36.431
    STEP: getting the test event 09/07/23 05:19:36.432
    STEP: patching the test event 09/07/23 05:19:36.434
    STEP: getting the test event 09/07/23 05:19:36.439
    STEP: updating the test event 09/07/23 05:19:36.44
    STEP: getting the test event 09/07/23 05:19:36.445
    STEP: deleting the test event 09/07/23 05:19:36.446
    STEP: listing events in all namespaces 09/07/23 05:19:36.449
    STEP: listing events in test namespace 09/07/23 05:19:36.452
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:36.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1653" for this suite. 09/07/23 05:19:36.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:36.459
Sep  7 05:19:36.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:19:36.46
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:36.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:36.47
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:19:36.478
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:19:36.831
STEP: Deploying the webhook pod 09/07/23 05:19:36.836
STEP: Wait for the deployment to be ready 09/07/23 05:19:36.844
Sep  7 05:19:36.847: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/07/23 05:19:38.854
STEP: Verifying the service has paired with the endpoint 09/07/23 05:19:38.861
Sep  7 05:19:39.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/07/23 05:19:39.865
STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:39.865
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/07/23 05:19:39.875
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/07/23 05:19:40.881
STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:40.881
STEP: Having no error when timeout is longer than webhook latency 09/07/23 05:19:41.9
STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:41.9
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/07/23 05:19:46.924
STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:46.924
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:19:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-758" for this suite. 09/07/23 05:19:51.962
STEP: Destroying namespace "webhook-758-markers" for this suite. 09/07/23 05:19:51.968
------------------------------
â€¢ [SLOW TEST] [15.518 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:36.459
    Sep  7 05:19:36.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:19:36.46
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:36.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:36.47
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:19:36.478
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:19:36.831
    STEP: Deploying the webhook pod 09/07/23 05:19:36.836
    STEP: Wait for the deployment to be ready 09/07/23 05:19:36.844
    Sep  7 05:19:36.847: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/07/23 05:19:38.854
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:19:38.861
    Sep  7 05:19:39.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/07/23 05:19:39.865
    STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:39.865
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/07/23 05:19:39.875
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/07/23 05:19:40.881
    STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:40.881
    STEP: Having no error when timeout is longer than webhook latency 09/07/23 05:19:41.9
    STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:41.9
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/07/23 05:19:46.924
    STEP: Registering slow webhook via the AdmissionRegistration API 09/07/23 05:19:46.924
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:19:51.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-758" for this suite. 09/07/23 05:19:51.962
    STEP: Destroying namespace "webhook-758-markers" for this suite. 09/07/23 05:19:51.968
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:19:51.977
Sep  7 05:19:51.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 05:19:51.977
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:51.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:51.987
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/07/23 05:19:51.989
STEP: waiting for pod running 09/07/23 05:19:51.994
Sep  7 05:19:51.994: INFO: Waiting up to 2m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120" to be "running"
Sep  7 05:19:51.995: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64782ms
Sep  7 05:19:53.997: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003857738s
Sep  7 05:19:53.998: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" satisfied condition "running"
STEP: creating a file in subpath 09/07/23 05:19:53.998
Sep  7 05:19:53.999: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4120 PodName:var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:19:53.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:19:54.000: INFO: ExecWithOptions: Clientset creation
Sep  7 05:19:54.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4120/pods/var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/07/23 05:19:54.111
Sep  7 05:19:54.113: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4120 PodName:var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:19:54.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:19:54.114: INFO: ExecWithOptions: Clientset creation
Sep  7 05:19:54.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4120/pods/var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/07/23 05:19:54.211
Sep  7 05:19:54.721: INFO: Successfully updated pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d"
STEP: waiting for annotated pod running 09/07/23 05:19:54.721
Sep  7 05:19:54.721: INFO: Waiting up to 2m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120" to be "running"
Sep  7 05:19:54.722: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Running", Reason="", readiness=true. Elapsed: 1.467971ms
Sep  7 05:19:54.722: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" satisfied condition "running"
STEP: deleting the pod gracefully 09/07/23 05:19:54.722
Sep  7 05:19:54.722: INFO: Deleting pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120"
Sep  7 05:19:54.727: INFO: Wait up to 5m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 05:20:30.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4120" for this suite. 09/07/23 05:20:30.734
------------------------------
â€¢ [SLOW TEST] [38.761 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:19:51.977
    Sep  7 05:19:51.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 05:19:51.977
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:19:51.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:19:51.987
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/07/23 05:19:51.989
    STEP: waiting for pod running 09/07/23 05:19:51.994
    Sep  7 05:19:51.994: INFO: Waiting up to 2m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120" to be "running"
    Sep  7 05:19:51.995: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64782ms
    Sep  7 05:19:53.997: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003857738s
    Sep  7 05:19:53.998: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" satisfied condition "running"
    STEP: creating a file in subpath 09/07/23 05:19:53.998
    Sep  7 05:19:53.999: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4120 PodName:var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:19:53.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:19:54.000: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:19:54.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4120/pods/var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/07/23 05:19:54.111
    Sep  7 05:19:54.113: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4120 PodName:var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:19:54.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:19:54.114: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:19:54.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-4120/pods/var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/07/23 05:19:54.211
    Sep  7 05:19:54.721: INFO: Successfully updated pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d"
    STEP: waiting for annotated pod running 09/07/23 05:19:54.721
    Sep  7 05:19:54.721: INFO: Waiting up to 2m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120" to be "running"
    Sep  7 05:19:54.722: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d": Phase="Running", Reason="", readiness=true. Elapsed: 1.467971ms
    Sep  7 05:19:54.722: INFO: Pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" satisfied condition "running"
    STEP: deleting the pod gracefully 09/07/23 05:19:54.722
    Sep  7 05:19:54.722: INFO: Deleting pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" in namespace "var-expansion-4120"
    Sep  7 05:19:54.727: INFO: Wait up to 5m0s for pod "var-expansion-466cb20f-1ba7-4e06-9aa8-b808a789a64d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:20:30.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4120" for this suite. 09/07/23 05:20:30.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:20:30.738
Sep  7 05:20:30.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename subpath 09/07/23 05:20:30.738
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:30.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:30.747
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/07/23 05:20:30.749
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-m286 09/07/23 05:20:30.759
STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:20:30.759
Sep  7 05:20:30.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m286" in namespace "subpath-8899" to be "Succeeded or Failed"
Sep  7 05:20:30.765: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3525ms
Sep  7 05:20:32.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 2.004218066s
Sep  7 05:20:34.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 4.00531218s
Sep  7 05:20:36.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 6.005539023s
Sep  7 05:20:38.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 8.004081954s
Sep  7 05:20:40.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 10.004073265s
Sep  7 05:20:42.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 12.004562784s
Sep  7 05:20:44.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 14.004133382s
Sep  7 05:20:46.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 16.00519031s
Sep  7 05:20:48.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 18.004729236s
Sep  7 05:20:50.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 20.004926841s
Sep  7 05:20:52.767: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=false. Elapsed: 22.003768015s
Sep  7 05:20:54.767: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003856748s
STEP: Saw pod success 09/07/23 05:20:54.767
Sep  7 05:20:54.767: INFO: Pod "pod-subpath-test-configmap-m286" satisfied condition "Succeeded or Failed"
Sep  7 05:20:54.769: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-m286 container test-container-subpath-configmap-m286: <nil>
STEP: delete the pod 09/07/23 05:20:54.777
Sep  7 05:20:54.785: INFO: Waiting for pod pod-subpath-test-configmap-m286 to disappear
Sep  7 05:20:54.786: INFO: Pod pod-subpath-test-configmap-m286 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-m286 09/07/23 05:20:54.787
Sep  7 05:20:54.787: INFO: Deleting pod "pod-subpath-test-configmap-m286" in namespace "subpath-8899"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  7 05:20:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8899" for this suite. 09/07/23 05:20:54.79
------------------------------
â€¢ [SLOW TEST] [24.055 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:20:30.738
    Sep  7 05:20:30.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename subpath 09/07/23 05:20:30.738
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:30.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:30.747
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/07/23 05:20:30.749
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-m286 09/07/23 05:20:30.759
    STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:20:30.759
    Sep  7 05:20:30.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m286" in namespace "subpath-8899" to be "Succeeded or Failed"
    Sep  7 05:20:30.765: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3525ms
    Sep  7 05:20:32.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 2.004218066s
    Sep  7 05:20:34.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 4.00531218s
    Sep  7 05:20:36.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 6.005539023s
    Sep  7 05:20:38.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 8.004081954s
    Sep  7 05:20:40.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 10.004073265s
    Sep  7 05:20:42.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 12.004562784s
    Sep  7 05:20:44.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 14.004133382s
    Sep  7 05:20:46.769: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 16.00519031s
    Sep  7 05:20:48.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 18.004729236s
    Sep  7 05:20:50.768: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=true. Elapsed: 20.004926841s
    Sep  7 05:20:52.767: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Running", Reason="", readiness=false. Elapsed: 22.003768015s
    Sep  7 05:20:54.767: INFO: Pod "pod-subpath-test-configmap-m286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003856748s
    STEP: Saw pod success 09/07/23 05:20:54.767
    Sep  7 05:20:54.767: INFO: Pod "pod-subpath-test-configmap-m286" satisfied condition "Succeeded or Failed"
    Sep  7 05:20:54.769: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-configmap-m286 container test-container-subpath-configmap-m286: <nil>
    STEP: delete the pod 09/07/23 05:20:54.777
    Sep  7 05:20:54.785: INFO: Waiting for pod pod-subpath-test-configmap-m286 to disappear
    Sep  7 05:20:54.786: INFO: Pod pod-subpath-test-configmap-m286 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-m286 09/07/23 05:20:54.787
    Sep  7 05:20:54.787: INFO: Deleting pod "pod-subpath-test-configmap-m286" in namespace "subpath-8899"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:20:54.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8899" for this suite. 09/07/23 05:20:54.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:20:54.795
Sep  7 05:20:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 05:20:54.796
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:54.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:54.804
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-gh9hq" 09/07/23 05:20:54.806
Sep  7 05:20:54.808: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
Sep  7 05:20:55.810: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
Sep  7 05:20:55.812: INFO: Found 1 replicas for "e2e-rc-gh9hq" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-gh9hq" 09/07/23 05:20:55.812
STEP: Updating a scale subresource 09/07/23 05:20:55.813
STEP: Verifying replicas where modified for replication controller "e2e-rc-gh9hq" 09/07/23 05:20:55.817
Sep  7 05:20:55.817: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
Sep  7 05:20:56.818: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
Sep  7 05:20:56.820: INFO: Found 2 replicas for "e2e-rc-gh9hq" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:20:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9985" for this suite. 09/07/23 05:20:56.822
------------------------------
â€¢ [2.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:20:54.795
    Sep  7 05:20:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 05:20:54.796
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:54.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:54.804
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-gh9hq" 09/07/23 05:20:54.806
    Sep  7 05:20:54.808: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
    Sep  7 05:20:55.810: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
    Sep  7 05:20:55.812: INFO: Found 1 replicas for "e2e-rc-gh9hq" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-gh9hq" 09/07/23 05:20:55.812
    STEP: Updating a scale subresource 09/07/23 05:20:55.813
    STEP: Verifying replicas where modified for replication controller "e2e-rc-gh9hq" 09/07/23 05:20:55.817
    Sep  7 05:20:55.817: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
    Sep  7 05:20:56.818: INFO: Get Replication Controller "e2e-rc-gh9hq" to confirm replicas
    Sep  7 05:20:56.820: INFO: Found 2 replicas for "e2e-rc-gh9hq" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:20:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9985" for this suite. 09/07/23 05:20:56.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:20:56.826
Sep  7 05:20:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 05:20:56.827
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:56.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:56.835
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/07/23 05:20:56.836
Sep  7 05:20:56.840: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  7 05:21:01.843: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 05:21:01.843
STEP: getting scale subresource 09/07/23 05:21:01.843
STEP: updating a scale subresource 09/07/23 05:21:01.844
STEP: verifying the replicaset Spec.Replicas was modified 09/07/23 05:21:01.848
STEP: Patch a scale subresource 09/07/23 05:21:01.851
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:01.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1947" for this suite. 09/07/23 05:21:01.86
------------------------------
â€¢ [SLOW TEST] [5.042 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:20:56.826
    Sep  7 05:20:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 05:20:56.827
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:20:56.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:20:56.835
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/07/23 05:20:56.836
    Sep  7 05:20:56.840: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  7 05:21:01.843: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 05:21:01.843
    STEP: getting scale subresource 09/07/23 05:21:01.843
    STEP: updating a scale subresource 09/07/23 05:21:01.844
    STEP: verifying the replicaset Spec.Replicas was modified 09/07/23 05:21:01.848
    STEP: Patch a scale subresource 09/07/23 05:21:01.851
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:01.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1947" for this suite. 09/07/23 05:21:01.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:01.868
Sep  7 05:21:01.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-webhook 09/07/23 05:21:01.869
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:01.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:01.879
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/07/23 05:21:01.881
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/07/23 05:21:02.128
STEP: Deploying the custom resource conversion webhook pod 09/07/23 05:21:02.133
STEP: Wait for the deployment to be ready 09/07/23 05:21:02.145
Sep  7 05:21:02.148: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:21:04.155
STEP: Verifying the service has paired with the endpoint 09/07/23 05:21:04.162
Sep  7 05:21:05.162: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  7 05:21:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Creating a v1 custom resource 09/07/23 05:21:07.716
STEP: v2 custom resource should be converted 09/07/23 05:21:07.721
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:08.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6765" for this suite. 09/07/23 05:21:08.255
------------------------------
â€¢ [SLOW TEST] [6.390 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:01.868
    Sep  7 05:21:01.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-webhook 09/07/23 05:21:01.869
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:01.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:01.879
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/07/23 05:21:01.881
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/07/23 05:21:02.128
    STEP: Deploying the custom resource conversion webhook pod 09/07/23 05:21:02.133
    STEP: Wait for the deployment to be ready 09/07/23 05:21:02.145
    Sep  7 05:21:02.148: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:21:04.155
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:21:04.162
    Sep  7 05:21:05.162: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  7 05:21:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Creating a v1 custom resource 09/07/23 05:21:07.716
    STEP: v2 custom resource should be converted 09/07/23 05:21:07.721
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:08.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6765" for this suite. 09/07/23 05:21:08.255
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:08.258
Sep  7 05:21:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:21:08.259
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:08.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:08.27
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/07/23 05:21:08.271
Sep  7 05:21:08.272: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  7 05:21:08.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:08.780: INFO: stderr: ""
Sep  7 05:21:08.780: INFO: stdout: "service/agnhost-replica created\n"
Sep  7 05:21:08.780: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  7 05:21:08.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:08.903: INFO: stderr: ""
Sep  7 05:21:08.903: INFO: stdout: "service/agnhost-primary created\n"
Sep  7 05:21:08.903: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  7 05:21:08.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:09.030: INFO: stderr: ""
Sep  7 05:21:09.030: INFO: stdout: "service/frontend created\n"
Sep  7 05:21:09.030: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  7 05:21:09.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:09.151: INFO: stderr: ""
Sep  7 05:21:09.151: INFO: stdout: "deployment.apps/frontend created\n"
Sep  7 05:21:09.151: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  7 05:21:09.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:09.275: INFO: stderr: ""
Sep  7 05:21:09.275: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  7 05:21:09.275: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  7 05:21:09.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
Sep  7 05:21:09.391: INFO: stderr: ""
Sep  7 05:21:09.391: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/07/23 05:21:09.391
Sep  7 05:21:09.391: INFO: Waiting for all frontend pods to be Running.
Sep  7 05:21:14.444: INFO: Waiting for frontend to serve content.
Sep  7 05:21:14.450: INFO: Trying to add a new entry to the guestbook.
Sep  7 05:21:14.457: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/07/23 05:21:14.463
Sep  7 05:21:14.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.524: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.524: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/07/23 05:21:14.524
Sep  7 05:21:14.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.580: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/07/23 05:21:14.58
Sep  7 05:21:14.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.637: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/07/23 05:21:14.637
Sep  7 05:21:14.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.689: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/07/23 05:21:14.689
Sep  7 05:21:14.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.746: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.746: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/07/23 05:21:14.746
Sep  7 05:21:14.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
Sep  7 05:21:14.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:21:14.800: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-49" for this suite. 09/07/23 05:21:14.807
------------------------------
â€¢ [SLOW TEST] [6.554 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:08.258
    Sep  7 05:21:08.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:21:08.259
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:08.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:08.27
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/07/23 05:21:08.271
    Sep  7 05:21:08.272: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  7 05:21:08.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:08.780: INFO: stderr: ""
    Sep  7 05:21:08.780: INFO: stdout: "service/agnhost-replica created\n"
    Sep  7 05:21:08.780: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  7 05:21:08.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:08.903: INFO: stderr: ""
    Sep  7 05:21:08.903: INFO: stdout: "service/agnhost-primary created\n"
    Sep  7 05:21:08.903: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  7 05:21:08.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:09.030: INFO: stderr: ""
    Sep  7 05:21:09.030: INFO: stdout: "service/frontend created\n"
    Sep  7 05:21:09.030: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  7 05:21:09.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:09.151: INFO: stderr: ""
    Sep  7 05:21:09.151: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  7 05:21:09.151: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  7 05:21:09.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:09.275: INFO: stderr: ""
    Sep  7 05:21:09.275: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  7 05:21:09.275: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  7 05:21:09.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 create -f -'
    Sep  7 05:21:09.391: INFO: stderr: ""
    Sep  7 05:21:09.391: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/07/23 05:21:09.391
    Sep  7 05:21:09.391: INFO: Waiting for all frontend pods to be Running.
    Sep  7 05:21:14.444: INFO: Waiting for frontend to serve content.
    Sep  7 05:21:14.450: INFO: Trying to add a new entry to the guestbook.
    Sep  7 05:21:14.457: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/07/23 05:21:14.463
    Sep  7 05:21:14.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.524: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.524: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/07/23 05:21:14.524
    Sep  7 05:21:14.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.580: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.580: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/07/23 05:21:14.58
    Sep  7 05:21:14.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.637: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.637: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/07/23 05:21:14.637
    Sep  7 05:21:14.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.689: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.689: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/07/23 05:21:14.689
    Sep  7 05:21:14.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.746: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.746: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/07/23 05:21:14.746
    Sep  7 05:21:14.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-49 delete --grace-period=0 --force -f -'
    Sep  7 05:21:14.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:21:14.800: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-49" for this suite. 09/07/23 05:21:14.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:14.814
Sep  7 05:21:14.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:21:14.814
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:14.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:14.823
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:21:14.832
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:21:15.051
STEP: Deploying the webhook pod 09/07/23 05:21:15.054
STEP: Wait for the deployment to be ready 09/07/23 05:21:15.062
Sep  7 05:21:15.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  7 05:21:17.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/07/23 05:21:19.077
STEP: Verifying the service has paired with the endpoint 09/07/23 05:21:19.085
Sep  7 05:21:20.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/07/23 05:21:20.087
STEP: create a configmap that should be updated by the webhook 09/07/23 05:21:20.097
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:20.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9198" for this suite. 09/07/23 05:21:20.134
STEP: Destroying namespace "webhook-9198-markers" for this suite. 09/07/23 05:21:20.137
------------------------------
â€¢ [SLOW TEST] [5.327 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:14.814
    Sep  7 05:21:14.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:21:14.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:14.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:14.823
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:21:14.832
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:21:15.051
    STEP: Deploying the webhook pod 09/07/23 05:21:15.054
    STEP: Wait for the deployment to be ready 09/07/23 05:21:15.062
    Sep  7 05:21:15.067: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  7 05:21:17.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 21, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/07/23 05:21:19.077
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:21:19.085
    Sep  7 05:21:20.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/07/23 05:21:20.087
    STEP: create a configmap that should be updated by the webhook 09/07/23 05:21:20.097
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:20.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9198" for this suite. 09/07/23 05:21:20.134
    STEP: Destroying namespace "webhook-9198-markers" for this suite. 09/07/23 05:21:20.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:20.141
Sep  7 05:21:20.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:21:20.142
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:20.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:20.152
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-7ffee4e4-b9f7-4ca5-b3a7-8a3d6b57b3e3 09/07/23 05:21:20.164
STEP: Creating a pod to test consume secrets 09/07/23 05:21:20.166
Sep  7 05:21:20.174: INFO: Waiting up to 5m0s for pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7" in namespace "secrets-8241" to be "Succeeded or Failed"
Sep  7 05:21:20.182: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.206861ms
Sep  7 05:21:22.184: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00950579s
Sep  7 05:21:24.185: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010866679s
STEP: Saw pod success 09/07/23 05:21:24.185
Sep  7 05:21:24.186: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7" satisfied condition "Succeeded or Failed"
Sep  7 05:21:24.187: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:21:24.196
Sep  7 05:21:24.204: INFO: Waiting for pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 to disappear
Sep  7 05:21:24.205: INFO: Pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:24.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8241" for this suite. 09/07/23 05:21:24.207
STEP: Destroying namespace "secret-namespace-7939" for this suite. 09/07/23 05:21:24.212
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:20.141
    Sep  7 05:21:20.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:21:20.142
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:20.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:20.152
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-7ffee4e4-b9f7-4ca5-b3a7-8a3d6b57b3e3 09/07/23 05:21:20.164
    STEP: Creating a pod to test consume secrets 09/07/23 05:21:20.166
    Sep  7 05:21:20.174: INFO: Waiting up to 5m0s for pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7" in namespace "secrets-8241" to be "Succeeded or Failed"
    Sep  7 05:21:20.182: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.206861ms
    Sep  7 05:21:22.184: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00950579s
    Sep  7 05:21:24.185: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010866679s
    STEP: Saw pod success 09/07/23 05:21:24.185
    Sep  7 05:21:24.186: INFO: Pod "pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7" satisfied condition "Succeeded or Failed"
    Sep  7 05:21:24.187: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:21:24.196
    Sep  7 05:21:24.204: INFO: Waiting for pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 to disappear
    Sep  7 05:21:24.205: INFO: Pod pod-secrets-03c6c959-d28f-4308-b04b-5e4fefb2ecf7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:24.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8241" for this suite. 09/07/23 05:21:24.207
    STEP: Destroying namespace "secret-namespace-7939" for this suite. 09/07/23 05:21:24.212
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:24.215
Sep  7 05:21:24.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename init-container 09/07/23 05:21:24.216
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:24.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:24.225
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/07/23 05:21:24.232
Sep  7 05:21:24.232: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:21:27.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9791" for this suite. 09/07/23 05:21:27.54
------------------------------
â€¢ [3.331 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:24.215
    Sep  7 05:21:24.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename init-container 09/07/23 05:21:24.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:24.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:24.225
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/07/23 05:21:24.232
    Sep  7 05:21:24.232: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:21:27.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9791" for this suite. 09/07/23 05:21:27.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:21:27.547
Sep  7 05:21:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename cronjob 09/07/23 05:21:27.547
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:27.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:27.557
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/07/23 05:21:27.558
STEP: Ensuring a job is scheduled 09/07/23 05:21:27.563
STEP: Ensuring exactly one is scheduled 09/07/23 05:22:01.567
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/07/23 05:22:01.569
STEP: Ensuring the job is replaced with a new one 09/07/23 05:22:01.57
STEP: Removing cronjob 09/07/23 05:23:01.574
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  7 05:23:01.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5627" for this suite. 09/07/23 05:23:01.58
------------------------------
â€¢ [SLOW TEST] [94.038 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:21:27.547
    Sep  7 05:21:27.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename cronjob 09/07/23 05:21:27.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:21:27.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:21:27.557
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/07/23 05:21:27.558
    STEP: Ensuring a job is scheduled 09/07/23 05:21:27.563
    STEP: Ensuring exactly one is scheduled 09/07/23 05:22:01.567
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/07/23 05:22:01.569
    STEP: Ensuring the job is replaced with a new one 09/07/23 05:22:01.57
    STEP: Removing cronjob 09/07/23 05:23:01.574
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:23:01.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5627" for this suite. 09/07/23 05:23:01.58
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:23:01.584
Sep  7 05:23:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 05:23:01.585
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:01.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:01.597
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/07/23 05:23:01.598
STEP: Ensuring active pods == parallelism 09/07/23 05:23:01.601
STEP: Orphaning one of the Job's Pods 09/07/23 05:23:03.604
Sep  7 05:23:04.115: INFO: Successfully updated pod "adopt-release-78vdl"
STEP: Checking that the Job readopts the Pod 09/07/23 05:23:04.115
Sep  7 05:23:04.115: INFO: Waiting up to 15m0s for pod "adopt-release-78vdl" in namespace "job-9557" to be "adopted"
Sep  7 05:23:04.116: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 1.5086ms
Sep  7 05:23:06.119: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.003841213s
Sep  7 05:23:06.119: INFO: Pod "adopt-release-78vdl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/07/23 05:23:06.119
Sep  7 05:23:06.627: INFO: Successfully updated pod "adopt-release-78vdl"
STEP: Checking that the Job releases the Pod 09/07/23 05:23:06.627
Sep  7 05:23:06.627: INFO: Waiting up to 15m0s for pod "adopt-release-78vdl" in namespace "job-9557" to be "released"
Sep  7 05:23:06.630: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.0949ms
Sep  7 05:23:08.634: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006151702s
Sep  7 05:23:08.634: INFO: Pod "adopt-release-78vdl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 05:23:08.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9557" for this suite. 09/07/23 05:23:08.636
------------------------------
â€¢ [SLOW TEST] [7.059 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:23:01.584
    Sep  7 05:23:01.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 05:23:01.585
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:01.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:01.597
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/07/23 05:23:01.598
    STEP: Ensuring active pods == parallelism 09/07/23 05:23:01.601
    STEP: Orphaning one of the Job's Pods 09/07/23 05:23:03.604
    Sep  7 05:23:04.115: INFO: Successfully updated pod "adopt-release-78vdl"
    STEP: Checking that the Job readopts the Pod 09/07/23 05:23:04.115
    Sep  7 05:23:04.115: INFO: Waiting up to 15m0s for pod "adopt-release-78vdl" in namespace "job-9557" to be "adopted"
    Sep  7 05:23:04.116: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 1.5086ms
    Sep  7 05:23:06.119: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.003841213s
    Sep  7 05:23:06.119: INFO: Pod "adopt-release-78vdl" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/07/23 05:23:06.119
    Sep  7 05:23:06.627: INFO: Successfully updated pod "adopt-release-78vdl"
    STEP: Checking that the Job releases the Pod 09/07/23 05:23:06.627
    Sep  7 05:23:06.627: INFO: Waiting up to 15m0s for pod "adopt-release-78vdl" in namespace "job-9557" to be "released"
    Sep  7 05:23:06.630: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.0949ms
    Sep  7 05:23:08.634: INFO: Pod "adopt-release-78vdl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006151702s
    Sep  7 05:23:08.634: INFO: Pod "adopt-release-78vdl" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:23:08.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9557" for this suite. 09/07/23 05:23:08.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:23:08.643
Sep  7 05:23:08.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename endpointslice 09/07/23 05:23:08.644
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:08.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:08.654
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/07/23 05:23:13.701
STEP: referencing matching pods with named port 09/07/23 05:23:18.706
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/07/23 05:23:23.711
STEP: recreating EndpointSlices after they've been deleted 09/07/23 05:23:28.716
Sep  7 05:23:28.729: INFO: EndpointSlice for Service endpointslice-9768/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  7 05:23:38.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9768" for this suite. 09/07/23 05:23:38.737
------------------------------
â€¢ [SLOW TEST] [30.096 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:23:08.643
    Sep  7 05:23:08.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename endpointslice 09/07/23 05:23:08.644
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:08.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:08.654
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/07/23 05:23:13.701
    STEP: referencing matching pods with named port 09/07/23 05:23:18.706
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/07/23 05:23:23.711
    STEP: recreating EndpointSlices after they've been deleted 09/07/23 05:23:28.716
    Sep  7 05:23:28.729: INFO: EndpointSlice for Service endpointslice-9768/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:23:38.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9768" for this suite. 09/07/23 05:23:38.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:23:38.74
Sep  7 05:23:38.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 05:23:38.741
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:38.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:38.751
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/07/23 05:23:38.753
Sep  7 05:23:38.759: INFO: Waiting up to 5m0s for pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451" in namespace "var-expansion-9268" to be "Succeeded or Failed"
Sep  7 05:23:38.760: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28558ms
Sep  7 05:23:40.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004668339s
Sep  7 05:23:42.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004127136s
STEP: Saw pod success 09/07/23 05:23:42.763
Sep  7 05:23:42.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451" satisfied condition "Succeeded or Failed"
Sep  7 05:23:42.765: INFO: Trying to get logs from node kind-worker pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 container dapi-container: <nil>
STEP: delete the pod 09/07/23 05:23:42.774
Sep  7 05:23:42.781: INFO: Waiting for pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 to disappear
Sep  7 05:23:42.782: INFO: Pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 05:23:42.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9268" for this suite. 09/07/23 05:23:42.784
------------------------------
â€¢ [4.047 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:23:38.74
    Sep  7 05:23:38.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 05:23:38.741
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:38.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:38.751
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/07/23 05:23:38.753
    Sep  7 05:23:38.759: INFO: Waiting up to 5m0s for pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451" in namespace "var-expansion-9268" to be "Succeeded or Failed"
    Sep  7 05:23:38.760: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28558ms
    Sep  7 05:23:40.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004668339s
    Sep  7 05:23:42.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004127136s
    STEP: Saw pod success 09/07/23 05:23:42.763
    Sep  7 05:23:42.763: INFO: Pod "var-expansion-485ad44e-b754-4de6-9446-af92a05bc451" satisfied condition "Succeeded or Failed"
    Sep  7 05:23:42.765: INFO: Trying to get logs from node kind-worker pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 05:23:42.774
    Sep  7 05:23:42.781: INFO: Waiting for pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 to disappear
    Sep  7 05:23:42.782: INFO: Pod var-expansion-485ad44e-b754-4de6-9446-af92a05bc451 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:23:42.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9268" for this suite. 09/07/23 05:23:42.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:23:42.789
Sep  7 05:23:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename cronjob 09/07/23 05:23:42.789
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:42.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:42.798
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/07/23 05:23:42.799
STEP: Ensuring more than one job is running at a time 09/07/23 05:23:42.803
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/07/23 05:25:00.807
STEP: Removing cronjob 09/07/23 05:25:00.809
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9239" for this suite. 09/07/23 05:25:00.814
------------------------------
â€¢ [SLOW TEST] [78.029 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:23:42.789
    Sep  7 05:23:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename cronjob 09/07/23 05:23:42.789
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:23:42.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:23:42.798
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/07/23 05:23:42.799
    STEP: Ensuring more than one job is running at a time 09/07/23 05:23:42.803
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/07/23 05:25:00.807
    STEP: Removing cronjob 09/07/23 05:25:00.809
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:00.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9239" for this suite. 09/07/23 05:25:00.814
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:00.818
Sep  7 05:25:00.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 05:25:00.818
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:00.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:00.828
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Sep  7 05:25:00.842: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/07/23 05:25:00.845
Sep  7 05:25:00.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:00.847: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/07/23 05:25:00.847
Sep  7 05:25:00.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:00.857: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  7 05:25:01.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:01.859: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  7 05:25:02.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:25:02.859: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/07/23 05:25:02.861
Sep  7 05:25:02.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:25:02.871: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep  7 05:25:03.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:03.874: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/07/23 05:25:03.874
Sep  7 05:25:03.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:03.881: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  7 05:25:04.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:04.883: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  7 05:25:05.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:05.883: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
Sep  7 05:25:06.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:25:06.884: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:25:06.887
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6812, will wait for the garbage collector to delete the pods 09/07/23 05:25:06.887
Sep  7 05:25:06.943: INFO: Deleting DaemonSet.extensions daemon-set took: 3.748231ms
Sep  7 05:25:07.044: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.998191ms
Sep  7 05:25:09.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:25:09.946: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 05:25:09.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6486"},"items":null}

Sep  7 05:25:09.949: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6486"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:09.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6812" for this suite. 09/07/23 05:25:09.962
------------------------------
â€¢ [SLOW TEST] [9.147 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:00.818
    Sep  7 05:25:00.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 05:25:00.818
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:00.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:00.828
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Sep  7 05:25:00.842: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/07/23 05:25:00.845
    Sep  7 05:25:00.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:00.847: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/07/23 05:25:00.847
    Sep  7 05:25:00.857: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:00.857: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  7 05:25:01.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:01.859: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  7 05:25:02.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:25:02.859: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/07/23 05:25:02.861
    Sep  7 05:25:02.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:25:02.871: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep  7 05:25:03.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:03.874: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/07/23 05:25:03.874
    Sep  7 05:25:03.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:03.881: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  7 05:25:04.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:04.883: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  7 05:25:05.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:05.883: INFO: Node kind-worker2 is running 0 daemon pod, expected 1
    Sep  7 05:25:06.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:25:06.884: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:25:06.887
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6812, will wait for the garbage collector to delete the pods 09/07/23 05:25:06.887
    Sep  7 05:25:06.943: INFO: Deleting DaemonSet.extensions daemon-set took: 3.748231ms
    Sep  7 05:25:07.044: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.998191ms
    Sep  7 05:25:09.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:25:09.946: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 05:25:09.948: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6486"},"items":null}

    Sep  7 05:25:09.949: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6486"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:09.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6812" for this suite. 09/07/23 05:25:09.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:09.966
Sep  7 05:25:09.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 05:25:09.966
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:09.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:09.977
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/07/23 05:25:09.981
Sep  7 05:25:09.986: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9909" to be "running and ready"
Sep  7 05:25:09.988: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62443ms
Sep  7 05:25:09.988: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:25:11.991: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004447217s
Sep  7 05:25:11.991: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  7 05:25:11.991: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/07/23 05:25:11.993
Sep  7 05:25:11.997: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9909" to be "running and ready"
Sep  7 05:25:11.998: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433541ms
Sep  7 05:25:11.998: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:25:14.002: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005017927s
Sep  7 05:25:14.002: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  7 05:25:14.002: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/07/23 05:25:14.004
Sep  7 05:25:14.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  7 05:25:14.009: INFO: Pod pod-with-prestop-http-hook still exists
Sep  7 05:25:16.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  7 05:25:16.012: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/07/23 05:25:16.012
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:16.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9909" for this suite. 09/07/23 05:25:16.021
------------------------------
â€¢ [SLOW TEST] [6.059 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:09.966
    Sep  7 05:25:09.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 05:25:09.966
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:09.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:09.977
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/07/23 05:25:09.981
    Sep  7 05:25:09.986: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9909" to be "running and ready"
    Sep  7 05:25:09.988: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62443ms
    Sep  7 05:25:09.988: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:25:11.991: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004447217s
    Sep  7 05:25:11.991: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  7 05:25:11.991: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/07/23 05:25:11.993
    Sep  7 05:25:11.997: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9909" to be "running and ready"
    Sep  7 05:25:11.998: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433541ms
    Sep  7 05:25:11.998: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:25:14.002: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005017927s
    Sep  7 05:25:14.002: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  7 05:25:14.002: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/07/23 05:25:14.004
    Sep  7 05:25:14.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  7 05:25:14.009: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  7 05:25:16.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  7 05:25:16.012: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/07/23 05:25:16.012
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:16.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9909" for this suite. 09/07/23 05:25:16.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:16.025
Sep  7 05:25:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 05:25:16.026
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:16.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:16.036
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  7 05:25:16.042: INFO: Waiting up to 2m0s for pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" in namespace "var-expansion-5347" to be "container 0 failed with reason CreateContainerConfigError"
Sep  7 05:25:16.043: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628": Phase="Pending", Reason="", readiness=false. Elapsed: 1.24024ms
Sep  7 05:25:18.046: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003951425s
Sep  7 05:25:18.046: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  7 05:25:18.046: INFO: Deleting pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" in namespace "var-expansion-5347"
Sep  7 05:25:18.051: INFO: Wait up to 5m0s for pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:20.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5347" for this suite. 09/07/23 05:25:20.058
------------------------------
â€¢ [4.036 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:16.025
    Sep  7 05:25:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 05:25:16.026
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:16.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:16.036
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  7 05:25:16.042: INFO: Waiting up to 2m0s for pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" in namespace "var-expansion-5347" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  7 05:25:16.043: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628": Phase="Pending", Reason="", readiness=false. Elapsed: 1.24024ms
    Sep  7 05:25:18.046: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003951425s
    Sep  7 05:25:18.046: INFO: Pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  7 05:25:18.046: INFO: Deleting pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" in namespace "var-expansion-5347"
    Sep  7 05:25:18.051: INFO: Wait up to 5m0s for pod "var-expansion-a29ee888-c274-46b6-a6e6-689fc67c7628" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:20.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5347" for this suite. 09/07/23 05:25:20.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:20.062
Sep  7 05:25:20.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:25:20.062
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:20.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:20.072
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2143" for this suite. 09/07/23 05:25:20.093
------------------------------
â€¢ [0.036 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:20.062
    Sep  7 05:25:20.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:25:20.062
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:20.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:20.072
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2143" for this suite. 09/07/23 05:25:20.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:20.098
Sep  7 05:25:20.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:25:20.098
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:20.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:20.105
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7111 09/07/23 05:25:20.107
STEP: creating service affinity-clusterip in namespace services-7111 09/07/23 05:25:20.107
STEP: creating replication controller affinity-clusterip in namespace services-7111 09/07/23 05:25:20.113
I0907 05:25:20.118523      29 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7111, replica count: 3
I0907 05:25:23.169594      29 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 05:25:23.172: INFO: Creating new exec pod
Sep  7 05:25:23.178: INFO: Waiting up to 5m0s for pod "execpod-affinityq6754" in namespace "services-7111" to be "running"
Sep  7 05:25:23.179: INFO: Pod "execpod-affinityq6754": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37791ms
Sep  7 05:25:25.182: INFO: Pod "execpod-affinityq6754": Phase="Running", Reason="", readiness=true. Elapsed: 2.004002773s
Sep  7 05:25:25.182: INFO: Pod "execpod-affinityq6754" satisfied condition "running"
Sep  7 05:25:26.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  7 05:25:26.359: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  7 05:25:26.359: INFO: stdout: ""
Sep  7 05:25:26.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c nc -v -z -w 2 10.96.38.234 80'
Sep  7 05:25:26.509: INFO: stderr: "+ nc -v -z -w 2 10.96.38.234 80\nConnection to 10.96.38.234 80 port [tcp/http] succeeded!\n"
Sep  7 05:25:26.509: INFO: stdout: ""
Sep  7 05:25:26.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.38.234:80/ ; done'
Sep  7 05:25:26.717: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n"
Sep  7 05:25:26.717: INFO: stdout: "\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx"
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
Sep  7 05:25:26.717: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7111, will wait for the garbage collector to delete the pods 09/07/23 05:25:26.724
Sep  7 05:25:26.781: INFO: Deleting ReplicationController affinity-clusterip took: 3.72853ms
Sep  7 05:25:26.881: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.094411ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:28.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7111" for this suite. 09/07/23 05:25:28.993
------------------------------
â€¢ [SLOW TEST] [8.899 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:20.098
    Sep  7 05:25:20.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:25:20.098
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:20.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:20.105
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7111 09/07/23 05:25:20.107
    STEP: creating service affinity-clusterip in namespace services-7111 09/07/23 05:25:20.107
    STEP: creating replication controller affinity-clusterip in namespace services-7111 09/07/23 05:25:20.113
    I0907 05:25:20.118523      29 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7111, replica count: 3
    I0907 05:25:23.169594      29 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 05:25:23.172: INFO: Creating new exec pod
    Sep  7 05:25:23.178: INFO: Waiting up to 5m0s for pod "execpod-affinityq6754" in namespace "services-7111" to be "running"
    Sep  7 05:25:23.179: INFO: Pod "execpod-affinityq6754": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37791ms
    Sep  7 05:25:25.182: INFO: Pod "execpod-affinityq6754": Phase="Running", Reason="", readiness=true. Elapsed: 2.004002773s
    Sep  7 05:25:25.182: INFO: Pod "execpod-affinityq6754" satisfied condition "running"
    Sep  7 05:25:26.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  7 05:25:26.359: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  7 05:25:26.359: INFO: stdout: ""
    Sep  7 05:25:26.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c nc -v -z -w 2 10.96.38.234 80'
    Sep  7 05:25:26.509: INFO: stderr: "+ nc -v -z -w 2 10.96.38.234 80\nConnection to 10.96.38.234 80 port [tcp/http] succeeded!\n"
    Sep  7 05:25:26.509: INFO: stdout: ""
    Sep  7 05:25:26.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7111 exec execpod-affinityq6754 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.38.234:80/ ; done'
    Sep  7 05:25:26.717: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.38.234:80/\n"
    Sep  7 05:25:26.717: INFO: stdout: "\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx\naffinity-clusterip-hsghx"
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Received response from host: affinity-clusterip-hsghx
    Sep  7 05:25:26.717: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7111, will wait for the garbage collector to delete the pods 09/07/23 05:25:26.724
    Sep  7 05:25:26.781: INFO: Deleting ReplicationController affinity-clusterip took: 3.72853ms
    Sep  7 05:25:26.881: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.094411ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:28.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7111" for this suite. 09/07/23 05:25:28.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:28.997
Sep  7 05:25:28.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:25:28.998
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:29.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:29.006
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/07/23 05:25:29.008
Sep  7 05:25:29.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  7 05:25:29.064: INFO: stderr: ""
Sep  7 05:25:29.064: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/07/23 05:25:29.064
Sep  7 05:25:29.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  7 05:25:29.065: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2170" to be "running and ready, or succeeded"
Sep  7 05:25:29.068: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44082ms
Sep  7 05:25:29.068: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kind-worker2' to be 'Running' but was 'Pending'
Sep  7 05:25:31.071: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005987041s
Sep  7 05:25:31.071: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  7 05:25:31.071: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/07/23 05:25:31.071
Sep  7 05:25:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator'
Sep  7 05:25:31.134: INFO: stderr: ""
Sep  7 05:25:31.134: INFO: stdout: "I0907 05:25:29.746052       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/kcm 213\nI0907 05:25:29.946463       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/x78n 476\nI0907 05:25:30.146838       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nnv 342\nI0907 05:25:30.346151       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/gqv 403\nI0907 05:25:30.546547       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mdl 517\nI0907 05:25:30.746945       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/p5mg 425\nI0907 05:25:30.946255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8s4 497\n"
STEP: limiting log lines 09/07/23 05:25:31.134
Sep  7 05:25:31.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --tail=1'
Sep  7 05:25:31.194: INFO: stderr: ""
Sep  7 05:25:31.194: INFO: stdout: "I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
Sep  7 05:25:31.194: INFO: got output "I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
STEP: limiting log bytes 09/07/23 05:25:31.194
Sep  7 05:25:31.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --limit-bytes=1'
Sep  7 05:25:31.249: INFO: stderr: ""
Sep  7 05:25:31.249: INFO: stdout: "I"
Sep  7 05:25:31.249: INFO: got output "I"
STEP: exposing timestamps 09/07/23 05:25:31.249
Sep  7 05:25:31.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  7 05:25:31.301: INFO: stderr: ""
Sep  7 05:25:31.301: INFO: stdout: "2023-09-07T05:25:31.146771610Z I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
Sep  7 05:25:31.301: INFO: got output "2023-09-07T05:25:31.146771610Z I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
STEP: restricting to a time range 09/07/23 05:25:31.301
Sep  7 05:25:33.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --since=1s'
Sep  7 05:25:33.857: INFO: stderr: ""
Sep  7 05:25:33.858: INFO: stdout: "I0907 05:25:32.947023       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/c2nl 514\nI0907 05:25:33.146323       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/858c 324\nI0907 05:25:33.346691       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/wjb7 530\nI0907 05:25:33.547118       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/sdkm 528\nI0907 05:25:33.746482       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/p74 451\n"
Sep  7 05:25:33.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --since=24h'
Sep  7 05:25:33.916: INFO: stderr: ""
Sep  7 05:25:33.916: INFO: stdout: "I0907 05:25:29.746052       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/kcm 213\nI0907 05:25:29.946463       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/x78n 476\nI0907 05:25:30.146838       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nnv 342\nI0907 05:25:30.346151       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/gqv 403\nI0907 05:25:30.546547       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mdl 517\nI0907 05:25:30.746945       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/p5mg 425\nI0907 05:25:30.946255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8s4 497\nI0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\nI0907 05:25:31.347018       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/p6mq 488\nI0907 05:25:31.546327       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v6j 485\nI0907 05:25:31.746717       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/lpz 344\nI0907 05:25:31.947095       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/rs9p 328\nI0907 05:25:32.146466       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/hqdf 371\nI0907 05:25:32.346846       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/k5k 506\nI0907 05:25:32.546168       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/759 565\nI0907 05:25:32.746538       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/m9r 316\nI0907 05:25:32.947023       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/c2nl 514\nI0907 05:25:33.146323       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/858c 324\nI0907 05:25:33.346691       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/wjb7 530\nI0907 05:25:33.547118       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/sdkm 528\nI0907 05:25:33.746482       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/p74 451\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  7 05:25:33.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 delete pod logs-generator'
Sep  7 05:25:35.965: INFO: stderr: ""
Sep  7 05:25:35.965: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:35.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2170" for this suite. 09/07/23 05:25:35.967
------------------------------
â€¢ [SLOW TEST] [6.974 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:28.997
    Sep  7 05:25:28.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:25:28.998
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:29.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:29.006
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/07/23 05:25:29.008
    Sep  7 05:25:29.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  7 05:25:29.064: INFO: stderr: ""
    Sep  7 05:25:29.064: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/07/23 05:25:29.064
    Sep  7 05:25:29.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  7 05:25:29.065: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2170" to be "running and ready, or succeeded"
    Sep  7 05:25:29.068: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.44082ms
    Sep  7 05:25:29.068: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kind-worker2' to be 'Running' but was 'Pending'
    Sep  7 05:25:31.071: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005987041s
    Sep  7 05:25:31.071: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  7 05:25:31.071: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/07/23 05:25:31.071
    Sep  7 05:25:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator'
    Sep  7 05:25:31.134: INFO: stderr: ""
    Sep  7 05:25:31.134: INFO: stdout: "I0907 05:25:29.746052       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/kcm 213\nI0907 05:25:29.946463       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/x78n 476\nI0907 05:25:30.146838       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nnv 342\nI0907 05:25:30.346151       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/gqv 403\nI0907 05:25:30.546547       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mdl 517\nI0907 05:25:30.746945       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/p5mg 425\nI0907 05:25:30.946255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8s4 497\n"
    STEP: limiting log lines 09/07/23 05:25:31.134
    Sep  7 05:25:31.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --tail=1'
    Sep  7 05:25:31.194: INFO: stderr: ""
    Sep  7 05:25:31.194: INFO: stdout: "I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
    Sep  7 05:25:31.194: INFO: got output "I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
    STEP: limiting log bytes 09/07/23 05:25:31.194
    Sep  7 05:25:31.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --limit-bytes=1'
    Sep  7 05:25:31.249: INFO: stderr: ""
    Sep  7 05:25:31.249: INFO: stdout: "I"
    Sep  7 05:25:31.249: INFO: got output "I"
    STEP: exposing timestamps 09/07/23 05:25:31.249
    Sep  7 05:25:31.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  7 05:25:31.301: INFO: stderr: ""
    Sep  7 05:25:31.301: INFO: stdout: "2023-09-07T05:25:31.146771610Z I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
    Sep  7 05:25:31.301: INFO: got output "2023-09-07T05:25:31.146771610Z I0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\n"
    STEP: restricting to a time range 09/07/23 05:25:31.301
    Sep  7 05:25:33.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --since=1s'
    Sep  7 05:25:33.857: INFO: stderr: ""
    Sep  7 05:25:33.858: INFO: stdout: "I0907 05:25:32.947023       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/c2nl 514\nI0907 05:25:33.146323       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/858c 324\nI0907 05:25:33.346691       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/wjb7 530\nI0907 05:25:33.547118       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/sdkm 528\nI0907 05:25:33.746482       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/p74 451\n"
    Sep  7 05:25:33.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 logs logs-generator logs-generator --since=24h'
    Sep  7 05:25:33.916: INFO: stderr: ""
    Sep  7 05:25:33.916: INFO: stdout: "I0907 05:25:29.746052       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/kcm 213\nI0907 05:25:29.946463       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/x78n 476\nI0907 05:25:30.146838       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nnv 342\nI0907 05:25:30.346151       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/gqv 403\nI0907 05:25:30.546547       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/mdl 517\nI0907 05:25:30.746945       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/p5mg 425\nI0907 05:25:30.946255       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/8s4 497\nI0907 05:25:31.146645       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/n7b 521\nI0907 05:25:31.347018       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/p6mq 488\nI0907 05:25:31.546327       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/v6j 485\nI0907 05:25:31.746717       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/lpz 344\nI0907 05:25:31.947095       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/rs9p 328\nI0907 05:25:32.146466       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/hqdf 371\nI0907 05:25:32.346846       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/k5k 506\nI0907 05:25:32.546168       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/759 565\nI0907 05:25:32.746538       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/m9r 316\nI0907 05:25:32.947023       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/c2nl 514\nI0907 05:25:33.146323       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/858c 324\nI0907 05:25:33.346691       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/wjb7 530\nI0907 05:25:33.547118       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/sdkm 528\nI0907 05:25:33.746482       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/p74 451\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  7 05:25:33.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-2170 delete pod logs-generator'
    Sep  7 05:25:35.965: INFO: stderr: ""
    Sep  7 05:25:35.965: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:35.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2170" for this suite. 09/07/23 05:25:35.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:35.972
Sep  7 05:25:35.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:25:35.972
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:35.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:35.983
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-b471de27-b760-4d07-af74-3986fd6456c2 09/07/23 05:25:35.985
STEP: Creating a pod to test consume secrets 09/07/23 05:25:35.987
Sep  7 05:25:35.993: INFO: Waiting up to 5m0s for pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3" in namespace "secrets-2707" to be "Succeeded or Failed"
Sep  7 05:25:35.995: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.353811ms
Sep  7 05:25:37.997: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003925469s
Sep  7 05:25:39.998: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004371598s
STEP: Saw pod success 09/07/23 05:25:39.998
Sep  7 05:25:39.998: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3" satisfied condition "Succeeded or Failed"
Sep  7 05:25:39.999: INFO: Trying to get logs from node kind-worker pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:25:40.003
Sep  7 05:25:40.010: INFO: Waiting for pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 to disappear
Sep  7 05:25:40.012: INFO: Pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:40.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2707" for this suite. 09/07/23 05:25:40.014
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:35.972
    Sep  7 05:25:35.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:25:35.972
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:35.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:35.983
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-b471de27-b760-4d07-af74-3986fd6456c2 09/07/23 05:25:35.985
    STEP: Creating a pod to test consume secrets 09/07/23 05:25:35.987
    Sep  7 05:25:35.993: INFO: Waiting up to 5m0s for pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3" in namespace "secrets-2707" to be "Succeeded or Failed"
    Sep  7 05:25:35.995: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.353811ms
    Sep  7 05:25:37.997: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003925469s
    Sep  7 05:25:39.998: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004371598s
    STEP: Saw pod success 09/07/23 05:25:39.998
    Sep  7 05:25:39.998: INFO: Pod "pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3" satisfied condition "Succeeded or Failed"
    Sep  7 05:25:39.999: INFO: Trying to get logs from node kind-worker pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:25:40.003
    Sep  7 05:25:40.010: INFO: Waiting for pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 to disappear
    Sep  7 05:25:40.012: INFO: Pod pod-secrets-5f435159-9767-446d-a3c6-9d73a029a7f3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:40.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2707" for this suite. 09/07/23 05:25:40.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:40.018
Sep  7 05:25:40.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:25:40.018
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:40.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:40.028
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-296" for this suite. 09/07/23 05:25:40.049
------------------------------
â€¢ [0.036 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:40.018
    Sep  7 05:25:40.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:25:40.018
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:40.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:40.028
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-296" for this suite. 09/07/23 05:25:40.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:40.054
Sep  7 05:25:40.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 05:25:40.055
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:40.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:40.062
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  7 05:25:40.070: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  7 05:25:45.072: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 05:25:45.072
STEP: Scaling up "test-rs" replicaset  09/07/23 05:25:45.073
Sep  7 05:25:45.077: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/07/23 05:25:45.077
W0907 05:25:45.084466      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  7 05:25:45.085: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
Sep  7 05:25:45.091: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
Sep  7 05:25:45.100: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
Sep  7 05:25:45.105: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
Sep  7 05:25:45.948: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 2, AvailableReplicas 2
Sep  7 05:25:45.974: INFO: observed Replicaset test-rs in namespace replicaset-3582 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:45.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3582" for this suite. 09/07/23 05:25:45.976
------------------------------
â€¢ [SLOW TEST] [5.926 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:40.054
    Sep  7 05:25:40.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 05:25:40.055
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:40.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:40.062
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  7 05:25:40.070: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  7 05:25:45.072: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 05:25:45.072
    STEP: Scaling up "test-rs" replicaset  09/07/23 05:25:45.073
    Sep  7 05:25:45.077: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/07/23 05:25:45.077
    W0907 05:25:45.084466      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  7 05:25:45.085: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
    Sep  7 05:25:45.091: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
    Sep  7 05:25:45.100: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
    Sep  7 05:25:45.105: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 1, AvailableReplicas 1
    Sep  7 05:25:45.948: INFO: observed ReplicaSet test-rs in namespace replicaset-3582 with ReadyReplicas 2, AvailableReplicas 2
    Sep  7 05:25:45.974: INFO: observed Replicaset test-rs in namespace replicaset-3582 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:45.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3582" for this suite. 09/07/23 05:25:45.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:45.981
Sep  7 05:25:45.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:25:45.981
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:45.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:45.989
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/07/23 05:25:45.991
Sep  7 05:25:45.995: INFO: Waiting up to 5m0s for pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5" in namespace "emptydir-2590" to be "Succeeded or Failed"
Sep  7 05:25:45.997: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55955ms
Sep  7 05:25:48.000: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004683786s
Sep  7 05:25:49.999: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004345231s
STEP: Saw pod success 09/07/23 05:25:50
Sep  7 05:25:50.000: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5" satisfied condition "Succeeded or Failed"
Sep  7 05:25:50.001: INFO: Trying to get logs from node kind-worker2 pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 container test-container: <nil>
STEP: delete the pod 09/07/23 05:25:50.006
Sep  7 05:25:50.014: INFO: Waiting for pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 to disappear
Sep  7 05:25:50.015: INFO: Pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:25:50.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2590" for this suite. 09/07/23 05:25:50.017
------------------------------
â€¢ [4.040 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:45.981
    Sep  7 05:25:45.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:25:45.981
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:45.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:45.989
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/07/23 05:25:45.991
    Sep  7 05:25:45.995: INFO: Waiting up to 5m0s for pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5" in namespace "emptydir-2590" to be "Succeeded or Failed"
    Sep  7 05:25:45.997: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55955ms
    Sep  7 05:25:48.000: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004683786s
    Sep  7 05:25:49.999: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004345231s
    STEP: Saw pod success 09/07/23 05:25:50
    Sep  7 05:25:50.000: INFO: Pod "pod-40dbc570-ae4d-435c-8056-1c2219cea1f5" satisfied condition "Succeeded or Failed"
    Sep  7 05:25:50.001: INFO: Trying to get logs from node kind-worker2 pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:25:50.006
    Sep  7 05:25:50.014: INFO: Waiting for pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 to disappear
    Sep  7 05:25:50.015: INFO: Pod pod-40dbc570-ae4d-435c-8056-1c2219cea1f5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:25:50.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2590" for this suite. 09/07/23 05:25:50.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:25:50.021
Sep  7 05:25:50.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 05:25:50.021
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:50.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:50.031
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/07/23 05:25:50.033
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_tcp@PTR;sleep 1; done
 09/07/23 05:25:50.041
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_tcp@PTR;sleep 1; done
 09/07/23 05:25:50.041
STEP: creating a pod to probe DNS 09/07/23 05:25:50.041
STEP: submitting the pod to kubernetes 09/07/23 05:25:50.041
Sep  7 05:25:50.049: INFO: Waiting up to 15m0s for pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85" in namespace "dns-6646" to be "running"
Sep  7 05:25:50.051: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66863ms
Sep  7 05:25:52.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305795s
Sep  7 05:25:54.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Running", Reason="", readiness=true. Elapsed: 4.004979219s
Sep  7 05:25:54.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:25:54.055
STEP: looking for the results for each expected name from probers 09/07/23 05:25:54.056
Sep  7 05:25:54.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.065: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.074: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.078: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.079: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.081: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:54.084: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local]

Sep  7 05:25:59.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.103: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.106: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:25:59.113: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

Sep  7 05:26:04.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.092: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.100: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.101: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.104: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:04.110: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

Sep  7 05:26:09.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.106: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:09.113: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

Sep  7 05:26:14.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:14.111: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

Sep  7 05:26:19.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
Sep  7 05:26:19.111: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

Sep  7 05:26:24.111: INFO: DNS probes using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 succeeded

STEP: deleting the pod 09/07/23 05:26:24.111
STEP: deleting the test service 09/07/23 05:26:24.121
STEP: deleting the test headless service 09/07/23 05:26:24.134
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:24.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6646" for this suite. 09/07/23 05:26:24.142
------------------------------
â€¢ [SLOW TEST] [34.125 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:25:50.021
    Sep  7 05:25:50.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 05:25:50.021
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:25:50.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:25:50.031
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/07/23 05:25:50.033
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_tcp@PTR;sleep 1; done
     09/07/23 05:25:50.041
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6646.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6646.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.135.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.135.70_tcp@PTR;sleep 1; done
     09/07/23 05:25:50.041
    STEP: creating a pod to probe DNS 09/07/23 05:25:50.041
    STEP: submitting the pod to kubernetes 09/07/23 05:25:50.041
    Sep  7 05:25:50.049: INFO: Waiting up to 15m0s for pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85" in namespace "dns-6646" to be "running"
    Sep  7 05:25:50.051: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66863ms
    Sep  7 05:25:52.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004305795s
    Sep  7 05:25:54.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85": Phase="Running", Reason="", readiness=true. Elapsed: 4.004979219s
    Sep  7 05:25:54.054: INFO: Pod "dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:25:54.055
    STEP: looking for the results for each expected name from probers 09/07/23 05:25:54.056
    Sep  7 05:25:54.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.063: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.065: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.074: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.078: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.079: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.081: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:54.084: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-6646.svc.cluster.local]

    Sep  7 05:25:59.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.103: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.106: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:25:59.113: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

    Sep  7 05:26:04.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.092: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.100: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.101: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.104: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:04.110: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

    Sep  7 05:26:09.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.106: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:09.113: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

    Sep  7 05:26:14.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:14.111: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

    Sep  7 05:26:19.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.101: INFO: Unable to read jessie_udp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.104: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local from pod dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85: the server could not find the requested resource (get pods dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85)
    Sep  7 05:26:19.111: INFO: Lookups using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 failed for: [wheezy_udp@dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@dns-test-service.dns-6646.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_udp@dns-test-service.dns-6646.svc.cluster.local jessie_tcp@dns-test-service.dns-6646.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6646.svc.cluster.local]

    Sep  7 05:26:24.111: INFO: DNS probes using dns-6646/dns-test-e2eb6f91-af2f-4944-85cf-74c5aa65ed85 succeeded

    STEP: deleting the pod 09/07/23 05:26:24.111
    STEP: deleting the test service 09/07/23 05:26:24.121
    STEP: deleting the test headless service 09/07/23 05:26:24.134
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:24.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6646" for this suite. 09/07/23 05:26:24.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:24.146
Sep  7 05:26:24.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:26:24.147
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:24.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:24.182
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:26:24.195
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:26:24.446
STEP: Deploying the webhook pod 09/07/23 05:26:24.451
STEP: Wait for the deployment to be ready 09/07/23 05:26:24.462
Sep  7 05:26:24.466: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:26:26.472
STEP: Verifying the service has paired with the endpoint 09/07/23 05:26:26.481
Sep  7 05:26:27.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/07/23 05:26:27.484
STEP: create a namespace for the webhook 09/07/23 05:26:27.495
STEP: create a configmap should be unconditionally rejected by the webhook 09/07/23 05:26:27.499
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:27.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-200" for this suite. 09/07/23 05:26:27.54
STEP: Destroying namespace "webhook-200-markers" for this suite. 09/07/23 05:26:27.544
------------------------------
â€¢ [3.402 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:24.146
    Sep  7 05:26:24.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:26:24.147
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:24.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:24.182
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:26:24.195
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:26:24.446
    STEP: Deploying the webhook pod 09/07/23 05:26:24.451
    STEP: Wait for the deployment to be ready 09/07/23 05:26:24.462
    Sep  7 05:26:24.466: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:26:26.472
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:26:26.481
    Sep  7 05:26:27.482: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/07/23 05:26:27.484
    STEP: create a namespace for the webhook 09/07/23 05:26:27.495
    STEP: create a configmap should be unconditionally rejected by the webhook 09/07/23 05:26:27.499
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:27.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-200" for this suite. 09/07/23 05:26:27.54
    STEP: Destroying namespace "webhook-200-markers" for this suite. 09/07/23 05:26:27.544
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:27.548
Sep  7 05:26:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 05:26:27.549
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:27.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:27.559
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/07/23 05:26:27.561
STEP: setting up watch 09/07/23 05:26:27.561
STEP: submitting the pod to kubernetes 09/07/23 05:26:27.663
STEP: verifying the pod is in kubernetes 09/07/23 05:26:27.67
STEP: verifying pod creation was observed 09/07/23 05:26:27.671
Sep  7 05:26:27.671: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d" in namespace "pods-9732" to be "running"
Sep  7 05:26:27.675: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465331ms
Sep  7 05:26:29.678: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006365955s
Sep  7 05:26:29.678: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d" satisfied condition "running"
STEP: deleting the pod gracefully 09/07/23 05:26:29.68
STEP: verifying pod deletion was observed 09/07/23 05:26:29.685
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:32.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9732" for this suite. 09/07/23 05:26:32.053
------------------------------
â€¢ [4.508 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:27.548
    Sep  7 05:26:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 05:26:27.549
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:27.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:27.559
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/07/23 05:26:27.561
    STEP: setting up watch 09/07/23 05:26:27.561
    STEP: submitting the pod to kubernetes 09/07/23 05:26:27.663
    STEP: verifying the pod is in kubernetes 09/07/23 05:26:27.67
    STEP: verifying pod creation was observed 09/07/23 05:26:27.671
    Sep  7 05:26:27.671: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d" in namespace "pods-9732" to be "running"
    Sep  7 05:26:27.675: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.465331ms
    Sep  7 05:26:29.678: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006365955s
    Sep  7 05:26:29.678: INFO: Pod "pod-submit-remove-c20d850b-e674-41c9-a4a5-952ba35f786d" satisfied condition "running"
    STEP: deleting the pod gracefully 09/07/23 05:26:29.68
    STEP: verifying pod deletion was observed 09/07/23 05:26:29.685
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:32.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9732" for this suite. 09/07/23 05:26:32.053
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:32.056
Sep  7 05:26:32.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:26:32.057
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:32.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:32.065
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 05:26:32.067
Sep  7 05:26:32.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  7 05:26:32.119: INFO: stderr: ""
Sep  7 05:26:32.119: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/07/23 05:26:32.119
Sep  7 05:26:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  7 05:26:32.240: INFO: stderr: ""
Sep  7 05:26:32.240: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 05:26:32.24
Sep  7 05:26:32.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 delete pods e2e-test-httpd-pod'
Sep  7 05:26:35.073: INFO: stderr: ""
Sep  7 05:26:35.073: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:35.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8750" for this suite. 09/07/23 05:26:35.076
------------------------------
â€¢ [3.024 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:32.056
    Sep  7 05:26:32.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:26:32.057
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:32.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:32.065
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 05:26:32.067
    Sep  7 05:26:32.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  7 05:26:32.119: INFO: stderr: ""
    Sep  7 05:26:32.119: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/07/23 05:26:32.119
    Sep  7 05:26:32.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  7 05:26:32.240: INFO: stderr: ""
    Sep  7 05:26:32.240: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 05:26:32.24
    Sep  7 05:26:32.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-8750 delete pods e2e-test-httpd-pod'
    Sep  7 05:26:35.073: INFO: stderr: ""
    Sep  7 05:26:35.073: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:35.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8750" for this suite. 09/07/23 05:26:35.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:35.081
Sep  7 05:26:35.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:26:35.082
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:35.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:35.091
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/07/23 05:26:35.093
STEP: Getting a ResourceQuota 09/07/23 05:26:35.097
STEP: Updating a ResourceQuota 09/07/23 05:26:35.099
STEP: Verifying a ResourceQuota was modified 09/07/23 05:26:35.103
STEP: Deleting a ResourceQuota 09/07/23 05:26:35.104
STEP: Verifying the deleted ResourceQuota 09/07/23 05:26:35.107
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:35.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4452" for this suite. 09/07/23 05:26:35.11
------------------------------
â€¢ [0.033 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:35.081
    Sep  7 05:26:35.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:26:35.082
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:35.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:35.091
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/07/23 05:26:35.093
    STEP: Getting a ResourceQuota 09/07/23 05:26:35.097
    STEP: Updating a ResourceQuota 09/07/23 05:26:35.099
    STEP: Verifying a ResourceQuota was modified 09/07/23 05:26:35.103
    STEP: Deleting a ResourceQuota 09/07/23 05:26:35.104
    STEP: Verifying the deleted ResourceQuota 09/07/23 05:26:35.107
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:35.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4452" for this suite. 09/07/23 05:26:35.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:35.115
Sep  7 05:26:35.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:26:35.116
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:35.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:35.124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-7e01a2b7-20aa-4ee5-9f6c-122b27d27078 09/07/23 05:26:35.126
STEP: Creating a pod to test consume secrets 09/07/23 05:26:35.129
Sep  7 05:26:35.134: INFO: Waiting up to 5m0s for pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc" in namespace "secrets-714" to be "Succeeded or Failed"
Sep  7 05:26:35.135: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30562ms
Sep  7 05:26:37.137: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003514982s
Sep  7 05:26:39.139: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004603883s
STEP: Saw pod success 09/07/23 05:26:39.139
Sep  7 05:26:39.139: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc" satisfied condition "Succeeded or Failed"
Sep  7 05:26:39.140: INFO: Trying to get logs from node kind-worker pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:26:39.144
Sep  7 05:26:39.152: INFO: Waiting for pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc to disappear
Sep  7 05:26:39.153: INFO: Pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-714" for this suite. 09/07/23 05:26:39.155
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:35.115
    Sep  7 05:26:35.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:26:35.116
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:35.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:35.124
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-7e01a2b7-20aa-4ee5-9f6c-122b27d27078 09/07/23 05:26:35.126
    STEP: Creating a pod to test consume secrets 09/07/23 05:26:35.129
    Sep  7 05:26:35.134: INFO: Waiting up to 5m0s for pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc" in namespace "secrets-714" to be "Succeeded or Failed"
    Sep  7 05:26:35.135: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30562ms
    Sep  7 05:26:37.137: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003514982s
    Sep  7 05:26:39.139: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004603883s
    STEP: Saw pod success 09/07/23 05:26:39.139
    Sep  7 05:26:39.139: INFO: Pod "pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc" satisfied condition "Succeeded or Failed"
    Sep  7 05:26:39.140: INFO: Trying to get logs from node kind-worker pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:26:39.144
    Sep  7 05:26:39.152: INFO: Waiting for pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc to disappear
    Sep  7 05:26:39.153: INFO: Pod pod-secrets-72f3c473-6c99-4c6a-8213-ce820f34c8dc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-714" for this suite. 09/07/23 05:26:39.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:39.159
Sep  7 05:26:39.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 05:26:39.16
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:39.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:39.17
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/07/23 05:26:39.172
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/07/23 05:26:39.172
STEP: creating a pod to probe DNS 09/07/23 05:26:39.172
STEP: submitting the pod to kubernetes 09/07/23 05:26:39.172
Sep  7 05:26:39.181: INFO: Waiting up to 15m0s for pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca" in namespace "dns-9587" to be "running"
Sep  7 05:26:39.184: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.497551ms
Sep  7 05:26:41.187: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca": Phase="Running", Reason="", readiness=true. Elapsed: 2.006554231s
Sep  7 05:26:41.187: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:26:41.187
STEP: looking for the results for each expected name from probers 09/07/23 05:26:41.189
Sep  7 05:26:41.195: INFO: DNS probes using dns-9587/dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca succeeded

STEP: deleting the pod 09/07/23 05:26:41.195
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:41.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9587" for this suite. 09/07/23 05:26:41.205
------------------------------
â€¢ [2.050 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:39.159
    Sep  7 05:26:39.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 05:26:39.16
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:39.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:39.17
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/07/23 05:26:39.172
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/07/23 05:26:39.172
    STEP: creating a pod to probe DNS 09/07/23 05:26:39.172
    STEP: submitting the pod to kubernetes 09/07/23 05:26:39.172
    Sep  7 05:26:39.181: INFO: Waiting up to 15m0s for pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca" in namespace "dns-9587" to be "running"
    Sep  7 05:26:39.184: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.497551ms
    Sep  7 05:26:41.187: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca": Phase="Running", Reason="", readiness=true. Elapsed: 2.006554231s
    Sep  7 05:26:41.187: INFO: Pod "dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:26:41.187
    STEP: looking for the results for each expected name from probers 09/07/23 05:26:41.189
    Sep  7 05:26:41.195: INFO: DNS probes using dns-9587/dns-test-9b4676f2-1860-4a17-8a00-37f7b95e3aca succeeded

    STEP: deleting the pod 09/07/23 05:26:41.195
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:41.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9587" for this suite. 09/07/23 05:26:41.205
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:41.21
Sep  7 05:26:41.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context 09/07/23 05:26:41.211
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:41.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:41.219
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/07/23 05:26:41.22
Sep  7 05:26:41.225: INFO: Waiting up to 5m0s for pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8" in namespace "security-context-4824" to be "Succeeded or Failed"
Sep  7 05:26:41.227: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.42474ms
Sep  7 05:26:43.230: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411837s
Sep  7 05:26:45.229: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00353734s
STEP: Saw pod success 09/07/23 05:26:45.229
Sep  7 05:26:45.229: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8" satisfied condition "Succeeded or Failed"
Sep  7 05:26:45.231: INFO: Trying to get logs from node kind-worker pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 container test-container: <nil>
STEP: delete the pod 09/07/23 05:26:45.234
Sep  7 05:26:45.243: INFO: Waiting for pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 to disappear
Sep  7 05:26:45.245: INFO: Pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:45.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4824" for this suite. 09/07/23 05:26:45.247
------------------------------
â€¢ [4.040 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:41.21
    Sep  7 05:26:41.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context 09/07/23 05:26:41.211
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:41.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:41.219
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/07/23 05:26:41.22
    Sep  7 05:26:41.225: INFO: Waiting up to 5m0s for pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8" in namespace "security-context-4824" to be "Succeeded or Failed"
    Sep  7 05:26:41.227: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.42474ms
    Sep  7 05:26:43.230: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411837s
    Sep  7 05:26:45.229: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00353734s
    STEP: Saw pod success 09/07/23 05:26:45.229
    Sep  7 05:26:45.229: INFO: Pod "security-context-9eb08bca-e67e-4a64-bf59-112c769340a8" satisfied condition "Succeeded or Failed"
    Sep  7 05:26:45.231: INFO: Trying to get logs from node kind-worker pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:26:45.234
    Sep  7 05:26:45.243: INFO: Waiting for pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 to disappear
    Sep  7 05:26:45.245: INFO: Pod security-context-9eb08bca-e67e-4a64-bf59-112c769340a8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:45.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4824" for this suite. 09/07/23 05:26:45.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:45.251
Sep  7 05:26:45.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:26:45.251
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:45.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:45.26
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:26:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5845" for this suite. 09/07/23 05:26:45.266
------------------------------
â€¢ [0.018 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:45.251
    Sep  7 05:26:45.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:26:45.251
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:45.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:45.26
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:26:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5845" for this suite. 09/07/23 05:26:45.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:26:45.269
Sep  7 05:26:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 05:26:45.27
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:45.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:45.278
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a in namespace container-probe-9256 09/07/23 05:26:45.28
Sep  7 05:26:45.285: INFO: Waiting up to 5m0s for pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a" in namespace "container-probe-9256" to be "not pending"
Sep  7 05:26:45.286: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40027ms
Sep  7 05:26:47.290: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00448622s
Sep  7 05:26:47.290: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a" satisfied condition "not pending"
Sep  7 05:26:47.290: INFO: Started pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a in namespace container-probe-9256
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:26:47.29
Sep  7 05:26:47.291: INFO: Initial restart count of pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a is 0
Sep  7 05:27:37.365: INFO: Restart count of pod container-probe-9256/busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a is now 1 (50.073702731s elapsed)
STEP: deleting the pod 09/07/23 05:27:37.365
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 05:27:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9256" for this suite. 09/07/23 05:27:37.375
------------------------------
â€¢ [SLOW TEST] [52.109 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:26:45.269
    Sep  7 05:26:45.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 05:26:45.27
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:26:45.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:26:45.278
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a in namespace container-probe-9256 09/07/23 05:26:45.28
    Sep  7 05:26:45.285: INFO: Waiting up to 5m0s for pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a" in namespace "container-probe-9256" to be "not pending"
    Sep  7 05:26:45.286: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40027ms
    Sep  7 05:26:47.290: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a": Phase="Running", Reason="", readiness=true. Elapsed: 2.00448622s
    Sep  7 05:26:47.290: INFO: Pod "busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a" satisfied condition "not pending"
    Sep  7 05:26:47.290: INFO: Started pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a in namespace container-probe-9256
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:26:47.29
    Sep  7 05:26:47.291: INFO: Initial restart count of pod busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a is 0
    Sep  7 05:27:37.365: INFO: Restart count of pod container-probe-9256/busybox-4a12d43d-e5ab-4375-9855-7f3c9bdc4f3a is now 1 (50.073702731s elapsed)
    STEP: deleting the pod 09/07/23 05:27:37.365
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:27:37.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9256" for this suite. 09/07/23 05:27:37.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:27:37.379
Sep  7 05:27:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-runtime 09/07/23 05:27:37.379
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:37.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:37.39
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/07/23 05:27:37.396
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/07/23 05:27:52.435
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/07/23 05:27:52.436
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/07/23 05:27:52.439
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/07/23 05:27:52.439
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/07/23 05:27:52.452
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/07/23 05:27:54.458
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/07/23 05:27:56.464
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/07/23 05:27:56.467
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/07/23 05:27:56.468
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/07/23 05:27:56.48
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/07/23 05:27:57.485
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/07/23 05:27:59.491
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/07/23 05:27:59.495
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/07/23 05:27:59.495
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  7 05:27:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3095" for this suite. 09/07/23 05:27:59.512
------------------------------
â€¢ [SLOW TEST] [22.137 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:27:37.379
    Sep  7 05:27:37.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-runtime 09/07/23 05:27:37.379
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:37.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:37.39
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/07/23 05:27:37.396
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/07/23 05:27:52.435
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/07/23 05:27:52.436
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/07/23 05:27:52.439
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/07/23 05:27:52.439
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/07/23 05:27:52.452
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/07/23 05:27:54.458
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/07/23 05:27:56.464
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/07/23 05:27:56.467
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/07/23 05:27:56.468
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/07/23 05:27:56.48
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/07/23 05:27:57.485
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/07/23 05:27:59.491
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/07/23 05:27:59.495
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/07/23 05:27:59.495
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:27:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3095" for this suite. 09/07/23 05:27:59.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:27:59.516
Sep  7 05:27:59.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 05:27:59.516
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:59.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:59.526
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/07/23 05:27:59.528
Sep  7 05:27:59.531: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/07/23 05:27:59.531
Sep  7 05:27:59.537: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/07/23 05:27:59.537
Sep  7 05:27:59.544: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:27:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5400" for this suite. 09/07/23 05:27:59.546
------------------------------
â€¢ [0.033 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:27:59.516
    Sep  7 05:27:59.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 05:27:59.516
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:59.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:59.526
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/07/23 05:27:59.528
    Sep  7 05:27:59.531: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/07/23 05:27:59.531
    Sep  7 05:27:59.537: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/07/23 05:27:59.537
    Sep  7 05:27:59.544: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:27:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5400" for this suite. 09/07/23 05:27:59.546
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:27:59.549
Sep  7 05:27:59.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:27:59.549
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:59.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:59.559
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:27:59.567
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:28:00.05
STEP: Deploying the webhook pod 09/07/23 05:28:00.055
STEP: Wait for the deployment to be ready 09/07/23 05:28:00.065
Sep  7 05:28:00.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:28:02.073
STEP: Verifying the service has paired with the endpoint 09/07/23 05:28:02.08
Sep  7 05:28:03.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/07/23 05:28:03.122
STEP: Creating a configMap that should be mutated 09/07/23 05:28:03.129
STEP: Deleting the collection of validation webhooks 09/07/23 05:28:03.145
STEP: Creating a configMap that should not be mutated 09/07/23 05:28:03.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:28:03.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3134" for this suite. 09/07/23 05:28:03.195
STEP: Destroying namespace "webhook-3134-markers" for this suite. 09/07/23 05:28:03.202
------------------------------
â€¢ [3.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:27:59.549
    Sep  7 05:27:59.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:27:59.549
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:27:59.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:27:59.559
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:27:59.567
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:28:00.05
    STEP: Deploying the webhook pod 09/07/23 05:28:00.055
    STEP: Wait for the deployment to be ready 09/07/23 05:28:00.065
    Sep  7 05:28:00.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:28:02.073
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:28:02.08
    Sep  7 05:28:03.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/07/23 05:28:03.122
    STEP: Creating a configMap that should be mutated 09/07/23 05:28:03.129
    STEP: Deleting the collection of validation webhooks 09/07/23 05:28:03.145
    STEP: Creating a configMap that should not be mutated 09/07/23 05:28:03.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:28:03.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3134" for this suite. 09/07/23 05:28:03.195
    STEP: Destroying namespace "webhook-3134-markers" for this suite. 09/07/23 05:28:03.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:28:03.205
Sep  7 05:28:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:28:03.205
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:03.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:03.213
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:28:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8251" for this suite. 09/07/23 05:28:07.226
------------------------------
â€¢ [4.025 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:28:03.205
    Sep  7 05:28:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:28:03.205
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:03.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:03.213
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:28:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8251" for this suite. 09/07/23 05:28:07.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:28:07.23
Sep  7 05:28:07.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:28:07.23
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:07.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:07.241
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-d630eaba-3ec6-404d-8f90-df3d1c1f383e 09/07/23 05:28:07.242
STEP: Creating a pod to test consume secrets 09/07/23 05:28:07.245
Sep  7 05:28:07.250: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81" in namespace "projected-2913" to be "Succeeded or Failed"
Sep  7 05:28:07.252: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40076ms
Sep  7 05:28:09.254: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003891519s
Sep  7 05:28:11.255: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005004367s
STEP: Saw pod success 09/07/23 05:28:11.255
Sep  7 05:28:11.256: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81" satisfied condition "Succeeded or Failed"
Sep  7 05:28:11.258: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:28:11.261
Sep  7 05:28:11.269: INFO: Waiting for pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 to disappear
Sep  7 05:28:11.271: INFO: Pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:28:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2913" for this suite. 09/07/23 05:28:11.274
------------------------------
â€¢ [4.048 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:28:07.23
    Sep  7 05:28:07.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:28:07.23
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:07.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:07.241
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-d630eaba-3ec6-404d-8f90-df3d1c1f383e 09/07/23 05:28:07.242
    STEP: Creating a pod to test consume secrets 09/07/23 05:28:07.245
    Sep  7 05:28:07.250: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81" in namespace "projected-2913" to be "Succeeded or Failed"
    Sep  7 05:28:07.252: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40076ms
    Sep  7 05:28:09.254: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003891519s
    Sep  7 05:28:11.255: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005004367s
    STEP: Saw pod success 09/07/23 05:28:11.255
    Sep  7 05:28:11.256: INFO: Pod "pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81" satisfied condition "Succeeded or Failed"
    Sep  7 05:28:11.258: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:28:11.261
    Sep  7 05:28:11.269: INFO: Waiting for pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 to disappear
    Sep  7 05:28:11.271: INFO: Pod pod-projected-secrets-24161a06-4626-465b-9c68-e22d8643eb81 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:28:11.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2913" for this suite. 09/07/23 05:28:11.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:28:11.279
Sep  7 05:28:11.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:28:11.279
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:11.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:11.288
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/07/23 05:28:11.289
Sep  7 05:28:11.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 create -f -'
Sep  7 05:28:11.405: INFO: stderr: ""
Sep  7 05:28:11.405: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 05:28:11.405
Sep  7 05:28:11.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 05:28:11.457: INFO: stderr: ""
Sep  7 05:28:11.457: INFO: stdout: "update-demo-nautilus-qdg6v update-demo-nautilus-zd56f "
Sep  7 05:28:11.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 05:28:11.508: INFO: stderr: ""
Sep  7 05:28:11.508: INFO: stdout: ""
Sep  7 05:28:11.508: INFO: update-demo-nautilus-qdg6v is created but not running
Sep  7 05:28:16.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 05:28:16.567: INFO: stderr: ""
Sep  7 05:28:16.567: INFO: stdout: "update-demo-nautilus-qdg6v update-demo-nautilus-zd56f "
Sep  7 05:28:16.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 05:28:16.615: INFO: stderr: ""
Sep  7 05:28:16.615: INFO: stdout: "true"
Sep  7 05:28:16.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 05:28:16.667: INFO: stderr: ""
Sep  7 05:28:16.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 05:28:16.667: INFO: validating pod update-demo-nautilus-qdg6v
Sep  7 05:28:16.670: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 05:28:16.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 05:28:16.670: INFO: update-demo-nautilus-qdg6v is verified up and running
Sep  7 05:28:16.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-zd56f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 05:28:16.724: INFO: stderr: ""
Sep  7 05:28:16.724: INFO: stdout: "true"
Sep  7 05:28:16.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-zd56f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 05:28:16.775: INFO: stderr: ""
Sep  7 05:28:16.775: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 05:28:16.775: INFO: validating pod update-demo-nautilus-zd56f
Sep  7 05:28:16.778: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 05:28:16.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 05:28:16.778: INFO: update-demo-nautilus-zd56f is verified up and running
STEP: using delete to clean up resources 09/07/23 05:28:16.778
Sep  7 05:28:16.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 delete --grace-period=0 --force -f -'
Sep  7 05:28:16.832: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:28:16.832: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  7 05:28:16.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get rc,svc -l name=update-demo --no-headers'
Sep  7 05:28:16.908: INFO: stderr: "No resources found in kubectl-6218 namespace.\n"
Sep  7 05:28:16.908: INFO: stdout: ""
Sep  7 05:28:16.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  7 05:28:16.963: INFO: stderr: ""
Sep  7 05:28:16.963: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:28:16.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6218" for this suite. 09/07/23 05:28:16.966
------------------------------
â€¢ [SLOW TEST] [5.763 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:28:11.279
    Sep  7 05:28:11.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:28:11.279
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:11.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:11.288
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/07/23 05:28:11.289
    Sep  7 05:28:11.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 create -f -'
    Sep  7 05:28:11.405: INFO: stderr: ""
    Sep  7 05:28:11.405: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 05:28:11.405
    Sep  7 05:28:11.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 05:28:11.457: INFO: stderr: ""
    Sep  7 05:28:11.457: INFO: stdout: "update-demo-nautilus-qdg6v update-demo-nautilus-zd56f "
    Sep  7 05:28:11.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 05:28:11.508: INFO: stderr: ""
    Sep  7 05:28:11.508: INFO: stdout: ""
    Sep  7 05:28:11.508: INFO: update-demo-nautilus-qdg6v is created but not running
    Sep  7 05:28:16.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 05:28:16.567: INFO: stderr: ""
    Sep  7 05:28:16.567: INFO: stdout: "update-demo-nautilus-qdg6v update-demo-nautilus-zd56f "
    Sep  7 05:28:16.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 05:28:16.615: INFO: stderr: ""
    Sep  7 05:28:16.615: INFO: stdout: "true"
    Sep  7 05:28:16.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-qdg6v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 05:28:16.667: INFO: stderr: ""
    Sep  7 05:28:16.667: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 05:28:16.667: INFO: validating pod update-demo-nautilus-qdg6v
    Sep  7 05:28:16.670: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 05:28:16.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 05:28:16.670: INFO: update-demo-nautilus-qdg6v is verified up and running
    Sep  7 05:28:16.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-zd56f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 05:28:16.724: INFO: stderr: ""
    Sep  7 05:28:16.724: INFO: stdout: "true"
    Sep  7 05:28:16.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods update-demo-nautilus-zd56f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 05:28:16.775: INFO: stderr: ""
    Sep  7 05:28:16.775: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 05:28:16.775: INFO: validating pod update-demo-nautilus-zd56f
    Sep  7 05:28:16.778: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 05:28:16.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 05:28:16.778: INFO: update-demo-nautilus-zd56f is verified up and running
    STEP: using delete to clean up resources 09/07/23 05:28:16.778
    Sep  7 05:28:16.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 delete --grace-period=0 --force -f -'
    Sep  7 05:28:16.832: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:28:16.832: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  7 05:28:16.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get rc,svc -l name=update-demo --no-headers'
    Sep  7 05:28:16.908: INFO: stderr: "No resources found in kubectl-6218 namespace.\n"
    Sep  7 05:28:16.908: INFO: stdout: ""
    Sep  7 05:28:16.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6218 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  7 05:28:16.963: INFO: stderr: ""
    Sep  7 05:28:16.963: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:28:16.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6218" for this suite. 09/07/23 05:28:16.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:28:17.042
Sep  7 05:28:17.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:28:17.043
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:17.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:17.152
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-278 09/07/23 05:28:17.154
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/07/23 05:28:17.165
STEP: creating service externalsvc in namespace services-278 09/07/23 05:28:17.166
STEP: creating replication controller externalsvc in namespace services-278 09/07/23 05:28:17.175
I0907 05:28:17.184886      29 runners.go:193] Created replication controller with name: externalsvc, namespace: services-278, replica count: 2
I0907 05:28:20.237362      29 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/07/23 05:28:20.239
Sep  7 05:28:20.251: INFO: Creating new exec pod
Sep  7 05:28:20.256: INFO: Waiting up to 5m0s for pod "execpodnd94z" in namespace "services-278" to be "running"
Sep  7 05:28:20.258: INFO: Pod "execpodnd94z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.68116ms
Sep  7 05:28:22.261: INFO: Pod "execpodnd94z": Phase="Running", Reason="", readiness=true. Elapsed: 2.005035986s
Sep  7 05:28:22.261: INFO: Pod "execpodnd94z" satisfied condition "running"
Sep  7 05:28:22.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-278 exec execpodnd94z -- /bin/sh -x -c nslookup nodeport-service.services-278.svc.cluster.local'
Sep  7 05:28:22.674: INFO: stderr: "+ nslookup nodeport-service.services-278.svc.cluster.local\n"
Sep  7 05:28:22.674: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-278.svc.cluster.local\tcanonical name = externalsvc.services-278.svc.cluster.local.\nName:\texternalsvc.services-278.svc.cluster.local\nAddress: 10.96.202.200\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-278, will wait for the garbage collector to delete the pods 09/07/23 05:28:22.674
Sep  7 05:28:22.733: INFO: Deleting ReplicationController externalsvc took: 6.90437ms
Sep  7 05:28:22.833: INFO: Terminating ReplicationController externalsvc pods took: 100.357479ms
Sep  7 05:28:26.345: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:28:26.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-278" for this suite. 09/07/23 05:28:26.353
------------------------------
â€¢ [SLOW TEST] [9.317 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:28:17.042
    Sep  7 05:28:17.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:28:17.043
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:17.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:17.152
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-278 09/07/23 05:28:17.154
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/07/23 05:28:17.165
    STEP: creating service externalsvc in namespace services-278 09/07/23 05:28:17.166
    STEP: creating replication controller externalsvc in namespace services-278 09/07/23 05:28:17.175
    I0907 05:28:17.184886      29 runners.go:193] Created replication controller with name: externalsvc, namespace: services-278, replica count: 2
    I0907 05:28:20.237362      29 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/07/23 05:28:20.239
    Sep  7 05:28:20.251: INFO: Creating new exec pod
    Sep  7 05:28:20.256: INFO: Waiting up to 5m0s for pod "execpodnd94z" in namespace "services-278" to be "running"
    Sep  7 05:28:20.258: INFO: Pod "execpodnd94z": Phase="Pending", Reason="", readiness=false. Elapsed: 1.68116ms
    Sep  7 05:28:22.261: INFO: Pod "execpodnd94z": Phase="Running", Reason="", readiness=true. Elapsed: 2.005035986s
    Sep  7 05:28:22.261: INFO: Pod "execpodnd94z" satisfied condition "running"
    Sep  7 05:28:22.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-278 exec execpodnd94z -- /bin/sh -x -c nslookup nodeport-service.services-278.svc.cluster.local'
    Sep  7 05:28:22.674: INFO: stderr: "+ nslookup nodeport-service.services-278.svc.cluster.local\n"
    Sep  7 05:28:22.674: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-278.svc.cluster.local\tcanonical name = externalsvc.services-278.svc.cluster.local.\nName:\texternalsvc.services-278.svc.cluster.local\nAddress: 10.96.202.200\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-278, will wait for the garbage collector to delete the pods 09/07/23 05:28:22.674
    Sep  7 05:28:22.733: INFO: Deleting ReplicationController externalsvc took: 6.90437ms
    Sep  7 05:28:22.833: INFO: Terminating ReplicationController externalsvc pods took: 100.357479ms
    Sep  7 05:28:26.345: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:28:26.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-278" for this suite. 09/07/23 05:28:26.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:28:26.36
Sep  7 05:28:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:28:26.361
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:26.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:26.369
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  7 05:28:26.378: INFO: created pod
Sep  7 05:28:26.378: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1984" to be "Succeeded or Failed"
Sep  7 05:28:26.380: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346701ms
Sep  7 05:28:28.383: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.004268336s
Sep  7 05:28:30.384: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00543726s
STEP: Saw pod success 09/07/23 05:28:30.384
Sep  7 05:28:30.384: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  7 05:29:00.387: INFO: polling logs
Sep  7 05:29:00.393: INFO: Pod logs: 
I0907 05:28:27.034612       1 log.go:198] OK: Got token
I0907 05:28:27.034652       1 log.go:198] validating with in-cluster discovery
I0907 05:28:27.034839       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0907 05:28:27.034865       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1984:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694065106, NotBefore:1694064506, IssuedAt:1694064506, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1984", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c2b5a8f-ebbe-4a92-a7c6-a215a254d6b9"}}}
I0907 05:28:27.278590       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0907 05:28:27.282399       1 log.go:198] OK: Validated signature on JWT
I0907 05:28:27.282481       1 log.go:198] OK: Got valid claims from token!
I0907 05:28:27.282516       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1984:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694065106, NotBefore:1694064506, IssuedAt:1694064506, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1984", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c2b5a8f-ebbe-4a92-a7c6-a215a254d6b9"}}}

Sep  7 05:29:00.393: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1984" for this suite. 09/07/23 05:29:00.398
------------------------------
â€¢ [SLOW TEST] [34.042 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:28:26.36
    Sep  7 05:28:26.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:28:26.361
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:28:26.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:28:26.369
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  7 05:28:26.378: INFO: created pod
    Sep  7 05:28:26.378: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1984" to be "Succeeded or Failed"
    Sep  7 05:28:26.380: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346701ms
    Sep  7 05:28:28.383: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 2.004268336s
    Sep  7 05:28:30.384: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00543726s
    STEP: Saw pod success 09/07/23 05:28:30.384
    Sep  7 05:28:30.384: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  7 05:29:00.387: INFO: polling logs
    Sep  7 05:29:00.393: INFO: Pod logs: 
    I0907 05:28:27.034612       1 log.go:198] OK: Got token
    I0907 05:28:27.034652       1 log.go:198] validating with in-cluster discovery
    I0907 05:28:27.034839       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0907 05:28:27.034865       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1984:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694065106, NotBefore:1694064506, IssuedAt:1694064506, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1984", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c2b5a8f-ebbe-4a92-a7c6-a215a254d6b9"}}}
    I0907 05:28:27.278590       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0907 05:28:27.282399       1 log.go:198] OK: Validated signature on JWT
    I0907 05:28:27.282481       1 log.go:198] OK: Got valid claims from token!
    I0907 05:28:27.282516       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1984:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694065106, NotBefore:1694064506, IssuedAt:1694064506, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1984", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9c2b5a8f-ebbe-4a92-a7c6-a215a254d6b9"}}}

    Sep  7 05:29:00.393: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:00.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1984" for this suite. 09/07/23 05:29:00.398
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:00.402
Sep  7 05:29:00.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-pred 09/07/23 05:29:00.403
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:00.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:00.414
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  7 05:29:00.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  7 05:29:00.419: INFO: Waiting for terminating namespaces to be deleted...
Sep  7 05:29:00.420: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  7 05:29:00.423: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.423: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 05:29:00.423: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.423: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 05:29:00.423: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  7 05:29:00.423: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:29:00.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:29:00.423: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  7 05:29:00.423: INFO: oidc-discovery-validator from svcaccounts-1984 started at 2023-09-07 05:28:26 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.423: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
Sep  7 05:29:00.423: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  7 05:29:00.425: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.425: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 05:29:00.425: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 05:29:00.425: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 05:29:00.425: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:29:00.425: INFO: 	Container e2e ready: true, restart count 0
Sep  7 05:29:00.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:29:00.425: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 05:29:00.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 05:29:00.425: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node kind-worker 09/07/23 05:29:00.435
STEP: verifying the node has the label node kind-worker2 09/07/23 05:29:00.442
Sep  7 05:29:00.447: INFO: Pod kindnet-69bgp requesting resource cpu=100m on Node kind-worker2
Sep  7 05:29:00.447: INFO: Pod kindnet-x7hxm requesting resource cpu=100m on Node kind-worker
Sep  7 05:29:00.447: INFO: Pod kube-proxy-7blqf requesting resource cpu=0m on Node kind-worker
Sep  7 05:29:00.447: INFO: Pod kube-proxy-9w4bc requesting resource cpu=0m on Node kind-worker2
Sep  7 05:29:00.447: INFO: Pod sonobuoy requesting resource cpu=0m on Node kind-worker
Sep  7 05:29:00.447: INFO: Pod sonobuoy-e2e-job-ac4880036cda42a0 requesting resource cpu=0m on Node kind-worker2
Sep  7 05:29:00.447: INFO: Pod sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb requesting resource cpu=0m on Node kind-worker
Sep  7 05:29:00.447: INFO: Pod sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx requesting resource cpu=0m on Node kind-worker2
STEP: Starting Pods to consume most of the cluster CPU. 09/07/23 05:29:00.447
Sep  7 05:29:00.447: INFO: Creating a pod which consumes cpu=33530m on Node kind-worker
Sep  7 05:29:00.451: INFO: Creating a pod which consumes cpu=33530m on Node kind-worker2
Sep  7 05:29:00.456: INFO: Waiting up to 5m0s for pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca" in namespace "sched-pred-2027" to be "running"
Sep  7 05:29:00.462: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97512ms
Sep  7 05:29:02.465: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571198s
Sep  7 05:29:02.465: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca" satisfied condition "running"
Sep  7 05:29:02.465: INFO: Waiting up to 5m0s for pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6" in namespace "sched-pred-2027" to be "running"
Sep  7 05:29:02.466: INFO: Pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6": Phase="Running", Reason="", readiness=true. Elapsed: 1.87529ms
Sep  7 05:29:02.466: INFO: Pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/07/23 05:29:02.466
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b958d81b44], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2027/filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6 to kind-worker2] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b97b61b84f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b97c6a836c], Reason = [Created], Message = [Created container filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b9822241da], Reason = [Started], Message = [Started container filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b9588e983e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2027/filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca to kind-worker] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b97b5c4847], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b97c58a304], Reason = [Created], Message = [Created container filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b981cf7cc0], Reason = [Started], Message = [Started container filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca] 09/07/23 05:29:02.469
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.178286b9d0fc24cf], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 09/07/23 05:29:02.481
STEP: removing the label node off the node kind-worker 09/07/23 05:29:03.48
STEP: verifying the node doesn't have the label node 09/07/23 05:29:03.489
STEP: removing the label node off the node kind-worker2 09/07/23 05:29:03.491
STEP: verifying the node doesn't have the label node 09/07/23 05:29:03.499
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:03.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2027" for this suite. 09/07/23 05:29:03.502
------------------------------
â€¢ [3.104 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:00.402
    Sep  7 05:29:00.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-pred 09/07/23 05:29:00.403
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:00.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:00.414
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  7 05:29:00.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  7 05:29:00.419: INFO: Waiting for terminating namespaces to be deleted...
    Sep  7 05:29:00.420: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  7 05:29:00.423: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.423: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 05:29:00.423: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.423: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 05:29:00.423: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  7 05:29:00.423: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:29:00.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:29:00.423: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  7 05:29:00.423: INFO: oidc-discovery-validator from svcaccounts-1984 started at 2023-09-07 05:28:26 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.423: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
    Sep  7 05:29:00.423: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  7 05:29:00.425: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.425: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 05:29:00.425: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 05:29:00.425: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 05:29:00.425: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:29:00.425: INFO: 	Container e2e ready: true, restart count 0
    Sep  7 05:29:00.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:29:00.425: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 05:29:00.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 05:29:00.425: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node kind-worker 09/07/23 05:29:00.435
    STEP: verifying the node has the label node kind-worker2 09/07/23 05:29:00.442
    Sep  7 05:29:00.447: INFO: Pod kindnet-69bgp requesting resource cpu=100m on Node kind-worker2
    Sep  7 05:29:00.447: INFO: Pod kindnet-x7hxm requesting resource cpu=100m on Node kind-worker
    Sep  7 05:29:00.447: INFO: Pod kube-proxy-7blqf requesting resource cpu=0m on Node kind-worker
    Sep  7 05:29:00.447: INFO: Pod kube-proxy-9w4bc requesting resource cpu=0m on Node kind-worker2
    Sep  7 05:29:00.447: INFO: Pod sonobuoy requesting resource cpu=0m on Node kind-worker
    Sep  7 05:29:00.447: INFO: Pod sonobuoy-e2e-job-ac4880036cda42a0 requesting resource cpu=0m on Node kind-worker2
    Sep  7 05:29:00.447: INFO: Pod sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb requesting resource cpu=0m on Node kind-worker
    Sep  7 05:29:00.447: INFO: Pod sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx requesting resource cpu=0m on Node kind-worker2
    STEP: Starting Pods to consume most of the cluster CPU. 09/07/23 05:29:00.447
    Sep  7 05:29:00.447: INFO: Creating a pod which consumes cpu=33530m on Node kind-worker
    Sep  7 05:29:00.451: INFO: Creating a pod which consumes cpu=33530m on Node kind-worker2
    Sep  7 05:29:00.456: INFO: Waiting up to 5m0s for pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca" in namespace "sched-pred-2027" to be "running"
    Sep  7 05:29:00.462: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97512ms
    Sep  7 05:29:02.465: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571198s
    Sep  7 05:29:02.465: INFO: Pod "filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca" satisfied condition "running"
    Sep  7 05:29:02.465: INFO: Waiting up to 5m0s for pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6" in namespace "sched-pred-2027" to be "running"
    Sep  7 05:29:02.466: INFO: Pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6": Phase="Running", Reason="", readiness=true. Elapsed: 1.87529ms
    Sep  7 05:29:02.466: INFO: Pod "filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/07/23 05:29:02.466
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b958d81b44], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2027/filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6 to kind-worker2] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b97b61b84f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b97c6a836c], Reason = [Created], Message = [Created container filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6.178286b9822241da], Reason = [Started], Message = [Started container filler-pod-83ab6c3e-5d06-4ef2-a6f3-cc51e1e0f2b6] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b9588e983e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2027/filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca to kind-worker] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b97b5c4847], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b97c58a304], Reason = [Created], Message = [Created container filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca.178286b981cf7cc0], Reason = [Started], Message = [Started container filler-pod-c513ea9f-3852-41a7-8b6e-b53d5de70dca] 09/07/23 05:29:02.469
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.178286b9d0fc24cf], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] 09/07/23 05:29:02.481
    STEP: removing the label node off the node kind-worker 09/07/23 05:29:03.48
    STEP: verifying the node doesn't have the label node 09/07/23 05:29:03.489
    STEP: removing the label node off the node kind-worker2 09/07/23 05:29:03.491
    STEP: verifying the node doesn't have the label node 09/07/23 05:29:03.499
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:03.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2027" for this suite. 09/07/23 05:29:03.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:03.506
Sep  7 05:29:03.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 05:29:03.507
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:03.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:03.519
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/07/23 05:29:03.521
STEP: Ensuring job reaches completions 09/07/23 05:29:03.524
STEP: Ensuring pods with index for job exist 09/07/23 05:29:11.527
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:11.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6116" for this suite. 09/07/23 05:29:11.531
------------------------------
â€¢ [SLOW TEST] [8.028 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:03.506
    Sep  7 05:29:03.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 05:29:03.507
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:03.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:03.519
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/07/23 05:29:03.521
    STEP: Ensuring job reaches completions 09/07/23 05:29:03.524
    STEP: Ensuring pods with index for job exist 09/07/23 05:29:11.527
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:11.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6116" for this suite. 09/07/23 05:29:11.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:11.535
Sep  7 05:29:11.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename containers 09/07/23 05:29:11.536
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:11.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:11.544
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/07/23 05:29:11.546
Sep  7 05:29:11.550: INFO: Waiting up to 5m0s for pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487" in namespace "containers-6345" to be "Succeeded or Failed"
Sep  7 05:29:11.551: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30651ms
Sep  7 05:29:13.554: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003801006s
Sep  7 05:29:15.555: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004845301s
STEP: Saw pod success 09/07/23 05:29:15.555
Sep  7 05:29:15.555: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487" satisfied condition "Succeeded or Failed"
Sep  7 05:29:15.556: INFO: Trying to get logs from node kind-worker pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:29:15.559
Sep  7 05:29:15.568: INFO: Waiting for pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 to disappear
Sep  7 05:29:15.570: INFO: Pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:15.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6345" for this suite. 09/07/23 05:29:15.571
------------------------------
â€¢ [4.039 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:11.535
    Sep  7 05:29:11.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename containers 09/07/23 05:29:11.536
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:11.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:11.544
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/07/23 05:29:11.546
    Sep  7 05:29:11.550: INFO: Waiting up to 5m0s for pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487" in namespace "containers-6345" to be "Succeeded or Failed"
    Sep  7 05:29:11.551: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30651ms
    Sep  7 05:29:13.554: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003801006s
    Sep  7 05:29:15.555: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004845301s
    STEP: Saw pod success 09/07/23 05:29:15.555
    Sep  7 05:29:15.555: INFO: Pod "client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487" satisfied condition "Succeeded or Failed"
    Sep  7 05:29:15.556: INFO: Trying to get logs from node kind-worker pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:29:15.559
    Sep  7 05:29:15.568: INFO: Waiting for pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 to disappear
    Sep  7 05:29:15.570: INFO: Pod client-containers-a6d2f376-d8e3-4b01-9469-58001cbe6487 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:15.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6345" for this suite. 09/07/23 05:29:15.571
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:15.575
Sep  7 05:29:15.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir-wrapper 09/07/23 05:29:15.575
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:15.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:15.584
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  7 05:29:15.596: INFO: Waiting up to 5m0s for pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e" in namespace "emptydir-wrapper-1852" to be "running and ready"
Sep  7 05:29:15.598: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36106ms
Sep  7 05:29:15.598: INFO: The phase of Pod pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:29:17.600: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003861865s
Sep  7 05:29:17.600: INFO: The phase of Pod pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e is Running (Ready = true)
Sep  7 05:29:17.600: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/07/23 05:29:17.602
STEP: Cleaning up the configmap 09/07/23 05:29:17.608
STEP: Cleaning up the pod 09/07/23 05:29:17.611
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:17.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1852" for this suite. 09/07/23 05:29:17.619
------------------------------
â€¢ [2.048 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:15.575
    Sep  7 05:29:15.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir-wrapper 09/07/23 05:29:15.575
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:15.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:15.584
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  7 05:29:15.596: INFO: Waiting up to 5m0s for pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e" in namespace "emptydir-wrapper-1852" to be "running and ready"
    Sep  7 05:29:15.598: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36106ms
    Sep  7 05:29:15.598: INFO: The phase of Pod pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:29:17.600: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.003861865s
    Sep  7 05:29:17.600: INFO: The phase of Pod pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e is Running (Ready = true)
    Sep  7 05:29:17.600: INFO: Pod "pod-secrets-e0fbce76-08aa-457a-9fec-cb4cd4fd7c6e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/07/23 05:29:17.602
    STEP: Cleaning up the configmap 09/07/23 05:29:17.608
    STEP: Cleaning up the pod 09/07/23 05:29:17.611
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:17.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1852" for this suite. 09/07/23 05:29:17.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:17.623
Sep  7 05:29:17.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename podtemplate 09/07/23 05:29:17.623
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:17.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:17.634
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/07/23 05:29:17.636
Sep  7 05:29:17.638: INFO: created test-podtemplate-1
Sep  7 05:29:17.641: INFO: created test-podtemplate-2
Sep  7 05:29:17.644: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/07/23 05:29:17.644
STEP: delete collection of pod templates 09/07/23 05:29:17.646
Sep  7 05:29:17.646: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/07/23 05:29:17.657
Sep  7 05:29:17.657: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:17.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6389" for this suite. 09/07/23 05:29:17.661
------------------------------
â€¢ [0.042 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:17.623
    Sep  7 05:29:17.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename podtemplate 09/07/23 05:29:17.623
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:17.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:17.634
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/07/23 05:29:17.636
    Sep  7 05:29:17.638: INFO: created test-podtemplate-1
    Sep  7 05:29:17.641: INFO: created test-podtemplate-2
    Sep  7 05:29:17.644: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/07/23 05:29:17.644
    STEP: delete collection of pod templates 09/07/23 05:29:17.646
    Sep  7 05:29:17.646: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/07/23 05:29:17.657
    Sep  7 05:29:17.657: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:17.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6389" for this suite. 09/07/23 05:29:17.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:17.665
Sep  7 05:29:17.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:29:17.666
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:17.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:17.674
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  7 05:29:17.677: INFO: Got root ca configmap in namespace "svcaccounts-5020"
Sep  7 05:29:17.680: INFO: Deleted root ca configmap in namespace "svcaccounts-5020"
STEP: waiting for a new root ca configmap created 09/07/23 05:29:18.181
Sep  7 05:29:18.183: INFO: Recreated root ca configmap in namespace "svcaccounts-5020"
Sep  7 05:29:18.186: INFO: Updated root ca configmap in namespace "svcaccounts-5020"
STEP: waiting for the root ca configmap reconciled 09/07/23 05:29:18.687
Sep  7 05:29:18.689: INFO: Reconciled root ca configmap in namespace "svcaccounts-5020"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:18.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5020" for this suite. 09/07/23 05:29:18.692
------------------------------
â€¢ [1.031 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:17.665
    Sep  7 05:29:17.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:29:17.666
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:17.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:17.674
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  7 05:29:17.677: INFO: Got root ca configmap in namespace "svcaccounts-5020"
    Sep  7 05:29:17.680: INFO: Deleted root ca configmap in namespace "svcaccounts-5020"
    STEP: waiting for a new root ca configmap created 09/07/23 05:29:18.181
    Sep  7 05:29:18.183: INFO: Recreated root ca configmap in namespace "svcaccounts-5020"
    Sep  7 05:29:18.186: INFO: Updated root ca configmap in namespace "svcaccounts-5020"
    STEP: waiting for the root ca configmap reconciled 09/07/23 05:29:18.687
    Sep  7 05:29:18.689: INFO: Reconciled root ca configmap in namespace "svcaccounts-5020"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:18.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5020" for this suite. 09/07/23 05:29:18.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:18.697
Sep  7 05:29:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename containers 09/07/23 05:29:18.697
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:18.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:18.708
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  7 05:29:18.716: INFO: Waiting up to 5m0s for pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3" in namespace "containers-5849" to be "running"
Sep  7 05:29:18.718: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30143ms
Sep  7 05:29:20.721: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004380854s
Sep  7 05:29:20.721: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:20.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5849" for this suite. 09/07/23 05:29:20.726
------------------------------
â€¢ [2.034 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:18.697
    Sep  7 05:29:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename containers 09/07/23 05:29:18.697
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:18.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:18.708
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  7 05:29:18.716: INFO: Waiting up to 5m0s for pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3" in namespace "containers-5849" to be "running"
    Sep  7 05:29:18.718: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30143ms
    Sep  7 05:29:20.721: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004380854s
    Sep  7 05:29:20.721: INFO: Pod "client-containers-849631a6-59b8-45db-93a3-2d01372560e3" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:20.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5849" for this suite. 09/07/23 05:29:20.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:20.731
Sep  7 05:29:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 05:29:20.731
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:20.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:20.742
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  7 05:29:20.743: INFO: Creating deployment "test-recreate-deployment"
Sep  7 05:29:20.746: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  7 05:29:20.751: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  7 05:29:22.756: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  7 05:29:22.757: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  7 05:29:22.764: INFO: Updating deployment test-recreate-deployment
Sep  7 05:29:22.764: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 05:29:22.819: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-528  d4f8ca38-81d0-420f-87b7-9963ee6f3406 8258 2 2023-09-07 05:29:20 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012bbe78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-07 05:29:22 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-07 05:29:22 +0000 UTC,LastTransitionTime:2023-09-07 05:29:20 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  7 05:29:22.820: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-528  55b80baa-e0f2-4e74-8ed3-e304c6b0a856 8255 1 2023-09-07 05:29:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d4f8ca38-81d0-420f-87b7-9963ee6f3406 0xc0051a86a0 0xc0051a86a1}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4f8ca38-81d0-420f-87b7-9963ee6f3406\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a8738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:29:22.820: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  7 05:29:22.820: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-528  0a4caa53-b397-4cde-997b-d808f08319e8 8246 2 2023-09-07 05:29:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d4f8ca38-81d0-420f-87b7-9963ee6f3406 0xc0051a8597 0xc0051a8598}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4f8ca38-81d0-420f-87b7-9963ee6f3406\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a8648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:29:22.822: INFO: Pod "test-recreate-deployment-cff6dc657-jgknc" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-jgknc test-recreate-deployment-cff6dc657- deployment-528  bb4eed58-6ffa-4210-8647-a892bef555e1 8257 0 2023-09-07 05:29:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 55b80baa-e0f2-4e74-8ed3-e304c6b0a856 0xc0051a8bb0 0xc0051a8bb1}] [] [{kube-controller-manager Update v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55b80baa-e0f2-4e74-8ed3-e304c6b0a856\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87rjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87rjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:,StartTime:2023-09-07 05:29:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:22.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-528" for this suite. 09/07/23 05:29:22.824
------------------------------
â€¢ [2.096 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:20.731
    Sep  7 05:29:20.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 05:29:20.731
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:20.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:20.742
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  7 05:29:20.743: INFO: Creating deployment "test-recreate-deployment"
    Sep  7 05:29:20.746: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  7 05:29:20.751: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Sep  7 05:29:22.756: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  7 05:29:22.757: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  7 05:29:22.764: INFO: Updating deployment test-recreate-deployment
    Sep  7 05:29:22.764: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 05:29:22.819: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-528  d4f8ca38-81d0-420f-87b7-9963ee6f3406 8258 2 2023-09-07 05:29:20 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0012bbe78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-07 05:29:22 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-07 05:29:22 +0000 UTC,LastTransitionTime:2023-09-07 05:29:20 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  7 05:29:22.820: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-528  55b80baa-e0f2-4e74-8ed3-e304c6b0a856 8255 1 2023-09-07 05:29:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d4f8ca38-81d0-420f-87b7-9963ee6f3406 0xc0051a86a0 0xc0051a86a1}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4f8ca38-81d0-420f-87b7-9963ee6f3406\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a8738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:29:22.820: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  7 05:29:22.820: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-528  0a4caa53-b397-4cde-997b-d808f08319e8 8246 2 2023-09-07 05:29:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d4f8ca38-81d0-420f-87b7-9963ee6f3406 0xc0051a8597 0xc0051a8598}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4f8ca38-81d0-420f-87b7-9963ee6f3406\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051a8648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:29:22.822: INFO: Pod "test-recreate-deployment-cff6dc657-jgknc" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-jgknc test-recreate-deployment-cff6dc657- deployment-528  bb4eed58-6ffa-4210-8647-a892bef555e1 8257 0 2023-09-07 05:29:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 55b80baa-e0f2-4e74-8ed3-e304c6b0a856 0xc0051a8bb0 0xc0051a8bb1}] [] [{kube-controller-manager Update v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55b80baa-e0f2-4e74-8ed3-e304c6b0a856\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:29:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87rjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87rjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:29:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:,StartTime:2023-09-07 05:29:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:22.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-528" for this suite. 09/07/23 05:29:22.824
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:22.827
Sep  7 05:29:22.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:29:22.828
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:22.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:22.838
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-19e4ca0c-9b1b-4174-a7f0-abe98daed681 09/07/23 05:29:22.84
STEP: Creating a pod to test consume configMaps 09/07/23 05:29:22.842
Sep  7 05:29:22.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d" in namespace "configmap-6744" to be "Succeeded or Failed"
Sep  7 05:29:22.849: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1929ms
Sep  7 05:29:24.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003848893s
Sep  7 05:29:26.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003702026s
STEP: Saw pod success 09/07/23 05:29:26.852
Sep  7 05:29:26.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d" satisfied condition "Succeeded or Failed"
Sep  7 05:29:26.853: INFO: Trying to get logs from node kind-worker pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d container configmap-volume-test: <nil>
STEP: delete the pod 09/07/23 05:29:26.856
Sep  7 05:29:26.863: INFO: Waiting for pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d to disappear
Sep  7 05:29:26.865: INFO: Pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:29:26.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6744" for this suite. 09/07/23 05:29:26.867
------------------------------
â€¢ [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:22.827
    Sep  7 05:29:22.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:29:22.828
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:22.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:22.838
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-19e4ca0c-9b1b-4174-a7f0-abe98daed681 09/07/23 05:29:22.84
    STEP: Creating a pod to test consume configMaps 09/07/23 05:29:22.842
    Sep  7 05:29:22.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d" in namespace "configmap-6744" to be "Succeeded or Failed"
    Sep  7 05:29:22.849: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.1929ms
    Sep  7 05:29:24.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003848893s
    Sep  7 05:29:26.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003702026s
    STEP: Saw pod success 09/07/23 05:29:26.852
    Sep  7 05:29:26.852: INFO: Pod "pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d" satisfied condition "Succeeded or Failed"
    Sep  7 05:29:26.853: INFO: Trying to get logs from node kind-worker pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d container configmap-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:29:26.856
    Sep  7 05:29:26.863: INFO: Waiting for pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d to disappear
    Sep  7 05:29:26.865: INFO: Pod pod-configmaps-735db9e5-31d7-4efd-8725-fa339ec5667d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:29:26.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6744" for this suite. 09/07/23 05:29:26.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:29:26.87
Sep  7 05:29:26.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 05:29:26.871
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:26.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:26.88
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/07/23 05:29:26.882
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:29:26.884
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:29:26.884
STEP: creating a pod to probe DNS 09/07/23 05:29:26.884
STEP: submitting the pod to kubernetes 09/07/23 05:29:26.885
Sep  7 05:29:26.891: INFO: Waiting up to 15m0s for pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948" in namespace "dns-6411" to be "running"
Sep  7 05:29:26.892: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948": Phase="Pending", Reason="", readiness=false. Elapsed: 1.21525ms
Sep  7 05:29:28.895: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948": Phase="Running", Reason="", readiness=true. Elapsed: 2.004316862s
Sep  7 05:29:28.895: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:29:28.895
STEP: looking for the results for each expected name from probers 09/07/23 05:29:28.897
Sep  7 05:29:28.903: INFO: DNS probes using dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948 succeeded

STEP: deleting the pod 09/07/23 05:29:28.903
STEP: changing the externalName to bar.example.com 09/07/23 05:29:28.911
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:29:28.916
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:29:28.916
STEP: creating a second pod to probe DNS 09/07/23 05:29:28.916
STEP: submitting the pod to kubernetes 09/07/23 05:29:28.916
Sep  7 05:29:28.920: INFO: Waiting up to 15m0s for pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6" in namespace "dns-6411" to be "running"
Sep  7 05:29:28.921: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25056ms
Sep  7 05:29:30.924: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004214822s
Sep  7 05:29:30.924: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:29:30.924
STEP: looking for the results for each expected name from probers 09/07/23 05:29:30.926
Sep  7 05:29:30.928: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:30.931: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:30.931: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:29:35.936: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains '' instead of 'bar.example.com.'
Sep  7 05:29:35.938: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:35.938: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:29:40.937: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:40.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:40.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:29:45.936: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:45.937: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:45.937: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:29:50.938: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:50.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:50.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:29:55.937: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:55.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep  7 05:29:55.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

Sep  7 05:30:00.938: INFO: DNS probes using dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 succeeded

STEP: deleting the pod 09/07/23 05:30:00.938
STEP: changing the service to type=ClusterIP 09/07/23 05:30:00.948
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:30:00.957
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
 09/07/23 05:30:00.957
STEP: creating a third pod to probe DNS 09/07/23 05:30:00.957
STEP: submitting the pod to kubernetes 09/07/23 05:30:00.958
Sep  7 05:30:00.965: INFO: Waiting up to 15m0s for pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7" in namespace "dns-6411" to be "running"
Sep  7 05:30:00.968: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358ms
Sep  7 05:30:02.972: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006795626s
Sep  7 05:30:02.972: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:30:02.972
STEP: looking for the results for each expected name from probers 09/07/23 05:30:02.973
Sep  7 05:30:02.978: INFO: DNS probes using dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7 succeeded

STEP: deleting the pod 09/07/23 05:30:02.978
STEP: deleting the test externalName service 09/07/23 05:30:02.986
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 05:30:02.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6411" for this suite. 09/07/23 05:30:02.996
------------------------------
â€¢ [SLOW TEST] [36.130 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:29:26.87
    Sep  7 05:29:26.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 05:29:26.871
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:29:26.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:29:26.88
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/07/23 05:29:26.882
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:29:26.884
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:29:26.884
    STEP: creating a pod to probe DNS 09/07/23 05:29:26.884
    STEP: submitting the pod to kubernetes 09/07/23 05:29:26.885
    Sep  7 05:29:26.891: INFO: Waiting up to 15m0s for pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948" in namespace "dns-6411" to be "running"
    Sep  7 05:29:26.892: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948": Phase="Pending", Reason="", readiness=false. Elapsed: 1.21525ms
    Sep  7 05:29:28.895: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948": Phase="Running", Reason="", readiness=true. Elapsed: 2.004316862s
    Sep  7 05:29:28.895: INFO: Pod "dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:29:28.895
    STEP: looking for the results for each expected name from probers 09/07/23 05:29:28.897
    Sep  7 05:29:28.903: INFO: DNS probes using dns-test-cdfc7926-38e1-4dde-b4ae-af22051d4948 succeeded

    STEP: deleting the pod 09/07/23 05:29:28.903
    STEP: changing the externalName to bar.example.com 09/07/23 05:29:28.911
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:29:28.916
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:29:28.916
    STEP: creating a second pod to probe DNS 09/07/23 05:29:28.916
    STEP: submitting the pod to kubernetes 09/07/23 05:29:28.916
    Sep  7 05:29:28.920: INFO: Waiting up to 15m0s for pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6" in namespace "dns-6411" to be "running"
    Sep  7 05:29:28.921: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25056ms
    Sep  7 05:29:30.924: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004214822s
    Sep  7 05:29:30.924: INFO: Pod "dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:29:30.924
    STEP: looking for the results for each expected name from probers 09/07/23 05:29:30.926
    Sep  7 05:29:30.928: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:30.931: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:30.931: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:29:35.936: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains '' instead of 'bar.example.com.'
    Sep  7 05:29:35.938: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:35.938: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:29:40.937: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:40.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:40.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:29:45.936: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:45.937: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:45.937: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:29:50.938: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:50.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:50.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:29:55.937: INFO: File wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:55.939: INFO: File jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local from pod  dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Sep  7 05:29:55.939: INFO: Lookups using dns-6411/dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 failed for: [wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local]

    Sep  7 05:30:00.938: INFO: DNS probes using dns-test-22291b71-4a5b-48de-acf0-291c3728e2e6 succeeded

    STEP: deleting the pod 09/07/23 05:30:00.938
    STEP: changing the service to type=ClusterIP 09/07/23 05:30:00.948
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:30:00.957
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6411.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6411.svc.cluster.local; sleep 1; done
     09/07/23 05:30:00.957
    STEP: creating a third pod to probe DNS 09/07/23 05:30:00.957
    STEP: submitting the pod to kubernetes 09/07/23 05:30:00.958
    Sep  7 05:30:00.965: INFO: Waiting up to 15m0s for pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7" in namespace "dns-6411" to be "running"
    Sep  7 05:30:00.968: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.358ms
    Sep  7 05:30:02.972: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006795626s
    Sep  7 05:30:02.972: INFO: Pod "dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:30:02.972
    STEP: looking for the results for each expected name from probers 09/07/23 05:30:02.973
    Sep  7 05:30:02.978: INFO: DNS probes using dns-test-55b79ed3-73c5-4d12-ac65-5433f1e2ccf7 succeeded

    STEP: deleting the pod 09/07/23 05:30:02.978
    STEP: deleting the test externalName service 09/07/23 05:30:02.986
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:30:02.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6411" for this suite. 09/07/23 05:30:02.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:30:03.002
Sep  7 05:30:03.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-runtime 09/07/23 05:30:03.003
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:30:03.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:30:03.012
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/07/23 05:30:03.014
STEP: wait for the container to reach Succeeded 09/07/23 05:30:03.018
STEP: get the container status 09/07/23 05:30:07.029
STEP: the container should be terminated 09/07/23 05:30:07.031
STEP: the termination message should be set 09/07/23 05:30:07.031
Sep  7 05:30:07.031: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/07/23 05:30:07.031
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  7 05:30:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-878" for this suite. 09/07/23 05:30:07.046
------------------------------
â€¢ [4.047 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:30:03.002
    Sep  7 05:30:03.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-runtime 09/07/23 05:30:03.003
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:30:03.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:30:03.012
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/07/23 05:30:03.014
    STEP: wait for the container to reach Succeeded 09/07/23 05:30:03.018
    STEP: get the container status 09/07/23 05:30:07.029
    STEP: the container should be terminated 09/07/23 05:30:07.031
    STEP: the termination message should be set 09/07/23 05:30:07.031
    Sep  7 05:30:07.031: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/07/23 05:30:07.031
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:30:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-878" for this suite. 09/07/23 05:30:07.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:30:07.049
Sep  7 05:30:07.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename cronjob 09/07/23 05:30:07.05
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:30:07.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:30:07.059
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/07/23 05:30:07.061
STEP: Ensuring no jobs are scheduled 09/07/23 05:30:07.063
STEP: Ensuring no job exists by listing jobs explicitly 09/07/23 05:35:07.068
STEP: Removing cronjob 09/07/23 05:35:07.069
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  7 05:35:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3958" for this suite. 09/07/23 05:35:07.075
------------------------------
â€¢ [SLOW TEST] [300.029 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:30:07.049
    Sep  7 05:30:07.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename cronjob 09/07/23 05:30:07.05
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:30:07.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:30:07.059
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/07/23 05:30:07.061
    STEP: Ensuring no jobs are scheduled 09/07/23 05:30:07.063
    STEP: Ensuring no job exists by listing jobs explicitly 09/07/23 05:35:07.068
    STEP: Removing cronjob 09/07/23 05:35:07.069
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:35:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3958" for this suite. 09/07/23 05:35:07.075
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:35:07.078
Sep  7 05:35:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:35:07.079
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:07.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:07.088
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-66217569-aa41-485d-bafb-4722cfdb660a 09/07/23 05:35:07.089
STEP: Creating a pod to test consume configMaps 09/07/23 05:35:07.092
Sep  7 05:35:07.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125" in namespace "configmap-5862" to be "Succeeded or Failed"
Sep  7 05:35:07.098: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33227ms
Sep  7 05:35:09.101: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Running", Reason="", readiness=false. Elapsed: 2.004268075s
Sep  7 05:35:11.102: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005418659s
STEP: Saw pod success 09/07/23 05:35:11.102
Sep  7 05:35:11.102: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125" satisfied condition "Succeeded or Failed"
Sep  7 05:35:11.104: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:35:11.113
Sep  7 05:35:11.122: INFO: Waiting for pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 to disappear
Sep  7 05:35:11.123: INFO: Pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:35:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5862" for this suite. 09/07/23 05:35:11.125
------------------------------
â€¢ [4.050 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:35:07.078
    Sep  7 05:35:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:35:07.079
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:07.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:07.088
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-66217569-aa41-485d-bafb-4722cfdb660a 09/07/23 05:35:07.089
    STEP: Creating a pod to test consume configMaps 09/07/23 05:35:07.092
    Sep  7 05:35:07.096: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125" in namespace "configmap-5862" to be "Succeeded or Failed"
    Sep  7 05:35:07.098: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33227ms
    Sep  7 05:35:09.101: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Running", Reason="", readiness=false. Elapsed: 2.004268075s
    Sep  7 05:35:11.102: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005418659s
    STEP: Saw pod success 09/07/23 05:35:11.102
    Sep  7 05:35:11.102: INFO: Pod "pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125" satisfied condition "Succeeded or Failed"
    Sep  7 05:35:11.104: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:35:11.113
    Sep  7 05:35:11.122: INFO: Waiting for pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 to disappear
    Sep  7 05:35:11.123: INFO: Pod pod-configmaps-b7b102b3-c12e-4cbd-946d-f0eb20187125 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:35:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5862" for this suite. 09/07/23 05:35:11.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:35:11.128
Sep  7 05:35:11.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:35:11.129
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:11.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:11.138
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/07/23 05:35:11.139
Sep  7 05:35:11.143: INFO: Waiting up to 5m0s for pod "pod-52df0364-7af2-4004-8911-0af7fb19907b" in namespace "emptydir-5735" to be "Succeeded or Failed"
Sep  7 05:35:11.145: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23325ms
Sep  7 05:35:13.147: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003481724s
Sep  7 05:35:15.148: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004042168s
STEP: Saw pod success 09/07/23 05:35:15.148
Sep  7 05:35:15.148: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b" satisfied condition "Succeeded or Failed"
Sep  7 05:35:15.149: INFO: Trying to get logs from node kind-worker pod pod-52df0364-7af2-4004-8911-0af7fb19907b container test-container: <nil>
STEP: delete the pod 09/07/23 05:35:15.158
Sep  7 05:35:15.168: INFO: Waiting for pod pod-52df0364-7af2-4004-8911-0af7fb19907b to disappear
Sep  7 05:35:15.169: INFO: Pod pod-52df0364-7af2-4004-8911-0af7fb19907b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:35:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5735" for this suite. 09/07/23 05:35:15.171
------------------------------
â€¢ [4.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:35:11.128
    Sep  7 05:35:11.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:35:11.129
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:11.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:11.138
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/07/23 05:35:11.139
    Sep  7 05:35:11.143: INFO: Waiting up to 5m0s for pod "pod-52df0364-7af2-4004-8911-0af7fb19907b" in namespace "emptydir-5735" to be "Succeeded or Failed"
    Sep  7 05:35:11.145: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23325ms
    Sep  7 05:35:13.147: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003481724s
    Sep  7 05:35:15.148: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004042168s
    STEP: Saw pod success 09/07/23 05:35:15.148
    Sep  7 05:35:15.148: INFO: Pod "pod-52df0364-7af2-4004-8911-0af7fb19907b" satisfied condition "Succeeded or Failed"
    Sep  7 05:35:15.149: INFO: Trying to get logs from node kind-worker pod pod-52df0364-7af2-4004-8911-0af7fb19907b container test-container: <nil>
    STEP: delete the pod 09/07/23 05:35:15.158
    Sep  7 05:35:15.168: INFO: Waiting for pod pod-52df0364-7af2-4004-8911-0af7fb19907b to disappear
    Sep  7 05:35:15.169: INFO: Pod pod-52df0364-7af2-4004-8911-0af7fb19907b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:35:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5735" for this suite. 09/07/23 05:35:15.171
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:35:15.174
Sep  7 05:35:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 05:35:15.175
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:15.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:15.183
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  7 05:35:15.189: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  7 05:35:20.193: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 05:35:20.193
Sep  7 05:35:20.193: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  7 05:35:22.195: INFO: Creating deployment "test-rollover-deployment"
Sep  7 05:35:22.201: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  7 05:35:24.206: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  7 05:35:24.210: INFO: Ensure that both replica sets have 1 created replica
Sep  7 05:35:24.213: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  7 05:35:24.219: INFO: Updating deployment test-rollover-deployment
Sep  7 05:35:24.219: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  7 05:35:26.223: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  7 05:35:26.226: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  7 05:35:26.229: INFO: all replica sets need to contain the pod-template-hash label
Sep  7 05:35:26.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:35:28.234: INFO: all replica sets need to contain the pod-template-hash label
Sep  7 05:35:28.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:35:30.235: INFO: all replica sets need to contain the pod-template-hash label
Sep  7 05:35:30.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:35:32.233: INFO: all replica sets need to contain the pod-template-hash label
Sep  7 05:35:32.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:35:34.235: INFO: all replica sets need to contain the pod-template-hash label
Sep  7 05:35:34.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  7 05:35:36.235: INFO: 
Sep  7 05:35:36.235: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 05:35:36.239: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6871  512e46b7-052d-4f4a-9b5b-e2351b0a5356 9157 2 2023-09-07 05:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044533e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 05:35:22 +0000 UTC,LastTransitionTime:2023-09-07 05:35:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-07 05:35:35 +0000 UTC,LastTransitionTime:2023-09-07 05:35:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  7 05:35:36.241: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6871  e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2 9147 2 2023-09-07 05:35:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc005666557 0xc005666558}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005666608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:35:36.241: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  7 05:35:36.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6871  5e929cc5-31e4-4995-abc5-454f2f15fbae 9156 2 2023-09-07 05:35:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc00566641f 0xc005666430}] [] [{e2e.test Update apps/v1 2023-09-07 05:35:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0056664e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:35:36.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6871  81d718ef-7436-4f62-8026-b8c442a2d23a 9117 2 2023-09-07 05:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc005666677 0xc005666678}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005666728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:35:36.242: INFO: Pod "test-rollover-deployment-6c6df9974f-vfksr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-vfksr test-rollover-deployment-6c6df9974f- deployment-6871  b5bc17fe-37ce-4116-b99e-97ff91a4a864 9129 0 2023-09-07 05:35:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2 0xc005666ca7 0xc005666ca8}] [] [{kube-controller-manager Update v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:35:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vpls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vpls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.68,StartTime:2023-09-07 05:35:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:35:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5f2ff577695e9df53b7dbb0bd7814c3090f965644bf240c058b9c4cae1ceda07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 05:35:36.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6871" for this suite. 09/07/23 05:35:36.244
------------------------------
â€¢ [SLOW TEST] [21.073 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:35:15.174
    Sep  7 05:35:15.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 05:35:15.175
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:15.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:15.183
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  7 05:35:15.189: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  7 05:35:20.193: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 05:35:20.193
    Sep  7 05:35:20.193: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  7 05:35:22.195: INFO: Creating deployment "test-rollover-deployment"
    Sep  7 05:35:22.201: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  7 05:35:24.206: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  7 05:35:24.210: INFO: Ensure that both replica sets have 1 created replica
    Sep  7 05:35:24.213: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  7 05:35:24.219: INFO: Updating deployment test-rollover-deployment
    Sep  7 05:35:24.219: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  7 05:35:26.223: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  7 05:35:26.226: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  7 05:35:26.229: INFO: all replica sets need to contain the pod-template-hash label
    Sep  7 05:35:26.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:35:28.234: INFO: all replica sets need to contain the pod-template-hash label
    Sep  7 05:35:28.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:35:30.235: INFO: all replica sets need to contain the pod-template-hash label
    Sep  7 05:35:30.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:35:32.233: INFO: all replica sets need to contain the pod-template-hash label
    Sep  7 05:35:32.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:35:34.235: INFO: all replica sets need to contain the pod-template-hash label
    Sep  7 05:35:34.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 5, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 5, 35, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  7 05:35:36.235: INFO: 
    Sep  7 05:35:36.235: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 05:35:36.239: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6871  512e46b7-052d-4f4a-9b5b-e2351b0a5356 9157 2 2023-09-07 05:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044533e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 05:35:22 +0000 UTC,LastTransitionTime:2023-09-07 05:35:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-07 05:35:35 +0000 UTC,LastTransitionTime:2023-09-07 05:35:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  7 05:35:36.241: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-6871  e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2 9147 2 2023-09-07 05:35:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc005666557 0xc005666558}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005666608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:35:36.241: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  7 05:35:36.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6871  5e929cc5-31e4-4995-abc5-454f2f15fbae 9156 2 2023-09-07 05:35:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc00566641f 0xc005666430}] [] [{e2e.test Update apps/v1 2023-09-07 05:35:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0056664e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:35:36.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6871  81d718ef-7436-4f62-8026-b8c442a2d23a 9117 2 2023-09-07 05:35:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 512e46b7-052d-4f4a-9b5b-e2351b0a5356 0xc005666677 0xc005666678}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"512e46b7-052d-4f4a-9b5b-e2351b0a5356\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005666728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:35:36.242: INFO: Pod "test-rollover-deployment-6c6df9974f-vfksr" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-vfksr test-rollover-deployment-6c6df9974f- deployment-6871  b5bc17fe-37ce-4116-b99e-97ff91a4a864 9129 0 2023-09-07 05:35:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2 0xc005666ca7 0xc005666ca8}] [] [{kube-controller-manager Update v1 2023-09-07 05:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2dd3e2a-fc0e-4f7f-98e3-da85c6bc36b2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:35:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vpls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vpls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:35:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.68,StartTime:2023-09-07 05:35:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:35:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5f2ff577695e9df53b7dbb0bd7814c3090f965644bf240c058b9c4cae1ceda07,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:35:36.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6871" for this suite. 09/07/23 05:35:36.244
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:35:36.248
Sep  7 05:35:36.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:35:36.248
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:36.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:36.257
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3156 09/07/23 05:35:36.258
STEP: creating service affinity-clusterip-transition in namespace services-3156 09/07/23 05:35:36.258
STEP: creating replication controller affinity-clusterip-transition in namespace services-3156 09/07/23 05:35:36.266
I0907 05:35:36.269330      29 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3156, replica count: 3
I0907 05:35:39.321164      29 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 05:35:39.324: INFO: Creating new exec pod
Sep  7 05:35:39.328: INFO: Waiting up to 5m0s for pod "execpod-affinityl9w4d" in namespace "services-3156" to be "running"
Sep  7 05:35:39.330: INFO: Pod "execpod-affinityl9w4d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80836ms
Sep  7 05:35:41.333: INFO: Pod "execpod-affinityl9w4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005138162s
Sep  7 05:35:41.333: INFO: Pod "execpod-affinityl9w4d" satisfied condition "running"
Sep  7 05:35:42.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  7 05:35:42.503: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  7 05:35:42.503: INFO: stdout: ""
Sep  7 05:35:42.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c nc -v -z -w 2 10.96.99.175 80'
Sep  7 05:35:42.662: INFO: stderr: "+ nc -v -z -w 2 10.96.99.175 80\nConnection to 10.96.99.175 80 port [tcp/http] succeeded!\n"
Sep  7 05:35:42.662: INFO: stdout: ""
Sep  7 05:35:42.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
Sep  7 05:35:42.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
Sep  7 05:35:42.910: INFO: stdout: "\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v"
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:42.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
Sep  7 05:35:43.148: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
Sep  7 05:35:43.148: INFO: stdout: "\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg"
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
Sep  7 05:36:13.386: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
Sep  7 05:36:13.386: INFO: stdout: "\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg"
Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
Sep  7 05:36:13.387: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3156, will wait for the garbage collector to delete the pods 09/07/23 05:36:13.393
Sep  7 05:36:13.449: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.61617ms
Sep  7 05:36:13.550: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.257385ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:17.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3156" for this suite. 09/07/23 05:36:17.062
------------------------------
â€¢ [SLOW TEST] [40.818 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:35:36.248
    Sep  7 05:35:36.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:35:36.248
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:35:36.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:35:36.257
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3156 09/07/23 05:35:36.258
    STEP: creating service affinity-clusterip-transition in namespace services-3156 09/07/23 05:35:36.258
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3156 09/07/23 05:35:36.266
    I0907 05:35:36.269330      29 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3156, replica count: 3
    I0907 05:35:39.321164      29 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 05:35:39.324: INFO: Creating new exec pod
    Sep  7 05:35:39.328: INFO: Waiting up to 5m0s for pod "execpod-affinityl9w4d" in namespace "services-3156" to be "running"
    Sep  7 05:35:39.330: INFO: Pod "execpod-affinityl9w4d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80836ms
    Sep  7 05:35:41.333: INFO: Pod "execpod-affinityl9w4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005138162s
    Sep  7 05:35:41.333: INFO: Pod "execpod-affinityl9w4d" satisfied condition "running"
    Sep  7 05:35:42.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  7 05:35:42.503: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  7 05:35:42.503: INFO: stdout: ""
    Sep  7 05:35:42.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c nc -v -z -w 2 10.96.99.175 80'
    Sep  7 05:35:42.662: INFO: stderr: "+ nc -v -z -w 2 10.96.99.175 80\nConnection to 10.96.99.175 80 port [tcp/http] succeeded!\n"
    Sep  7 05:35:42.662: INFO: stdout: ""
    Sep  7 05:35:42.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
    Sep  7 05:35:42.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
    Sep  7 05:35:42.910: INFO: stdout: "\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v"
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:42.910: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:42.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
    Sep  7 05:35:43.148: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
    Sep  7 05:35:43.148: INFO: stdout: "\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-ml22v\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-bqwd2\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg"
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-ml22v
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-bqwd2
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:35:43.148: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3156 exec execpod-affinityl9w4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.99.175:80/ ; done'
    Sep  7 05:36:13.386: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.99.175:80/\n"
    Sep  7 05:36:13.386: INFO: stdout: "\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg\naffinity-clusterip-transition-6r9zg"
    Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.386: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Received response from host: affinity-clusterip-transition-6r9zg
    Sep  7 05:36:13.387: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3156, will wait for the garbage collector to delete the pods 09/07/23 05:36:13.393
    Sep  7 05:36:13.449: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.61617ms
    Sep  7 05:36:13.550: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.257385ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:17.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3156" for this suite. 09/07/23 05:36:17.062
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:17.066
Sep  7 05:36:17.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:36:17.067
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:17.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:17.077
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-87cbaa19-934d-4db5-8dfe-c17b736a0927 09/07/23 05:36:17.079
STEP: Creating a pod to test consume secrets 09/07/23 05:36:17.082
Sep  7 05:36:17.089: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf" in namespace "projected-4425" to be "Succeeded or Failed"
Sep  7 05:36:17.091: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.771789ms
Sep  7 05:36:19.093: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004425717s
Sep  7 05:36:21.094: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00564024s
STEP: Saw pod success 09/07/23 05:36:21.094
Sep  7 05:36:21.094: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf" satisfied condition "Succeeded or Failed"
Sep  7 05:36:21.096: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf container projected-secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:36:21.1
Sep  7 05:36:21.108: INFO: Waiting for pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf to disappear
Sep  7 05:36:21.109: INFO: Pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:21.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4425" for this suite. 09/07/23 05:36:21.111
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:17.066
    Sep  7 05:36:17.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:36:17.067
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:17.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:17.077
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-87cbaa19-934d-4db5-8dfe-c17b736a0927 09/07/23 05:36:17.079
    STEP: Creating a pod to test consume secrets 09/07/23 05:36:17.082
    Sep  7 05:36:17.089: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf" in namespace "projected-4425" to be "Succeeded or Failed"
    Sep  7 05:36:17.091: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.771789ms
    Sep  7 05:36:19.093: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004425717s
    Sep  7 05:36:21.094: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00564024s
    STEP: Saw pod success 09/07/23 05:36:21.094
    Sep  7 05:36:21.094: INFO: Pod "pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf" satisfied condition "Succeeded or Failed"
    Sep  7 05:36:21.096: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:36:21.1
    Sep  7 05:36:21.108: INFO: Waiting for pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf to disappear
    Sep  7 05:36:21.109: INFO: Pod pod-projected-secrets-4ef30751-4738-4732-abf8-16e71a4b2cdf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:21.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4425" for this suite. 09/07/23 05:36:21.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:21.114
Sep  7 05:36:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:36:21.115
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:21.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:21.125
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-34b225cb-1851-48f8-9e19-81074ece2aa7 09/07/23 05:36:21.126
STEP: Creating a pod to test consume configMaps 09/07/23 05:36:21.129
Sep  7 05:36:21.132: INFO: Waiting up to 5m0s for pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde" in namespace "configmap-4560" to be "Succeeded or Failed"
Sep  7 05:36:21.134: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29884ms
Sep  7 05:36:23.136: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003835985s
Sep  7 05:36:25.138: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005328184s
STEP: Saw pod success 09/07/23 05:36:25.138
Sep  7 05:36:25.138: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde" satisfied condition "Succeeded or Failed"
Sep  7 05:36:25.140: INFO: Trying to get logs from node kind-worker pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:36:25.143
Sep  7 05:36:25.152: INFO: Waiting for pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde to disappear
Sep  7 05:36:25.153: INFO: Pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4560" for this suite. 09/07/23 05:36:25.155
------------------------------
â€¢ [4.046 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:21.114
    Sep  7 05:36:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:36:21.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:21.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:21.125
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-34b225cb-1851-48f8-9e19-81074ece2aa7 09/07/23 05:36:21.126
    STEP: Creating a pod to test consume configMaps 09/07/23 05:36:21.129
    Sep  7 05:36:21.132: INFO: Waiting up to 5m0s for pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde" in namespace "configmap-4560" to be "Succeeded or Failed"
    Sep  7 05:36:21.134: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29884ms
    Sep  7 05:36:23.136: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003835985s
    Sep  7 05:36:25.138: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005328184s
    STEP: Saw pod success 09/07/23 05:36:25.138
    Sep  7 05:36:25.138: INFO: Pod "pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde" satisfied condition "Succeeded or Failed"
    Sep  7 05:36:25.140: INFO: Trying to get logs from node kind-worker pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:36:25.143
    Sep  7 05:36:25.152: INFO: Waiting for pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde to disappear
    Sep  7 05:36:25.153: INFO: Pod pod-configmaps-97d8dfd6-b4fe-4096-92d9-20ec71a2acde no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4560" for this suite. 09/07/23 05:36:25.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:25.16
Sep  7 05:36:25.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 05:36:25.161
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:25.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:25.17
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/07/23 05:36:25.171
Sep  7 05:36:25.178: INFO: Waiting up to 5m0s for pod "pod-tpmsr" in namespace "pods-6124" to be "running"
Sep  7 05:36:25.180: INFO: Pod "pod-tpmsr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666989ms
Sep  7 05:36:27.183: INFO: Pod "pod-tpmsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073801s
Sep  7 05:36:27.183: INFO: Pod "pod-tpmsr" satisfied condition "running"
STEP: patching /status 09/07/23 05:36:27.183
Sep  7 05:36:27.189: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:27.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6124" for this suite. 09/07/23 05:36:27.192
------------------------------
â€¢ [2.037 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:25.16
    Sep  7 05:36:25.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 05:36:25.161
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:25.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:25.17
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/07/23 05:36:25.171
    Sep  7 05:36:25.178: INFO: Waiting up to 5m0s for pod "pod-tpmsr" in namespace "pods-6124" to be "running"
    Sep  7 05:36:25.180: INFO: Pod "pod-tpmsr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666989ms
    Sep  7 05:36:27.183: INFO: Pod "pod-tpmsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073801s
    Sep  7 05:36:27.183: INFO: Pod "pod-tpmsr" satisfied condition "running"
    STEP: patching /status 09/07/23 05:36:27.183
    Sep  7 05:36:27.189: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:27.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6124" for this suite. 09/07/23 05:36:27.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:27.198
Sep  7 05:36:27.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:36:27.199
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:27.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:27.213
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:36:27.221
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:36:27.607
STEP: Deploying the webhook pod 09/07/23 05:36:27.614
STEP: Wait for the deployment to be ready 09/07/23 05:36:27.623
Sep  7 05:36:27.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:36:29.635
STEP: Verifying the service has paired with the endpoint 09/07/23 05:36:29.643
Sep  7 05:36:30.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/07/23 05:36:30.646
STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.658
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/07/23 05:36:30.663
STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.67
STEP: Patching a validating webhook configuration's rules to include the create operation 09/07/23 05:36:30.675
STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.681
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:30.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4368" for this suite. 09/07/23 05:36:30.714
STEP: Destroying namespace "webhook-4368-markers" for this suite. 09/07/23 05:36:30.719
------------------------------
â€¢ [3.525 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:27.198
    Sep  7 05:36:27.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:36:27.199
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:27.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:27.213
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:36:27.221
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:36:27.607
    STEP: Deploying the webhook pod 09/07/23 05:36:27.614
    STEP: Wait for the deployment to be ready 09/07/23 05:36:27.623
    Sep  7 05:36:27.628: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:36:29.635
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:36:29.643
    Sep  7 05:36:30.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/07/23 05:36:30.646
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.658
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/07/23 05:36:30.663
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.67
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/07/23 05:36:30.675
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 05:36:30.681
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:30.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4368" for this suite. 09/07/23 05:36:30.714
    STEP: Destroying namespace "webhook-4368-markers" for this suite. 09/07/23 05:36:30.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:30.724
Sep  7 05:36:30.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 05:36:30.725
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:30.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:30.735
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 09/07/23 05:36:30.746
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:36:30.749
Sep  7 05:36:30.751: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:30.755: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:36:30.755: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:31.758: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:31.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:36:31.760: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:32.758: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:32.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:36:32.760: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/07/23 05:36:32.762
Sep  7 05:36:32.771: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:32.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:36:32.773: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:33.775: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:33.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:36:33.777: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:34.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:34.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:36:34.778: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:35.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:35.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:36:35.778: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:36.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:36.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 05:36:36.779: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:36:37.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:36:37.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 05:36:37.778: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:36:37.78
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7037, will wait for the garbage collector to delete the pods 09/07/23 05:36:37.78
Sep  7 05:36:37.836: INFO: Deleting DaemonSet.extensions daemon-set took: 3.278559ms
Sep  7 05:36:37.936: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.635755ms
Sep  7 05:36:41.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 05:36:41.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 05:36:41.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9607"},"items":null}

Sep  7 05:36:41.141: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9607"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:36:41.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7037" for this suite. 09/07/23 05:36:41.147
------------------------------
â€¢ [SLOW TEST] [10.428 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:30.724
    Sep  7 05:36:30.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 05:36:30.725
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:30.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:30.735
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 09/07/23 05:36:30.746
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:36:30.749
    Sep  7 05:36:30.751: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:30.755: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:36:30.755: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:31.758: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:31.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:36:31.760: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:32.758: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:32.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:36:32.760: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/07/23 05:36:32.762
    Sep  7 05:36:32.771: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:32.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:36:32.773: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:33.775: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:33.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:36:33.777: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:34.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:34.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:36:34.778: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:35.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:35.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:36:35.778: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:36.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:36.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 05:36:36.779: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:36:37.776: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:36:37.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 05:36:37.778: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 05:36:37.78
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7037, will wait for the garbage collector to delete the pods 09/07/23 05:36:37.78
    Sep  7 05:36:37.836: INFO: Deleting DaemonSet.extensions daemon-set took: 3.278559ms
    Sep  7 05:36:37.936: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.635755ms
    Sep  7 05:36:41.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 05:36:41.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 05:36:41.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9607"},"items":null}

    Sep  7 05:36:41.141: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9607"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:36:41.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7037" for this suite. 09/07/23 05:36:41.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:36:41.152
Sep  7 05:36:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir-wrapper 09/07/23 05:36:41.153
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:41.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:41.161
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/07/23 05:36:41.162
STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:36:41.406
Sep  7 05:36:41.508: INFO: Pod name wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/07/23 05:36:41.508
Sep  7 05:36:41.508: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:36:41.556: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 47.487719ms
Sep  7 05:36:43.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051116825s
Sep  7 05:36:45.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050785937s
Sep  7 05:36:47.558: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049960872s
Sep  7 05:36:49.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051032409s
Sep  7 05:36:51.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050658769s
Sep  7 05:36:53.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.051955862s
Sep  7 05:36:55.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Running", Reason="", readiness=true. Elapsed: 14.051338649s
Sep  7 05:36:55.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn" satisfied condition "running"
Sep  7 05:36:55.560: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:36:55.561: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8": Phase="Running", Reason="", readiness=true. Elapsed: 1.681479ms
Sep  7 05:36:55.561: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8" satisfied condition "running"
Sep  7 05:36:55.561: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:36:55.563: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh": Phase="Running", Reason="", readiness=true. Elapsed: 1.970239ms
Sep  7 05:36:55.563: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh" satisfied condition "running"
Sep  7 05:36:55.563: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:36:55.565: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.102489ms
Sep  7 05:36:55.565: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp" satisfied condition "running"
Sep  7 05:36:55.565: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:36:55.567: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz": Phase="Running", Reason="", readiness=true. Elapsed: 1.819389ms
Sep  7 05:36:55.567: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:36:55.567
Sep  7 05:36:55.624: INFO: Deleting ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 took: 4.298658ms
Sep  7 05:36:55.724: INFO: Terminating ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 pods took: 100.461718ms
STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:36:59.327
Sep  7 05:36:59.336: INFO: Pod name wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b: Found 0 pods out of 5
Sep  7 05:37:04.342: INFO: Pod name wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/07/23 05:37:04.342
Sep  7 05:37:04.342: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:04.343: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78707ms
Sep  7 05:37:06.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00545846s
Sep  7 05:37:08.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005038086s
Sep  7 05:37:10.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006204023s
Sep  7 05:37:12.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005529695s
Sep  7 05:37:14.346: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Running", Reason="", readiness=true. Elapsed: 10.004485439s
Sep  7 05:37:14.346: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4" satisfied condition "running"
Sep  7 05:37:14.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:14.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs": Phase="Running", Reason="", readiness=true. Elapsed: 1.911069ms
Sep  7 05:37:14.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs" satisfied condition "running"
Sep  7 05:37:14.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:14.350: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.076659ms
Sep  7 05:37:14.350: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2" satisfied condition "running"
Sep  7 05:37:14.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:14.352: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm": Phase="Running", Reason="", readiness=true. Elapsed: 1.74025ms
Sep  7 05:37:14.352: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm" satisfied condition "running"
Sep  7 05:37:14.352: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:14.354: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8": Phase="Running", Reason="", readiness=true. Elapsed: 1.93241ms
Sep  7 05:37:14.354: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:37:14.354
Sep  7 05:37:14.411: INFO: Deleting ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b took: 4.233968ms
Sep  7 05:37:14.512: INFO: Terminating ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b pods took: 101.089199ms
STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:37:17.417
Sep  7 05:37:17.428: INFO: Pod name wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75: Found 0 pods out of 5
Sep  7 05:37:22.432: INFO: Pod name wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/07/23 05:37:22.432
Sep  7 05:37:22.432: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:22.435: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488379ms
Sep  7 05:37:24.438: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162815s
Sep  7 05:37:26.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006394967s
Sep  7 05:37:28.438: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006072441s
Sep  7 05:37:30.437: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Running", Reason="", readiness=true. Elapsed: 8.004935217s
Sep  7 05:37:30.437: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk" satisfied condition "running"
Sep  7 05:37:30.437: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:30.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6": Phase="Running", Reason="", readiness=true. Elapsed: 2.038249ms
Sep  7 05:37:30.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6" satisfied condition "running"
Sep  7 05:37:30.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:30.442: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.284759ms
Sep  7 05:37:30.442: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f" satisfied condition "running"
Sep  7 05:37:30.442: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:30.443: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837799ms
Sep  7 05:37:32.447: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.005627207s
Sep  7 05:37:32.447: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs" satisfied condition "running"
Sep  7 05:37:32.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8" in namespace "emptydir-wrapper-1303" to be "running"
Sep  7 05:37:32.449: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8": Phase="Running", Reason="", readiness=true. Elapsed: 1.677109ms
Sep  7 05:37:32.449: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:37:32.449
Sep  7 05:37:32.506: INFO: Deleting ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 took: 3.752128ms
Sep  7 05:37:32.606: INFO: Terminating ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 pods took: 100.7045ms
STEP: Cleaning up the configMaps 09/07/23 05:37:34.708
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:34.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1303" for this suite. 09/07/23 05:37:34.863
------------------------------
â€¢ [SLOW TEST] [53.714 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:36:41.152
    Sep  7 05:36:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir-wrapper 09/07/23 05:36:41.153
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:36:41.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:36:41.161
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/07/23 05:36:41.162
    STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:36:41.406
    Sep  7 05:36:41.508: INFO: Pod name wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/07/23 05:36:41.508
    Sep  7 05:36:41.508: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:36:41.556: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 47.487719ms
    Sep  7 05:36:43.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051116825s
    Sep  7 05:36:45.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050785937s
    Sep  7 05:36:47.558: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049960872s
    Sep  7 05:36:49.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051032409s
    Sep  7 05:36:51.559: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050658769s
    Sep  7 05:36:53.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Pending", Reason="", readiness=false. Elapsed: 12.051955862s
    Sep  7 05:36:55.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn": Phase="Running", Reason="", readiness=true. Elapsed: 14.051338649s
    Sep  7 05:36:55.560: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-76sqn" satisfied condition "running"
    Sep  7 05:36:55.560: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:36:55.561: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8": Phase="Running", Reason="", readiness=true. Elapsed: 1.681479ms
    Sep  7 05:36:55.561: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9cdc8" satisfied condition "running"
    Sep  7 05:36:55.561: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:36:55.563: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh": Phase="Running", Reason="", readiness=true. Elapsed: 1.970239ms
    Sep  7 05:36:55.563: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-9htjh" satisfied condition "running"
    Sep  7 05:36:55.563: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:36:55.565: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp": Phase="Running", Reason="", readiness=true. Elapsed: 2.102489ms
    Sep  7 05:36:55.565: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-c9cjp" satisfied condition "running"
    Sep  7 05:36:55.565: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:36:55.567: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz": Phase="Running", Reason="", readiness=true. Elapsed: 1.819389ms
    Sep  7 05:36:55.567: INFO: Pod "wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6-mnrwz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:36:55.567
    Sep  7 05:36:55.624: INFO: Deleting ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 took: 4.298658ms
    Sep  7 05:36:55.724: INFO: Terminating ReplicationController wrapped-volume-race-8bef36d9-281e-4964-9c47-9a961e7565b6 pods took: 100.461718ms
    STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:36:59.327
    Sep  7 05:36:59.336: INFO: Pod name wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b: Found 0 pods out of 5
    Sep  7 05:37:04.342: INFO: Pod name wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/07/23 05:37:04.342
    Sep  7 05:37:04.342: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:04.343: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78707ms
    Sep  7 05:37:06.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00545846s
    Sep  7 05:37:08.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005038086s
    Sep  7 05:37:10.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006204023s
    Sep  7 05:37:12.347: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005529695s
    Sep  7 05:37:14.346: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4": Phase="Running", Reason="", readiness=true. Elapsed: 10.004485439s
    Sep  7 05:37:14.346: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-4z5z4" satisfied condition "running"
    Sep  7 05:37:14.346: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:14.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs": Phase="Running", Reason="", readiness=true. Elapsed: 1.911069ms
    Sep  7 05:37:14.348: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-9dlqs" satisfied condition "running"
    Sep  7 05:37:14.348: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:14.350: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.076659ms
    Sep  7 05:37:14.350: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-g9tr2" satisfied condition "running"
    Sep  7 05:37:14.350: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:14.352: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm": Phase="Running", Reason="", readiness=true. Elapsed: 1.74025ms
    Sep  7 05:37:14.352: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ns6fm" satisfied condition "running"
    Sep  7 05:37:14.352: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:14.354: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8": Phase="Running", Reason="", readiness=true. Elapsed: 1.93241ms
    Sep  7 05:37:14.354: INFO: Pod "wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b-ppmq8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:37:14.354
    Sep  7 05:37:14.411: INFO: Deleting ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b took: 4.233968ms
    Sep  7 05:37:14.512: INFO: Terminating ReplicationController wrapped-volume-race-e1684c06-6958-4b2b-a52d-b8740df36d2b pods took: 101.089199ms
    STEP: Creating RC which spawns configmap-volume pods 09/07/23 05:37:17.417
    Sep  7 05:37:17.428: INFO: Pod name wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75: Found 0 pods out of 5
    Sep  7 05:37:22.432: INFO: Pod name wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/07/23 05:37:22.432
    Sep  7 05:37:22.432: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:22.435: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488379ms
    Sep  7 05:37:24.438: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162815s
    Sep  7 05:37:26.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006394967s
    Sep  7 05:37:28.438: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006072441s
    Sep  7 05:37:30.437: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk": Phase="Running", Reason="", readiness=true. Elapsed: 8.004935217s
    Sep  7 05:37:30.437: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-428wk" satisfied condition "running"
    Sep  7 05:37:30.437: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:30.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6": Phase="Running", Reason="", readiness=true. Elapsed: 2.038249ms
    Sep  7 05:37:30.439: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-9vzs6" satisfied condition "running"
    Sep  7 05:37:30.439: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:30.442: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.284759ms
    Sep  7 05:37:30.442: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-b2l7f" satisfied condition "running"
    Sep  7 05:37:30.442: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:30.443: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837799ms
    Sep  7 05:37:32.447: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.005627207s
    Sep  7 05:37:32.447: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-cbfxs" satisfied condition "running"
    Sep  7 05:37:32.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8" in namespace "emptydir-wrapper-1303" to be "running"
    Sep  7 05:37:32.449: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8": Phase="Running", Reason="", readiness=true. Elapsed: 1.677109ms
    Sep  7 05:37:32.449: INFO: Pod "wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75-rtxx8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 in namespace emptydir-wrapper-1303, will wait for the garbage collector to delete the pods 09/07/23 05:37:32.449
    Sep  7 05:37:32.506: INFO: Deleting ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 took: 3.752128ms
    Sep  7 05:37:32.606: INFO: Terminating ReplicationController wrapped-volume-race-9106164e-154f-49fb-850c-de333a63ab75 pods took: 100.7045ms
    STEP: Cleaning up the configMaps 09/07/23 05:37:34.708
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:34.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1303" for this suite. 09/07/23 05:37:34.863
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:34.867
Sep  7 05:37:34.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:37:34.867
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:34.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:34.878
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-c55e18fe-47e7-411d-97af-72beae861d04 09/07/23 05:37:34.88
STEP: Creating secret with name secret-projected-all-test-volume-696a075e-f844-455c-b1e9-defe1421d743 09/07/23 05:37:34.882
STEP: Creating a pod to test Check all projections for projected volume plugin 09/07/23 05:37:34.884
Sep  7 05:37:34.890: INFO: Waiting up to 5m0s for pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8" in namespace "projected-6012" to be "Succeeded or Failed"
Sep  7 05:37:34.892: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26669ms
Sep  7 05:37:36.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004460774s
Sep  7 05:37:38.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004720703s
STEP: Saw pod success 09/07/23 05:37:38.895
Sep  7 05:37:38.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8" satisfied condition "Succeeded or Failed"
Sep  7 05:37:38.897: INFO: Trying to get logs from node kind-worker2 pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 container projected-all-volume-test: <nil>
STEP: delete the pod 09/07/23 05:37:38.904
Sep  7 05:37:38.911: INFO: Waiting for pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 to disappear
Sep  7 05:37:38.913: INFO: Pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:38.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6012" for this suite. 09/07/23 05:37:38.915
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:34.867
    Sep  7 05:37:34.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:37:34.867
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:34.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:34.878
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-c55e18fe-47e7-411d-97af-72beae861d04 09/07/23 05:37:34.88
    STEP: Creating secret with name secret-projected-all-test-volume-696a075e-f844-455c-b1e9-defe1421d743 09/07/23 05:37:34.882
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/07/23 05:37:34.884
    Sep  7 05:37:34.890: INFO: Waiting up to 5m0s for pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8" in namespace "projected-6012" to be "Succeeded or Failed"
    Sep  7 05:37:34.892: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26669ms
    Sep  7 05:37:36.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004460774s
    Sep  7 05:37:38.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004720703s
    STEP: Saw pod success 09/07/23 05:37:38.895
    Sep  7 05:37:38.895: INFO: Pod "projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8" satisfied condition "Succeeded or Failed"
    Sep  7 05:37:38.897: INFO: Trying to get logs from node kind-worker2 pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:37:38.904
    Sep  7 05:37:38.911: INFO: Waiting for pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 to disappear
    Sep  7 05:37:38.913: INFO: Pod projected-volume-1da13cb1-8eab-420e-bfbf-438ff173e4b8 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:38.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6012" for this suite. 09/07/23 05:37:38.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:38.922
Sep  7 05:37:38.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:37:38.923
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:38.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:38.931
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/07/23 05:37:38.933
Sep  7 05:37:38.939: INFO: Waiting up to 5m0s for pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4" in namespace "emptydir-3833" to be "Succeeded or Failed"
Sep  7 05:37:38.940: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.292059ms
Sep  7 05:37:40.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004140979s
Sep  7 05:37:42.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818183s
STEP: Saw pod success 09/07/23 05:37:42.943
Sep  7 05:37:42.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4" satisfied condition "Succeeded or Failed"
Sep  7 05:37:42.945: INFO: Trying to get logs from node kind-worker pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 container test-container: <nil>
STEP: delete the pod 09/07/23 05:37:42.949
Sep  7 05:37:42.958: INFO: Waiting for pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 to disappear
Sep  7 05:37:42.959: INFO: Pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:42.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3833" for this suite. 09/07/23 05:37:42.961
------------------------------
â€¢ [4.042 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:38.922
    Sep  7 05:37:38.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:37:38.923
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:38.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:38.931
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/07/23 05:37:38.933
    Sep  7 05:37:38.939: INFO: Waiting up to 5m0s for pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4" in namespace "emptydir-3833" to be "Succeeded or Failed"
    Sep  7 05:37:38.940: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.292059ms
    Sep  7 05:37:40.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004140979s
    Sep  7 05:37:42.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818183s
    STEP: Saw pod success 09/07/23 05:37:42.943
    Sep  7 05:37:42.943: INFO: Pod "pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4" satisfied condition "Succeeded or Failed"
    Sep  7 05:37:42.945: INFO: Trying to get logs from node kind-worker pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:37:42.949
    Sep  7 05:37:42.958: INFO: Waiting for pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 to disappear
    Sep  7 05:37:42.959: INFO: Pod pod-fe7da3c7-7cf0-4a27-b2fb-2ef5716a77b4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:42.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3833" for this suite. 09/07/23 05:37:42.961
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:42.964
Sep  7 05:37:42.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:37:42.965
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:42.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:42.977
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/07/23 05:37:42.979
STEP: Getting a ResourceQuota 09/07/23 05:37:42.981
STEP: Listing all ResourceQuotas with LabelSelector 09/07/23 05:37:42.983
STEP: Patching the ResourceQuota 09/07/23 05:37:42.986
STEP: Deleting a Collection of ResourceQuotas 09/07/23 05:37:42.989
STEP: Verifying the deleted ResourceQuota 09/07/23 05:37:42.993
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:42.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-517" for this suite. 09/07/23 05:37:42.996
------------------------------
â€¢ [0.035 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:42.964
    Sep  7 05:37:42.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:37:42.965
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:42.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:42.977
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/07/23 05:37:42.979
    STEP: Getting a ResourceQuota 09/07/23 05:37:42.981
    STEP: Listing all ResourceQuotas with LabelSelector 09/07/23 05:37:42.983
    STEP: Patching the ResourceQuota 09/07/23 05:37:42.986
    STEP: Deleting a Collection of ResourceQuotas 09/07/23 05:37:42.989
    STEP: Verifying the deleted ResourceQuota 09/07/23 05:37:42.993
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:42.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-517" for this suite. 09/07/23 05:37:42.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:43
Sep  7 05:37:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:37:43
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:43.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:43.011
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-49lkh"  09/07/23 05:37:43.012
Sep  7 05:37:43.014: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-49lkh"  09/07/23 05:37:43.015
Sep  7 05:37:43.019: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:43.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5595" for this suite. 09/07/23 05:37:43.02
------------------------------
â€¢ [0.024 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:43
    Sep  7 05:37:43.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 05:37:43
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:43.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:43.011
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-49lkh"  09/07/23 05:37:43.012
    Sep  7 05:37:43.014: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-49lkh"  09/07/23 05:37:43.015
    Sep  7 05:37:43.019: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:43.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5595" for this suite. 09/07/23 05:37:43.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:43.024
Sep  7 05:37:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:37:43.024
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:43.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:43.034
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/07/23 05:37:43.036
STEP: Creating a ResourceQuota 09/07/23 05:37:48.038
STEP: Ensuring resource quota status is calculated 09/07/23 05:37:48.041
STEP: Creating a Service 09/07/23 05:37:50.044
STEP: Creating a NodePort Service 09/07/23 05:37:50.057
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/07/23 05:37:50.071
STEP: Ensuring resource quota status captures service creation 09/07/23 05:37:50.085
STEP: Deleting Services 09/07/23 05:37:52.087
STEP: Ensuring resource quota status released usage 09/07/23 05:37:52.107
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:54.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5374" for this suite. 09/07/23 05:37:54.113
------------------------------
â€¢ [SLOW TEST] [11.093 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:43.024
    Sep  7 05:37:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:37:43.024
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:43.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:43.034
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/07/23 05:37:43.036
    STEP: Creating a ResourceQuota 09/07/23 05:37:48.038
    STEP: Ensuring resource quota status is calculated 09/07/23 05:37:48.041
    STEP: Creating a Service 09/07/23 05:37:50.044
    STEP: Creating a NodePort Service 09/07/23 05:37:50.057
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/07/23 05:37:50.071
    STEP: Ensuring resource quota status captures service creation 09/07/23 05:37:50.085
    STEP: Deleting Services 09/07/23 05:37:52.087
    STEP: Ensuring resource quota status released usage 09/07/23 05:37:52.107
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:54.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5374" for this suite. 09/07/23 05:37:54.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:54.117
Sep  7 05:37:54.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:37:54.117
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:54.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:54.127
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  7 05:37:54.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:37:57.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-915" for this suite. 09/07/23 05:37:57.211
------------------------------
â€¢ [3.098 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:54.117
    Sep  7 05:37:54.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:37:54.117
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:54.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:54.127
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  7 05:37:54.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:37:57.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-915" for this suite. 09/07/23 05:37:57.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:37:57.215
Sep  7 05:37:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 05:37:57.216
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:57.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:57.226
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/07/23 05:37:57.228
STEP: Patching the Job 09/07/23 05:37:57.232
STEP: Watching for Job to be patched 09/07/23 05:37:57.242
Sep  7 05:37:57.243: INFO: Event ADDED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  7 05:37:57.243: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  7 05:37:57.243: INFO: Event MODIFIED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/07/23 05:37:57.243
STEP: Watching for Job to be updated 09/07/23 05:37:57.248
Sep  7 05:37:57.248: INFO: Event MODIFIED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:37:57.248: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/07/23 05:37:57.248
Sep  7 05:37:57.250: INFO: Job: e2e-qfkpx as labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched]
STEP: Waiting for job to complete 09/07/23 05:37:57.25
STEP: Delete a job collection with a labelselector 09/07/23 05:38:05.253
STEP: Watching for Job to be deleted 09/07/23 05:38:05.256
Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  7 05:38:05.257: INFO: Event DELETED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/07/23 05:38:05.257
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 05:38:05.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1385" for this suite. 09/07/23 05:38:05.261
------------------------------
â€¢ [SLOW TEST] [8.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:37:57.215
    Sep  7 05:37:57.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 05:37:57.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:37:57.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:37:57.226
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/07/23 05:37:57.228
    STEP: Patching the Job 09/07/23 05:37:57.232
    STEP: Watching for Job to be patched 09/07/23 05:37:57.242
    Sep  7 05:37:57.243: INFO: Event ADDED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  7 05:37:57.243: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  7 05:37:57.243: INFO: Event MODIFIED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/07/23 05:37:57.243
    STEP: Watching for Job to be updated 09/07/23 05:37:57.248
    Sep  7 05:37:57.248: INFO: Event MODIFIED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:37:57.248: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/07/23 05:37:57.248
    Sep  7 05:37:57.250: INFO: Job: e2e-qfkpx as labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched]
    STEP: Waiting for job to complete 09/07/23 05:37:57.25
    STEP: Delete a job collection with a labelselector 09/07/23 05:38:05.253
    STEP: Watching for Job to be deleted 09/07/23 05:38:05.256
    Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:38:05.257: INFO: Event MODIFIED observed for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  7 05:38:05.257: INFO: Event DELETED found for Job e2e-qfkpx in namespace job-1385 with labels: map[e2e-job-label:e2e-qfkpx e2e-qfkpx:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/07/23 05:38:05.257
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:38:05.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1385" for this suite. 09/07/23 05:38:05.261
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:38:05.264
Sep  7 05:38:05.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-watch 09/07/23 05:38:05.265
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:38:05.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:38:05.28
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  7 05:38:05.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Creating first CR  09/07/23 05:38:07.815
Sep  7 05:38:07.818: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:07Z]] name:name1 resourceVersion:10794 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/07/23 05:38:17.819
Sep  7 05:38:17.822: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:17Z]] name:name2 resourceVersion:10838 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/07/23 05:38:27.823
Sep  7 05:38:27.828: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:27Z]] name:name1 resourceVersion:10853 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/07/23 05:38:37.829
Sep  7 05:38:37.836: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:37Z]] name:name2 resourceVersion:10870 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/07/23 05:38:47.837
Sep  7 05:38:47.842: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:27Z]] name:name1 resourceVersion:10887 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/07/23 05:38:57.843
Sep  7 05:38:57.848: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:37Z]] name:name2 resourceVersion:10903 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:08.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2592" for this suite. 09/07/23 05:39:08.36
------------------------------
â€¢ [SLOW TEST] [63.099 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:38:05.264
    Sep  7 05:38:05.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-watch 09/07/23 05:38:05.265
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:38:05.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:38:05.28
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  7 05:38:05.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Creating first CR  09/07/23 05:38:07.815
    Sep  7 05:38:07.818: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:07Z]] name:name1 resourceVersion:10794 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/07/23 05:38:17.819
    Sep  7 05:38:17.822: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:17Z]] name:name2 resourceVersion:10838 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/07/23 05:38:27.823
    Sep  7 05:38:27.828: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:27Z]] name:name1 resourceVersion:10853 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/07/23 05:38:37.829
    Sep  7 05:38:37.836: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:37Z]] name:name2 resourceVersion:10870 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/07/23 05:38:47.837
    Sep  7 05:38:47.842: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:27Z]] name:name1 resourceVersion:10887 uid:ef4e6603-7757-433d-85e1-b0eb98cb4b61] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/07/23 05:38:57.843
    Sep  7 05:38:57.848: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-07T05:38:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-07T05:38:37Z]] name:name2 resourceVersion:10903 uid:6231dcef-370a-4067-8a13-4ee5d0bff00c] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:08.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2592" for this suite. 09/07/23 05:39:08.36
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:08.364
Sep  7 05:39:08.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:39:08.364
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:08.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:08.373
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-1da424e1-c0be-4020-8cc5-25fc84778590 09/07/23 05:39:08.376
STEP: Creating secret with name s-test-opt-upd-1a26bdcd-4621-418f-8c7e-cef93caeac31 09/07/23 05:39:08.378
STEP: Creating the pod 09/07/23 05:39:08.381
Sep  7 05:39:08.387: INFO: Waiting up to 5m0s for pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d" in namespace "secrets-6678" to be "running and ready"
Sep  7 05:39:08.388: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.194759ms
Sep  7 05:39:08.388: INFO: The phase of Pod pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:39:10.391: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003847971s
Sep  7 05:39:10.391: INFO: The phase of Pod pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d is Running (Ready = true)
Sep  7 05:39:10.391: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1da424e1-c0be-4020-8cc5-25fc84778590 09/07/23 05:39:10.401
STEP: Updating secret s-test-opt-upd-1a26bdcd-4621-418f-8c7e-cef93caeac31 09/07/23 05:39:10.404
STEP: Creating secret with name s-test-opt-create-aab1ed4d-9975-41e5-bcda-df307e8669d2 09/07/23 05:39:10.406
STEP: waiting to observe update in volume 09/07/23 05:39:10.409
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6678" for this suite. 09/07/23 05:39:12.425
------------------------------
â€¢ [4.065 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:08.364
    Sep  7 05:39:08.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:39:08.364
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:08.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:08.373
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-1da424e1-c0be-4020-8cc5-25fc84778590 09/07/23 05:39:08.376
    STEP: Creating secret with name s-test-opt-upd-1a26bdcd-4621-418f-8c7e-cef93caeac31 09/07/23 05:39:08.378
    STEP: Creating the pod 09/07/23 05:39:08.381
    Sep  7 05:39:08.387: INFO: Waiting up to 5m0s for pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d" in namespace "secrets-6678" to be "running and ready"
    Sep  7 05:39:08.388: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.194759ms
    Sep  7 05:39:08.388: INFO: The phase of Pod pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:39:10.391: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.003847971s
    Sep  7 05:39:10.391: INFO: The phase of Pod pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d is Running (Ready = true)
    Sep  7 05:39:10.391: INFO: Pod "pod-secrets-5ae11518-7401-47eb-b957-72327f1b6c0d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1da424e1-c0be-4020-8cc5-25fc84778590 09/07/23 05:39:10.401
    STEP: Updating secret s-test-opt-upd-1a26bdcd-4621-418f-8c7e-cef93caeac31 09/07/23 05:39:10.404
    STEP: Creating secret with name s-test-opt-create-aab1ed4d-9975-41e5-bcda-df307e8669d2 09/07/23 05:39:10.406
    STEP: waiting to observe update in volume 09/07/23 05:39:10.409
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:12.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6678" for this suite. 09/07/23 05:39:12.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:12.431
Sep  7 05:39:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context-test 09/07/23 05:39:12.431
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:12.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:12.442
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  7 05:39:12.447: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef" in namespace "security-context-test-5107" to be "Succeeded or Failed"
Sep  7 05:39:12.448: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29486ms
Sep  7 05:39:14.451: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003798627s
Sep  7 05:39:16.452: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004896546s
Sep  7 05:39:16.452: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef" satisfied condition "Succeeded or Failed"
Sep  7 05:39:16.460: INFO: Got logs for pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:16.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5107" for this suite. 09/07/23 05:39:16.462
------------------------------
â€¢ [4.035 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:12.431
    Sep  7 05:39:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context-test 09/07/23 05:39:12.431
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:12.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:12.442
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  7 05:39:12.447: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef" in namespace "security-context-test-5107" to be "Succeeded or Failed"
    Sep  7 05:39:12.448: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29486ms
    Sep  7 05:39:14.451: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003798627s
    Sep  7 05:39:16.452: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004896546s
    Sep  7 05:39:16.452: INFO: Pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef" satisfied condition "Succeeded or Failed"
    Sep  7 05:39:16.460: INFO: Got logs for pod "busybox-privileged-false-6c669f2d-a912-4dc4-b5c2-1efc21a791ef": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:16.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5107" for this suite. 09/07/23 05:39:16.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:16.466
Sep  7 05:39:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename ingressclass 09/07/23 05:39:16.467
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:16.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:16.475
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/07/23 05:39:16.477
STEP: getting /apis/networking.k8s.io 09/07/23 05:39:16.478
STEP: getting /apis/networking.k8s.iov1 09/07/23 05:39:16.479
STEP: creating 09/07/23 05:39:16.479
STEP: getting 09/07/23 05:39:16.486
STEP: listing 09/07/23 05:39:16.487
STEP: watching 09/07/23 05:39:16.489
Sep  7 05:39:16.489: INFO: starting watch
STEP: patching 09/07/23 05:39:16.489
STEP: updating 09/07/23 05:39:16.492
Sep  7 05:39:16.495: INFO: waiting for watch events with expected annotations
Sep  7 05:39:16.495: INFO: saw patched and updated annotations
STEP: deleting 09/07/23 05:39:16.495
STEP: deleting a collection 09/07/23 05:39:16.502
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2192" for this suite. 09/07/23 05:39:16.509
------------------------------
â€¢ [0.046 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:16.466
    Sep  7 05:39:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename ingressclass 09/07/23 05:39:16.467
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:16.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:16.475
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/07/23 05:39:16.477
    STEP: getting /apis/networking.k8s.io 09/07/23 05:39:16.478
    STEP: getting /apis/networking.k8s.iov1 09/07/23 05:39:16.479
    STEP: creating 09/07/23 05:39:16.479
    STEP: getting 09/07/23 05:39:16.486
    STEP: listing 09/07/23 05:39:16.487
    STEP: watching 09/07/23 05:39:16.489
    Sep  7 05:39:16.489: INFO: starting watch
    STEP: patching 09/07/23 05:39:16.489
    STEP: updating 09/07/23 05:39:16.492
    Sep  7 05:39:16.495: INFO: waiting for watch events with expected annotations
    Sep  7 05:39:16.495: INFO: saw patched and updated annotations
    STEP: deleting 09/07/23 05:39:16.495
    STEP: deleting a collection 09/07/23 05:39:16.502
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:16.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2192" for this suite. 09/07/23 05:39:16.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:16.512
Sep  7 05:39:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:39:16.513
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:16.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:16.52
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-4418/configmap-test-4b7197da-75f2-4776-b18c-7fe4120e79a7 09/07/23 05:39:16.522
STEP: Creating a pod to test consume configMaps 09/07/23 05:39:16.524
Sep  7 05:39:16.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094" in namespace "configmap-4418" to be "Succeeded or Failed"
Sep  7 05:39:16.530: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469159ms
Sep  7 05:39:18.533: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00457628s
Sep  7 05:39:20.534: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005560345s
STEP: Saw pod success 09/07/23 05:39:20.534
Sep  7 05:39:20.534: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094" satisfied condition "Succeeded or Failed"
Sep  7 05:39:20.536: INFO: Trying to get logs from node kind-worker pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 container env-test: <nil>
STEP: delete the pod 09/07/23 05:39:20.539
Sep  7 05:39:20.548: INFO: Waiting for pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 to disappear
Sep  7 05:39:20.549: INFO: Pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:20.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4418" for this suite. 09/07/23 05:39:20.551
------------------------------
â€¢ [4.042 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:16.512
    Sep  7 05:39:16.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:39:16.513
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:16.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:16.52
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-4418/configmap-test-4b7197da-75f2-4776-b18c-7fe4120e79a7 09/07/23 05:39:16.522
    STEP: Creating a pod to test consume configMaps 09/07/23 05:39:16.524
    Sep  7 05:39:16.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094" in namespace "configmap-4418" to be "Succeeded or Failed"
    Sep  7 05:39:16.530: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469159ms
    Sep  7 05:39:18.533: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00457628s
    Sep  7 05:39:20.534: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005560345s
    STEP: Saw pod success 09/07/23 05:39:20.534
    Sep  7 05:39:20.534: INFO: Pod "pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094" satisfied condition "Succeeded or Failed"
    Sep  7 05:39:20.536: INFO: Trying to get logs from node kind-worker pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 container env-test: <nil>
    STEP: delete the pod 09/07/23 05:39:20.539
    Sep  7 05:39:20.548: INFO: Waiting for pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 to disappear
    Sep  7 05:39:20.549: INFO: Pod pod-configmaps-7cee1e9d-45bf-4cba-82ec-c9c264e77094 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:20.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4418" for this suite. 09/07/23 05:39:20.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:20.555
Sep  7 05:39:20.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename watch 09/07/23 05:39:20.556
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:20.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:20.566
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/07/23 05:39:20.568
STEP: creating a new configmap 09/07/23 05:39:20.569
STEP: modifying the configmap once 09/07/23 05:39:20.571
STEP: closing the watch once it receives two notifications 09/07/23 05:39:20.575
Sep  7 05:39:20.575: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11036 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:39:20.575: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11037 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/07/23 05:39:20.575
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/07/23 05:39:20.579
STEP: deleting the configmap 09/07/23 05:39:20.579
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/07/23 05:39:20.583
Sep  7 05:39:20.583: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11038 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:39:20.583: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11039 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:20.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9529" for this suite. 09/07/23 05:39:20.585
------------------------------
â€¢ [0.033 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:20.555
    Sep  7 05:39:20.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename watch 09/07/23 05:39:20.556
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:20.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:20.566
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/07/23 05:39:20.568
    STEP: creating a new configmap 09/07/23 05:39:20.569
    STEP: modifying the configmap once 09/07/23 05:39:20.571
    STEP: closing the watch once it receives two notifications 09/07/23 05:39:20.575
    Sep  7 05:39:20.575: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11036 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:39:20.575: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11037 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/07/23 05:39:20.575
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/07/23 05:39:20.579
    STEP: deleting the configmap 09/07/23 05:39:20.579
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/07/23 05:39:20.583
    Sep  7 05:39:20.583: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11038 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:39:20.583: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9529  22b3584e-f05a-4581-8b34-9839a5d55516 11039 0 2023-09-07 05:39:20 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-07 05:39:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:20.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9529" for this suite. 09/07/23 05:39:20.585
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:20.588
Sep  7 05:39:20.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename init-container 09/07/23 05:39:20.589
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:20.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:20.596
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/07/23 05:39:20.598
Sep  7 05:39:20.598: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:25.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2923" for this suite. 09/07/23 05:39:25.399
------------------------------
â€¢ [4.814 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:20.588
    Sep  7 05:39:20.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename init-container 09/07/23 05:39:20.589
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:20.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:20.596
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/07/23 05:39:20.598
    Sep  7 05:39:20.598: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:25.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2923" for this suite. 09/07/23 05:39:25.399
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:25.402
Sep  7 05:39:25.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename prestop 09/07/23 05:39:25.403
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:25.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:25.414
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6311 09/07/23 05:39:25.416
STEP: Waiting for pods to come up. 09/07/23 05:39:25.421
Sep  7 05:39:25.421: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6311" to be "running"
Sep  7 05:39:25.422: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.43499ms
Sep  7 05:39:27.425: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571901s
Sep  7 05:39:27.425: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6311 09/07/23 05:39:27.427
Sep  7 05:39:27.430: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6311" to be "running"
Sep  7 05:39:27.431: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20352ms
Sep  7 05:39:29.434: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003756704s
Sep  7 05:39:29.434: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/07/23 05:39:29.434
Sep  7 05:39:34.443: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/07/23 05:39:34.443
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:34.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6311" for this suite. 09/07/23 05:39:34.455
------------------------------
â€¢ [SLOW TEST] [9.056 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:25.402
    Sep  7 05:39:25.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename prestop 09/07/23 05:39:25.403
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:25.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:25.414
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6311 09/07/23 05:39:25.416
    STEP: Waiting for pods to come up. 09/07/23 05:39:25.421
    Sep  7 05:39:25.421: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6311" to be "running"
    Sep  7 05:39:25.422: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.43499ms
    Sep  7 05:39:27.425: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004571901s
    Sep  7 05:39:27.425: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6311 09/07/23 05:39:27.427
    Sep  7 05:39:27.430: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6311" to be "running"
    Sep  7 05:39:27.431: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20352ms
    Sep  7 05:39:29.434: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.003756704s
    Sep  7 05:39:29.434: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/07/23 05:39:29.434
    Sep  7 05:39:34.443: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/07/23 05:39:34.443
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:34.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6311" for this suite. 09/07/23 05:39:34.455
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:34.458
Sep  7 05:39:34.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:39:34.459
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:34.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:34.468
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/07/23 05:39:34.47
Sep  7 05:39:34.474: INFO: Waiting up to 5m0s for pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4" in namespace "emptydir-3823" to be "Succeeded or Failed"
Sep  7 05:39:34.476: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.304529ms
Sep  7 05:39:36.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403738s
Sep  7 05:39:38.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866195s
STEP: Saw pod success 09/07/23 05:39:38.478
Sep  7 05:39:38.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4" satisfied condition "Succeeded or Failed"
Sep  7 05:39:38.480: INFO: Trying to get logs from node kind-worker pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 container test-container: <nil>
STEP: delete the pod 09/07/23 05:39:38.483
Sep  7 05:39:38.490: INFO: Waiting for pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 to disappear
Sep  7 05:39:38.491: INFO: Pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:38.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3823" for this suite. 09/07/23 05:39:38.495
------------------------------
â€¢ [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:34.458
    Sep  7 05:39:34.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:39:34.459
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:34.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:34.468
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/07/23 05:39:34.47
    Sep  7 05:39:34.474: INFO: Waiting up to 5m0s for pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4" in namespace "emptydir-3823" to be "Succeeded or Failed"
    Sep  7 05:39:34.476: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.304529ms
    Sep  7 05:39:36.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00403738s
    Sep  7 05:39:38.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866195s
    STEP: Saw pod success 09/07/23 05:39:38.478
    Sep  7 05:39:38.478: INFO: Pod "pod-5c4f936e-877f-4ef4-9b23-d1d803989df4" satisfied condition "Succeeded or Failed"
    Sep  7 05:39:38.480: INFO: Trying to get logs from node kind-worker pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:39:38.483
    Sep  7 05:39:38.490: INFO: Waiting for pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 to disappear
    Sep  7 05:39:38.491: INFO: Pod pod-5c4f936e-877f-4ef4-9b23-d1d803989df4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:38.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3823" for this suite. 09/07/23 05:39:38.495
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:38.498
Sep  7 05:39:38.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:39:38.498
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:38.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:38.506
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/07/23 05:39:38.508
Sep  7 05:39:38.508: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-9584 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/07/23 05:39:38.544
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:38.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9584" for this suite. 09/07/23 05:39:38.551
------------------------------
â€¢ [0.056 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:38.498
    Sep  7 05:39:38.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:39:38.498
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:38.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:38.506
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/07/23 05:39:38.508
    Sep  7 05:39:38.508: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-9584 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/07/23 05:39:38.544
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:38.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9584" for this suite. 09/07/23 05:39:38.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:38.554
Sep  7 05:39:38.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-runtime 09/07/23 05:39:38.555
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:38.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:38.563
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/07/23 05:39:38.564
STEP: wait for the container to reach Succeeded 09/07/23 05:39:38.569
STEP: get the container status 09/07/23 05:39:41.578
STEP: the container should be terminated 09/07/23 05:39:41.579
STEP: the termination message should be set 09/07/23 05:39:41.579
Sep  7 05:39:41.579: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/07/23 05:39:41.579
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:41.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4597" for this suite. 09/07/23 05:39:41.59
------------------------------
â€¢ [3.039 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:38.554
    Sep  7 05:39:38.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-runtime 09/07/23 05:39:38.555
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:38.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:38.563
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/07/23 05:39:38.564
    STEP: wait for the container to reach Succeeded 09/07/23 05:39:38.569
    STEP: get the container status 09/07/23 05:39:41.578
    STEP: the container should be terminated 09/07/23 05:39:41.579
    STEP: the termination message should be set 09/07/23 05:39:41.579
    Sep  7 05:39:41.579: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/07/23 05:39:41.579
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:41.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4597" for this suite. 09/07/23 05:39:41.59
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:41.593
Sep  7 05:39:41.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename events 09/07/23 05:39:41.593
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:41.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:41.601
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/07/23 05:39:41.603
STEP: get a list of Events with a label in the current namespace 09/07/23 05:39:41.611
STEP: delete a list of events 09/07/23 05:39:41.612
Sep  7 05:39:41.613: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/07/23 05:39:41.622
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  7 05:39:41.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-761" for this suite. 09/07/23 05:39:41.625
------------------------------
â€¢ [0.034 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:41.593
    Sep  7 05:39:41.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename events 09/07/23 05:39:41.593
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:41.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:41.601
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/07/23 05:39:41.603
    STEP: get a list of Events with a label in the current namespace 09/07/23 05:39:41.611
    STEP: delete a list of events 09/07/23 05:39:41.612
    Sep  7 05:39:41.613: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/07/23 05:39:41.622
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:39:41.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-761" for this suite. 09/07/23 05:39:41.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:39:41.628
Sep  7 05:39:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption 09/07/23 05:39:41.628
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:41.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:41.637
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  7 05:39:41.646: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 05:40:41.660: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/07/23 05:40:41.661
Sep  7 05:40:41.675: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  7 05:40:41.680: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  7 05:40:41.691: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  7 05:40:41.695: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/07/23 05:40:41.695
Sep  7 05:40:41.695: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9545" to be "running"
Sep  7 05:40:41.699: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810369ms
Sep  7 05:40:43.702: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607549s
Sep  7 05:40:43.702: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  7 05:40:43.702: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
Sep  7 05:40:43.704: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.688939ms
Sep  7 05:40:43.704: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  7 05:40:43.704: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
Sep  7 05:40:43.705: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.73891ms
Sep  7 05:40:43.705: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  7 05:40:43.705: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
Sep  7 05:40:43.707: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.76129ms
Sep  7 05:40:43.707: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/07/23 05:40:43.707
Sep  7 05:40:43.713: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  7 05:40:43.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57065ms
Sep  7 05:40:45.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004250652s
Sep  7 05:40:47.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003923677s
Sep  7 05:40:49.718: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005079993s
Sep  7 05:40:49.718: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:40:49.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9545" for this suite. 09/07/23 05:40:49.753
------------------------------
â€¢ [SLOW TEST] [68.129 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:39:41.628
    Sep  7 05:39:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption 09/07/23 05:39:41.628
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:39:41.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:39:41.637
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  7 05:39:41.646: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 05:40:41.660: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/07/23 05:40:41.661
    Sep  7 05:40:41.675: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  7 05:40:41.680: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  7 05:40:41.691: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  7 05:40:41.695: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/07/23 05:40:41.695
    Sep  7 05:40:41.695: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9545" to be "running"
    Sep  7 05:40:41.699: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.810369ms
    Sep  7 05:40:43.702: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607549s
    Sep  7 05:40:43.702: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  7 05:40:43.702: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
    Sep  7 05:40:43.704: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.688939ms
    Sep  7 05:40:43.704: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  7 05:40:43.704: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
    Sep  7 05:40:43.705: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.73891ms
    Sep  7 05:40:43.705: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  7 05:40:43.705: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9545" to be "running"
    Sep  7 05:40:43.707: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.76129ms
    Sep  7 05:40:43.707: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/07/23 05:40:43.707
    Sep  7 05:40:43.713: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  7 05:40:43.714: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57065ms
    Sep  7 05:40:45.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004250652s
    Sep  7 05:40:47.717: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003923677s
    Sep  7 05:40:49.718: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005079993s
    Sep  7 05:40:49.718: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:40:49.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9545" for this suite. 09/07/23 05:40:49.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:40:49.758
Sep  7 05:40:49.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename ephemeral-containers-test 09/07/23 05:40:49.759
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:40:49.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:40:49.767
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/07/23 05:40:49.769
Sep  7 05:40:49.773: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2939" to be "running and ready"
Sep  7 05:40:49.775: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34993ms
Sep  7 05:40:49.775: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:40:51.777: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003845137s
Sep  7 05:40:51.777: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  7 05:40:51.777: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/07/23 05:40:51.779
Sep  7 05:40:51.786: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2939" to be "container debugger running"
Sep  7 05:40:51.788: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.701059ms
Sep  7 05:40:53.791: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005373589s
Sep  7 05:40:53.791: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/07/23 05:40:53.792
Sep  7 05:40:53.792: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2939 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:40:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:40:53.792: INFO: ExecWithOptions: Clientset creation
Sep  7 05:40:53.792: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2939/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  7 05:40:53.897: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:40:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2939" for this suite. 09/07/23 05:40:53.908
------------------------------
â€¢ [4.154 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:40:49.758
    Sep  7 05:40:49.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/07/23 05:40:49.759
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:40:49.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:40:49.767
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/07/23 05:40:49.769
    Sep  7 05:40:49.773: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2939" to be "running and ready"
    Sep  7 05:40:49.775: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34993ms
    Sep  7 05:40:49.775: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:40:51.777: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003845137s
    Sep  7 05:40:51.777: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  7 05:40:51.777: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/07/23 05:40:51.779
    Sep  7 05:40:51.786: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2939" to be "container debugger running"
    Sep  7 05:40:51.788: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.701059ms
    Sep  7 05:40:53.791: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005373589s
    Sep  7 05:40:53.791: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/07/23 05:40:53.792
    Sep  7 05:40:53.792: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2939 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:40:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:40:53.792: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:40:53.792: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2939/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  7 05:40:53.897: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:40:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2939" for this suite. 09/07/23 05:40:53.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:40:53.913
Sep  7 05:40:53.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename limitrange 09/07/23 05:40:53.914
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:40:53.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:40:53.923
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/07/23 05:40:53.925
STEP: Setting up watch 09/07/23 05:40:53.925
STEP: Submitting a LimitRange 09/07/23 05:40:54.028
STEP: Verifying LimitRange creation was observed 09/07/23 05:40:54.034
STEP: Fetching the LimitRange to ensure it has proper values 09/07/23 05:40:54.034
Sep  7 05:40:54.036: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  7 05:40:54.036: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/07/23 05:40:54.036
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/07/23 05:40:54.039
Sep  7 05:40:54.041: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  7 05:40:54.041: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/07/23 05:40:54.041
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/07/23 05:40:54.045
Sep  7 05:40:54.046: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  7 05:40:54.046: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/07/23 05:40:54.047
STEP: Failing to create a Pod with more than max resources 09/07/23 05:40:54.048
STEP: Updating a LimitRange 09/07/23 05:40:54.049
STEP: Verifying LimitRange updating is effective 09/07/23 05:40:54.054
STEP: Creating a Pod with less than former min resources 09/07/23 05:40:56.057
STEP: Failing to create a Pod with more than max resources 09/07/23 05:40:56.062
STEP: Deleting a LimitRange 09/07/23 05:40:56.064
STEP: Verifying the LimitRange was deleted 09/07/23 05:40:56.067
Sep  7 05:41:01.070: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/07/23 05:41:01.07
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:01.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7341" for this suite. 09/07/23 05:41:01.077
------------------------------
â€¢ [SLOW TEST] [7.169 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:40:53.913
    Sep  7 05:40:53.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename limitrange 09/07/23 05:40:53.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:40:53.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:40:53.923
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/07/23 05:40:53.925
    STEP: Setting up watch 09/07/23 05:40:53.925
    STEP: Submitting a LimitRange 09/07/23 05:40:54.028
    STEP: Verifying LimitRange creation was observed 09/07/23 05:40:54.034
    STEP: Fetching the LimitRange to ensure it has proper values 09/07/23 05:40:54.034
    Sep  7 05:40:54.036: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  7 05:40:54.036: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/07/23 05:40:54.036
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/07/23 05:40:54.039
    Sep  7 05:40:54.041: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  7 05:40:54.041: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/07/23 05:40:54.041
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/07/23 05:40:54.045
    Sep  7 05:40:54.046: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  7 05:40:54.046: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/07/23 05:40:54.047
    STEP: Failing to create a Pod with more than max resources 09/07/23 05:40:54.048
    STEP: Updating a LimitRange 09/07/23 05:40:54.049
    STEP: Verifying LimitRange updating is effective 09/07/23 05:40:54.054
    STEP: Creating a Pod with less than former min resources 09/07/23 05:40:56.057
    STEP: Failing to create a Pod with more than max resources 09/07/23 05:40:56.062
    STEP: Deleting a LimitRange 09/07/23 05:40:56.064
    STEP: Verifying the LimitRange was deleted 09/07/23 05:40:56.067
    Sep  7 05:41:01.070: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/07/23 05:41:01.07
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:01.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7341" for this suite. 09/07/23 05:41:01.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:01.082
Sep  7 05:41:01.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:41:01.083
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:01.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:01.09
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  7 05:41:01.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:02.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2627" for this suite. 09/07/23 05:41:02.108
------------------------------
â€¢ [1.030 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:01.082
    Sep  7 05:41:01.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename custom-resource-definition 09/07/23 05:41:01.083
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:01.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:01.09
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  7 05:41:01.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:02.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2627" for this suite. 09/07/23 05:41:02.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:02.113
Sep  7 05:41:02.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 05:41:02.114
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:02.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:02.123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-b1cd1a6a-ce27-4c05-9088-6161364bb2f4 09/07/23 05:41:02.124
STEP: Creating a pod to test consume configMaps 09/07/23 05:41:02.127
Sep  7 05:41:02.131: INFO: Waiting up to 5m0s for pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb" in namespace "configmap-6472" to be "Succeeded or Failed"
Sep  7 05:41:02.132: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37002ms
Sep  7 05:41:04.134: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00315638s
Sep  7 05:41:06.136: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004753141s
STEP: Saw pod success 09/07/23 05:41:06.136
Sep  7 05:41:06.136: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb" satisfied condition "Succeeded or Failed"
Sep  7 05:41:06.137: INFO: Trying to get logs from node kind-worker pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:41:06.142
Sep  7 05:41:06.147: INFO: Waiting for pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb to disappear
Sep  7 05:41:06.148: INFO: Pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:06.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6472" for this suite. 09/07/23 05:41:06.15
------------------------------
â€¢ [4.040 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:02.113
    Sep  7 05:41:02.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 05:41:02.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:02.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:02.123
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-b1cd1a6a-ce27-4c05-9088-6161364bb2f4 09/07/23 05:41:02.124
    STEP: Creating a pod to test consume configMaps 09/07/23 05:41:02.127
    Sep  7 05:41:02.131: INFO: Waiting up to 5m0s for pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb" in namespace "configmap-6472" to be "Succeeded or Failed"
    Sep  7 05:41:02.132: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37002ms
    Sep  7 05:41:04.134: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00315638s
    Sep  7 05:41:06.136: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004753141s
    STEP: Saw pod success 09/07/23 05:41:06.136
    Sep  7 05:41:06.136: INFO: Pod "pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb" satisfied condition "Succeeded or Failed"
    Sep  7 05:41:06.137: INFO: Trying to get logs from node kind-worker pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:41:06.142
    Sep  7 05:41:06.147: INFO: Waiting for pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb to disappear
    Sep  7 05:41:06.148: INFO: Pod pod-configmaps-7de63fab-443f-4022-9415-20e13669bbcb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:06.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6472" for this suite. 09/07/23 05:41:06.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:06.153
Sep  7 05:41:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 05:41:06.154
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:06.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:06.163
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/07/23 05:41:06.165
STEP: delete the rc 09/07/23 05:41:11.172
STEP: wait for all pods to be garbage collected 09/07/23 05:41:11.175
STEP: Gathering metrics 09/07/23 05:41:16.179
Sep  7 05:41:16.190: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 05:41:16.192: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.57503ms
Sep  7 05:41:16.192: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 05:41:16.192: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 05:41:16.230: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:16.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9609" for this suite. 09/07/23 05:41:16.232
------------------------------
â€¢ [SLOW TEST] [10.082 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:06.153
    Sep  7 05:41:06.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 05:41:06.154
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:06.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:06.163
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/07/23 05:41:06.165
    STEP: delete the rc 09/07/23 05:41:11.172
    STEP: wait for all pods to be garbage collected 09/07/23 05:41:11.175
    STEP: Gathering metrics 09/07/23 05:41:16.179
    Sep  7 05:41:16.190: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 05:41:16.192: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.57503ms
    Sep  7 05:41:16.192: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 05:41:16.192: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 05:41:16.230: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:16.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9609" for this suite. 09/07/23 05:41:16.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:16.236
Sep  7 05:41:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 05:41:16.236
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:16.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:16.247
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6469 09/07/23 05:41:16.249
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  7 05:41:16.258: INFO: Found 0 stateful pods, waiting for 1
Sep  7 05:41:26.261: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/07/23 05:41:26.264
W0907 05:41:26.277821      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  7 05:41:26.283: INFO: Found 1 stateful pods, waiting for 2
Sep  7 05:41:36.285: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:41:36.285: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/07/23 05:41:36.29
STEP: Delete all of the StatefulSets 09/07/23 05:41:36.291
STEP: Verify that StatefulSets have been deleted 09/07/23 05:41:36.295
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 05:41:36.296: INFO: Deleting all statefulset in ns statefulset-6469
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6469" for this suite. 09/07/23 05:41:36.302
------------------------------
â€¢ [SLOW TEST] [20.071 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:16.236
    Sep  7 05:41:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 05:41:16.236
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:16.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:16.247
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6469 09/07/23 05:41:16.249
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  7 05:41:16.258: INFO: Found 0 stateful pods, waiting for 1
    Sep  7 05:41:26.261: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/07/23 05:41:26.264
    W0907 05:41:26.277821      29 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  7 05:41:26.283: INFO: Found 1 stateful pods, waiting for 2
    Sep  7 05:41:36.285: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:41:36.285: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/07/23 05:41:36.29
    STEP: Delete all of the StatefulSets 09/07/23 05:41:36.291
    STEP: Verify that StatefulSets have been deleted 09/07/23 05:41:36.295
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 05:41:36.296: INFO: Deleting all statefulset in ns statefulset-6469
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:36.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6469" for this suite. 09/07/23 05:41:36.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:36.307
Sep  7 05:41:36.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:41:36.308
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:36.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:36.32
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/07/23 05:41:36.322
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3064" for this suite. 09/07/23 05:41:36.326
------------------------------
â€¢ [0.023 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:36.307
    Sep  7 05:41:36.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:41:36.308
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:36.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:36.32
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/07/23 05:41:36.322
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3064" for this suite. 09/07/23 05:41:36.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:36.33
Sep  7 05:41:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:41:36.331
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:36.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:36.343
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8331 09/07/23 05:41:36.345
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[] 09/07/23 05:41:36.355
Sep  7 05:41:36.357: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep  7 05:41:37.362: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8331 09/07/23 05:41:37.362
Sep  7 05:41:37.368: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8331" to be "running and ready"
Sep  7 05:41:37.370: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4423ms
Sep  7 05:41:37.370: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:41:39.373: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00431578s
Sep  7 05:41:39.373: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  7 05:41:39.373: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod1:[80]] 09/07/23 05:41:39.374
Sep  7 05:41:39.380: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/07/23 05:41:39.38
Sep  7 05:41:39.380: INFO: Creating new exec pod
Sep  7 05:41:39.386: INFO: Waiting up to 5m0s for pod "execpodqlf6c" in namespace "services-8331" to be "running"
Sep  7 05:41:39.388: INFO: Pod "execpodqlf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531599ms
Sep  7 05:41:41.390: INFO: Pod "execpodqlf6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207652s
Sep  7 05:41:41.390: INFO: Pod "execpodqlf6c" satisfied condition "running"
Sep  7 05:41:42.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  7 05:41:42.559: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:42.559: INFO: stdout: ""
Sep  7 05:41:42.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
Sep  7 05:41:42.726: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:42.726: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8331 09/07/23 05:41:42.726
Sep  7 05:41:42.729: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8331" to be "running and ready"
Sep  7 05:41:42.731: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62969ms
Sep  7 05:41:42.731: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:41:44.733: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078304s
Sep  7 05:41:44.733: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  7 05:41:44.733: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod1:[80] pod2:[80]] 09/07/23 05:41:44.735
Sep  7 05:41:44.742: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/07/23 05:41:44.742
Sep  7 05:41:45.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  7 05:41:45.926: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:45.926: INFO: stdout: ""
Sep  7 05:41:45.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
Sep  7 05:41:46.086: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:46.086: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8331 09/07/23 05:41:46.086
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod2:[80]] 09/07/23 05:41:46.097
Sep  7 05:41:46.103: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/07/23 05:41:46.103
Sep  7 05:41:47.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  7 05:41:47.274: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:47.274: INFO: stdout: ""
Sep  7 05:41:47.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
Sep  7 05:41:47.426: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
Sep  7 05:41:47.426: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8331 09/07/23 05:41:47.426
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[] 09/07/23 05:41:47.435
Sep  7 05:41:47.442: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:41:47.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8331" for this suite. 09/07/23 05:41:47.453
------------------------------
â€¢ [SLOW TEST] [11.126 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:36.33
    Sep  7 05:41:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:41:36.331
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:36.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:36.343
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8331 09/07/23 05:41:36.345
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[] 09/07/23 05:41:36.355
    Sep  7 05:41:36.357: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Sep  7 05:41:37.362: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8331 09/07/23 05:41:37.362
    Sep  7 05:41:37.368: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8331" to be "running and ready"
    Sep  7 05:41:37.370: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4423ms
    Sep  7 05:41:37.370: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:41:39.373: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00431578s
    Sep  7 05:41:39.373: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  7 05:41:39.373: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod1:[80]] 09/07/23 05:41:39.374
    Sep  7 05:41:39.380: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/07/23 05:41:39.38
    Sep  7 05:41:39.380: INFO: Creating new exec pod
    Sep  7 05:41:39.386: INFO: Waiting up to 5m0s for pod "execpodqlf6c" in namespace "services-8331" to be "running"
    Sep  7 05:41:39.388: INFO: Pod "execpodqlf6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.531599ms
    Sep  7 05:41:41.390: INFO: Pod "execpodqlf6c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004207652s
    Sep  7 05:41:41.390: INFO: Pod "execpodqlf6c" satisfied condition "running"
    Sep  7 05:41:42.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  7 05:41:42.559: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:42.559: INFO: stdout: ""
    Sep  7 05:41:42.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
    Sep  7 05:41:42.726: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:42.726: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8331 09/07/23 05:41:42.726
    Sep  7 05:41:42.729: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8331" to be "running and ready"
    Sep  7 05:41:42.731: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62969ms
    Sep  7 05:41:42.731: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:41:44.733: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078304s
    Sep  7 05:41:44.733: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  7 05:41:44.733: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod1:[80] pod2:[80]] 09/07/23 05:41:44.735
    Sep  7 05:41:44.742: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/07/23 05:41:44.742
    Sep  7 05:41:45.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  7 05:41:45.926: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:45.926: INFO: stdout: ""
    Sep  7 05:41:45.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
    Sep  7 05:41:46.086: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:46.086: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8331 09/07/23 05:41:46.086
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[pod2:[80]] 09/07/23 05:41:46.097
    Sep  7 05:41:46.103: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/07/23 05:41:46.103
    Sep  7 05:41:47.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  7 05:41:47.274: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:47.274: INFO: stdout: ""
    Sep  7 05:41:47.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-8331 exec execpodqlf6c -- /bin/sh -x -c nc -v -z -w 2 10.96.178.31 80'
    Sep  7 05:41:47.426: INFO: stderr: "+ nc -v -z -w 2 10.96.178.31 80\nConnection to 10.96.178.31 80 port [tcp/http] succeeded!\n"
    Sep  7 05:41:47.426: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8331 09/07/23 05:41:47.426
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8331 to expose endpoints map[] 09/07/23 05:41:47.435
    Sep  7 05:41:47.442: INFO: successfully validated that service endpoint-test2 in namespace services-8331 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:41:47.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8331" for this suite. 09/07/23 05:41:47.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:41:47.458
Sep  7 05:41:47.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 05:41:47.458
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:47.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:47.471
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/07/23 05:41:47.475
STEP: create the rc2 09/07/23 05:41:47.479
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/07/23 05:41:52.487
STEP: delete the rc simpletest-rc-to-be-deleted 09/07/23 05:41:52.767
STEP: wait for the rc to be deleted 09/07/23 05:41:52.773
Sep  7 05:41:57.782: INFO: 71 pods remaining
Sep  7 05:41:57.782: INFO: 71 pods has nil DeletionTimestamp
Sep  7 05:41:57.782: INFO: 
STEP: Gathering metrics 09/07/23 05:42:02.779
Sep  7 05:42:02.785: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 05:42:02.787: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.34494ms
Sep  7 05:42:02.787: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 05:42:02.787: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 05:42:02.826: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  7 05:42:02.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-24hgq" in namespace "gc-5490"
Sep  7 05:42:02.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gjpm" in namespace "gc-5490"
Sep  7 05:42:02.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-47pwk" in namespace "gc-5490"
Sep  7 05:42:02.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-49l4w" in namespace "gc-5490"
Sep  7 05:42:02.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b489" in namespace "gc-5490"
Sep  7 05:42:02.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c77p" in namespace "gc-5490"
Sep  7 05:42:02.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dvb8" in namespace "gc-5490"
Sep  7 05:42:02.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rk6j" in namespace "gc-5490"
Sep  7 05:42:02.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-59trh" in namespace "gc-5490"
Sep  7 05:42:02.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kcbs" in namespace "gc-5490"
Sep  7 05:42:02.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lrvv" in namespace "gc-5490"
Sep  7 05:42:02.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xjnm" in namespace "gc-5490"
Sep  7 05:42:02.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-62jj7" in namespace "gc-5490"
Sep  7 05:42:02.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dtw6" in namespace "gc-5490"
Sep  7 05:42:02.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lz5n" in namespace "gc-5490"
Sep  7 05:42:02.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p45l" in namespace "gc-5490"
Sep  7 05:42:02.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q85g" in namespace "gc-5490"
Sep  7 05:42:02.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xxn9" in namespace "gc-5490"
Sep  7 05:42:02.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-86q9w" in namespace "gc-5490"
Sep  7 05:42:02.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pslx" in namespace "gc-5490"
Sep  7 05:42:02.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-b65ck" in namespace "gc-5490"
Sep  7 05:42:02.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjkds" in namespace "gc-5490"
Sep  7 05:42:02.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqcg8" in namespace "gc-5490"
Sep  7 05:42:02.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs8s" in namespace "gc-5490"
Sep  7 05:42:02.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvvs8" in namespace "gc-5490"
Sep  7 05:42:03.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9l59" in namespace "gc-5490"
Sep  7 05:42:03.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfcjp" in namespace "gc-5490"
Sep  7 05:42:03.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnbqk" in namespace "gc-5490"
Sep  7 05:42:03.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxvkt" in namespace "gc-5490"
Sep  7 05:42:03.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9bn6" in namespace "gc-5490"
Sep  7 05:42:03.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlth5" in namespace "gc-5490"
Sep  7 05:42:03.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqpgg" in namespace "gc-5490"
Sep  7 05:42:03.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvzlp" in namespace "gc-5490"
Sep  7 05:42:03.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqq89" in namespace "gc-5490"
Sep  7 05:42:03.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdqk2" in namespace "gc-5490"
Sep  7 05:42:03.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-glvmj" in namespace "gc-5490"
Sep  7 05:42:03.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmtkw" in namespace "gc-5490"
Sep  7 05:42:03.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8kg8" in namespace "gc-5490"
Sep  7 05:42:03.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9mzs" in namespace "gc-5490"
Sep  7 05:42:03.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9qz4" in namespace "gc-5490"
Sep  7 05:42:03.113: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk84v" in namespace "gc-5490"
Sep  7 05:42:03.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwk67" in namespace "gc-5490"
Sep  7 05:42:03.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-kdfq4" in namespace "gc-5490"
Sep  7 05:42:03.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-l64w4" in namespace "gc-5490"
Sep  7 05:42:03.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-l89t8" in namespace "gc-5490"
Sep  7 05:42:03.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcpfg" in namespace "gc-5490"
Sep  7 05:42:03.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-lngtn" in namespace "gc-5490"
Sep  7 05:42:03.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-lzmkq" in namespace "gc-5490"
Sep  7 05:42:03.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-m6mwx" in namespace "gc-5490"
Sep  7 05:42:03.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-mlbls" in namespace "gc-5490"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 05:42:03.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5490" for this suite. 09/07/23 05:42:03.192
------------------------------
â€¢ [SLOW TEST] [15.738 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:41:47.458
    Sep  7 05:41:47.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 05:41:47.458
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:41:47.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:41:47.471
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/07/23 05:41:47.475
    STEP: create the rc2 09/07/23 05:41:47.479
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/07/23 05:41:52.487
    STEP: delete the rc simpletest-rc-to-be-deleted 09/07/23 05:41:52.767
    STEP: wait for the rc to be deleted 09/07/23 05:41:52.773
    Sep  7 05:41:57.782: INFO: 71 pods remaining
    Sep  7 05:41:57.782: INFO: 71 pods has nil DeletionTimestamp
    Sep  7 05:41:57.782: INFO: 
    STEP: Gathering metrics 09/07/23 05:42:02.779
    Sep  7 05:42:02.785: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 05:42:02.787: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.34494ms
    Sep  7 05:42:02.787: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 05:42:02.787: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 05:42:02.826: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  7 05:42:02.826: INFO: Deleting pod "simpletest-rc-to-be-deleted-24hgq" in namespace "gc-5490"
    Sep  7 05:42:02.832: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gjpm" in namespace "gc-5490"
    Sep  7 05:42:02.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-47pwk" in namespace "gc-5490"
    Sep  7 05:42:02.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-49l4w" in namespace "gc-5490"
    Sep  7 05:42:02.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b489" in namespace "gc-5490"
    Sep  7 05:42:02.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c77p" in namespace "gc-5490"
    Sep  7 05:42:02.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dvb8" in namespace "gc-5490"
    Sep  7 05:42:02.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rk6j" in namespace "gc-5490"
    Sep  7 05:42:02.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-59trh" in namespace "gc-5490"
    Sep  7 05:42:02.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kcbs" in namespace "gc-5490"
    Sep  7 05:42:02.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lrvv" in namespace "gc-5490"
    Sep  7 05:42:02.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xjnm" in namespace "gc-5490"
    Sep  7 05:42:02.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-62jj7" in namespace "gc-5490"
    Sep  7 05:42:02.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dtw6" in namespace "gc-5490"
    Sep  7 05:42:02.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lz5n" in namespace "gc-5490"
    Sep  7 05:42:02.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p45l" in namespace "gc-5490"
    Sep  7 05:42:02.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q85g" in namespace "gc-5490"
    Sep  7 05:42:02.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xxn9" in namespace "gc-5490"
    Sep  7 05:42:02.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-86q9w" in namespace "gc-5490"
    Sep  7 05:42:02.954: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pslx" in namespace "gc-5490"
    Sep  7 05:42:02.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-b65ck" in namespace "gc-5490"
    Sep  7 05:42:02.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjkds" in namespace "gc-5490"
    Sep  7 05:42:02.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqcg8" in namespace "gc-5490"
    Sep  7 05:42:02.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvs8s" in namespace "gc-5490"
    Sep  7 05:42:02.994: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvvs8" in namespace "gc-5490"
    Sep  7 05:42:03.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9l59" in namespace "gc-5490"
    Sep  7 05:42:03.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfcjp" in namespace "gc-5490"
    Sep  7 05:42:03.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnbqk" in namespace "gc-5490"
    Sep  7 05:42:03.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxvkt" in namespace "gc-5490"
    Sep  7 05:42:03.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9bn6" in namespace "gc-5490"
    Sep  7 05:42:03.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlth5" in namespace "gc-5490"
    Sep  7 05:42:03.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqpgg" in namespace "gc-5490"
    Sep  7 05:42:03.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvzlp" in namespace "gc-5490"
    Sep  7 05:42:03.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqq89" in namespace "gc-5490"
    Sep  7 05:42:03.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdqk2" in namespace "gc-5490"
    Sep  7 05:42:03.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-glvmj" in namespace "gc-5490"
    Sep  7 05:42:03.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmtkw" in namespace "gc-5490"
    Sep  7 05:42:03.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8kg8" in namespace "gc-5490"
    Sep  7 05:42:03.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9mzs" in namespace "gc-5490"
    Sep  7 05:42:03.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9qz4" in namespace "gc-5490"
    Sep  7 05:42:03.113: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk84v" in namespace "gc-5490"
    Sep  7 05:42:03.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwk67" in namespace "gc-5490"
    Sep  7 05:42:03.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-kdfq4" in namespace "gc-5490"
    Sep  7 05:42:03.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-l64w4" in namespace "gc-5490"
    Sep  7 05:42:03.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-l89t8" in namespace "gc-5490"
    Sep  7 05:42:03.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcpfg" in namespace "gc-5490"
    Sep  7 05:42:03.162: INFO: Deleting pod "simpletest-rc-to-be-deleted-lngtn" in namespace "gc-5490"
    Sep  7 05:42:03.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-lzmkq" in namespace "gc-5490"
    Sep  7 05:42:03.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-m6mwx" in namespace "gc-5490"
    Sep  7 05:42:03.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-mlbls" in namespace "gc-5490"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:42:03.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5490" for this suite. 09/07/23 05:42:03.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:42:03.196
Sep  7 05:42:03.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context-test 09/07/23 05:42:03.197
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:03.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:03.207
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  7 05:42:03.214: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1" in namespace "security-context-test-7285" to be "Succeeded or Failed"
Sep  7 05:42:03.216: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59763ms
Sep  7 05:42:05.219: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004631792s
Sep  7 05:42:07.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003742466s
Sep  7 05:42:09.219: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004646002s
Sep  7 05:42:11.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00363869s
Sep  7 05:42:13.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.004183539s
Sep  7 05:42:13.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 05:42:13.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7285" for this suite. 09/07/23 05:42:13.22
------------------------------
â€¢ [SLOW TEST] [10.028 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:42:03.196
    Sep  7 05:42:03.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context-test 09/07/23 05:42:03.197
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:03.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:03.207
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  7 05:42:03.214: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1" in namespace "security-context-test-7285" to be "Succeeded or Failed"
    Sep  7 05:42:03.216: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59763ms
    Sep  7 05:42:05.219: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004631792s
    Sep  7 05:42:07.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003742466s
    Sep  7 05:42:09.219: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004646002s
    Sep  7 05:42:11.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00363869s
    Sep  7 05:42:13.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.004183539s
    Sep  7 05:42:13.218: INFO: Pod "busybox-user-65534-b7285eff-8c91-4530-87c5-c8163644dcb1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:42:13.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7285" for this suite. 09/07/23 05:42:13.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:42:13.225
Sep  7 05:42:13.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 05:42:13.226
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:13.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:13.24
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  7 05:42:13.248: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  7 05:42:18.250: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 05:42:18.25
Sep  7 05:42:18.250: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/07/23 05:42:18.255
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 05:42:18.262: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8519  0e3b4012-b3b3-41a1-9b01-e5a40f3901e1 13205 1 2023-09-07 05:42:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-07 05:42:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00485b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  7 05:42:18.264: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep  7 05:42:18.264: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  7 05:42:18.264: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8519  62d3409f-98c3-4801-97dc-44ab4d981041 13206 1 2023-09-07 05:42:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0e3b4012-b3b3-41a1-9b01-e5a40f3901e1 0xc004e63fe7 0xc004e63fe8}] [] [{e2e.test Update apps/v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 05:42:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3b4012-b3b3-41a1-9b01-e5a40f3901e1\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c1e0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:42:18.266: INFO: Pod "test-cleanup-controller-k5twh" is available:
&Pod{ObjectMeta:{test-cleanup-controller-k5twh test-cleanup-controller- deployment-8519  6c53bbbd-8eb8-490b-b1ed-1680bc786bfd 13194 0 2023-09-07 05:42:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 62d3409f-98c3-4801-97dc-44ab4d981041 0xc003c1e397 0xc003c1e398}] [] [{kube-controller-manager Update v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62d3409f-98c3-4801-97dc-44ab4d981041\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p976f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p976f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.168,StartTime:2023-09-07 05:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:42:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://00736f2c5918bfefb157416418dd1e7d930765a2828eb2f44a281ea05cef1b1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 05:42:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8519" for this suite. 09/07/23 05:42:18.272
------------------------------
â€¢ [SLOW TEST] [5.052 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:42:13.225
    Sep  7 05:42:13.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 05:42:13.226
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:13.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:13.24
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  7 05:42:13.248: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  7 05:42:18.250: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 05:42:18.25
    Sep  7 05:42:18.250: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/07/23 05:42:18.255
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 05:42:18.262: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8519  0e3b4012-b3b3-41a1-9b01-e5a40f3901e1 13205 1 2023-09-07 05:42:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-07 05:42:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00485b058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  7 05:42:18.264: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Sep  7 05:42:18.264: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  7 05:42:18.264: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8519  62d3409f-98c3-4801-97dc-44ab4d981041 13206 1 2023-09-07 05:42:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0e3b4012-b3b3-41a1-9b01-e5a40f3901e1 0xc004e63fe7 0xc004e63fe8}] [] [{e2e.test Update apps/v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 05:42:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"0e3b4012-b3b3-41a1-9b01-e5a40f3901e1\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c1e0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:42:18.266: INFO: Pod "test-cleanup-controller-k5twh" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-k5twh test-cleanup-controller- deployment-8519  6c53bbbd-8eb8-490b-b1ed-1680bc786bfd 13194 0 2023-09-07 05:42:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 62d3409f-98c3-4801-97dc-44ab4d981041 0xc003c1e397 0xc003c1e398}] [] [{kube-controller-manager Update v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62d3409f-98c3-4801-97dc-44ab4d981041\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:42:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p976f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p976f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.168,StartTime:2023-09-07 05:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:42:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://00736f2c5918bfefb157416418dd1e7d930765a2828eb2f44a281ea05cef1b1a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:42:18.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8519" for this suite. 09/07/23 05:42:18.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:42:18.277
Sep  7 05:42:18.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 05:42:18.278
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:18.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:18.29
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 in namespace container-probe-9549 09/07/23 05:42:18.292
Sep  7 05:42:18.297: INFO: Waiting up to 5m0s for pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87" in namespace "container-probe-9549" to be "not pending"
Sep  7 05:42:18.298: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381089ms
Sep  7 05:42:20.301: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003901774s
Sep  7 05:42:20.301: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87" satisfied condition "not pending"
Sep  7 05:42:20.301: INFO: Started pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 in namespace container-probe-9549
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:42:20.301
Sep  7 05:42:20.303: INFO: Initial restart count of pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 is 0
STEP: deleting the pod 09/07/23 05:46:20.673
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:20.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9549" for this suite. 09/07/23 05:46:20.686
------------------------------
â€¢ [SLOW TEST] [242.412 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:42:18.277
    Sep  7 05:42:18.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 05:42:18.278
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:42:18.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:42:18.29
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 in namespace container-probe-9549 09/07/23 05:42:18.292
    Sep  7 05:42:18.297: INFO: Waiting up to 5m0s for pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87" in namespace "container-probe-9549" to be "not pending"
    Sep  7 05:42:18.298: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381089ms
    Sep  7 05:42:20.301: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87": Phase="Running", Reason="", readiness=true. Elapsed: 2.003901774s
    Sep  7 05:42:20.301: INFO: Pod "test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87" satisfied condition "not pending"
    Sep  7 05:42:20.301: INFO: Started pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 in namespace container-probe-9549
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:42:20.301
    Sep  7 05:42:20.303: INFO: Initial restart count of pod test-webserver-730a6db1-787f-480d-8ab1-3899f4576f87 is 0
    STEP: deleting the pod 09/07/23 05:46:20.673
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:20.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9549" for this suite. 09/07/23 05:46:20.686
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:20.689
Sep  7 05:46:20.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pod-network-test 09/07/23 05:46:20.69
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:20.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:20.704
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2520 09/07/23 05:46:20.706
STEP: creating a selector 09/07/23 05:46:20.706
STEP: Creating the service pods in kubernetes 09/07/23 05:46:20.706
Sep  7 05:46:20.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  7 05:46:20.719: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2520" to be "running and ready"
Sep  7 05:46:20.721: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.59427ms
Sep  7 05:46:20.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:46:22.724: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004856957s
Sep  7 05:46:22.724: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:46:24.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006125675s
Sep  7 05:46:24.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:46:26.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006659234s
Sep  7 05:46:26.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:46:28.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005931044s
Sep  7 05:46:28.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:46:30.724: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005226326s
Sep  7 05:46:30.724: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:46:32.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006017448s
Sep  7 05:46:32.725: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  7 05:46:32.725: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  7 05:46:32.726: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2520" to be "running and ready"
Sep  7 05:46:32.728: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.367009ms
Sep  7 05:46:32.728: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  7 05:46:32.728: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/07/23 05:46:32.729
Sep  7 05:46:32.732: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2520" to be "running"
Sep  7 05:46:32.734: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33555ms
Sep  7 05:46:34.737: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004324282s
Sep  7 05:46:34.737: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  7 05:46:34.738: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  7 05:46:34.738: INFO: Breadth first check of 10.244.1.170 on host 192.168.8.3...
Sep  7 05:46:34.740: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.171:9080/dial?request=hostname&protocol=udp&host=10.244.1.170&port=8081&tries=1'] Namespace:pod-network-test-2520 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:46:34.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:46:34.740: INFO: ExecWithOptions: Clientset creation
Sep  7 05:46:34.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2520/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.170%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  7 05:46:34.873: INFO: Waiting for responses: map[]
Sep  7 05:46:34.873: INFO: reached 10.244.1.170 after 0/1 tries
Sep  7 05:46:34.873: INFO: Breadth first check of 10.244.2.140 on host 192.168.8.6...
Sep  7 05:46:34.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.171:9080/dial?request=hostname&protocol=udp&host=10.244.2.140&port=8081&tries=1'] Namespace:pod-network-test-2520 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:46:34.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:46:34.875: INFO: ExecWithOptions: Clientset creation
Sep  7 05:46:34.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2520/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.140%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  7 05:46:34.978: INFO: Waiting for responses: map[]
Sep  7 05:46:34.978: INFO: reached 10.244.2.140 after 0/1 tries
Sep  7 05:46:34.978: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:34.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2520" for this suite. 09/07/23 05:46:34.981
------------------------------
â€¢ [SLOW TEST] [14.295 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:20.689
    Sep  7 05:46:20.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pod-network-test 09/07/23 05:46:20.69
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:20.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:20.704
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2520 09/07/23 05:46:20.706
    STEP: creating a selector 09/07/23 05:46:20.706
    STEP: Creating the service pods in kubernetes 09/07/23 05:46:20.706
    Sep  7 05:46:20.706: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  7 05:46:20.719: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2520" to be "running and ready"
    Sep  7 05:46:20.721: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.59427ms
    Sep  7 05:46:20.721: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:46:22.724: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004856957s
    Sep  7 05:46:22.724: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:46:24.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006125675s
    Sep  7 05:46:24.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:46:26.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.006659234s
    Sep  7 05:46:26.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:46:28.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005931044s
    Sep  7 05:46:28.725: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:46:30.724: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005226326s
    Sep  7 05:46:30.724: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:46:32.725: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006017448s
    Sep  7 05:46:32.725: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  7 05:46:32.725: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  7 05:46:32.726: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2520" to be "running and ready"
    Sep  7 05:46:32.728: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.367009ms
    Sep  7 05:46:32.728: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  7 05:46:32.728: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/07/23 05:46:32.729
    Sep  7 05:46:32.732: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2520" to be "running"
    Sep  7 05:46:32.734: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33555ms
    Sep  7 05:46:34.737: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004324282s
    Sep  7 05:46:34.737: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  7 05:46:34.738: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  7 05:46:34.738: INFO: Breadth first check of 10.244.1.170 on host 192.168.8.3...
    Sep  7 05:46:34.740: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.171:9080/dial?request=hostname&protocol=udp&host=10.244.1.170&port=8081&tries=1'] Namespace:pod-network-test-2520 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:46:34.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:46:34.740: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:46:34.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2520/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.170%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  7 05:46:34.873: INFO: Waiting for responses: map[]
    Sep  7 05:46:34.873: INFO: reached 10.244.1.170 after 0/1 tries
    Sep  7 05:46:34.873: INFO: Breadth first check of 10.244.2.140 on host 192.168.8.6...
    Sep  7 05:46:34.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.171:9080/dial?request=hostname&protocol=udp&host=10.244.2.140&port=8081&tries=1'] Namespace:pod-network-test-2520 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:46:34.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:46:34.875: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:46:34.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2520/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.140%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  7 05:46:34.978: INFO: Waiting for responses: map[]
    Sep  7 05:46:34.978: INFO: reached 10.244.2.140 after 0/1 tries
    Sep  7 05:46:34.978: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:34.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2520" for this suite. 09/07/23 05:46:34.981
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:34.985
Sep  7 05:46:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:46:34.985
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:34.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:34.997
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/07/23 05:46:34.999
Sep  7 05:46:35.003: INFO: Waiting up to 5m0s for pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e" in namespace "emptydir-8715" to be "Succeeded or Failed"
Sep  7 05:46:35.005: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.73683ms
Sep  7 05:46:37.008: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004331214s
Sep  7 05:46:39.009: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00545975s
STEP: Saw pod success 09/07/23 05:46:39.009
Sep  7 05:46:39.009: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e" satisfied condition "Succeeded or Failed"
Sep  7 05:46:39.011: INFO: Trying to get logs from node kind-worker2 pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e container test-container: <nil>
STEP: delete the pod 09/07/23 05:46:39.02
Sep  7 05:46:39.029: INFO: Waiting for pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e to disappear
Sep  7 05:46:39.030: INFO: Pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:39.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8715" for this suite. 09/07/23 05:46:39.032
------------------------------
â€¢ [4.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:34.985
    Sep  7 05:46:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:46:34.985
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:34.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:34.997
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/07/23 05:46:34.999
    Sep  7 05:46:35.003: INFO: Waiting up to 5m0s for pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e" in namespace "emptydir-8715" to be "Succeeded or Failed"
    Sep  7 05:46:35.005: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.73683ms
    Sep  7 05:46:37.008: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004331214s
    Sep  7 05:46:39.009: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00545975s
    STEP: Saw pod success 09/07/23 05:46:39.009
    Sep  7 05:46:39.009: INFO: Pod "pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e" satisfied condition "Succeeded or Failed"
    Sep  7 05:46:39.011: INFO: Trying to get logs from node kind-worker2 pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e container test-container: <nil>
    STEP: delete the pod 09/07/23 05:46:39.02
    Sep  7 05:46:39.029: INFO: Waiting for pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e to disappear
    Sep  7 05:46:39.030: INFO: Pod pod-64ec4ef7-50c9-4bfb-a7b1-ae04e7a8297e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:39.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8715" for this suite. 09/07/23 05:46:39.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:39.036
Sep  7 05:46:39.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:46:39.037
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:39.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:39.047
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/07/23 05:46:39.049
Sep  7 05:46:39.053: INFO: Waiting up to 5m0s for pod "pod-f8323682-5227-4660-9206-54bd22002104" in namespace "emptydir-4855" to be "Succeeded or Failed"
Sep  7 05:46:39.054: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45193ms
Sep  7 05:46:41.058: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871046s
Sep  7 05:46:43.057: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004050613s
STEP: Saw pod success 09/07/23 05:46:43.057
Sep  7 05:46:43.057: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104" satisfied condition "Succeeded or Failed"
Sep  7 05:46:43.059: INFO: Trying to get logs from node kind-worker pod pod-f8323682-5227-4660-9206-54bd22002104 container test-container: <nil>
STEP: delete the pod 09/07/23 05:46:43.067
Sep  7 05:46:43.074: INFO: Waiting for pod pod-f8323682-5227-4660-9206-54bd22002104 to disappear
Sep  7 05:46:43.076: INFO: Pod pod-f8323682-5227-4660-9206-54bd22002104 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:43.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4855" for this suite. 09/07/23 05:46:43.078
------------------------------
â€¢ [4.045 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:39.036
    Sep  7 05:46:39.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:46:39.037
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:39.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:39.047
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/07/23 05:46:39.049
    Sep  7 05:46:39.053: INFO: Waiting up to 5m0s for pod "pod-f8323682-5227-4660-9206-54bd22002104" in namespace "emptydir-4855" to be "Succeeded or Failed"
    Sep  7 05:46:39.054: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45193ms
    Sep  7 05:46:41.058: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004871046s
    Sep  7 05:46:43.057: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004050613s
    STEP: Saw pod success 09/07/23 05:46:43.057
    Sep  7 05:46:43.057: INFO: Pod "pod-f8323682-5227-4660-9206-54bd22002104" satisfied condition "Succeeded or Failed"
    Sep  7 05:46:43.059: INFO: Trying to get logs from node kind-worker pod pod-f8323682-5227-4660-9206-54bd22002104 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:46:43.067
    Sep  7 05:46:43.074: INFO: Waiting for pod pod-f8323682-5227-4660-9206-54bd22002104 to disappear
    Sep  7 05:46:43.076: INFO: Pod pod-f8323682-5227-4660-9206-54bd22002104 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:43.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4855" for this suite. 09/07/23 05:46:43.078
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:43.082
Sep  7 05:46:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename runtimeclass 09/07/23 05:46:43.082
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:43.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:43.09
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  7 05:46:43.098: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9058 to be scheduled
Sep  7 05:46:43.099: INFO: 1 pods are not scheduled: [runtimeclass-9058/test-runtimeclass-runtimeclass-9058-preconfigured-handler-x6x4l(1824c5be-77f9-463a-86ff-295e1acaaa59)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:45.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9058" for this suite. 09/07/23 05:46:45.108
------------------------------
â€¢ [2.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:43.082
    Sep  7 05:46:43.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename runtimeclass 09/07/23 05:46:43.082
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:43.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:43.09
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  7 05:46:43.098: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9058 to be scheduled
    Sep  7 05:46:43.099: INFO: 1 pods are not scheduled: [runtimeclass-9058/test-runtimeclass-runtimeclass-9058-preconfigured-handler-x6x4l(1824c5be-77f9-463a-86ff-295e1acaaa59)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:45.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9058" for this suite. 09/07/23 05:46:45.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:45.114
Sep  7 05:46:45.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:46:45.115
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:45.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:45.123
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/07/23 05:46:45.125
Sep  7 05:46:45.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 create -f -'
Sep  7 05:46:45.593: INFO: stderr: ""
Sep  7 05:46:45.593: INFO: stdout: "pod/pause created\n"
Sep  7 05:46:45.593: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  7 05:46:45.593: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6490" to be "running and ready"
Sep  7 05:46:45.595: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.83684ms
Sep  7 05:46:45.595: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kind-worker' to be 'Running' but was 'Pending'
Sep  7 05:46:47.597: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459409s
Sep  7 05:46:47.597: INFO: Pod "pause" satisfied condition "running and ready"
Sep  7 05:46:47.597: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/07/23 05:46:47.597
Sep  7 05:46:47.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 label pods pause testing-label=testing-label-value'
Sep  7 05:46:47.657: INFO: stderr: ""
Sep  7 05:46:47.657: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/07/23 05:46:47.657
Sep  7 05:46:47.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pod pause -L testing-label'
Sep  7 05:46:47.708: INFO: stderr: ""
Sep  7 05:46:47.708: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/07/23 05:46:47.708
Sep  7 05:46:47.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 label pods pause testing-label-'
Sep  7 05:46:47.766: INFO: stderr: ""
Sep  7 05:46:47.766: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/07/23 05:46:47.766
Sep  7 05:46:47.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pod pause -L testing-label'
Sep  7 05:46:47.818: INFO: stderr: ""
Sep  7 05:46:47.818: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/07/23 05:46:47.818
Sep  7 05:46:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 delete --grace-period=0 --force -f -'
Sep  7 05:46:47.872: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 05:46:47.872: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  7 05:46:47.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get rc,svc -l name=pause --no-headers'
Sep  7 05:46:47.931: INFO: stderr: "No resources found in kubectl-6490 namespace.\n"
Sep  7 05:46:47.931: INFO: stdout: ""
Sep  7 05:46:47.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  7 05:46:47.980: INFO: stderr: ""
Sep  7 05:46:47.980: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:47.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6490" for this suite. 09/07/23 05:46:47.982
------------------------------
â€¢ [2.872 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:45.114
    Sep  7 05:46:45.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:46:45.115
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:45.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:45.123
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/07/23 05:46:45.125
    Sep  7 05:46:45.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 create -f -'
    Sep  7 05:46:45.593: INFO: stderr: ""
    Sep  7 05:46:45.593: INFO: stdout: "pod/pause created\n"
    Sep  7 05:46:45.593: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  7 05:46:45.593: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6490" to be "running and ready"
    Sep  7 05:46:45.595: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.83684ms
    Sep  7 05:46:45.595: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kind-worker' to be 'Running' but was 'Pending'
    Sep  7 05:46:47.597: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004459409s
    Sep  7 05:46:47.597: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  7 05:46:47.597: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/07/23 05:46:47.597
    Sep  7 05:46:47.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 label pods pause testing-label=testing-label-value'
    Sep  7 05:46:47.657: INFO: stderr: ""
    Sep  7 05:46:47.657: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/07/23 05:46:47.657
    Sep  7 05:46:47.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pod pause -L testing-label'
    Sep  7 05:46:47.708: INFO: stderr: ""
    Sep  7 05:46:47.708: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/07/23 05:46:47.708
    Sep  7 05:46:47.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 label pods pause testing-label-'
    Sep  7 05:46:47.766: INFO: stderr: ""
    Sep  7 05:46:47.766: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/07/23 05:46:47.766
    Sep  7 05:46:47.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pod pause -L testing-label'
    Sep  7 05:46:47.818: INFO: stderr: ""
    Sep  7 05:46:47.818: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/07/23 05:46:47.818
    Sep  7 05:46:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 delete --grace-period=0 --force -f -'
    Sep  7 05:46:47.872: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 05:46:47.872: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  7 05:46:47.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get rc,svc -l name=pause --no-headers'
    Sep  7 05:46:47.931: INFO: stderr: "No resources found in kubectl-6490 namespace.\n"
    Sep  7 05:46:47.931: INFO: stdout: ""
    Sep  7 05:46:47.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6490 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  7 05:46:47.980: INFO: stderr: ""
    Sep  7 05:46:47.980: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:47.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6490" for this suite. 09/07/23 05:46:47.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:47.986
Sep  7 05:46:47.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 05:46:47.987
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:47.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:47.997
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/07/23 05:46:47.999
STEP: submitting the pod to kubernetes 09/07/23 05:46:47.999
STEP: verifying QOS class is set on the pod 09/07/23 05:46:48.005
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:48.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5403" for this suite. 09/07/23 05:46:48.009
------------------------------
â€¢ [0.026 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:47.986
    Sep  7 05:46:47.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 05:46:47.987
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:47.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:47.997
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/07/23 05:46:47.999
    STEP: submitting the pod to kubernetes 09/07/23 05:46:47.999
    STEP: verifying QOS class is set on the pod 09/07/23 05:46:48.005
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:48.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5403" for this suite. 09/07/23 05:46:48.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:48.012
Sep  7 05:46:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:46:48.013
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:48.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:48.021
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:46:48.028
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:46:48.253
STEP: Deploying the webhook pod 09/07/23 05:46:48.258
STEP: Wait for the deployment to be ready 09/07/23 05:46:48.266
Sep  7 05:46:48.269: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/07/23 05:46:50.275
STEP: Verifying the service has paired with the endpoint 09/07/23 05:46:50.286
Sep  7 05:46:51.286: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/07/23 05:46:51.288
STEP: create a pod 09/07/23 05:46:51.299
Sep  7 05:46:51.301: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4462" to be "running"
Sep  7 05:46:51.303: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53046ms
Sep  7 05:46:53.306: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004748631s
Sep  7 05:46:53.306: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/07/23 05:46:53.306
Sep  7 05:46:53.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=webhook-4462 attach --namespace=webhook-4462 to-be-attached-pod -i -c=container1'
Sep  7 05:46:53.368: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:53.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4462" for this suite. 09/07/23 05:46:53.393
STEP: Destroying namespace "webhook-4462-markers" for this suite. 09/07/23 05:46:53.397
------------------------------
â€¢ [SLOW TEST] [5.390 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:48.012
    Sep  7 05:46:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:46:48.013
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:48.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:48.021
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:46:48.028
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:46:48.253
    STEP: Deploying the webhook pod 09/07/23 05:46:48.258
    STEP: Wait for the deployment to be ready 09/07/23 05:46:48.266
    Sep  7 05:46:48.269: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/07/23 05:46:50.275
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:46:50.286
    Sep  7 05:46:51.286: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/07/23 05:46:51.288
    STEP: create a pod 09/07/23 05:46:51.299
    Sep  7 05:46:51.301: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4462" to be "running"
    Sep  7 05:46:51.303: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53046ms
    Sep  7 05:46:53.306: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004748631s
    Sep  7 05:46:53.306: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/07/23 05:46:53.306
    Sep  7 05:46:53.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=webhook-4462 attach --namespace=webhook-4462 to-be-attached-pod -i -c=container1'
    Sep  7 05:46:53.368: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:53.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4462" for this suite. 09/07/23 05:46:53.393
    STEP: Destroying namespace "webhook-4462-markers" for this suite. 09/07/23 05:46:53.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:53.403
Sep  7 05:46:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:46:53.404
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.413
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/07/23 05:46:53.417
STEP: waiting for available Endpoint 09/07/23 05:46:53.42
STEP: listing all Endpoints 09/07/23 05:46:53.421
STEP: updating the Endpoint 09/07/23 05:46:53.423
STEP: fetching the Endpoint 09/07/23 05:46:53.429
STEP: patching the Endpoint 09/07/23 05:46:53.431
STEP: fetching the Endpoint 09/07/23 05:46:53.437
STEP: deleting the Endpoint by Collection 09/07/23 05:46:53.439
STEP: waiting for Endpoint deletion 09/07/23 05:46:53.443
STEP: fetching the Endpoint 09/07/23 05:46:53.443
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:53.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-757" for this suite. 09/07/23 05:46:53.447
------------------------------
â€¢ [0.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:53.403
    Sep  7 05:46:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:46:53.404
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.413
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/07/23 05:46:53.417
    STEP: waiting for available Endpoint 09/07/23 05:46:53.42
    STEP: listing all Endpoints 09/07/23 05:46:53.421
    STEP: updating the Endpoint 09/07/23 05:46:53.423
    STEP: fetching the Endpoint 09/07/23 05:46:53.429
    STEP: patching the Endpoint 09/07/23 05:46:53.431
    STEP: fetching the Endpoint 09/07/23 05:46:53.437
    STEP: deleting the Endpoint by Collection 09/07/23 05:46:53.439
    STEP: waiting for Endpoint deletion 09/07/23 05:46:53.443
    STEP: fetching the Endpoint 09/07/23 05:46:53.443
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:53.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-757" for this suite. 09/07/23 05:46:53.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:53.45
Sep  7 05:46:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename discovery 09/07/23 05:46:53.451
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.464
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/07/23 05:46:53.466
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  7 05:46:53.726: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  7 05:46:53.726: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  7 05:46:53.726: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  7 05:46:53.726: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  7 05:46:53.726: INFO: Checking APIGroup: apps
Sep  7 05:46:53.727: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  7 05:46:53.727: INFO: Versions found [{apps/v1 v1}]
Sep  7 05:46:53.727: INFO: apps/v1 matches apps/v1
Sep  7 05:46:53.727: INFO: Checking APIGroup: events.k8s.io
Sep  7 05:46:53.728: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  7 05:46:53.728: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  7 05:46:53.728: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  7 05:46:53.728: INFO: Checking APIGroup: authentication.k8s.io
Sep  7 05:46:53.728: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  7 05:46:53.728: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  7 05:46:53.728: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  7 05:46:53.728: INFO: Checking APIGroup: authorization.k8s.io
Sep  7 05:46:53.729: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  7 05:46:53.729: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  7 05:46:53.729: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  7 05:46:53.729: INFO: Checking APIGroup: autoscaling
Sep  7 05:46:53.729: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  7 05:46:53.729: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  7 05:46:53.729: INFO: autoscaling/v2 matches autoscaling/v2
Sep  7 05:46:53.729: INFO: Checking APIGroup: batch
Sep  7 05:46:53.730: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  7 05:46:53.730: INFO: Versions found [{batch/v1 v1}]
Sep  7 05:46:53.730: INFO: batch/v1 matches batch/v1
Sep  7 05:46:53.730: INFO: Checking APIGroup: certificates.k8s.io
Sep  7 05:46:53.730: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  7 05:46:53.730: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  7 05:46:53.730: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  7 05:46:53.731: INFO: Checking APIGroup: networking.k8s.io
Sep  7 05:46:53.731: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  7 05:46:53.731: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  7 05:46:53.731: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  7 05:46:53.731: INFO: Checking APIGroup: policy
Sep  7 05:46:53.732: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  7 05:46:53.732: INFO: Versions found [{policy/v1 v1}]
Sep  7 05:46:53.732: INFO: policy/v1 matches policy/v1
Sep  7 05:46:53.732: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  7 05:46:53.732: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  7 05:46:53.732: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  7 05:46:53.732: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  7 05:46:53.732: INFO: Checking APIGroup: storage.k8s.io
Sep  7 05:46:53.733: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  7 05:46:53.733: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  7 05:46:53.733: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  7 05:46:53.733: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  7 05:46:53.733: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  7 05:46:53.733: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  7 05:46:53.733: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  7 05:46:53.733: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  7 05:46:53.734: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  7 05:46:53.734: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  7 05:46:53.734: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  7 05:46:53.734: INFO: Checking APIGroup: scheduling.k8s.io
Sep  7 05:46:53.734: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  7 05:46:53.734: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  7 05:46:53.734: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  7 05:46:53.734: INFO: Checking APIGroup: coordination.k8s.io
Sep  7 05:46:53.735: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  7 05:46:53.735: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  7 05:46:53.735: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  7 05:46:53.735: INFO: Checking APIGroup: node.k8s.io
Sep  7 05:46:53.736: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  7 05:46:53.736: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  7 05:46:53.736: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  7 05:46:53.736: INFO: Checking APIGroup: discovery.k8s.io
Sep  7 05:46:53.736: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  7 05:46:53.736: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  7 05:46:53.736: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  7 05:46:53.736: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  7 05:46:53.737: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  7 05:46:53.737: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  7 05:46:53.737: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:53.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3319" for this suite. 09/07/23 05:46:53.739
------------------------------
â€¢ [0.292 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:53.45
    Sep  7 05:46:53.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename discovery 09/07/23 05:46:53.451
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.464
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/07/23 05:46:53.466
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  7 05:46:53.726: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  7 05:46:53.726: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  7 05:46:53.726: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  7 05:46:53.726: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  7 05:46:53.726: INFO: Checking APIGroup: apps
    Sep  7 05:46:53.727: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  7 05:46:53.727: INFO: Versions found [{apps/v1 v1}]
    Sep  7 05:46:53.727: INFO: apps/v1 matches apps/v1
    Sep  7 05:46:53.727: INFO: Checking APIGroup: events.k8s.io
    Sep  7 05:46:53.728: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  7 05:46:53.728: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  7 05:46:53.728: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  7 05:46:53.728: INFO: Checking APIGroup: authentication.k8s.io
    Sep  7 05:46:53.728: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  7 05:46:53.728: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  7 05:46:53.728: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  7 05:46:53.728: INFO: Checking APIGroup: authorization.k8s.io
    Sep  7 05:46:53.729: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  7 05:46:53.729: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  7 05:46:53.729: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  7 05:46:53.729: INFO: Checking APIGroup: autoscaling
    Sep  7 05:46:53.729: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  7 05:46:53.729: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  7 05:46:53.729: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  7 05:46:53.729: INFO: Checking APIGroup: batch
    Sep  7 05:46:53.730: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  7 05:46:53.730: INFO: Versions found [{batch/v1 v1}]
    Sep  7 05:46:53.730: INFO: batch/v1 matches batch/v1
    Sep  7 05:46:53.730: INFO: Checking APIGroup: certificates.k8s.io
    Sep  7 05:46:53.730: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  7 05:46:53.730: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  7 05:46:53.730: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  7 05:46:53.731: INFO: Checking APIGroup: networking.k8s.io
    Sep  7 05:46:53.731: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  7 05:46:53.731: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  7 05:46:53.731: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  7 05:46:53.731: INFO: Checking APIGroup: policy
    Sep  7 05:46:53.732: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  7 05:46:53.732: INFO: Versions found [{policy/v1 v1}]
    Sep  7 05:46:53.732: INFO: policy/v1 matches policy/v1
    Sep  7 05:46:53.732: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  7 05:46:53.732: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  7 05:46:53.732: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  7 05:46:53.732: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  7 05:46:53.732: INFO: Checking APIGroup: storage.k8s.io
    Sep  7 05:46:53.733: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  7 05:46:53.733: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  7 05:46:53.733: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  7 05:46:53.733: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  7 05:46:53.733: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  7 05:46:53.733: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  7 05:46:53.733: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  7 05:46:53.733: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  7 05:46:53.734: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  7 05:46:53.734: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  7 05:46:53.734: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  7 05:46:53.734: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  7 05:46:53.734: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  7 05:46:53.734: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  7 05:46:53.734: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  7 05:46:53.734: INFO: Checking APIGroup: coordination.k8s.io
    Sep  7 05:46:53.735: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  7 05:46:53.735: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  7 05:46:53.735: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  7 05:46:53.735: INFO: Checking APIGroup: node.k8s.io
    Sep  7 05:46:53.736: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  7 05:46:53.736: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  7 05:46:53.736: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  7 05:46:53.736: INFO: Checking APIGroup: discovery.k8s.io
    Sep  7 05:46:53.736: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  7 05:46:53.736: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  7 05:46:53.736: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  7 05:46:53.736: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  7 05:46:53.737: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  7 05:46:53.737: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  7 05:46:53.737: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:53.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3319" for this suite. 09/07/23 05:46:53.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:53.744
Sep  7 05:46:53.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 05:46:53.745
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.755
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-drdlf" 09/07/23 05:46:53.757
Sep  7 05:46:53.765: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-drdlf-1003" 09/07/23 05:46:53.765
Sep  7 05:46:53.769: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-drdlf-1003" 09/07/23 05:46:53.769
Sep  7 05:46:53.773: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:53.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1236" for this suite. 09/07/23 05:46:53.775
STEP: Destroying namespace "e2e-ns-drdlf-1003" for this suite. 09/07/23 05:46:53.778
------------------------------
â€¢ [0.038 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:53.744
    Sep  7 05:46:53.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 05:46:53.745
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.755
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-drdlf" 09/07/23 05:46:53.757
    Sep  7 05:46:53.765: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-drdlf-1003" 09/07/23 05:46:53.765
    Sep  7 05:46:53.769: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-drdlf-1003" 09/07/23 05:46:53.769
    Sep  7 05:46:53.773: INFO: Namespace "e2e-ns-drdlf-1003" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:53.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1236" for this suite. 09/07/23 05:46:53.775
    STEP: Destroying namespace "e2e-ns-drdlf-1003" for this suite. 09/07/23 05:46:53.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:53.783
Sep  7 05:46:53.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename endpointslicemirroring 09/07/23 05:46:53.783
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.792
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/07/23 05:46:53.8
Sep  7 05:46:53.805: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/07/23 05:46:55.807
Sep  7 05:46:55.812: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 09/07/23 05:46:57.815
Sep  7 05:46:57.820: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  7 05:46:59.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-5516" for this suite. 09/07/23 05:46:59.826
------------------------------
â€¢ [SLOW TEST] [6.047 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:53.783
    Sep  7 05:46:53.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename endpointslicemirroring 09/07/23 05:46:53.783
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:53.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:53.792
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/07/23 05:46:53.8
    Sep  7 05:46:53.805: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/07/23 05:46:55.807
    Sep  7 05:46:55.812: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 09/07/23 05:46:57.815
    Sep  7 05:46:57.820: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:46:59.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-5516" for this suite. 09/07/23 05:46:59.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:46:59.831
Sep  7 05:46:59.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 05:46:59.831
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:59.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:59.842
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/07/23 05:46:59.844
STEP: Ensuring job reaches completions 09/07/23 05:46:59.847
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:09.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7354" for this suite. 09/07/23 05:47:09.853
------------------------------
â€¢ [SLOW TEST] [10.025 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:46:59.831
    Sep  7 05:46:59.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 05:46:59.831
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:46:59.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:46:59.842
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/07/23 05:46:59.844
    STEP: Ensuring job reaches completions 09/07/23 05:46:59.847
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:09.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7354" for this suite. 09/07/23 05:47:09.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:09.857
Sep  7 05:47:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:47:09.857
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:09.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:09.869
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/07/23 05:47:09.875
Sep  7 05:47:09.875: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9" in namespace "kubelet-test-4755" to be "completed"
Sep  7 05:47:09.876: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35115ms
Sep  7 05:47:11.879: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00418009s
Sep  7 05:47:13.880: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005159981s
Sep  7 05:47:13.880: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4755" for this suite. 09/07/23 05:47:13.887
------------------------------
â€¢ [4.034 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:09.857
    Sep  7 05:47:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubelet-test 09/07/23 05:47:09.857
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:09.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:09.869
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/07/23 05:47:09.875
    Sep  7 05:47:09.875: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9" in namespace "kubelet-test-4755" to be "completed"
    Sep  7 05:47:09.876: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35115ms
    Sep  7 05:47:11.879: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00418009s
    Sep  7 05:47:13.880: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005159981s
    Sep  7 05:47:13.880: INFO: Pod "agnhost-host-aliases86c5af77-319e-4a8c-b4f0-d4a46ac07dd9" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4755" for this suite. 09/07/23 05:47:13.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:13.893
Sep  7 05:47:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:47:13.894
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:13.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:13.906
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/07/23 05:47:13.907
Sep  7 05:47:13.912: INFO: Waiting up to 5m0s for pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c" in namespace "emptydir-7512" to be "Succeeded or Failed"
Sep  7 05:47:13.913: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46895ms
Sep  7 05:47:15.916: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003676871s
Sep  7 05:47:17.917: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004876275s
STEP: Saw pod success 09/07/23 05:47:17.917
Sep  7 05:47:17.917: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c" satisfied condition "Succeeded or Failed"
Sep  7 05:47:17.919: INFO: Trying to get logs from node kind-worker pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c container test-container: <nil>
STEP: delete the pod 09/07/23 05:47:17.922
Sep  7 05:47:17.930: INFO: Waiting for pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c to disappear
Sep  7 05:47:17.932: INFO: Pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7512" for this suite. 09/07/23 05:47:17.934
------------------------------
â€¢ [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:13.893
    Sep  7 05:47:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:47:13.894
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:13.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:13.906
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/07/23 05:47:13.907
    Sep  7 05:47:13.912: INFO: Waiting up to 5m0s for pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c" in namespace "emptydir-7512" to be "Succeeded or Failed"
    Sep  7 05:47:13.913: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46895ms
    Sep  7 05:47:15.916: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003676871s
    Sep  7 05:47:17.917: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004876275s
    STEP: Saw pod success 09/07/23 05:47:17.917
    Sep  7 05:47:17.917: INFO: Pod "pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c" satisfied condition "Succeeded or Failed"
    Sep  7 05:47:17.919: INFO: Trying to get logs from node kind-worker pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c container test-container: <nil>
    STEP: delete the pod 09/07/23 05:47:17.922
    Sep  7 05:47:17.930: INFO: Waiting for pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c to disappear
    Sep  7 05:47:17.932: INFO: Pod pod-5712a4cd-4510-4b93-a96f-0ceb1644ab0c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7512" for this suite. 09/07/23 05:47:17.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:17.937
Sep  7 05:47:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename runtimeclass 09/07/23 05:47:17.937
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:17.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:17.947
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  7 05:47:17.957: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9932 to be scheduled
Sep  7 05:47:17.958: INFO: 1 pods are not scheduled: [runtimeclass-9932/test-runtimeclass-runtimeclass-9932-preconfigured-handler-h8gg2(0cd2aba2-1984-458d-93e9-538ee6e48ad0)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9932" for this suite. 09/07/23 05:47:19.967
------------------------------
â€¢ [2.035 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:17.937
    Sep  7 05:47:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename runtimeclass 09/07/23 05:47:17.937
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:17.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:17.947
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  7 05:47:17.957: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9932 to be scheduled
    Sep  7 05:47:17.958: INFO: 1 pods are not scheduled: [runtimeclass-9932/test-runtimeclass-runtimeclass-9932-preconfigured-handler-h8gg2(0cd2aba2-1984-458d-93e9-538ee6e48ad0)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9932" for this suite. 09/07/23 05:47:19.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:19.973
Sep  7 05:47:19.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pod-network-test 09/07/23 05:47:19.974
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:19.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:19.982
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-2829 09/07/23 05:47:19.984
STEP: creating a selector 09/07/23 05:47:19.984
STEP: Creating the service pods in kubernetes 09/07/23 05:47:19.984
Sep  7 05:47:19.984: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  7 05:47:19.998: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2829" to be "running and ready"
Sep  7 05:47:19.999: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58746ms
Sep  7 05:47:19.999: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:47:22.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004587094s
Sep  7 05:47:22.002: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:47:24.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00464946s
Sep  7 05:47:24.002: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:47:26.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005194557s
Sep  7 05:47:26.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:47:28.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005317504s
Sep  7 05:47:28.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:47:30.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005189443s
Sep  7 05:47:30.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 05:47:32.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004735292s
Sep  7 05:47:32.002: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  7 05:47:32.002: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  7 05:47:32.005: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2829" to be "running and ready"
Sep  7 05:47:32.007: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.421429ms
Sep  7 05:47:32.007: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  7 05:47:32.007: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/07/23 05:47:32.009
Sep  7 05:47:32.019: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2829" to be "running"
Sep  7 05:47:32.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.76241ms
Sep  7 05:47:34.023: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00408157s
Sep  7 05:47:34.023: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  7 05:47:34.025: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2829" to be "running"
Sep  7 05:47:34.026: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.52117ms
Sep  7 05:47:34.026: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  7 05:47:34.027: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  7 05:47:34.027: INFO: Going to poll 10.244.1.178 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  7 05:47:34.029: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2829 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:47:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:47:34.029: INFO: ExecWithOptions: Clientset creation
Sep  7 05:47:34.029: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2829/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  7 05:47:35.131: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  7 05:47:35.131: INFO: Going to poll 10.244.2.148 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  7 05:47:35.133: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2829 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 05:47:35.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:47:35.133: INFO: ExecWithOptions: Clientset creation
Sep  7 05:47:35.133: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2829/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  7 05:47:36.256: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:36.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2829" for this suite. 09/07/23 05:47:36.259
------------------------------
â€¢ [SLOW TEST] [16.289 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:19.973
    Sep  7 05:47:19.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pod-network-test 09/07/23 05:47:19.974
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:19.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:19.982
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-2829 09/07/23 05:47:19.984
    STEP: creating a selector 09/07/23 05:47:19.984
    STEP: Creating the service pods in kubernetes 09/07/23 05:47:19.984
    Sep  7 05:47:19.984: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  7 05:47:19.998: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2829" to be "running and ready"
    Sep  7 05:47:19.999: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58746ms
    Sep  7 05:47:19.999: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:47:22.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004587094s
    Sep  7 05:47:22.002: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:47:24.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00464946s
    Sep  7 05:47:24.002: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:47:26.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005194557s
    Sep  7 05:47:26.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:47:28.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005317504s
    Sep  7 05:47:28.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:47:30.003: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005189443s
    Sep  7 05:47:30.003: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 05:47:32.002: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004735292s
    Sep  7 05:47:32.002: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  7 05:47:32.002: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  7 05:47:32.005: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2829" to be "running and ready"
    Sep  7 05:47:32.007: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.421429ms
    Sep  7 05:47:32.007: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  7 05:47:32.007: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/07/23 05:47:32.009
    Sep  7 05:47:32.019: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2829" to be "running"
    Sep  7 05:47:32.020: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.76241ms
    Sep  7 05:47:34.023: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00408157s
    Sep  7 05:47:34.023: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  7 05:47:34.025: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2829" to be "running"
    Sep  7 05:47:34.026: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.52117ms
    Sep  7 05:47:34.026: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  7 05:47:34.027: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  7 05:47:34.027: INFO: Going to poll 10.244.1.178 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  7 05:47:34.029: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2829 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:47:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:47:34.029: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:47:34.029: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2829/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  7 05:47:35.131: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  7 05:47:35.131: INFO: Going to poll 10.244.2.148 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  7 05:47:35.133: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2829 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 05:47:35.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:47:35.133: INFO: ExecWithOptions: Clientset creation
    Sep  7 05:47:35.133: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2829/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  7 05:47:36.256: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:36.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2829" for this suite. 09/07/23 05:47:36.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:36.263
Sep  7 05:47:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename proxy 09/07/23 05:47:36.264
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:36.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:36.274
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  7 05:47:36.276: INFO: Creating pod...
Sep  7 05:47:36.280: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2347" to be "running"
Sep  7 05:47:36.282: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71145ms
Sep  7 05:47:38.285: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004973992s
Sep  7 05:47:38.285: INFO: Pod "agnhost" satisfied condition "running"
Sep  7 05:47:38.285: INFO: Creating service...
Sep  7 05:47:38.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=DELETE
Sep  7 05:47:38.296: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  7 05:47:38.296: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=OPTIONS
Sep  7 05:47:38.299: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  7 05:47:38.299: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=PATCH
Sep  7 05:47:38.301: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  7 05:47:38.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=POST
Sep  7 05:47:38.303: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  7 05:47:38.303: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=PUT
Sep  7 05:47:38.305: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  7 05:47:38.305: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  7 05:47:38.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  7 05:47:38.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  7 05:47:38.309: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  7 05:47:38.309: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  7 05:47:38.311: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  7 05:47:38.311: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=POST
Sep  7 05:47:38.314: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  7 05:47:38.314: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=PUT
Sep  7 05:47:38.316: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  7 05:47:38.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=GET
Sep  7 05:47:38.317: INFO: http.Client request:GET StatusCode:301
Sep  7 05:47:38.318: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=GET
Sep  7 05:47:38.319: INFO: http.Client request:GET StatusCode:301
Sep  7 05:47:38.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=HEAD
Sep  7 05:47:38.321: INFO: http.Client request:HEAD StatusCode:301
Sep  7 05:47:38.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  7 05:47:38.323: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:38.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2347" for this suite. 09/07/23 05:47:38.325
------------------------------
â€¢ [2.066 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:36.263
    Sep  7 05:47:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename proxy 09/07/23 05:47:36.264
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:36.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:36.274
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  7 05:47:36.276: INFO: Creating pod...
    Sep  7 05:47:36.280: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2347" to be "running"
    Sep  7 05:47:36.282: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71145ms
    Sep  7 05:47:38.285: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004973992s
    Sep  7 05:47:38.285: INFO: Pod "agnhost" satisfied condition "running"
    Sep  7 05:47:38.285: INFO: Creating service...
    Sep  7 05:47:38.293: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=DELETE
    Sep  7 05:47:38.296: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  7 05:47:38.296: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=OPTIONS
    Sep  7 05:47:38.299: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  7 05:47:38.299: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=PATCH
    Sep  7 05:47:38.301: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  7 05:47:38.301: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=POST
    Sep  7 05:47:38.303: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  7 05:47:38.303: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=PUT
    Sep  7 05:47:38.305: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  7 05:47:38.305: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  7 05:47:38.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  7 05:47:38.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  7 05:47:38.309: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  7 05:47:38.309: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  7 05:47:38.311: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  7 05:47:38.311: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=POST
    Sep  7 05:47:38.314: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  7 05:47:38.314: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  7 05:47:38.316: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  7 05:47:38.316: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=GET
    Sep  7 05:47:38.317: INFO: http.Client request:GET StatusCode:301
    Sep  7 05:47:38.318: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=GET
    Sep  7 05:47:38.319: INFO: http.Client request:GET StatusCode:301
    Sep  7 05:47:38.319: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/pods/agnhost/proxy?method=HEAD
    Sep  7 05:47:38.321: INFO: http.Client request:HEAD StatusCode:301
    Sep  7 05:47:38.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2347/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  7 05:47:38.323: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:38.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2347" for this suite. 09/07/23 05:47:38.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:38.33
Sep  7 05:47:38.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:47:38.331
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:38.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:38.342
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:47:38.351
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:47:38.517
STEP: Deploying the webhook pod 09/07/23 05:47:38.522
STEP: Wait for the deployment to be ready 09/07/23 05:47:38.532
Sep  7 05:47:38.538: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:47:40.545
STEP: Verifying the service has paired with the endpoint 09/07/23 05:47:40.553
Sep  7 05:47:41.553: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/07/23 05:47:41.555
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/07/23 05:47:41.567
STEP: Creating a dummy validating-webhook-configuration object 09/07/23 05:47:41.576
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/07/23 05:47:41.583
STEP: Creating a dummy mutating-webhook-configuration object 09/07/23 05:47:41.589
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/07/23 05:47:41.594
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:41.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4971" for this suite. 09/07/23 05:47:41.628
STEP: Destroying namespace "webhook-4971-markers" for this suite. 09/07/23 05:47:41.631
------------------------------
â€¢ [3.305 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:38.33
    Sep  7 05:47:38.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:47:38.331
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:38.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:38.342
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:47:38.351
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:47:38.517
    STEP: Deploying the webhook pod 09/07/23 05:47:38.522
    STEP: Wait for the deployment to be ready 09/07/23 05:47:38.532
    Sep  7 05:47:38.538: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:47:40.545
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:47:40.553
    Sep  7 05:47:41.553: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/07/23 05:47:41.555
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/07/23 05:47:41.567
    STEP: Creating a dummy validating-webhook-configuration object 09/07/23 05:47:41.576
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/07/23 05:47:41.583
    STEP: Creating a dummy mutating-webhook-configuration object 09/07/23 05:47:41.589
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/07/23 05:47:41.594
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:41.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4971" for this suite. 09/07/23 05:47:41.628
    STEP: Destroying namespace "webhook-4971-markers" for this suite. 09/07/23 05:47:41.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:41.635
Sep  7 05:47:41.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 05:47:41.636
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:41.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:41.646
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/07/23 05:47:41.65
Sep  7 05:47:41.656: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2389" to be "running and ready"
Sep  7 05:47:41.657: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35212ms
Sep  7 05:47:41.657: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:47:43.661: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004963733s
Sep  7 05:47:43.661: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  7 05:47:43.661: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/07/23 05:47:43.663
Sep  7 05:47:43.668: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2389" to be "running and ready"
Sep  7 05:47:43.669: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31928ms
Sep  7 05:47:43.669: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:47:45.672: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003944285s
Sep  7 05:47:45.672: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  7 05:47:45.672: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/07/23 05:47:45.674
STEP: delete the pod with lifecycle hook 09/07/23 05:47:45.678
Sep  7 05:47:45.681: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  7 05:47:45.683: INFO: Pod pod-with-poststart-http-hook still exists
Sep  7 05:47:47.684: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  7 05:47:47.686: INFO: Pod pod-with-poststart-http-hook still exists
Sep  7 05:47:49.684: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  7 05:47:49.687: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:49.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2389" for this suite. 09/07/23 05:47:49.689
------------------------------
â€¢ [SLOW TEST] [8.057 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:41.635
    Sep  7 05:47:41.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 05:47:41.636
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:41.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:41.646
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/07/23 05:47:41.65
    Sep  7 05:47:41.656: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2389" to be "running and ready"
    Sep  7 05:47:41.657: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35212ms
    Sep  7 05:47:41.657: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:47:43.661: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004963733s
    Sep  7 05:47:43.661: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  7 05:47:43.661: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/07/23 05:47:43.663
    Sep  7 05:47:43.668: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2389" to be "running and ready"
    Sep  7 05:47:43.669: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31928ms
    Sep  7 05:47:43.669: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:47:45.672: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.003944285s
    Sep  7 05:47:45.672: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  7 05:47:45.672: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/07/23 05:47:45.674
    STEP: delete the pod with lifecycle hook 09/07/23 05:47:45.678
    Sep  7 05:47:45.681: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  7 05:47:45.683: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  7 05:47:47.684: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  7 05:47:47.686: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  7 05:47:49.684: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  7 05:47:49.687: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:49.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2389" for this suite. 09/07/23 05:47:49.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:49.692
Sep  7 05:47:49.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:47:49.693
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:49.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:49.703
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-695ca30e-c67d-4bc0-ba52-8d1b5fe1b960 09/07/23 05:47:49.705
STEP: Creating a pod to test consume secrets 09/07/23 05:47:49.707
Sep  7 05:47:49.713: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0" in namespace "projected-6857" to be "Succeeded or Failed"
Sep  7 05:47:49.714: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63289ms
Sep  7 05:47:51.719: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006048647s
Sep  7 05:47:53.718: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005063856s
STEP: Saw pod success 09/07/23 05:47:53.718
Sep  7 05:47:53.718: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0" satisfied condition "Succeeded or Failed"
Sep  7 05:47:53.720: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:47:53.723
Sep  7 05:47:53.731: INFO: Waiting for pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 to disappear
Sep  7 05:47:53.733: INFO: Pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:53.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6857" for this suite. 09/07/23 05:47:53.734
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:49.692
    Sep  7 05:47:49.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:47:49.693
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:49.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:49.703
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-695ca30e-c67d-4bc0-ba52-8d1b5fe1b960 09/07/23 05:47:49.705
    STEP: Creating a pod to test consume secrets 09/07/23 05:47:49.707
    Sep  7 05:47:49.713: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0" in namespace "projected-6857" to be "Succeeded or Failed"
    Sep  7 05:47:49.714: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63289ms
    Sep  7 05:47:51.719: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006048647s
    Sep  7 05:47:53.718: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005063856s
    STEP: Saw pod success 09/07/23 05:47:53.718
    Sep  7 05:47:53.718: INFO: Pod "pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0" satisfied condition "Succeeded or Failed"
    Sep  7 05:47:53.720: INFO: Trying to get logs from node kind-worker2 pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:47:53.723
    Sep  7 05:47:53.731: INFO: Waiting for pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 to disappear
    Sep  7 05:47:53.733: INFO: Pod pod-projected-secrets-a7047154-a058-4588-946f-34a28d305ae0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:53.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6857" for this suite. 09/07/23 05:47:53.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:53.738
Sep  7 05:47:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption 09/07/23 05:47:53.738
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:53.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:53.748
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/07/23 05:47:53.752
STEP: Updating PodDisruptionBudget status 09/07/23 05:47:55.755
STEP: Waiting for all pods to be running 09/07/23 05:47:55.762
Sep  7 05:47:55.764: INFO: running pods: 0 < 1
STEP: locating a running pod 09/07/23 05:47:57.767
STEP: Waiting for the pdb to be processed 09/07/23 05:47:57.776
STEP: Patching PodDisruptionBudget status 09/07/23 05:47:57.78
STEP: Waiting for the pdb to be processed 09/07/23 05:47:57.786
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:47:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-539" for this suite. 09/07/23 05:47:57.789
------------------------------
â€¢ [4.056 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:53.738
    Sep  7 05:47:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption 09/07/23 05:47:53.738
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:53.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:53.748
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/07/23 05:47:53.752
    STEP: Updating PodDisruptionBudget status 09/07/23 05:47:55.755
    STEP: Waiting for all pods to be running 09/07/23 05:47:55.762
    Sep  7 05:47:55.764: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/07/23 05:47:57.767
    STEP: Waiting for the pdb to be processed 09/07/23 05:47:57.776
    STEP: Patching PodDisruptionBudget status 09/07/23 05:47:57.78
    STEP: Waiting for the pdb to be processed 09/07/23 05:47:57.786
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:47:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-539" for this suite. 09/07/23 05:47:57.789
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:47:57.794
Sep  7 05:47:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:47:57.794
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:57.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:57.803
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3671 09/07/23 05:47:57.805
STEP: changing the ExternalName service to type=NodePort 09/07/23 05:47:57.808
STEP: creating replication controller externalname-service in namespace services-3671 09/07/23 05:47:57.821
I0907 05:47:57.825310      29 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3671, replica count: 2
I0907 05:48:00.876594      29 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 05:48:00.876: INFO: Creating new exec pod
Sep  7 05:48:00.880: INFO: Waiting up to 5m0s for pod "execpodpxbl5" in namespace "services-3671" to be "running"
Sep  7 05:48:00.883: INFO: Pod "execpodpxbl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71423ms
Sep  7 05:48:02.884: INFO: Pod "execpodpxbl5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454482s
Sep  7 05:48:02.885: INFO: Pod "execpodpxbl5" satisfied condition "running"
Sep  7 05:48:03.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  7 05:48:04.043: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  7 05:48:04.043: INFO: stdout: ""
Sep  7 05:48:04.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 10.96.159.81 80'
Sep  7 05:48:04.202: INFO: stderr: "+ nc -v -z -w 2 10.96.159.81 80\nConnection to 10.96.159.81 80 port [tcp/http] succeeded!\n"
Sep  7 05:48:04.202: INFO: stdout: ""
Sep  7 05:48:04.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31164'
Sep  7 05:48:04.365: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31164\nConnection to 192.168.8.3 31164 port [tcp/*] succeeded!\n"
Sep  7 05:48:04.365: INFO: stdout: ""
Sep  7 05:48:04.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31164'
Sep  7 05:48:04.537: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31164\nConnection to 192.168.8.6 31164 port [tcp/*] succeeded!\n"
Sep  7 05:48:04.537: INFO: stdout: ""
Sep  7 05:48:04.537: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:04.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3671" for this suite. 09/07/23 05:48:04.555
------------------------------
â€¢ [SLOW TEST] [6.764 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:47:57.794
    Sep  7 05:47:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:47:57.794
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:47:57.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:47:57.803
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3671 09/07/23 05:47:57.805
    STEP: changing the ExternalName service to type=NodePort 09/07/23 05:47:57.808
    STEP: creating replication controller externalname-service in namespace services-3671 09/07/23 05:47:57.821
    I0907 05:47:57.825310      29 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3671, replica count: 2
    I0907 05:48:00.876594      29 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 05:48:00.876: INFO: Creating new exec pod
    Sep  7 05:48:00.880: INFO: Waiting up to 5m0s for pod "execpodpxbl5" in namespace "services-3671" to be "running"
    Sep  7 05:48:00.883: INFO: Pod "execpodpxbl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71423ms
    Sep  7 05:48:02.884: INFO: Pod "execpodpxbl5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004454482s
    Sep  7 05:48:02.885: INFO: Pod "execpodpxbl5" satisfied condition "running"
    Sep  7 05:48:03.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  7 05:48:04.043: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  7 05:48:04.043: INFO: stdout: ""
    Sep  7 05:48:04.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 10.96.159.81 80'
    Sep  7 05:48:04.202: INFO: stderr: "+ nc -v -z -w 2 10.96.159.81 80\nConnection to 10.96.159.81 80 port [tcp/http] succeeded!\n"
    Sep  7 05:48:04.202: INFO: stdout: ""
    Sep  7 05:48:04.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31164'
    Sep  7 05:48:04.365: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31164\nConnection to 192.168.8.3 31164 port [tcp/*] succeeded!\n"
    Sep  7 05:48:04.365: INFO: stdout: ""
    Sep  7 05:48:04.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-3671 exec execpodpxbl5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31164'
    Sep  7 05:48:04.537: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31164\nConnection to 192.168.8.6 31164 port [tcp/*] succeeded!\n"
    Sep  7 05:48:04.537: INFO: stdout: ""
    Sep  7 05:48:04.537: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:04.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3671" for this suite. 09/07/23 05:48:04.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:04.558
Sep  7 05:48:04.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename subpath 09/07/23 05:48:04.559
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:04.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:04.57
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/07/23 05:48:04.572
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-dwbd 09/07/23 05:48:04.577
STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:48:04.578
Sep  7 05:48:04.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dwbd" in namespace "subpath-6818" to be "Succeeded or Failed"
Sep  7 05:48:04.584: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56703ms
Sep  7 05:48:06.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005221603s
Sep  7 05:48:08.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.004489087s
Sep  7 05:48:10.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 6.005278442s
Sep  7 05:48:12.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004637818s
Sep  7 05:48:14.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 10.004131365s
Sep  7 05:48:16.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 12.005161493s
Sep  7 05:48:18.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 14.004170872s
Sep  7 05:48:20.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 16.005150261s
Sep  7 05:48:22.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004526341s
Sep  7 05:48:24.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 20.005520182s
Sep  7 05:48:26.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=false. Elapsed: 22.005293133s
Sep  7 05:48:28.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004773966s
STEP: Saw pod success 09/07/23 05:48:28.587
Sep  7 05:48:28.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd" satisfied condition "Succeeded or Failed"
Sep  7 05:48:28.589: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-downwardapi-dwbd container test-container-subpath-downwardapi-dwbd: <nil>
STEP: delete the pod 09/07/23 05:48:28.593
Sep  7 05:48:28.604: INFO: Waiting for pod pod-subpath-test-downwardapi-dwbd to disappear
Sep  7 05:48:28.605: INFO: Pod pod-subpath-test-downwardapi-dwbd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-dwbd 09/07/23 05:48:28.605
Sep  7 05:48:28.605: INFO: Deleting pod "pod-subpath-test-downwardapi-dwbd" in namespace "subpath-6818"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6818" for this suite. 09/07/23 05:48:28.609
------------------------------
â€¢ [SLOW TEST] [24.054 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:04.558
    Sep  7 05:48:04.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename subpath 09/07/23 05:48:04.559
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:04.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:04.57
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/07/23 05:48:04.572
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-dwbd 09/07/23 05:48:04.577
    STEP: Creating a pod to test atomic-volume-subpath 09/07/23 05:48:04.578
    Sep  7 05:48:04.583: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-dwbd" in namespace "subpath-6818" to be "Succeeded or Failed"
    Sep  7 05:48:04.584: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56703ms
    Sep  7 05:48:06.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005221603s
    Sep  7 05:48:08.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.004489087s
    Sep  7 05:48:10.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 6.005278442s
    Sep  7 05:48:12.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004637818s
    Sep  7 05:48:14.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 10.004131365s
    Sep  7 05:48:16.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 12.005161493s
    Sep  7 05:48:18.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 14.004170872s
    Sep  7 05:48:20.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 16.005150261s
    Sep  7 05:48:22.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004526341s
    Sep  7 05:48:24.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=true. Elapsed: 20.005520182s
    Sep  7 05:48:26.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Running", Reason="", readiness=false. Elapsed: 22.005293133s
    Sep  7 05:48:28.587: INFO: Pod "pod-subpath-test-downwardapi-dwbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004773966s
    STEP: Saw pod success 09/07/23 05:48:28.587
    Sep  7 05:48:28.588: INFO: Pod "pod-subpath-test-downwardapi-dwbd" satisfied condition "Succeeded or Failed"
    Sep  7 05:48:28.589: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-downwardapi-dwbd container test-container-subpath-downwardapi-dwbd: <nil>
    STEP: delete the pod 09/07/23 05:48:28.593
    Sep  7 05:48:28.604: INFO: Waiting for pod pod-subpath-test-downwardapi-dwbd to disappear
    Sep  7 05:48:28.605: INFO: Pod pod-subpath-test-downwardapi-dwbd no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-dwbd 09/07/23 05:48:28.605
    Sep  7 05:48:28.605: INFO: Deleting pod "pod-subpath-test-downwardapi-dwbd" in namespace "subpath-6818"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:28.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6818" for this suite. 09/07/23 05:48:28.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:28.613
Sep  7 05:48:28.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename watch 09/07/23 05:48:28.613
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:28.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:28.622
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/07/23 05:48:28.624
STEP: starting a background goroutine to produce watch events 09/07/23 05:48:28.625
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/07/23 05:48:28.625
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3683" for this suite. 09/07/23 05:48:31.466
------------------------------
â€¢ [2.905 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:28.613
    Sep  7 05:48:28.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename watch 09/07/23 05:48:28.613
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:28.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:28.622
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/07/23 05:48:28.624
    STEP: starting a background goroutine to produce watch events 09/07/23 05:48:28.625
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/07/23 05:48:28.625
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3683" for this suite. 09/07/23 05:48:31.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:31.52
Sep  7 05:48:31.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:48:31.521
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:31.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:31.53
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:48:31.531
Sep  7 05:48:31.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247" in namespace "downward-api-1202" to be "Succeeded or Failed"
Sep  7 05:48:31.537: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32921ms
Sep  7 05:48:33.540: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004064854s
Sep  7 05:48:35.542: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005715058s
STEP: Saw pod success 09/07/23 05:48:35.542
Sep  7 05:48:35.542: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247" satisfied condition "Succeeded or Failed"
Sep  7 05:48:35.544: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 container client-container: <nil>
STEP: delete the pod 09/07/23 05:48:35.547
Sep  7 05:48:35.555: INFO: Waiting for pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 to disappear
Sep  7 05:48:35.556: INFO: Pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:35.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1202" for this suite. 09/07/23 05:48:35.558
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:31.52
    Sep  7 05:48:31.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:48:31.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:31.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:31.53
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:48:31.531
    Sep  7 05:48:31.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247" in namespace "downward-api-1202" to be "Succeeded or Failed"
    Sep  7 05:48:31.537: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32921ms
    Sep  7 05:48:33.540: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004064854s
    Sep  7 05:48:35.542: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005715058s
    STEP: Saw pod success 09/07/23 05:48:35.542
    Sep  7 05:48:35.542: INFO: Pod "downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247" satisfied condition "Succeeded or Failed"
    Sep  7 05:48:35.544: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 container client-container: <nil>
    STEP: delete the pod 09/07/23 05:48:35.547
    Sep  7 05:48:35.555: INFO: Waiting for pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 to disappear
    Sep  7 05:48:35.556: INFO: Pod downwardapi-volume-4cef173b-801b-45c5-bd1e-3a7b835f3247 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:35.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1202" for this suite. 09/07/23 05:48:35.558
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:35.563
Sep  7 05:48:35.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sysctl 09/07/23 05:48:35.563
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:35.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:35.571
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/07/23 05:48:35.573
STEP: Watching for error events or started pod 09/07/23 05:48:35.579
STEP: Waiting for pod completion 09/07/23 05:48:37.582
Sep  7 05:48:37.582: INFO: Waiting up to 3m0s for pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc" in namespace "sysctl-8617" to be "completed"
Sep  7 05:48:37.584: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86576ms
Sep  7 05:48:39.586: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003979167s
Sep  7 05:48:39.586: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/07/23 05:48:39.587
STEP: Getting logs from the pod 09/07/23 05:48:39.587
STEP: Checking that the sysctl is actually updated 09/07/23 05:48:39.59
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:39.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8617" for this suite. 09/07/23 05:48:39.592
------------------------------
â€¢ [4.033 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:35.563
    Sep  7 05:48:35.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sysctl 09/07/23 05:48:35.563
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:35.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:35.571
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/07/23 05:48:35.573
    STEP: Watching for error events or started pod 09/07/23 05:48:35.579
    STEP: Waiting for pod completion 09/07/23 05:48:37.582
    Sep  7 05:48:37.582: INFO: Waiting up to 3m0s for pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc" in namespace "sysctl-8617" to be "completed"
    Sep  7 05:48:37.584: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86576ms
    Sep  7 05:48:39.586: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003979167s
    Sep  7 05:48:39.586: INFO: Pod "sysctl-34346c17-3661-46a3-877d-54491f61d1fc" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/07/23 05:48:39.587
    STEP: Getting logs from the pod 09/07/23 05:48:39.587
    STEP: Checking that the sysctl is actually updated 09/07/23 05:48:39.59
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:39.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8617" for this suite. 09/07/23 05:48:39.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:39.597
Sep  7 05:48:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:48:39.597
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:39.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:39.608
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/07/23 05:48:39.61
Sep  7 05:48:39.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/07/23 05:48:44.726
Sep  7 05:48:44.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 05:48:46.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:51.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2599" for this suite. 09/07/23 05:48:51.101
------------------------------
â€¢ [SLOW TEST] [11.508 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:39.597
    Sep  7 05:48:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:48:39.597
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:39.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:39.608
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/07/23 05:48:39.61
    Sep  7 05:48:39.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/07/23 05:48:44.726
    Sep  7 05:48:44.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 05:48:46.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:51.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2599" for this suite. 09/07/23 05:48:51.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:51.105
Sep  7 05:48:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption 09/07/23 05:48:51.106
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:51.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:51.117
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/07/23 05:48:51.119
STEP: Waiting for the pdb to be processed 09/07/23 05:48:51.121
STEP: First trying to evict a pod which shouldn't be evictable 09/07/23 05:48:53.129
STEP: Waiting for all pods to be running 09/07/23 05:48:53.129
Sep  7 05:48:53.131: INFO: pods: 0 < 3
STEP: locating a running pod 09/07/23 05:48:55.135
STEP: Updating the pdb to allow a pod to be evicted 09/07/23 05:48:55.14
STEP: Waiting for the pdb to be processed 09/07/23 05:48:55.145
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/07/23 05:48:57.149
STEP: Waiting for all pods to be running 09/07/23 05:48:57.149
STEP: Waiting for the pdb to observed all healthy pods 09/07/23 05:48:57.151
STEP: Patching the pdb to disallow a pod to be evicted 09/07/23 05:48:57.165
STEP: Waiting for the pdb to be processed 09/07/23 05:48:57.178
STEP: Waiting for all pods to be running 09/07/23 05:48:59.182
STEP: locating a running pod 09/07/23 05:48:59.184
STEP: Deleting the pdb to allow a pod to be evicted 09/07/23 05:48:59.19
STEP: Waiting for the pdb to be deleted 09/07/23 05:48:59.193
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/07/23 05:48:59.194
STEP: Waiting for all pods to be running 09/07/23 05:48:59.195
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:48:59.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6831" for this suite. 09/07/23 05:48:59.208
------------------------------
â€¢ [SLOW TEST] [8.110 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:51.105
    Sep  7 05:48:51.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption 09/07/23 05:48:51.106
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:51.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:51.117
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/07/23 05:48:51.119
    STEP: Waiting for the pdb to be processed 09/07/23 05:48:51.121
    STEP: First trying to evict a pod which shouldn't be evictable 09/07/23 05:48:53.129
    STEP: Waiting for all pods to be running 09/07/23 05:48:53.129
    Sep  7 05:48:53.131: INFO: pods: 0 < 3
    STEP: locating a running pod 09/07/23 05:48:55.135
    STEP: Updating the pdb to allow a pod to be evicted 09/07/23 05:48:55.14
    STEP: Waiting for the pdb to be processed 09/07/23 05:48:55.145
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/07/23 05:48:57.149
    STEP: Waiting for all pods to be running 09/07/23 05:48:57.149
    STEP: Waiting for the pdb to observed all healthy pods 09/07/23 05:48:57.151
    STEP: Patching the pdb to disallow a pod to be evicted 09/07/23 05:48:57.165
    STEP: Waiting for the pdb to be processed 09/07/23 05:48:57.178
    STEP: Waiting for all pods to be running 09/07/23 05:48:59.182
    STEP: locating a running pod 09/07/23 05:48:59.184
    STEP: Deleting the pdb to allow a pod to be evicted 09/07/23 05:48:59.19
    STEP: Waiting for the pdb to be deleted 09/07/23 05:48:59.193
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/07/23 05:48:59.194
    STEP: Waiting for all pods to be running 09/07/23 05:48:59.195
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:48:59.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6831" for this suite. 09/07/23 05:48:59.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:48:59.215
Sep  7 05:48:59.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 05:48:59.216
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:59.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:59.227
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-c69c7554-c86e-4c00-aace-f4917c0c238c 09/07/23 05:48:59.229
STEP: Creating a pod to test consume secrets 09/07/23 05:48:59.232
Sep  7 05:48:59.241: INFO: Waiting up to 5m0s for pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464" in namespace "secrets-7731" to be "Succeeded or Failed"
Sep  7 05:48:59.243: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65658ms
Sep  7 05:49:01.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004558824s
Sep  7 05:49:03.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005187099s
STEP: Saw pod success 09/07/23 05:49:03.246
Sep  7 05:49:03.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464" satisfied condition "Succeeded or Failed"
Sep  7 05:49:03.248: INFO: Trying to get logs from node kind-worker pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:49:03.252
Sep  7 05:49:03.260: INFO: Waiting for pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 to disappear
Sep  7 05:49:03.261: INFO: Pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 05:49:03.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7731" for this suite. 09/07/23 05:49:03.263
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:48:59.215
    Sep  7 05:48:59.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 05:48:59.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:48:59.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:48:59.227
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-c69c7554-c86e-4c00-aace-f4917c0c238c 09/07/23 05:48:59.229
    STEP: Creating a pod to test consume secrets 09/07/23 05:48:59.232
    Sep  7 05:48:59.241: INFO: Waiting up to 5m0s for pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464" in namespace "secrets-7731" to be "Succeeded or Failed"
    Sep  7 05:48:59.243: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65658ms
    Sep  7 05:49:01.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004558824s
    Sep  7 05:49:03.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005187099s
    STEP: Saw pod success 09/07/23 05:49:03.246
    Sep  7 05:49:03.246: INFO: Pod "pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464" satisfied condition "Succeeded or Failed"
    Sep  7 05:49:03.248: INFO: Trying to get logs from node kind-worker pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:49:03.252
    Sep  7 05:49:03.260: INFO: Waiting for pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 to disappear
    Sep  7 05:49:03.261: INFO: Pod pod-secrets-6bdf5eeb-08ee-4668-8fd2-667f83105464 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:49:03.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7731" for this suite. 09/07/23 05:49:03.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:49:03.267
Sep  7 05:49:03.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename watch 09/07/23 05:49:03.268
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:03.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:03.277
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/07/23 05:49:03.278
STEP: creating a new configmap 09/07/23 05:49:03.279
STEP: modifying the configmap once 09/07/23 05:49:03.282
STEP: changing the label value of the configmap 09/07/23 05:49:03.286
STEP: Expecting to observe a delete notification for the watched object 09/07/23 05:49:03.289
Sep  7 05:49:03.289: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15074 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:49:03.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15075 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:49:03.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15076 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/07/23 05:49:03.29
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/07/23 05:49:03.294
STEP: changing the label value of the configmap back 09/07/23 05:49:13.295
STEP: modifying the configmap a third time 09/07/23 05:49:13.3
STEP: deleting the configmap 09/07/23 05:49:13.304
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/07/23 05:49:13.307
Sep  7 05:49:13.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15142 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:49:13.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15143 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 05:49:13.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15144 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  7 05:49:13.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7466" for this suite. 09/07/23 05:49:13.31
------------------------------
â€¢ [SLOW TEST] [10.046 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:49:03.267
    Sep  7 05:49:03.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename watch 09/07/23 05:49:03.268
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:03.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:03.277
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/07/23 05:49:03.278
    STEP: creating a new configmap 09/07/23 05:49:03.279
    STEP: modifying the configmap once 09/07/23 05:49:03.282
    STEP: changing the label value of the configmap 09/07/23 05:49:03.286
    STEP: Expecting to observe a delete notification for the watched object 09/07/23 05:49:03.289
    Sep  7 05:49:03.289: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15074 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:49:03.290: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15075 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:49:03.290: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15076 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/07/23 05:49:03.29
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/07/23 05:49:03.294
    STEP: changing the label value of the configmap back 09/07/23 05:49:13.295
    STEP: modifying the configmap a third time 09/07/23 05:49:13.3
    STEP: deleting the configmap 09/07/23 05:49:13.304
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/07/23 05:49:13.307
    Sep  7 05:49:13.307: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15142 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:49:13.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15143 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 05:49:13.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7466  d61ad08d-cc80-42c4-8539-42c878130881 15144 0 2023-09-07 05:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-07 05:49:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:49:13.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7466" for this suite. 09/07/23 05:49:13.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:49:13.314
Sep  7 05:49:13.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:49:13.315
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:13.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:13.324
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9441 09/07/23 05:49:13.326
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[] 09/07/23 05:49:13.332
Sep  7 05:49:13.334: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep  7 05:49:14.338: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9441 09/07/23 05:49:14.338
Sep  7 05:49:14.342: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9441" to be "running and ready"
Sep  7 05:49:14.344: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30931ms
Sep  7 05:49:14.344: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:49:16.346: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004089839s
Sep  7 05:49:16.347: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  7 05:49:16.347: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod1:[100]] 09/07/23 05:49:16.348
Sep  7 05:49:16.353: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9441 09/07/23 05:49:16.353
Sep  7 05:49:16.359: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9441" to be "running and ready"
Sep  7 05:49:16.360: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142719ms
Sep  7 05:49:16.360: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:49:18.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00327075s
Sep  7 05:49:18.363: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  7 05:49:18.363: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod1:[100] pod2:[101]] 09/07/23 05:49:18.364
Sep  7 05:49:18.370: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/07/23 05:49:18.37
Sep  7 05:49:18.370: INFO: Creating new exec pod
Sep  7 05:49:18.376: INFO: Waiting up to 5m0s for pod "execpod5q79j" in namespace "services-9441" to be "running"
Sep  7 05:49:18.378: INFO: Pod "execpod5q79j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39439ms
Sep  7 05:49:20.381: INFO: Pod "execpod5q79j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00483711s
Sep  7 05:49:20.381: INFO: Pod "execpod5q79j" satisfied condition "running"
Sep  7 05:49:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  7 05:49:21.560: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  7 05:49:21.560: INFO: stdout: ""
Sep  7 05:49:21.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 10.96.16.43 80'
Sep  7 05:49:21.714: INFO: stderr: "+ nc -v -z -w 2 10.96.16.43 80\nConnection to 10.96.16.43 80 port [tcp/http] succeeded!\n"
Sep  7 05:49:21.714: INFO: stdout: ""
Sep  7 05:49:21.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  7 05:49:21.890: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  7 05:49:21.890: INFO: stdout: ""
Sep  7 05:49:21.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 10.96.16.43 81'
Sep  7 05:49:22.038: INFO: stderr: "+ nc -v -z -w 2 10.96.16.43 81\nConnection to 10.96.16.43 81 port [tcp/*] succeeded!\n"
Sep  7 05:49:22.038: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9441 09/07/23 05:49:22.038
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod2:[101]] 09/07/23 05:49:22.046
Sep  7 05:49:23.057: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9441 09/07/23 05:49:23.057
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[] 09/07/23 05:49:23.063
Sep  7 05:49:23.072: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:49:23.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9441" for this suite. 09/07/23 05:49:23.083
------------------------------
â€¢ [SLOW TEST] [9.774 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:49:13.314
    Sep  7 05:49:13.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:49:13.315
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:13.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:13.324
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9441 09/07/23 05:49:13.326
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[] 09/07/23 05:49:13.332
    Sep  7 05:49:13.334: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Sep  7 05:49:14.338: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9441 09/07/23 05:49:14.338
    Sep  7 05:49:14.342: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9441" to be "running and ready"
    Sep  7 05:49:14.344: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.30931ms
    Sep  7 05:49:14.344: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:49:16.346: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004089839s
    Sep  7 05:49:16.347: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  7 05:49:16.347: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod1:[100]] 09/07/23 05:49:16.348
    Sep  7 05:49:16.353: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9441 09/07/23 05:49:16.353
    Sep  7 05:49:16.359: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9441" to be "running and ready"
    Sep  7 05:49:16.360: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.142719ms
    Sep  7 05:49:16.360: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:49:18.363: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00327075s
    Sep  7 05:49:18.363: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  7 05:49:18.363: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod1:[100] pod2:[101]] 09/07/23 05:49:18.364
    Sep  7 05:49:18.370: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/07/23 05:49:18.37
    Sep  7 05:49:18.370: INFO: Creating new exec pod
    Sep  7 05:49:18.376: INFO: Waiting up to 5m0s for pod "execpod5q79j" in namespace "services-9441" to be "running"
    Sep  7 05:49:18.378: INFO: Pod "execpod5q79j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39439ms
    Sep  7 05:49:20.381: INFO: Pod "execpod5q79j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00483711s
    Sep  7 05:49:20.381: INFO: Pod "execpod5q79j" satisfied condition "running"
    Sep  7 05:49:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  7 05:49:21.560: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  7 05:49:21.560: INFO: stdout: ""
    Sep  7 05:49:21.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 10.96.16.43 80'
    Sep  7 05:49:21.714: INFO: stderr: "+ nc -v -z -w 2 10.96.16.43 80\nConnection to 10.96.16.43 80 port [tcp/http] succeeded!\n"
    Sep  7 05:49:21.714: INFO: stdout: ""
    Sep  7 05:49:21.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  7 05:49:21.890: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  7 05:49:21.890: INFO: stdout: ""
    Sep  7 05:49:21.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-9441 exec execpod5q79j -- /bin/sh -x -c nc -v -z -w 2 10.96.16.43 81'
    Sep  7 05:49:22.038: INFO: stderr: "+ nc -v -z -w 2 10.96.16.43 81\nConnection to 10.96.16.43 81 port [tcp/*] succeeded!\n"
    Sep  7 05:49:22.038: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9441 09/07/23 05:49:22.038
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[pod2:[101]] 09/07/23 05:49:22.046
    Sep  7 05:49:23.057: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9441 09/07/23 05:49:23.057
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9441 to expose endpoints map[] 09/07/23 05:49:23.063
    Sep  7 05:49:23.072: INFO: successfully validated that service multi-endpoint-test in namespace services-9441 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:49:23.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9441" for this suite. 09/07/23 05:49:23.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:49:23.09
Sep  7 05:49:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:49:23.091
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:23.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:23.103
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:49:23.105
Sep  7 05:49:23.111: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4" in namespace "projected-2028" to be "Succeeded or Failed"
Sep  7 05:49:23.113: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63947ms
Sep  7 05:49:25.115: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004106173s
Sep  7 05:49:27.115: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004101857s
STEP: Saw pod success 09/07/23 05:49:27.116
Sep  7 05:49:27.116: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4" satisfied condition "Succeeded or Failed"
Sep  7 05:49:27.117: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 container client-container: <nil>
STEP: delete the pod 09/07/23 05:49:27.121
Sep  7 05:49:27.129: INFO: Waiting for pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 to disappear
Sep  7 05:49:27.130: INFO: Pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 05:49:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2028" for this suite. 09/07/23 05:49:27.132
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:49:23.09
    Sep  7 05:49:23.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:49:23.091
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:23.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:23.103
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:49:23.105
    Sep  7 05:49:23.111: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4" in namespace "projected-2028" to be "Succeeded or Failed"
    Sep  7 05:49:23.113: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63947ms
    Sep  7 05:49:25.115: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004106173s
    Sep  7 05:49:27.115: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004101857s
    STEP: Saw pod success 09/07/23 05:49:27.116
    Sep  7 05:49:27.116: INFO: Pod "downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4" satisfied condition "Succeeded or Failed"
    Sep  7 05:49:27.117: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 container client-container: <nil>
    STEP: delete the pod 09/07/23 05:49:27.121
    Sep  7 05:49:27.129: INFO: Waiting for pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 to disappear
    Sep  7 05:49:27.130: INFO: Pod downwardapi-volume-36cad228-12cf-49b5-9072-68fe6d1363a4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:49:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2028" for this suite. 09/07/23 05:49:27.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:49:27.137
Sep  7 05:49:27.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:49:27.137
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:27.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:27.148
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:49:27.155
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:49:27.337
STEP: Deploying the webhook pod 09/07/23 05:49:27.342
STEP: Wait for the deployment to be ready 09/07/23 05:49:27.35
Sep  7 05:49:27.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:49:29.361
STEP: Verifying the service has paired with the endpoint 09/07/23 05:49:29.368
Sep  7 05:49:30.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/07/23 05:49:30.371
STEP: create a pod that should be updated by the webhook 09/07/23 05:49:30.381
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:49:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3796" for this suite. 09/07/23 05:49:30.418
STEP: Destroying namespace "webhook-3796-markers" for this suite. 09/07/23 05:49:30.424
------------------------------
â€¢ [3.293 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:49:27.137
    Sep  7 05:49:27.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:49:27.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:27.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:27.148
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:49:27.155
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:49:27.337
    STEP: Deploying the webhook pod 09/07/23 05:49:27.342
    STEP: Wait for the deployment to be ready 09/07/23 05:49:27.35
    Sep  7 05:49:27.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:49:29.361
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:49:29.368
    Sep  7 05:49:30.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/07/23 05:49:30.371
    STEP: create a pod that should be updated by the webhook 09/07/23 05:49:30.381
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:49:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3796" for this suite. 09/07/23 05:49:30.418
    STEP: Destroying namespace "webhook-3796-markers" for this suite. 09/07/23 05:49:30.424
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:49:30.43
Sep  7 05:49:30.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 05:49:30.431
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:30.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:30.44
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9460 09/07/23 05:49:30.442
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/07/23 05:49:30.447
STEP: Creating stateful set ss in namespace statefulset-9460 09/07/23 05:49:30.448
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9460 09/07/23 05:49:30.453
Sep  7 05:49:30.454: INFO: Found 0 stateful pods, waiting for 1
Sep  7 05:49:40.457: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/07/23 05:49:40.457
Sep  7 05:49:40.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:49:40.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:49:40.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:49:40.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 05:49:40.640: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  7 05:49:50.644: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 05:49:50.644: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 05:49:50.654: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
Sep  7 05:49:51.656: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998334339s
Sep  7 05:49:52.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996143348s
Sep  7 05:49:53.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993825727s
Sep  7 05:49:54.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991527196s
Sep  7 05:49:55.665: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989204364s
Sep  7 05:49:56.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.986989193s
Sep  7 05:49:57.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984664471s
Sep  7 05:49:58.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981216609s
Sep  7 05:49:59.675: INFO: Verifying statefulset ss doesn't scale past 1 for another 978.819217ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9460 09/07/23 05:50:00.676
Sep  7 05:50:00.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:50:00.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:50:00.845: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:50:00.845: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 05:50:00.847: INFO: Found 1 stateful pods, waiting for 3
Sep  7 05:50:10.854: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:50:10.854: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:50:10.854: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/07/23 05:50:10.854
STEP: Scale down will halt with unhealthy stateful pod 09/07/23 05:50:10.854
Sep  7 05:50:10.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:50:10.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:50:10.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:50:10.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 05:50:10.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:50:11.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:50:11.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:50:11.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 05:50:11.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:50:11.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:50:11.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:50:11.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 05:50:11.305: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 05:50:11.307: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  7 05:50:21.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 05:50:21.315: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 05:50:21.315: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 05:50:21.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999988s
Sep  7 05:50:22.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998161785s
Sep  7 05:50:23.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996069699s
Sep  7 05:50:24.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993154503s
Sep  7 05:50:25.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991043287s
Sep  7 05:50:26.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988317221s
Sep  7 05:50:27.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985566034s
Sep  7 05:50:28.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981965338s
Sep  7 05:50:29.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979569102s
Sep  7 05:50:30.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 977.062725ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9460 09/07/23 05:50:31.346
Sep  7 05:50:31.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:50:31.490: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:50:31.490: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:50:31.490: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 05:50:31.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:50:31.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:50:31.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:50:31.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 05:50:31.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:50:31.778: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:50:31.778: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:50:31.778: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 05:50:31.778: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/07/23 05:50:41.791
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 05:50:41.791: INFO: Deleting all statefulset in ns statefulset-9460
Sep  7 05:50:41.792: INFO: Scaling statefulset ss to 0
Sep  7 05:50:41.798: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 05:50:41.800: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:50:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9460" for this suite. 09/07/23 05:50:41.807
------------------------------
â€¢ [SLOW TEST] [71.382 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:49:30.43
    Sep  7 05:49:30.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 05:49:30.431
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:49:30.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:49:30.44
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9460 09/07/23 05:49:30.442
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/07/23 05:49:30.447
    STEP: Creating stateful set ss in namespace statefulset-9460 09/07/23 05:49:30.448
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9460 09/07/23 05:49:30.453
    Sep  7 05:49:30.454: INFO: Found 0 stateful pods, waiting for 1
    Sep  7 05:49:40.457: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/07/23 05:49:40.457
    Sep  7 05:49:40.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:49:40.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:49:40.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:49:40.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 05:49:40.640: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  7 05:49:50.644: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 05:49:50.644: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 05:49:50.654: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999998s
    Sep  7 05:49:51.656: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998334339s
    Sep  7 05:49:52.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996143348s
    Sep  7 05:49:53.661: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993825727s
    Sep  7 05:49:54.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991527196s
    Sep  7 05:49:55.665: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989204364s
    Sep  7 05:49:56.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.986989193s
    Sep  7 05:49:57.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984664471s
    Sep  7 05:49:58.673: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981216609s
    Sep  7 05:49:59.675: INFO: Verifying statefulset ss doesn't scale past 1 for another 978.819217ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9460 09/07/23 05:50:00.676
    Sep  7 05:50:00.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:50:00.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:50:00.845: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:50:00.845: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 05:50:00.847: INFO: Found 1 stateful pods, waiting for 3
    Sep  7 05:50:10.854: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:50:10.854: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:50:10.854: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/07/23 05:50:10.854
    STEP: Scale down will halt with unhealthy stateful pod 09/07/23 05:50:10.854
    Sep  7 05:50:10.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:50:10.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:50:10.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:50:10.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 05:50:10.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:50:11.133: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:50:11.133: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:50:11.133: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 05:50:11.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:50:11.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:50:11.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:50:11.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 05:50:11.305: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 05:50:11.307: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  7 05:50:21.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 05:50:21.315: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 05:50:21.315: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 05:50:21.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999988s
    Sep  7 05:50:22.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998161785s
    Sep  7 05:50:23.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.996069699s
    Sep  7 05:50:24.329: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993154503s
    Sep  7 05:50:25.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991043287s
    Sep  7 05:50:26.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988317221s
    Sep  7 05:50:27.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985566034s
    Sep  7 05:50:28.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981965338s
    Sep  7 05:50:29.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979569102s
    Sep  7 05:50:30.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 977.062725ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9460 09/07/23 05:50:31.346
    Sep  7 05:50:31.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:50:31.490: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:50:31.490: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:50:31.490: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 05:50:31.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:50:31.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:50:31.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:50:31.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 05:50:31.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-9460 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:50:31.778: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:50:31.778: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:50:31.778: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 05:50:31.778: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/07/23 05:50:41.791
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 05:50:41.791: INFO: Deleting all statefulset in ns statefulset-9460
    Sep  7 05:50:41.792: INFO: Scaling statefulset ss to 0
    Sep  7 05:50:41.798: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 05:50:41.800: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:50:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9460" for this suite. 09/07/23 05:50:41.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:50:41.813
Sep  7 05:50:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 05:50:41.814
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:50:41.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:50:41.824
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8044 09/07/23 05:50:41.826
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/07/23 05:50:41.831
Sep  7 05:50:41.836: INFO: Found 0 stateful pods, waiting for 3
Sep  7 05:50:51.840: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:50:51.840: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:50:51.840: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 05:50:51.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:50:51.980: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:50:51.981: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:50:51.981: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/07/23 05:51:01.989
Sep  7 05:51:02.005: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/07/23 05:51:02.005
STEP: Updating Pods in reverse ordinal order 09/07/23 05:51:12.014
Sep  7 05:51:12.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:51:12.169: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:51:12.170: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:51:12.170: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 05:51:22.182: INFO: Waiting for StatefulSet statefulset-8044/ss2 to complete update
Sep  7 05:51:22.182: INFO: Waiting for Pod statefulset-8044/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Rolling back to a previous revision 09/07/23 05:51:32.187
Sep  7 05:51:32.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 05:51:32.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 05:51:32.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 05:51:32.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 05:51:42.374: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/07/23 05:51:52.382
Sep  7 05:51:52.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 05:51:52.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 05:51:52.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 05:51:52.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 05:52:02.541: INFO: Deleting all statefulset in ns statefulset-8044
Sep  7 05:52:02.543: INFO: Scaling statefulset ss2 to 0
Sep  7 05:52:12.555: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 05:52:12.556: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:12.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8044" for this suite. 09/07/23 05:52:12.565
------------------------------
â€¢ [SLOW TEST] [90.755 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:50:41.813
    Sep  7 05:50:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 05:50:41.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:50:41.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:50:41.824
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8044 09/07/23 05:50:41.826
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/07/23 05:50:41.831
    Sep  7 05:50:41.836: INFO: Found 0 stateful pods, waiting for 3
    Sep  7 05:50:51.840: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:50:51.840: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:50:51.840: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 05:50:51.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:50:51.980: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:50:51.981: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:50:51.981: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/07/23 05:51:01.989
    Sep  7 05:51:02.005: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/07/23 05:51:02.005
    STEP: Updating Pods in reverse ordinal order 09/07/23 05:51:12.014
    Sep  7 05:51:12.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:51:12.169: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:51:12.170: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:51:12.170: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 05:51:22.182: INFO: Waiting for StatefulSet statefulset-8044/ss2 to complete update
    Sep  7 05:51:22.182: INFO: Waiting for Pod statefulset-8044/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Rolling back to a previous revision 09/07/23 05:51:32.187
    Sep  7 05:51:32.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 05:51:32.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 05:51:32.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 05:51:32.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 05:51:42.374: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/07/23 05:51:52.382
    Sep  7 05:51:52.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-8044 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 05:51:52.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 05:51:52.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 05:51:52.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 05:52:02.541: INFO: Deleting all statefulset in ns statefulset-8044
    Sep  7 05:52:02.543: INFO: Scaling statefulset ss2 to 0
    Sep  7 05:52:12.555: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 05:52:12.556: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:12.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8044" for this suite. 09/07/23 05:52:12.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:12.568
Sep  7 05:52:12.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 05:52:12.569
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:12.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:12.58
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  7 05:52:12.582: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/07/23 05:52:13.589
STEP: Checking rc "condition-test" has the desired failure condition set 09/07/23 05:52:13.595
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/07/23 05:52:14.599
Sep  7 05:52:14.606: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/07/23 05:52:14.606
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:15.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5760" for this suite. 09/07/23 05:52:15.613
------------------------------
â€¢ [3.048 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:12.568
    Sep  7 05:52:12.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 05:52:12.569
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:12.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:12.58
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  7 05:52:12.582: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/07/23 05:52:13.589
    STEP: Checking rc "condition-test" has the desired failure condition set 09/07/23 05:52:13.595
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/07/23 05:52:14.599
    Sep  7 05:52:14.606: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/07/23 05:52:14.606
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:15.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5760" for this suite. 09/07/23 05:52:15.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:15.617
Sep  7 05:52:15.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 05:52:15.618
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:15.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:15.629
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/07/23 05:52:15.631
STEP: Wait for the Deployment to create new ReplicaSet 09/07/23 05:52:15.634
STEP: delete the deployment 09/07/23 05:52:16.14
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/07/23 05:52:16.144
STEP: Gathering metrics 09/07/23 05:52:16.656
Sep  7 05:52:16.668: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 05:52:16.670: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.77171ms
Sep  7 05:52:16.670: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 05:52:16.670: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 05:52:16.711: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:16.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6638" for this suite. 09/07/23 05:52:16.714
------------------------------
â€¢ [1.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:15.617
    Sep  7 05:52:15.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 05:52:15.618
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:15.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:15.629
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/07/23 05:52:15.631
    STEP: Wait for the Deployment to create new ReplicaSet 09/07/23 05:52:15.634
    STEP: delete the deployment 09/07/23 05:52:16.14
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/07/23 05:52:16.144
    STEP: Gathering metrics 09/07/23 05:52:16.656
    Sep  7 05:52:16.668: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 05:52:16.670: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.77171ms
    Sep  7 05:52:16.670: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 05:52:16.670: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 05:52:16.711: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:16.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6638" for this suite. 09/07/23 05:52:16.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:16.718
Sep  7 05:52:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 05:52:16.719
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:16.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:16.727
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/07/23 05:52:16.729
Sep  7 05:52:16.737: INFO: Waiting up to 5m0s for pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393" in namespace "pods-3111" to be "running and ready"
Sep  7 05:52:16.738: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48443ms
Sep  7 05:52:16.738: INFO: The phase of Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:52:18.741: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393": Phase="Running", Reason="", readiness=true. Elapsed: 2.003943253s
Sep  7 05:52:18.741: INFO: The phase of Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 is Running (Ready = true)
Sep  7 05:52:18.741: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393" satisfied condition "running and ready"
Sep  7 05:52:18.743: INFO: Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 has hostIP: 192.168.8.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:18.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3111" for this suite. 09/07/23 05:52:18.746
------------------------------
â€¢ [2.031 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:16.718
    Sep  7 05:52:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 05:52:16.719
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:16.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:16.727
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/07/23 05:52:16.729
    Sep  7 05:52:16.737: INFO: Waiting up to 5m0s for pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393" in namespace "pods-3111" to be "running and ready"
    Sep  7 05:52:16.738: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48443ms
    Sep  7 05:52:16.738: INFO: The phase of Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:52:18.741: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393": Phase="Running", Reason="", readiness=true. Elapsed: 2.003943253s
    Sep  7 05:52:18.741: INFO: The phase of Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 is Running (Ready = true)
    Sep  7 05:52:18.741: INFO: Pod "pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393" satisfied condition "running and ready"
    Sep  7 05:52:18.743: INFO: Pod pod-hostip-a86bb029-4701-4cc8-87f0-d557ceb90393 has hostIP: 192.168.8.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:18.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3111" for this suite. 09/07/23 05:52:18.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:18.75
Sep  7 05:52:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 05:52:18.751
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:18.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:18.761
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  7 05:52:18.763: INFO: Creating simple deployment test-new-deployment
Sep  7 05:52:18.769: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 09/07/23 05:52:20.775
STEP: updating a scale subresource 09/07/23 05:52:20.776
STEP: verifying the deployment Spec.Replicas was modified 09/07/23 05:52:20.782
STEP: Patch a scale subresource 09/07/23 05:52:20.784
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 05:52:20.793: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4669  1b26d43f-e1a8-4475-b9c0-bce50e816959 16384 3 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-07 05:52:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fee138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 05:52:19 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-07 05:52:19 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  7 05:52:20.794: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4669  63ebd1f6-4856-46c4-b327-bcbc8562e221 16383 2 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1b26d43f-e1a8-4475-b9c0-bce50e816959 0xc000fee567 0xc000fee568}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b26d43f-e1a8-4475-b9c0-bce50e816959\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fee5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  7 05:52:20.800: INFO: Pod "test-new-deployment-7f5969cbc7-c9lv8" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-c9lv8 test-new-deployment-7f5969cbc7- deployment-4669  e7f4ef3b-36c3-4ccd-9117-a8dc7c8cd9ae 16347 0 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 63ebd1f6-4856-46c4-b327-bcbc8562e221 0xc003f78317 0xc003f78318}] [] [{kube-controller-manager Update v1 2023-09-07 05:52:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63ebd1f6-4856-46c4-b327-bcbc8562e221\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mphf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mphf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.168,StartTime:2023-09-07 05:52:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:52:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://522fc60916f61d0e50b294c8d8312b5c7aab5b37453cd3bc47800a3b03fe5108,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 05:52:20.800: INFO: Pod "test-new-deployment-7f5969cbc7-cr924" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-cr924 test-new-deployment-7f5969cbc7- deployment-4669  c5f8bcec-5086-4098-b2fd-5c185991edd1 16389 0 2023-09-07 05:52:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 63ebd1f6-4856-46c4-b327-bcbc8562e221 0xc003f78517 0xc003f78518}] [] [{kube-controller-manager Update v1 2023-09-07 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63ebd1f6-4856-46c4-b327-bcbc8562e221\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2zr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2zr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:20.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4669" for this suite. 09/07/23 05:52:20.806
------------------------------
â€¢ [2.063 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:18.75
    Sep  7 05:52:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 05:52:18.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:18.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:18.761
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  7 05:52:18.763: INFO: Creating simple deployment test-new-deployment
    Sep  7 05:52:18.769: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 09/07/23 05:52:20.775
    STEP: updating a scale subresource 09/07/23 05:52:20.776
    STEP: verifying the deployment Spec.Replicas was modified 09/07/23 05:52:20.782
    STEP: Patch a scale subresource 09/07/23 05:52:20.784
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 05:52:20.793: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4669  1b26d43f-e1a8-4475-b9c0-bce50e816959 16384 3 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-07 05:52:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fee138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 05:52:19 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-07 05:52:19 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  7 05:52:20.794: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4669  63ebd1f6-4856-46c4-b327-bcbc8562e221 16383 2 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 1b26d43f-e1a8-4475-b9c0-bce50e816959 0xc000fee567 0xc000fee568}] [] [{kube-controller-manager Update apps/v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b26d43f-e1a8-4475-b9c0-bce50e816959\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000fee5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 05:52:20.800: INFO: Pod "test-new-deployment-7f5969cbc7-c9lv8" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-c9lv8 test-new-deployment-7f5969cbc7- deployment-4669  e7f4ef3b-36c3-4ccd-9117-a8dc7c8cd9ae 16347 0 2023-09-07 05:52:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 63ebd1f6-4856-46c4-b327-bcbc8562e221 0xc003f78317 0xc003f78318}] [] [{kube-controller-manager Update v1 2023-09-07 05:52:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63ebd1f6-4856-46c4-b327-bcbc8562e221\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 05:52:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.168\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mphf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mphf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.168,StartTime:2023-09-07 05:52:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 05:52:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://522fc60916f61d0e50b294c8d8312b5c7aab5b37453cd3bc47800a3b03fe5108,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 05:52:20.800: INFO: Pod "test-new-deployment-7f5969cbc7-cr924" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-cr924 test-new-deployment-7f5969cbc7- deployment-4669  c5f8bcec-5086-4098-b2fd-5c185991edd1 16389 0 2023-09-07 05:52:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 63ebd1f6-4856-46c4-b327-bcbc8562e221 0xc003f78517 0xc003f78518}] [] [{kube-controller-manager Update v1 2023-09-07 05:52:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63ebd1f6-4856-46c4-b327-bcbc8562e221\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2zr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2zr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 05:52:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:20.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4669" for this suite. 09/07/23 05:52:20.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:20.814
Sep  7 05:52:20.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:52:20.814
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:20.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:20.822
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-2539c5ae-c3ad-4442-80de-f7c6ec323863 09/07/23 05:52:20.824
STEP: Creating a pod to test consume secrets 09/07/23 05:52:20.828
Sep  7 05:52:20.834: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9" in namespace "projected-9678" to be "Succeeded or Failed"
Sep  7 05:52:20.836: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60236ms
Sep  7 05:52:22.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005297824s
Sep  7 05:52:24.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005452199s
STEP: Saw pod success 09/07/23 05:52:24.84
Sep  7 05:52:24.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9" satisfied condition "Succeeded or Failed"
Sep  7 05:52:24.842: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/07/23 05:52:24.851
Sep  7 05:52:24.860: INFO: Waiting for pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 to disappear
Sep  7 05:52:24.862: INFO: Pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:24.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9678" for this suite. 09/07/23 05:52:24.864
------------------------------
â€¢ [4.053 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:20.814
    Sep  7 05:52:20.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:52:20.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:20.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:20.822
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-2539c5ae-c3ad-4442-80de-f7c6ec323863 09/07/23 05:52:20.824
    STEP: Creating a pod to test consume secrets 09/07/23 05:52:20.828
    Sep  7 05:52:20.834: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9" in namespace "projected-9678" to be "Succeeded or Failed"
    Sep  7 05:52:20.836: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60236ms
    Sep  7 05:52:22.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005297824s
    Sep  7 05:52:24.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005452199s
    STEP: Saw pod success 09/07/23 05:52:24.84
    Sep  7 05:52:24.840: INFO: Pod "pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:24.842: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 05:52:24.851
    Sep  7 05:52:24.860: INFO: Waiting for pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 to disappear
    Sep  7 05:52:24.862: INFO: Pod pod-projected-secrets-ebc899ae-0d8f-4560-8a20-6a2e3843bcf9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:24.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9678" for this suite. 09/07/23 05:52:24.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:24.867
Sep  7 05:52:24.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 05:52:24.868
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:24.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:24.879
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  7 05:52:24.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 create -f -'
Sep  7 05:52:25.187: INFO: stderr: ""
Sep  7 05:52:25.187: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  7 05:52:25.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 create -f -'
Sep  7 05:52:25.324: INFO: stderr: ""
Sep  7 05:52:25.324: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/07/23 05:52:25.324
Sep  7 05:52:26.327: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:52:26.327: INFO: Found 1 / 1
Sep  7 05:52:26.327: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  7 05:52:26.329: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 05:52:26.329: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  7 05:52:26.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe pod agnhost-primary-s5fxn'
Sep  7 05:52:26.390: INFO: stderr: ""
Sep  7 05:52:26.390: INFO: stdout: "Name:             agnhost-primary-s5fxn\nNamespace:        kubectl-7044\nPriority:         0\nService Account:  default\nNode:             kind-worker2/192.168.8.6\nStart Time:       Thu, 07 Sep 2023 05:52:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.2.169\nIPs:\n  IP:           10.244.2.169\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://96f4b2c740962118e1e6a5819c18dc40e157c625e8c4e8ef8cf229d869c057ba\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 07 Sep 2023 05:52:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jk5bh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jk5bh:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7044/agnhost-primary-s5fxn to kind-worker2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep  7 05:52:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe rc agnhost-primary'
Sep  7 05:52:26.451: INFO: stderr: ""
Sep  7 05:52:26.451: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7044\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-s5fxn\n"
Sep  7 05:52:26.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe service agnhost-primary'
Sep  7 05:52:26.509: INFO: stderr: ""
Sep  7 05:52:26.509: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7044\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.168.237\nIPs:               10.96.168.237\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.169:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  7 05:52:26.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe node kind-control-plane'
Sep  7 05:52:26.580: INFO: stderr: ""
Sep  7 05:52:26.580: INFO: stdout: "Name:               kind-control-plane\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kind-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 07 Sep 2023 04:56:59 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kind-control-plane\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 07 Sep 2023 05:52:17 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:57:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.8.7\n  Hostname:    kind-control-plane\nCapacity:\n  cpu:                48\n  ephemeral-storage:  1049996220Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             198049708Ki\n  pods:               110\nAllocatable:\n  cpu:                48\n  ephemeral-storage:  1049996220Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             198049708Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c920fe984fa54174956b8ac6b7b64695\n  System UUID:                e1af6536-27bf-458c-85fe-6d343503a67d\n  Boot ID:                    f23236e4-8d59-4af6-accd-384fbc5b206a\n  Kernel Version:             6.3.11-1rodete1-amd64\n  OS Image:                   Debian GNU/Linux 11 (bullseye)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   kind://docker/kind/kind-control-plane\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-c78ln                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     55m\n  kube-system                 coredns-787d4945fb-lhwpf                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     55m\n  kube-system                 etcd-kind-control-plane                                    100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         55m\n  kube-system                 kindnet-vxcrn                                              100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      55m\n  kube-system                 kube-apiserver-kind-control-plane                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-controller-manager-kind-control-plane                 200m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-proxy-pkkmr                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-scheduler-kind-control-plane                          100m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  local-path-storage          local-path-provisioner-6bd6454576-lj4v2                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-79rqp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (1%)   100m (0%)\n  memory             290Mi (0%)  390Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age   From             Message\n  ----    ------                   ----  ----             -------\n  Normal  Starting                 55m   kube-proxy       \n  Normal  Starting                 55m   kubelet          Starting kubelet.\n  Normal  NodeAllocatableEnforced  55m   kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  55m   kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    55m   kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     55m   kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           55m   node-controller  Node kind-control-plane event: Registered Node kind-control-plane in Controller\n  Normal  NodeReady                55m   kubelet          Node kind-control-plane status is now: NodeReady\n"
Sep  7 05:52:26.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe namespace kubectl-7044'
Sep  7 05:52:26.635: INFO: stderr: ""
Sep  7 05:52:26.635: INFO: stdout: "Name:         kubectl-7044\nLabels:       e2e-framework=kubectl\n              e2e-run=b2ea5160-bc07-49e5-8e7a-ce95b6c6d343\n              kubernetes.io/metadata.name=kubectl-7044\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:26.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7044" for this suite. 09/07/23 05:52:26.637
------------------------------
â€¢ [1.773 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:24.867
    Sep  7 05:52:24.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 05:52:24.868
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:24.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:24.879
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  7 05:52:24.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 create -f -'
    Sep  7 05:52:25.187: INFO: stderr: ""
    Sep  7 05:52:25.187: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  7 05:52:25.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 create -f -'
    Sep  7 05:52:25.324: INFO: stderr: ""
    Sep  7 05:52:25.324: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/07/23 05:52:25.324
    Sep  7 05:52:26.327: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:52:26.327: INFO: Found 1 / 1
    Sep  7 05:52:26.327: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  7 05:52:26.329: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 05:52:26.329: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  7 05:52:26.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe pod agnhost-primary-s5fxn'
    Sep  7 05:52:26.390: INFO: stderr: ""
    Sep  7 05:52:26.390: INFO: stdout: "Name:             agnhost-primary-s5fxn\nNamespace:        kubectl-7044\nPriority:         0\nService Account:  default\nNode:             kind-worker2/192.168.8.6\nStart Time:       Thu, 07 Sep 2023 05:52:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.2.169\nIPs:\n  IP:           10.244.2.169\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://96f4b2c740962118e1e6a5819c18dc40e157c625e8c4e8ef8cf229d869c057ba\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 07 Sep 2023 05:52:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jk5bh (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jk5bh:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7044/agnhost-primary-s5fxn to kind-worker2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Sep  7 05:52:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe rc agnhost-primary'
    Sep  7 05:52:26.451: INFO: stderr: ""
    Sep  7 05:52:26.451: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7044\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-s5fxn\n"
    Sep  7 05:52:26.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe service agnhost-primary'
    Sep  7 05:52:26.509: INFO: stderr: ""
    Sep  7 05:52:26.509: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7044\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.168.237\nIPs:               10.96.168.237\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.169:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  7 05:52:26.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe node kind-control-plane'
    Sep  7 05:52:26.580: INFO: stderr: ""
    Sep  7 05:52:26.580: INFO: stdout: "Name:               kind-control-plane\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kind-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 07 Sep 2023 04:56:59 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  kind-control-plane\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 07 Sep 2023 05:52:17 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:56:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 07 Sep 2023 05:51:26 +0000   Thu, 07 Sep 2023 04:57:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.8.7\n  Hostname:    kind-control-plane\nCapacity:\n  cpu:                48\n  ephemeral-storage:  1049996220Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             198049708Ki\n  pods:               110\nAllocatable:\n  cpu:                48\n  ephemeral-storage:  1049996220Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             198049708Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c920fe984fa54174956b8ac6b7b64695\n  System UUID:                e1af6536-27bf-458c-85fe-6d343503a67d\n  Boot ID:                    f23236e4-8d59-4af6-accd-384fbc5b206a\n  Kernel Version:             6.3.11-1rodete1-amd64\n  OS Image:                   Debian GNU/Linux 11 (bullseye)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.6\n  Kube-Proxy Version:         v1.26.6\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   kind://docker/kind/kind-control-plane\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-c78ln                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     55m\n  kube-system                 coredns-787d4945fb-lhwpf                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     55m\n  kube-system                 etcd-kind-control-plane                                    100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         55m\n  kube-system                 kindnet-vxcrn                                              100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      55m\n  kube-system                 kube-apiserver-kind-control-plane                          250m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-controller-manager-kind-control-plane                 200m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-proxy-pkkmr                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-scheduler-kind-control-plane                          100m (0%)     0 (0%)      0 (0%)           0 (0%)         55m\n  local-path-storage          local-path-provisioner-6bd6454576-lj4v2                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-79rqp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                950m (1%)   100m (0%)\n  memory             290Mi (0%)  390Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age   From             Message\n  ----    ------                   ----  ----             -------\n  Normal  Starting                 55m   kube-proxy       \n  Normal  Starting                 55m   kubelet          Starting kubelet.\n  Normal  NodeAllocatableEnforced  55m   kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  55m   kubelet          Node kind-control-plane status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    55m   kubelet          Node kind-control-plane status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     55m   kubelet          Node kind-control-plane status is now: NodeHasSufficientPID\n  Normal  RegisteredNode           55m   node-controller  Node kind-control-plane event: Registered Node kind-control-plane in Controller\n  Normal  NodeReady                55m   kubelet          Node kind-control-plane status is now: NodeReady\n"
    Sep  7 05:52:26.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7044 describe namespace kubectl-7044'
    Sep  7 05:52:26.635: INFO: stderr: ""
    Sep  7 05:52:26.635: INFO: stdout: "Name:         kubectl-7044\nLabels:       e2e-framework=kubectl\n              e2e-run=b2ea5160-bc07-49e5-8e7a-ce95b6c6d343\n              kubernetes.io/metadata.name=kubectl-7044\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:26.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7044" for this suite. 09/07/23 05:52:26.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:26.641
Sep  7 05:52:26.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:52:26.642
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:26.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:26.649
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:52:26.651
Sep  7 05:52:26.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc" in namespace "projected-7" to be "Succeeded or Failed"
Sep  7 05:52:26.657: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.333839ms
Sep  7 05:52:28.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004424075s
Sep  7 05:52:30.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004424182s
STEP: Saw pod success 09/07/23 05:52:30.66
Sep  7 05:52:30.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc" satisfied condition "Succeeded or Failed"
Sep  7 05:52:30.662: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc container client-container: <nil>
STEP: delete the pod 09/07/23 05:52:30.671
Sep  7 05:52:30.678: INFO: Waiting for pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc to disappear
Sep  7 05:52:30.679: INFO: Pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:30.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7" for this suite. 09/07/23 05:52:30.681
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:26.641
    Sep  7 05:52:26.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:52:26.642
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:26.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:26.649
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:52:26.651
    Sep  7 05:52:26.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc" in namespace "projected-7" to be "Succeeded or Failed"
    Sep  7 05:52:26.657: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.333839ms
    Sep  7 05:52:28.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004424075s
    Sep  7 05:52:30.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004424182s
    STEP: Saw pod success 09/07/23 05:52:30.66
    Sep  7 05:52:30.660: INFO: Pod "downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:30.662: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc container client-container: <nil>
    STEP: delete the pod 09/07/23 05:52:30.671
    Sep  7 05:52:30.678: INFO: Waiting for pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc to disappear
    Sep  7 05:52:30.679: INFO: Pod downwardapi-volume-36d3a2eb-b0de-47d7-a3d2-640b214ed7dc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:30.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7" for this suite. 09/07/23 05:52:30.681
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:30.686
Sep  7 05:52:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:52:30.687
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:30.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:30.694
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-47860b54-fd92-48a4-9737-c2be35f2ac6c 09/07/23 05:52:30.696
STEP: Creating a pod to test consume configMaps 09/07/23 05:52:30.698
Sep  7 05:52:30.704: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd" in namespace "projected-764" to be "Succeeded or Failed"
Sep  7 05:52:30.705: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55965ms
Sep  7 05:52:32.708: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003774487s
Sep  7 05:52:34.708: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004527564s
STEP: Saw pod success 09/07/23 05:52:34.708
Sep  7 05:52:34.709: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd" satisfied condition "Succeeded or Failed"
Sep  7 05:52:34.710: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:52:34.714
Sep  7 05:52:34.721: INFO: Waiting for pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd to disappear
Sep  7 05:52:34.722: INFO: Pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:34.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-764" for this suite. 09/07/23 05:52:34.724
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:30.686
    Sep  7 05:52:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:52:30.687
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:30.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:30.694
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-47860b54-fd92-48a4-9737-c2be35f2ac6c 09/07/23 05:52:30.696
    STEP: Creating a pod to test consume configMaps 09/07/23 05:52:30.698
    Sep  7 05:52:30.704: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd" in namespace "projected-764" to be "Succeeded or Failed"
    Sep  7 05:52:30.705: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55965ms
    Sep  7 05:52:32.708: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003774487s
    Sep  7 05:52:34.708: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004527564s
    STEP: Saw pod success 09/07/23 05:52:34.708
    Sep  7 05:52:34.709: INFO: Pod "pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:34.710: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:52:34.714
    Sep  7 05:52:34.721: INFO: Waiting for pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd to disappear
    Sep  7 05:52:34.722: INFO: Pod pod-projected-configmaps-9b894e44-6a2d-49b4-9672-5b327546bbbd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:34.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-764" for this suite. 09/07/23 05:52:34.724
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:34.73
Sep  7 05:52:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:52:34.73
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:34.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:34.739
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/07/23 05:52:34.741
Sep  7 05:52:34.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4" in namespace "downward-api-3096" to be "Succeeded or Failed"
Sep  7 05:52:34.748: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50406ms
Sep  7 05:52:36.750: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003633268s
Sep  7 05:52:38.752: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005020646s
STEP: Saw pod success 09/07/23 05:52:38.752
Sep  7 05:52:38.752: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4" satisfied condition "Succeeded or Failed"
Sep  7 05:52:38.753: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 container client-container: <nil>
STEP: delete the pod 09/07/23 05:52:38.757
Sep  7 05:52:38.765: INFO: Waiting for pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 to disappear
Sep  7 05:52:38.767: INFO: Pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3096" for this suite. 09/07/23 05:52:38.769
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:34.73
    Sep  7 05:52:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:52:34.73
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:34.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:34.739
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/07/23 05:52:34.741
    Sep  7 05:52:34.747: INFO: Waiting up to 5m0s for pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4" in namespace "downward-api-3096" to be "Succeeded or Failed"
    Sep  7 05:52:34.748: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50406ms
    Sep  7 05:52:36.750: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003633268s
    Sep  7 05:52:38.752: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005020646s
    STEP: Saw pod success 09/07/23 05:52:38.752
    Sep  7 05:52:38.752: INFO: Pod "downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:38.753: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 container client-container: <nil>
    STEP: delete the pod 09/07/23 05:52:38.757
    Sep  7 05:52:38.765: INFO: Waiting for pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 to disappear
    Sep  7 05:52:38.767: INFO: Pod downwardapi-volume-332fba5c-82ed-43b5-b151-5ac392e164f4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3096" for this suite. 09/07/23 05:52:38.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:38.772
Sep  7 05:52:38.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 05:52:38.773
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:38.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:38.783
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-314c58d1-0666-44f2-ab7b-bdcf8922c7a2 09/07/23 05:52:38.785
STEP: Creating a pod to test consume configMaps 09/07/23 05:52:38.787
Sep  7 05:52:38.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b" in namespace "projected-1858" to be "Succeeded or Failed"
Sep  7 05:52:38.793: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346779ms
Sep  7 05:52:40.796: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004043198s
Sep  7 05:52:42.797: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005126518s
STEP: Saw pod success 09/07/23 05:52:42.797
Sep  7 05:52:42.797: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b" satisfied condition "Succeeded or Failed"
Sep  7 05:52:42.799: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b container agnhost-container: <nil>
STEP: delete the pod 09/07/23 05:52:42.803
Sep  7 05:52:42.813: INFO: Waiting for pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b to disappear
Sep  7 05:52:42.815: INFO: Pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1858" for this suite. 09/07/23 05:52:42.817
------------------------------
â€¢ [4.048 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:38.772
    Sep  7 05:52:38.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 05:52:38.773
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:38.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:38.783
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-314c58d1-0666-44f2-ab7b-bdcf8922c7a2 09/07/23 05:52:38.785
    STEP: Creating a pod to test consume configMaps 09/07/23 05:52:38.787
    Sep  7 05:52:38.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b" in namespace "projected-1858" to be "Succeeded or Failed"
    Sep  7 05:52:38.793: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346779ms
    Sep  7 05:52:40.796: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004043198s
    Sep  7 05:52:42.797: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005126518s
    STEP: Saw pod success 09/07/23 05:52:42.797
    Sep  7 05:52:42.797: INFO: Pod "pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:42.799: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 05:52:42.803
    Sep  7 05:52:42.813: INFO: Waiting for pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b to disappear
    Sep  7 05:52:42.815: INFO: Pod pod-projected-configmaps-f31f417f-950a-4b36-bfbe-aaa27c298c5b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:42.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1858" for this suite. 09/07/23 05:52:42.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:42.821
Sep  7 05:52:42.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 05:52:42.821
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:42.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:42.832
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/07/23 05:52:42.833
Sep  7 05:52:42.837: INFO: Waiting up to 5m0s for pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9" in namespace "downward-api-9538" to be "Succeeded or Failed"
Sep  7 05:52:42.839: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.24364ms
Sep  7 05:52:44.841: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00368006s
Sep  7 05:52:46.842: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00438409s
STEP: Saw pod success 09/07/23 05:52:46.842
Sep  7 05:52:46.842: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9" satisfied condition "Succeeded or Failed"
Sep  7 05:52:46.844: INFO: Trying to get logs from node kind-worker2 pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 container dapi-container: <nil>
STEP: delete the pod 09/07/23 05:52:46.848
Sep  7 05:52:46.856: INFO: Waiting for pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 to disappear
Sep  7 05:52:46.857: INFO: Pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  7 05:52:46.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9538" for this suite. 09/07/23 05:52:46.859
------------------------------
â€¢ [4.042 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:42.821
    Sep  7 05:52:42.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 05:52:42.821
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:42.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:42.832
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/07/23 05:52:42.833
    Sep  7 05:52:42.837: INFO: Waiting up to 5m0s for pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9" in namespace "downward-api-9538" to be "Succeeded or Failed"
    Sep  7 05:52:42.839: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.24364ms
    Sep  7 05:52:44.841: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00368006s
    Sep  7 05:52:46.842: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00438409s
    STEP: Saw pod success 09/07/23 05:52:46.842
    Sep  7 05:52:46.842: INFO: Pod "downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9" satisfied condition "Succeeded or Failed"
    Sep  7 05:52:46.844: INFO: Trying to get logs from node kind-worker2 pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 05:52:46.848
    Sep  7 05:52:46.856: INFO: Waiting for pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 to disappear
    Sep  7 05:52:46.857: INFO: Pod downward-api-5953d8d6-04b0-4a46-bf92-02ee76b05da9 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:52:46.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9538" for this suite. 09/07/23 05:52:46.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:52:46.864
Sep  7 05:52:46.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 05:52:46.865
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:46.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:46.874
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da in namespace container-probe-8728 09/07/23 05:52:46.876
Sep  7 05:52:46.881: INFO: Waiting up to 5m0s for pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da" in namespace "container-probe-8728" to be "not pending"
Sep  7 05:52:46.883: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57616ms
Sep  7 05:52:48.886: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da": Phase="Running", Reason="", readiness=true. Elapsed: 2.00460507s
Sep  7 05:52:48.886: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da" satisfied condition "not pending"
Sep  7 05:52:48.886: INFO: Started pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da in namespace container-probe-8728
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:52:48.886
Sep  7 05:52:48.888: INFO: Initial restart count of pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da is 0
STEP: deleting the pod 09/07/23 05:56:49.248
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 05:56:49.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8728" for this suite. 09/07/23 05:56:49.262
------------------------------
â€¢ [SLOW TEST] [242.402 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:52:46.864
    Sep  7 05:52:46.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 05:52:46.865
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:52:46.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:52:46.874
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da in namespace container-probe-8728 09/07/23 05:52:46.876
    Sep  7 05:52:46.881: INFO: Waiting up to 5m0s for pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da" in namespace "container-probe-8728" to be "not pending"
    Sep  7 05:52:46.883: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57616ms
    Sep  7 05:52:48.886: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da": Phase="Running", Reason="", readiness=true. Elapsed: 2.00460507s
    Sep  7 05:52:48.886: INFO: Pod "busybox-1d28d468-2b98-4640-bac3-6aa2355541da" satisfied condition "not pending"
    Sep  7 05:52:48.886: INFO: Started pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da in namespace container-probe-8728
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 05:52:48.886
    Sep  7 05:52:48.888: INFO: Initial restart count of pod busybox-1d28d468-2b98-4640-bac3-6aa2355541da is 0
    STEP: deleting the pod 09/07/23 05:56:49.248
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:56:49.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8728" for this suite. 09/07/23 05:56:49.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:56:49.266
Sep  7 05:56:49.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sysctl 09/07/23 05:56:49.267
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:56:49.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:56:49.275
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/07/23 05:56:49.277
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:56:49.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8457" for this suite. 09/07/23 05:56:49.282
------------------------------
â€¢ [0.020 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:56:49.266
    Sep  7 05:56:49.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sysctl 09/07/23 05:56:49.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:56:49.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:56:49.275
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/07/23 05:56:49.277
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:56:49.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8457" for this suite. 09/07/23 05:56:49.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:56:49.287
Sep  7 05:56:49.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 05:56:49.287
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:56:49.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:56:49.296
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/07/23 05:56:49.297
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5558.svc.cluster.local;sleep 1; done
 09/07/23 05:56:49.301
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5558.svc.cluster.local;sleep 1; done
 09/07/23 05:56:49.302
STEP: creating a pod to probe DNS 09/07/23 05:56:49.302
STEP: submitting the pod to kubernetes 09/07/23 05:56:49.302
Sep  7 05:56:49.308: INFO: Waiting up to 15m0s for pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d" in namespace "dns-5558" to be "running"
Sep  7 05:56:49.310: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62709ms
Sep  7 05:56:51.314: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005826785s
Sep  7 05:56:51.314: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d" satisfied condition "running"
STEP: retrieving the pod 09/07/23 05:56:51.314
STEP: looking for the results for each expected name from probers 09/07/23 05:56:51.316
Sep  7 05:56:51.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:51.320: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:51.326: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:51.328: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:51.331: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:56:56.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:56.336: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:56.342: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:56.343: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:56:56.347: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:57:01.335: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:01.336: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:01.341: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:01.343: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:01.346: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:57:06.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:06.337: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:06.342: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:06.344: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:06.347: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:57:11.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:11.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:11.344: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:11.346: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:11.349: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:57:16.337: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:16.339: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:16.344: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:16.346: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
Sep  7 05:57:16.349: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

Sep  7 05:57:21.346: INFO: DNS probes using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d succeeded

STEP: deleting the pod 09/07/23 05:57:21.346
STEP: deleting the test headless service 09/07/23 05:57:21.354
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:21.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5558" for this suite. 09/07/23 05:57:21.369
------------------------------
â€¢ [SLOW TEST] [32.086 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:56:49.287
    Sep  7 05:56:49.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 05:56:49.287
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:56:49.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:56:49.296
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/07/23 05:56:49.297
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5558.svc.cluster.local;sleep 1; done
     09/07/23 05:56:49.301
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5558.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5558.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5558.svc.cluster.local;sleep 1; done
     09/07/23 05:56:49.302
    STEP: creating a pod to probe DNS 09/07/23 05:56:49.302
    STEP: submitting the pod to kubernetes 09/07/23 05:56:49.302
    Sep  7 05:56:49.308: INFO: Waiting up to 15m0s for pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d" in namespace "dns-5558" to be "running"
    Sep  7 05:56:49.310: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62709ms
    Sep  7 05:56:51.314: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005826785s
    Sep  7 05:56:51.314: INFO: Pod "dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 05:56:51.314
    STEP: looking for the results for each expected name from probers 09/07/23 05:56:51.316
    Sep  7 05:56:51.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:51.320: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:51.326: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:51.328: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:51.331: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:56:56.334: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:56.336: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:56.342: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:56.343: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:56:56.347: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:57:01.335: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:01.336: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:01.341: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:01.343: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:01.346: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:57:06.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:06.337: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:06.342: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:06.344: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:06.347: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:57:11.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:11.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:11.344: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:11.346: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:11.349: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:57:16.337: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:16.339: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:16.344: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:16.346: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local from pod dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d: the server could not find the requested resource (get pods dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d)
    Sep  7 05:57:16.349: INFO: Lookups using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5558.svc.cluster.local]

    Sep  7 05:57:21.346: INFO: DNS probes using dns-5558/dns-test-d813c7ae-560b-48dd-8a30-d6fbe547d83d succeeded

    STEP: deleting the pod 09/07/23 05:57:21.346
    STEP: deleting the test headless service 09/07/23 05:57:21.354
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:21.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5558" for this suite. 09/07/23 05:57:21.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:21.374
Sep  7 05:57:21.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:57:21.374
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:21.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:21.384
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  7 05:57:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/07/23 05:57:22.681
Sep  7 05:57:22.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
Sep  7 05:57:23.078: INFO: stderr: ""
Sep  7 05:57:23.078: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  7 05:57:23.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 delete e2e-test-crd-publish-openapi-2202-crds test-foo'
Sep  7 05:57:23.133: INFO: stderr: ""
Sep  7 05:57:23.133: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  7 05:57:23.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
Sep  7 05:57:23.249: INFO: stderr: ""
Sep  7 05:57:23.249: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  7 05:57:23.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 delete e2e-test-crd-publish-openapi-2202-crds test-foo'
Sep  7 05:57:23.306: INFO: stderr: ""
Sep  7 05:57:23.306: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/07/23 05:57:23.306
Sep  7 05:57:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
Sep  7 05:57:23.426: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/07/23 05:57:23.426
Sep  7 05:57:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
Sep  7 05:57:23.544: INFO: rc: 1
Sep  7 05:57:23.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
Sep  7 05:57:23.663: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/07/23 05:57:23.663
Sep  7 05:57:23.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
Sep  7 05:57:23.781: INFO: rc: 1
Sep  7 05:57:23.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
Sep  7 05:57:23.903: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/07/23 05:57:23.903
Sep  7 05:57:23.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds'
Sep  7 05:57:24.017: INFO: stderr: ""
Sep  7 05:57:24.017: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/07/23 05:57:24.017
Sep  7 05:57:24.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.metadata'
Sep  7 05:57:24.134: INFO: stderr: ""
Sep  7 05:57:24.135: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  7 05:57:24.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec'
Sep  7 05:57:24.245: INFO: stderr: ""
Sep  7 05:57:24.245: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  7 05:57:24.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec.bars'
Sep  7 05:57:24.359: INFO: stderr: ""
Sep  7 05:57:24.359: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/07/23 05:57:24.359
Sep  7 05:57:24.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec.bars2'
Sep  7 05:57:24.470: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:26.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2333" for this suite. 09/07/23 05:57:26.27
------------------------------
â€¢ [4.899 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:21.374
    Sep  7 05:57:21.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 05:57:21.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:21.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:21.384
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  7 05:57:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/07/23 05:57:22.681
    Sep  7 05:57:22.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
    Sep  7 05:57:23.078: INFO: stderr: ""
    Sep  7 05:57:23.078: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  7 05:57:23.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 delete e2e-test-crd-publish-openapi-2202-crds test-foo'
    Sep  7 05:57:23.133: INFO: stderr: ""
    Sep  7 05:57:23.133: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  7 05:57:23.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
    Sep  7 05:57:23.249: INFO: stderr: ""
    Sep  7 05:57:23.249: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  7 05:57:23.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 delete e2e-test-crd-publish-openapi-2202-crds test-foo'
    Sep  7 05:57:23.306: INFO: stderr: ""
    Sep  7 05:57:23.306: INFO: stdout: "e2e-test-crd-publish-openapi-2202-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/07/23 05:57:23.306
    Sep  7 05:57:23.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
    Sep  7 05:57:23.426: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/07/23 05:57:23.426
    Sep  7 05:57:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
    Sep  7 05:57:23.544: INFO: rc: 1
    Sep  7 05:57:23.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
    Sep  7 05:57:23.663: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/07/23 05:57:23.663
    Sep  7 05:57:23.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 create -f -'
    Sep  7 05:57:23.781: INFO: rc: 1
    Sep  7 05:57:23.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 --namespace=crd-publish-openapi-2333 apply -f -'
    Sep  7 05:57:23.903: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/07/23 05:57:23.903
    Sep  7 05:57:23.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds'
    Sep  7 05:57:24.017: INFO: stderr: ""
    Sep  7 05:57:24.017: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/07/23 05:57:24.017
    Sep  7 05:57:24.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.metadata'
    Sep  7 05:57:24.134: INFO: stderr: ""
    Sep  7 05:57:24.135: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  7 05:57:24.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec'
    Sep  7 05:57:24.245: INFO: stderr: ""
    Sep  7 05:57:24.245: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  7 05:57:24.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec.bars'
    Sep  7 05:57:24.359: INFO: stderr: ""
    Sep  7 05:57:24.359: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2202-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/07/23 05:57:24.359
    Sep  7 05:57:24.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-2333 explain e2e-test-crd-publish-openapi-2202-crds.spec.bars2'
    Sep  7 05:57:24.470: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:26.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2333" for this suite. 09/07/23 05:57:26.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:26.274
Sep  7 05:57:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context 09/07/23 05:57:26.274
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:26.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:26.286
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/07/23 05:57:26.288
Sep  7 05:57:26.293: INFO: Waiting up to 5m0s for pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194" in namespace "security-context-8998" to be "Succeeded or Failed"
Sep  7 05:57:26.294: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32714ms
Sep  7 05:57:28.297: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00445572s
Sep  7 05:57:30.298: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005114751s
STEP: Saw pod success 09/07/23 05:57:30.298
Sep  7 05:57:30.298: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194" satisfied condition "Succeeded or Failed"
Sep  7 05:57:30.300: INFO: Trying to get logs from node kind-worker2 pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 container test-container: <nil>
STEP: delete the pod 09/07/23 05:57:30.308
Sep  7 05:57:30.316: INFO: Waiting for pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 to disappear
Sep  7 05:57:30.317: INFO: Pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8998" for this suite. 09/07/23 05:57:30.319
------------------------------
â€¢ [4.048 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:26.274
    Sep  7 05:57:26.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context 09/07/23 05:57:26.274
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:26.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:26.286
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/07/23 05:57:26.288
    Sep  7 05:57:26.293: INFO: Waiting up to 5m0s for pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194" in namespace "security-context-8998" to be "Succeeded or Failed"
    Sep  7 05:57:26.294: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32714ms
    Sep  7 05:57:28.297: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00445572s
    Sep  7 05:57:30.298: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005114751s
    STEP: Saw pod success 09/07/23 05:57:30.298
    Sep  7 05:57:30.298: INFO: Pod "security-context-acef980c-67d8-4ffd-92ef-d14913b30194" satisfied condition "Succeeded or Failed"
    Sep  7 05:57:30.300: INFO: Trying to get logs from node kind-worker2 pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 container test-container: <nil>
    STEP: delete the pod 09/07/23 05:57:30.308
    Sep  7 05:57:30.316: INFO: Waiting for pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 to disappear
    Sep  7 05:57:30.317: INFO: Pod security-context-acef980c-67d8-4ffd-92ef-d14913b30194 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:30.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8998" for this suite. 09/07/23 05:57:30.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:30.322
Sep  7 05:57:30.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename ingress 09/07/23 05:57:30.323
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:30.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:30.332
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/07/23 05:57:30.333
STEP: getting /apis/networking.k8s.io 09/07/23 05:57:30.334
STEP: getting /apis/networking.k8s.iov1 09/07/23 05:57:30.335
STEP: creating 09/07/23 05:57:30.336
STEP: getting 09/07/23 05:57:30.343
STEP: listing 09/07/23 05:57:30.345
STEP: watching 09/07/23 05:57:30.346
Sep  7 05:57:30.346: INFO: starting watch
STEP: cluster-wide listing 09/07/23 05:57:30.347
STEP: cluster-wide watching 09/07/23 05:57:30.348
Sep  7 05:57:30.348: INFO: starting watch
STEP: patching 09/07/23 05:57:30.349
STEP: updating 09/07/23 05:57:30.353
Sep  7 05:57:30.357: INFO: waiting for watch events with expected annotations
Sep  7 05:57:30.357: INFO: saw patched and updated annotations
STEP: patching /status 09/07/23 05:57:30.357
STEP: updating /status 09/07/23 05:57:30.361
STEP: get /status 09/07/23 05:57:30.365
STEP: deleting 09/07/23 05:57:30.366
STEP: deleting a collection 09/07/23 05:57:30.374
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-6295" for this suite. 09/07/23 05:57:30.384
------------------------------
â€¢ [0.064 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:30.322
    Sep  7 05:57:30.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename ingress 09/07/23 05:57:30.323
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:30.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:30.332
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/07/23 05:57:30.333
    STEP: getting /apis/networking.k8s.io 09/07/23 05:57:30.334
    STEP: getting /apis/networking.k8s.iov1 09/07/23 05:57:30.335
    STEP: creating 09/07/23 05:57:30.336
    STEP: getting 09/07/23 05:57:30.343
    STEP: listing 09/07/23 05:57:30.345
    STEP: watching 09/07/23 05:57:30.346
    Sep  7 05:57:30.346: INFO: starting watch
    STEP: cluster-wide listing 09/07/23 05:57:30.347
    STEP: cluster-wide watching 09/07/23 05:57:30.348
    Sep  7 05:57:30.348: INFO: starting watch
    STEP: patching 09/07/23 05:57:30.349
    STEP: updating 09/07/23 05:57:30.353
    Sep  7 05:57:30.357: INFO: waiting for watch events with expected annotations
    Sep  7 05:57:30.357: INFO: saw patched and updated annotations
    STEP: patching /status 09/07/23 05:57:30.357
    STEP: updating /status 09/07/23 05:57:30.361
    STEP: get /status 09/07/23 05:57:30.365
    STEP: deleting 09/07/23 05:57:30.366
    STEP: deleting a collection 09/07/23 05:57:30.374
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-6295" for this suite. 09/07/23 05:57:30.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:30.388
Sep  7 05:57:30.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 05:57:30.388
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:30.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:30.396
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7723 09/07/23 05:57:30.398
STEP: creating service affinity-nodeport-transition in namespace services-7723 09/07/23 05:57:30.398
STEP: creating replication controller affinity-nodeport-transition in namespace services-7723 09/07/23 05:57:30.407
I0907 05:57:30.411964      29 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7723, replica count: 3
I0907 05:57:33.463312      29 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 05:57:33.468: INFO: Creating new exec pod
Sep  7 05:57:33.474: INFO: Waiting up to 5m0s for pod "execpod-affinitys4rs5" in namespace "services-7723" to be "running"
Sep  7 05:57:33.476: INFO: Pod "execpod-affinitys4rs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62774ms
Sep  7 05:57:35.478: INFO: Pod "execpod-affinitys4rs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004372791s
Sep  7 05:57:35.478: INFO: Pod "execpod-affinitys4rs5" satisfied condition "running"
Sep  7 05:57:36.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  7 05:57:36.634: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  7 05:57:36.634: INFO: stdout: ""
Sep  7 05:57:36.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 10.96.106.38 80'
Sep  7 05:57:36.801: INFO: stderr: "+ nc -v -z -w 2 10.96.106.38 80\nConnection to 10.96.106.38 80 port [tcp/http] succeeded!\n"
Sep  7 05:57:36.801: INFO: stdout: ""
Sep  7 05:57:36.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 30448'
Sep  7 05:57:36.973: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 30448\nConnection to 192.168.8.3 30448 port [tcp/*] succeeded!\n"
Sep  7 05:57:36.973: INFO: stdout: ""
Sep  7 05:57:36.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 30448'
Sep  7 05:57:37.126: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 30448\nConnection to 192.168.8.6 30448 port [tcp/*] succeeded!\n"
Sep  7 05:57:37.126: INFO: stdout: ""
Sep  7 05:57:37.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:30448/ ; done'
Sep  7 05:57:37.369: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n"
Sep  7 05:57:37.369: INFO: stdout: "\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-2nsq7"
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:30448/ ; done'
Sep  7 05:57:37.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n"
Sep  7 05:57:37.615: INFO: stdout: "\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7"
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
Sep  7 05:57:37.615: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7723, will wait for the garbage collector to delete the pods 09/07/23 05:57:37.624
Sep  7 05:57:37.681: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.27758ms
Sep  7 05:57:37.781: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.364302ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:41.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7723" for this suite. 09/07/23 05:57:41.501
------------------------------
â€¢ [SLOW TEST] [11.116 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:30.388
    Sep  7 05:57:30.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 05:57:30.388
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:30.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:30.396
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7723 09/07/23 05:57:30.398
    STEP: creating service affinity-nodeport-transition in namespace services-7723 09/07/23 05:57:30.398
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7723 09/07/23 05:57:30.407
    I0907 05:57:30.411964      29 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7723, replica count: 3
    I0907 05:57:33.463312      29 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 05:57:33.468: INFO: Creating new exec pod
    Sep  7 05:57:33.474: INFO: Waiting up to 5m0s for pod "execpod-affinitys4rs5" in namespace "services-7723" to be "running"
    Sep  7 05:57:33.476: INFO: Pod "execpod-affinitys4rs5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.62774ms
    Sep  7 05:57:35.478: INFO: Pod "execpod-affinitys4rs5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004372791s
    Sep  7 05:57:35.478: INFO: Pod "execpod-affinitys4rs5" satisfied condition "running"
    Sep  7 05:57:36.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  7 05:57:36.634: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  7 05:57:36.634: INFO: stdout: ""
    Sep  7 05:57:36.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 10.96.106.38 80'
    Sep  7 05:57:36.801: INFO: stderr: "+ nc -v -z -w 2 10.96.106.38 80\nConnection to 10.96.106.38 80 port [tcp/http] succeeded!\n"
    Sep  7 05:57:36.801: INFO: stdout: ""
    Sep  7 05:57:36.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 30448'
    Sep  7 05:57:36.973: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 30448\nConnection to 192.168.8.3 30448 port [tcp/*] succeeded!\n"
    Sep  7 05:57:36.973: INFO: stdout: ""
    Sep  7 05:57:36.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 30448'
    Sep  7 05:57:37.126: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 30448\nConnection to 192.168.8.6 30448 port [tcp/*] succeeded!\n"
    Sep  7 05:57:37.126: INFO: stdout: ""
    Sep  7 05:57:37.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:30448/ ; done'
    Sep  7 05:57:37.369: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n"
    Sep  7 05:57:37.369: INFO: stdout: "\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-7x8kk\naffinity-nodeport-transition-f9b8d\naffinity-nodeport-transition-2nsq7"
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-7x8kk
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-f9b8d
    Sep  7 05:57:37.369: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7723 exec execpod-affinitys4rs5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:30448/ ; done'
    Sep  7 05:57:37.615: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:30448/\n"
    Sep  7 05:57:37.615: INFO: stdout: "\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7\naffinity-nodeport-transition-2nsq7"
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Received response from host: affinity-nodeport-transition-2nsq7
    Sep  7 05:57:37.615: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7723, will wait for the garbage collector to delete the pods 09/07/23 05:57:37.624
    Sep  7 05:57:37.681: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.27758ms
    Sep  7 05:57:37.781: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.364302ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:41.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7723" for this suite. 09/07/23 05:57:41.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:41.505
Sep  7 05:57:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 05:57:41.505
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:41.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:41.516
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5700" 09/07/23 05:57:41.518
Sep  7 05:57:41.522: INFO: Namespace "namespaces-5700" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b2ea5160-bc07-49e5-8e7a-ce95b6c6d343", "kubernetes.io/metadata.name":"namespaces-5700", "namespaces-5700":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:41.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5700" for this suite. 09/07/23 05:57:41.524
------------------------------
â€¢ [0.022 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:41.505
    Sep  7 05:57:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 05:57:41.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:41.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:41.516
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5700" 09/07/23 05:57:41.518
    Sep  7 05:57:41.522: INFO: Namespace "namespaces-5700" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b2ea5160-bc07-49e5-8e7a-ce95b6c6d343", "kubernetes.io/metadata.name":"namespaces-5700", "namespaces-5700":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:41.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5700" for this suite. 09/07/23 05:57:41.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:41.527
Sep  7 05:57:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-runtime 09/07/23 05:57:41.528
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:41.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:41.538
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/07/23 05:57:41.54
STEP: wait for the container to reach Failed 09/07/23 05:57:41.545
STEP: get the container status 09/07/23 05:57:44.555
STEP: the container should be terminated 09/07/23 05:57:44.556
STEP: the termination message should be set 09/07/23 05:57:44.556
Sep  7 05:57:44.556: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/07/23 05:57:44.556
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  7 05:57:44.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4578" for this suite. 09/07/23 05:57:44.568
------------------------------
â€¢ [3.045 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:41.527
    Sep  7 05:57:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-runtime 09/07/23 05:57:41.528
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:41.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:41.538
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/07/23 05:57:41.54
    STEP: wait for the container to reach Failed 09/07/23 05:57:41.545
    STEP: get the container status 09/07/23 05:57:44.555
    STEP: the container should be terminated 09/07/23 05:57:44.556
    STEP: the termination message should be set 09/07/23 05:57:44.556
    Sep  7 05:57:44.556: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/07/23 05:57:44.556
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:57:44.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4578" for this suite. 09/07/23 05:57:44.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:57:44.572
Sep  7 05:57:44.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 05:57:44.573
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:44.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:44.582
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/07/23 05:57:44.586
STEP: delete the rc 09/07/23 05:57:49.592
STEP: wait for the rc to be deleted 09/07/23 05:57:49.596
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/07/23 05:57:54.599
STEP: Gathering metrics 09/07/23 05:58:24.608
Sep  7 05:58:24.620: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 05:58:24.622: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.63508ms
Sep  7 05:58:24.622: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 05:58:24.622: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 05:58:24.666: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  7 05:58:24.666: INFO: Deleting pod "simpletest.rc-25vql" in namespace "gc-3346"
Sep  7 05:58:24.672: INFO: Deleting pod "simpletest.rc-2lch4" in namespace "gc-3346"
Sep  7 05:58:24.681: INFO: Deleting pod "simpletest.rc-2pphs" in namespace "gc-3346"
Sep  7 05:58:24.688: INFO: Deleting pod "simpletest.rc-4mlrd" in namespace "gc-3346"
Sep  7 05:58:24.696: INFO: Deleting pod "simpletest.rc-4qhn4" in namespace "gc-3346"
Sep  7 05:58:24.702: INFO: Deleting pod "simpletest.rc-4tnsv" in namespace "gc-3346"
Sep  7 05:58:24.709: INFO: Deleting pod "simpletest.rc-4zlgd" in namespace "gc-3346"
Sep  7 05:58:24.716: INFO: Deleting pod "simpletest.rc-58zts" in namespace "gc-3346"
Sep  7 05:58:24.722: INFO: Deleting pod "simpletest.rc-5ggwg" in namespace "gc-3346"
Sep  7 05:58:24.728: INFO: Deleting pod "simpletest.rc-5mbkd" in namespace "gc-3346"
Sep  7 05:58:24.734: INFO: Deleting pod "simpletest.rc-5rkzn" in namespace "gc-3346"
Sep  7 05:58:24.740: INFO: Deleting pod "simpletest.rc-5tkv9" in namespace "gc-3346"
Sep  7 05:58:24.746: INFO: Deleting pod "simpletest.rc-6cdk6" in namespace "gc-3346"
Sep  7 05:58:24.753: INFO: Deleting pod "simpletest.rc-6ntxx" in namespace "gc-3346"
Sep  7 05:58:24.759: INFO: Deleting pod "simpletest.rc-6tmsx" in namespace "gc-3346"
Sep  7 05:58:24.766: INFO: Deleting pod "simpletest.rc-6zkvk" in namespace "gc-3346"
Sep  7 05:58:24.774: INFO: Deleting pod "simpletest.rc-7klfd" in namespace "gc-3346"
Sep  7 05:58:24.783: INFO: Deleting pod "simpletest.rc-7nksg" in namespace "gc-3346"
Sep  7 05:58:24.791: INFO: Deleting pod "simpletest.rc-7tfvl" in namespace "gc-3346"
Sep  7 05:58:24.798: INFO: Deleting pod "simpletest.rc-7w5zq" in namespace "gc-3346"
Sep  7 05:58:24.805: INFO: Deleting pod "simpletest.rc-7xv6j" in namespace "gc-3346"
Sep  7 05:58:24.815: INFO: Deleting pod "simpletest.rc-8l4mt" in namespace "gc-3346"
Sep  7 05:58:24.825: INFO: Deleting pod "simpletest.rc-8pt7g" in namespace "gc-3346"
Sep  7 05:58:24.835: INFO: Deleting pod "simpletest.rc-8r7qb" in namespace "gc-3346"
Sep  7 05:58:24.842: INFO: Deleting pod "simpletest.rc-928zp" in namespace "gc-3346"
Sep  7 05:58:24.848: INFO: Deleting pod "simpletest.rc-95q7k" in namespace "gc-3346"
Sep  7 05:58:24.857: INFO: Deleting pod "simpletest.rc-98h7f" in namespace "gc-3346"
Sep  7 05:58:24.863: INFO: Deleting pod "simpletest.rc-98jxt" in namespace "gc-3346"
Sep  7 05:58:24.871: INFO: Deleting pod "simpletest.rc-99ps4" in namespace "gc-3346"
Sep  7 05:58:24.877: INFO: Deleting pod "simpletest.rc-9rkrx" in namespace "gc-3346"
Sep  7 05:58:24.885: INFO: Deleting pod "simpletest.rc-b7k99" in namespace "gc-3346"
Sep  7 05:58:24.892: INFO: Deleting pod "simpletest.rc-bdpr6" in namespace "gc-3346"
Sep  7 05:58:24.901: INFO: Deleting pod "simpletest.rc-blhj7" in namespace "gc-3346"
Sep  7 05:58:24.912: INFO: Deleting pod "simpletest.rc-bsgg5" in namespace "gc-3346"
Sep  7 05:58:24.923: INFO: Deleting pod "simpletest.rc-bx8dp" in namespace "gc-3346"
Sep  7 05:58:24.931: INFO: Deleting pod "simpletest.rc-cdxlf" in namespace "gc-3346"
Sep  7 05:58:24.940: INFO: Deleting pod "simpletest.rc-cn26w" in namespace "gc-3346"
Sep  7 05:58:24.946: INFO: Deleting pod "simpletest.rc-cqm6x" in namespace "gc-3346"
Sep  7 05:58:24.952: INFO: Deleting pod "simpletest.rc-cvs7f" in namespace "gc-3346"
Sep  7 05:58:24.960: INFO: Deleting pod "simpletest.rc-dr5zn" in namespace "gc-3346"
Sep  7 05:58:24.966: INFO: Deleting pod "simpletest.rc-gg52f" in namespace "gc-3346"
Sep  7 05:58:24.972: INFO: Deleting pod "simpletest.rc-gh7pk" in namespace "gc-3346"
Sep  7 05:58:24.978: INFO: Deleting pod "simpletest.rc-gkhph" in namespace "gc-3346"
Sep  7 05:58:24.985: INFO: Deleting pod "simpletest.rc-grmnh" in namespace "gc-3346"
Sep  7 05:58:24.991: INFO: Deleting pod "simpletest.rc-gzvnm" in namespace "gc-3346"
Sep  7 05:58:25.001: INFO: Deleting pod "simpletest.rc-ht2sg" in namespace "gc-3346"
Sep  7 05:58:25.012: INFO: Deleting pod "simpletest.rc-hvnbd" in namespace "gc-3346"
Sep  7 05:58:25.022: INFO: Deleting pod "simpletest.rc-j98s7" in namespace "gc-3346"
Sep  7 05:58:25.034: INFO: Deleting pod "simpletest.rc-jh7rb" in namespace "gc-3346"
Sep  7 05:58:25.043: INFO: Deleting pod "simpletest.rc-jzvjf" in namespace "gc-3346"
Sep  7 05:58:25.053: INFO: Deleting pod "simpletest.rc-k9wdg" in namespace "gc-3346"
Sep  7 05:58:25.061: INFO: Deleting pod "simpletest.rc-kl4sd" in namespace "gc-3346"
Sep  7 05:58:25.068: INFO: Deleting pod "simpletest.rc-kt7vg" in namespace "gc-3346"
Sep  7 05:58:25.078: INFO: Deleting pod "simpletest.rc-kxksb" in namespace "gc-3346"
Sep  7 05:58:25.088: INFO: Deleting pod "simpletest.rc-lcwm2" in namespace "gc-3346"
Sep  7 05:58:25.097: INFO: Deleting pod "simpletest.rc-m42qc" in namespace "gc-3346"
Sep  7 05:58:25.108: INFO: Deleting pod "simpletest.rc-mbxwd" in namespace "gc-3346"
Sep  7 05:58:25.159: INFO: Deleting pod "simpletest.rc-mlnpm" in namespace "gc-3346"
Sep  7 05:58:25.213: INFO: Deleting pod "simpletest.rc-n5g4k" in namespace "gc-3346"
Sep  7 05:58:25.267: INFO: Deleting pod "simpletest.rc-n9gxm" in namespace "gc-3346"
Sep  7 05:58:25.313: INFO: Deleting pod "simpletest.rc-njq62" in namespace "gc-3346"
Sep  7 05:58:25.360: INFO: Deleting pod "simpletest.rc-nlw85" in namespace "gc-3346"
Sep  7 05:58:25.414: INFO: Deleting pod "simpletest.rc-nswvx" in namespace "gc-3346"
Sep  7 05:58:25.462: INFO: Deleting pod "simpletest.rc-nvhmt" in namespace "gc-3346"
Sep  7 05:58:25.509: INFO: Deleting pod "simpletest.rc-p79zh" in namespace "gc-3346"
Sep  7 05:58:25.565: INFO: Deleting pod "simpletest.rc-pm9br" in namespace "gc-3346"
Sep  7 05:58:25.611: INFO: Deleting pod "simpletest.rc-pxdmp" in namespace "gc-3346"
Sep  7 05:58:25.661: INFO: Deleting pod "simpletest.rc-pzxc4" in namespace "gc-3346"
Sep  7 05:58:25.713: INFO: Deleting pod "simpletest.rc-qsrq5" in namespace "gc-3346"
Sep  7 05:58:25.761: INFO: Deleting pod "simpletest.rc-qwh6s" in namespace "gc-3346"
Sep  7 05:58:25.814: INFO: Deleting pod "simpletest.rc-qxstj" in namespace "gc-3346"
Sep  7 05:58:25.861: INFO: Deleting pod "simpletest.rc-qzwlv" in namespace "gc-3346"
Sep  7 05:58:25.909: INFO: Deleting pod "simpletest.rc-r94pw" in namespace "gc-3346"
Sep  7 05:58:25.964: INFO: Deleting pod "simpletest.rc-r956h" in namespace "gc-3346"
Sep  7 05:58:26.013: INFO: Deleting pod "simpletest.rc-rcpzm" in namespace "gc-3346"
Sep  7 05:58:26.059: INFO: Deleting pod "simpletest.rc-rct2k" in namespace "gc-3346"
Sep  7 05:58:26.114: INFO: Deleting pod "simpletest.rc-rdlbc" in namespace "gc-3346"
Sep  7 05:58:26.158: INFO: Deleting pod "simpletest.rc-rfz24" in namespace "gc-3346"
Sep  7 05:58:26.212: INFO: Deleting pod "simpletest.rc-rl48v" in namespace "gc-3346"
Sep  7 05:58:26.260: INFO: Deleting pod "simpletest.rc-rn7qz" in namespace "gc-3346"
Sep  7 05:58:26.311: INFO: Deleting pod "simpletest.rc-rnnsq" in namespace "gc-3346"
Sep  7 05:58:26.361: INFO: Deleting pod "simpletest.rc-rxd25" in namespace "gc-3346"
Sep  7 05:58:26.408: INFO: Deleting pod "simpletest.rc-sh78z" in namespace "gc-3346"
Sep  7 05:58:26.461: INFO: Deleting pod "simpletest.rc-sj7x5" in namespace "gc-3346"
Sep  7 05:58:26.511: INFO: Deleting pod "simpletest.rc-t8lc9" in namespace "gc-3346"
Sep  7 05:58:26.561: INFO: Deleting pod "simpletest.rc-tjz4q" in namespace "gc-3346"
Sep  7 05:58:26.613: INFO: Deleting pod "simpletest.rc-tmx5s" in namespace "gc-3346"
Sep  7 05:58:26.659: INFO: Deleting pod "simpletest.rc-tvqrq" in namespace "gc-3346"
Sep  7 05:58:26.714: INFO: Deleting pod "simpletest.rc-v2gwj" in namespace "gc-3346"
Sep  7 05:58:26.759: INFO: Deleting pod "simpletest.rc-vd4qg" in namespace "gc-3346"
Sep  7 05:58:26.811: INFO: Deleting pod "simpletest.rc-vhvzl" in namespace "gc-3346"
Sep  7 05:58:26.863: INFO: Deleting pod "simpletest.rc-vl5wk" in namespace "gc-3346"
Sep  7 05:58:26.913: INFO: Deleting pod "simpletest.rc-w6sn2" in namespace "gc-3346"
Sep  7 05:58:26.961: INFO: Deleting pod "simpletest.rc-w9rcq" in namespace "gc-3346"
Sep  7 05:58:27.009: INFO: Deleting pod "simpletest.rc-wr2ch" in namespace "gc-3346"
Sep  7 05:58:27.061: INFO: Deleting pod "simpletest.rc-xkdzm" in namespace "gc-3346"
Sep  7 05:58:27.108: INFO: Deleting pod "simpletest.rc-z5ngw" in namespace "gc-3346"
Sep  7 05:58:27.159: INFO: Deleting pod "simpletest.rc-z62q7" in namespace "gc-3346"
Sep  7 05:58:27.209: INFO: Deleting pod "simpletest.rc-zlgcj" in namespace "gc-3346"
Sep  7 05:58:27.260: INFO: Deleting pod "simpletest.rc-zs5cc" in namespace "gc-3346"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3346" for this suite. 09/07/23 05:58:27.355
------------------------------
â€¢ [SLOW TEST] [42.835 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:57:44.572
    Sep  7 05:57:44.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 05:57:44.573
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:57:44.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:57:44.582
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/07/23 05:57:44.586
    STEP: delete the rc 09/07/23 05:57:49.592
    STEP: wait for the rc to be deleted 09/07/23 05:57:49.596
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/07/23 05:57:54.599
    STEP: Gathering metrics 09/07/23 05:58:24.608
    Sep  7 05:58:24.620: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 05:58:24.622: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.63508ms
    Sep  7 05:58:24.622: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 05:58:24.622: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 05:58:24.666: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  7 05:58:24.666: INFO: Deleting pod "simpletest.rc-25vql" in namespace "gc-3346"
    Sep  7 05:58:24.672: INFO: Deleting pod "simpletest.rc-2lch4" in namespace "gc-3346"
    Sep  7 05:58:24.681: INFO: Deleting pod "simpletest.rc-2pphs" in namespace "gc-3346"
    Sep  7 05:58:24.688: INFO: Deleting pod "simpletest.rc-4mlrd" in namespace "gc-3346"
    Sep  7 05:58:24.696: INFO: Deleting pod "simpletest.rc-4qhn4" in namespace "gc-3346"
    Sep  7 05:58:24.702: INFO: Deleting pod "simpletest.rc-4tnsv" in namespace "gc-3346"
    Sep  7 05:58:24.709: INFO: Deleting pod "simpletest.rc-4zlgd" in namespace "gc-3346"
    Sep  7 05:58:24.716: INFO: Deleting pod "simpletest.rc-58zts" in namespace "gc-3346"
    Sep  7 05:58:24.722: INFO: Deleting pod "simpletest.rc-5ggwg" in namespace "gc-3346"
    Sep  7 05:58:24.728: INFO: Deleting pod "simpletest.rc-5mbkd" in namespace "gc-3346"
    Sep  7 05:58:24.734: INFO: Deleting pod "simpletest.rc-5rkzn" in namespace "gc-3346"
    Sep  7 05:58:24.740: INFO: Deleting pod "simpletest.rc-5tkv9" in namespace "gc-3346"
    Sep  7 05:58:24.746: INFO: Deleting pod "simpletest.rc-6cdk6" in namespace "gc-3346"
    Sep  7 05:58:24.753: INFO: Deleting pod "simpletest.rc-6ntxx" in namespace "gc-3346"
    Sep  7 05:58:24.759: INFO: Deleting pod "simpletest.rc-6tmsx" in namespace "gc-3346"
    Sep  7 05:58:24.766: INFO: Deleting pod "simpletest.rc-6zkvk" in namespace "gc-3346"
    Sep  7 05:58:24.774: INFO: Deleting pod "simpletest.rc-7klfd" in namespace "gc-3346"
    Sep  7 05:58:24.783: INFO: Deleting pod "simpletest.rc-7nksg" in namespace "gc-3346"
    Sep  7 05:58:24.791: INFO: Deleting pod "simpletest.rc-7tfvl" in namespace "gc-3346"
    Sep  7 05:58:24.798: INFO: Deleting pod "simpletest.rc-7w5zq" in namespace "gc-3346"
    Sep  7 05:58:24.805: INFO: Deleting pod "simpletest.rc-7xv6j" in namespace "gc-3346"
    Sep  7 05:58:24.815: INFO: Deleting pod "simpletest.rc-8l4mt" in namespace "gc-3346"
    Sep  7 05:58:24.825: INFO: Deleting pod "simpletest.rc-8pt7g" in namespace "gc-3346"
    Sep  7 05:58:24.835: INFO: Deleting pod "simpletest.rc-8r7qb" in namespace "gc-3346"
    Sep  7 05:58:24.842: INFO: Deleting pod "simpletest.rc-928zp" in namespace "gc-3346"
    Sep  7 05:58:24.848: INFO: Deleting pod "simpletest.rc-95q7k" in namespace "gc-3346"
    Sep  7 05:58:24.857: INFO: Deleting pod "simpletest.rc-98h7f" in namespace "gc-3346"
    Sep  7 05:58:24.863: INFO: Deleting pod "simpletest.rc-98jxt" in namespace "gc-3346"
    Sep  7 05:58:24.871: INFO: Deleting pod "simpletest.rc-99ps4" in namespace "gc-3346"
    Sep  7 05:58:24.877: INFO: Deleting pod "simpletest.rc-9rkrx" in namespace "gc-3346"
    Sep  7 05:58:24.885: INFO: Deleting pod "simpletest.rc-b7k99" in namespace "gc-3346"
    Sep  7 05:58:24.892: INFO: Deleting pod "simpletest.rc-bdpr6" in namespace "gc-3346"
    Sep  7 05:58:24.901: INFO: Deleting pod "simpletest.rc-blhj7" in namespace "gc-3346"
    Sep  7 05:58:24.912: INFO: Deleting pod "simpletest.rc-bsgg5" in namespace "gc-3346"
    Sep  7 05:58:24.923: INFO: Deleting pod "simpletest.rc-bx8dp" in namespace "gc-3346"
    Sep  7 05:58:24.931: INFO: Deleting pod "simpletest.rc-cdxlf" in namespace "gc-3346"
    Sep  7 05:58:24.940: INFO: Deleting pod "simpletest.rc-cn26w" in namespace "gc-3346"
    Sep  7 05:58:24.946: INFO: Deleting pod "simpletest.rc-cqm6x" in namespace "gc-3346"
    Sep  7 05:58:24.952: INFO: Deleting pod "simpletest.rc-cvs7f" in namespace "gc-3346"
    Sep  7 05:58:24.960: INFO: Deleting pod "simpletest.rc-dr5zn" in namespace "gc-3346"
    Sep  7 05:58:24.966: INFO: Deleting pod "simpletest.rc-gg52f" in namespace "gc-3346"
    Sep  7 05:58:24.972: INFO: Deleting pod "simpletest.rc-gh7pk" in namespace "gc-3346"
    Sep  7 05:58:24.978: INFO: Deleting pod "simpletest.rc-gkhph" in namespace "gc-3346"
    Sep  7 05:58:24.985: INFO: Deleting pod "simpletest.rc-grmnh" in namespace "gc-3346"
    Sep  7 05:58:24.991: INFO: Deleting pod "simpletest.rc-gzvnm" in namespace "gc-3346"
    Sep  7 05:58:25.001: INFO: Deleting pod "simpletest.rc-ht2sg" in namespace "gc-3346"
    Sep  7 05:58:25.012: INFO: Deleting pod "simpletest.rc-hvnbd" in namespace "gc-3346"
    Sep  7 05:58:25.022: INFO: Deleting pod "simpletest.rc-j98s7" in namespace "gc-3346"
    Sep  7 05:58:25.034: INFO: Deleting pod "simpletest.rc-jh7rb" in namespace "gc-3346"
    Sep  7 05:58:25.043: INFO: Deleting pod "simpletest.rc-jzvjf" in namespace "gc-3346"
    Sep  7 05:58:25.053: INFO: Deleting pod "simpletest.rc-k9wdg" in namespace "gc-3346"
    Sep  7 05:58:25.061: INFO: Deleting pod "simpletest.rc-kl4sd" in namespace "gc-3346"
    Sep  7 05:58:25.068: INFO: Deleting pod "simpletest.rc-kt7vg" in namespace "gc-3346"
    Sep  7 05:58:25.078: INFO: Deleting pod "simpletest.rc-kxksb" in namespace "gc-3346"
    Sep  7 05:58:25.088: INFO: Deleting pod "simpletest.rc-lcwm2" in namespace "gc-3346"
    Sep  7 05:58:25.097: INFO: Deleting pod "simpletest.rc-m42qc" in namespace "gc-3346"
    Sep  7 05:58:25.108: INFO: Deleting pod "simpletest.rc-mbxwd" in namespace "gc-3346"
    Sep  7 05:58:25.159: INFO: Deleting pod "simpletest.rc-mlnpm" in namespace "gc-3346"
    Sep  7 05:58:25.213: INFO: Deleting pod "simpletest.rc-n5g4k" in namespace "gc-3346"
    Sep  7 05:58:25.267: INFO: Deleting pod "simpletest.rc-n9gxm" in namespace "gc-3346"
    Sep  7 05:58:25.313: INFO: Deleting pod "simpletest.rc-njq62" in namespace "gc-3346"
    Sep  7 05:58:25.360: INFO: Deleting pod "simpletest.rc-nlw85" in namespace "gc-3346"
    Sep  7 05:58:25.414: INFO: Deleting pod "simpletest.rc-nswvx" in namespace "gc-3346"
    Sep  7 05:58:25.462: INFO: Deleting pod "simpletest.rc-nvhmt" in namespace "gc-3346"
    Sep  7 05:58:25.509: INFO: Deleting pod "simpletest.rc-p79zh" in namespace "gc-3346"
    Sep  7 05:58:25.565: INFO: Deleting pod "simpletest.rc-pm9br" in namespace "gc-3346"
    Sep  7 05:58:25.611: INFO: Deleting pod "simpletest.rc-pxdmp" in namespace "gc-3346"
    Sep  7 05:58:25.661: INFO: Deleting pod "simpletest.rc-pzxc4" in namespace "gc-3346"
    Sep  7 05:58:25.713: INFO: Deleting pod "simpletest.rc-qsrq5" in namespace "gc-3346"
    Sep  7 05:58:25.761: INFO: Deleting pod "simpletest.rc-qwh6s" in namespace "gc-3346"
    Sep  7 05:58:25.814: INFO: Deleting pod "simpletest.rc-qxstj" in namespace "gc-3346"
    Sep  7 05:58:25.861: INFO: Deleting pod "simpletest.rc-qzwlv" in namespace "gc-3346"
    Sep  7 05:58:25.909: INFO: Deleting pod "simpletest.rc-r94pw" in namespace "gc-3346"
    Sep  7 05:58:25.964: INFO: Deleting pod "simpletest.rc-r956h" in namespace "gc-3346"
    Sep  7 05:58:26.013: INFO: Deleting pod "simpletest.rc-rcpzm" in namespace "gc-3346"
    Sep  7 05:58:26.059: INFO: Deleting pod "simpletest.rc-rct2k" in namespace "gc-3346"
    Sep  7 05:58:26.114: INFO: Deleting pod "simpletest.rc-rdlbc" in namespace "gc-3346"
    Sep  7 05:58:26.158: INFO: Deleting pod "simpletest.rc-rfz24" in namespace "gc-3346"
    Sep  7 05:58:26.212: INFO: Deleting pod "simpletest.rc-rl48v" in namespace "gc-3346"
    Sep  7 05:58:26.260: INFO: Deleting pod "simpletest.rc-rn7qz" in namespace "gc-3346"
    Sep  7 05:58:26.311: INFO: Deleting pod "simpletest.rc-rnnsq" in namespace "gc-3346"
    Sep  7 05:58:26.361: INFO: Deleting pod "simpletest.rc-rxd25" in namespace "gc-3346"
    Sep  7 05:58:26.408: INFO: Deleting pod "simpletest.rc-sh78z" in namespace "gc-3346"
    Sep  7 05:58:26.461: INFO: Deleting pod "simpletest.rc-sj7x5" in namespace "gc-3346"
    Sep  7 05:58:26.511: INFO: Deleting pod "simpletest.rc-t8lc9" in namespace "gc-3346"
    Sep  7 05:58:26.561: INFO: Deleting pod "simpletest.rc-tjz4q" in namespace "gc-3346"
    Sep  7 05:58:26.613: INFO: Deleting pod "simpletest.rc-tmx5s" in namespace "gc-3346"
    Sep  7 05:58:26.659: INFO: Deleting pod "simpletest.rc-tvqrq" in namespace "gc-3346"
    Sep  7 05:58:26.714: INFO: Deleting pod "simpletest.rc-v2gwj" in namespace "gc-3346"
    Sep  7 05:58:26.759: INFO: Deleting pod "simpletest.rc-vd4qg" in namespace "gc-3346"
    Sep  7 05:58:26.811: INFO: Deleting pod "simpletest.rc-vhvzl" in namespace "gc-3346"
    Sep  7 05:58:26.863: INFO: Deleting pod "simpletest.rc-vl5wk" in namespace "gc-3346"
    Sep  7 05:58:26.913: INFO: Deleting pod "simpletest.rc-w6sn2" in namespace "gc-3346"
    Sep  7 05:58:26.961: INFO: Deleting pod "simpletest.rc-w9rcq" in namespace "gc-3346"
    Sep  7 05:58:27.009: INFO: Deleting pod "simpletest.rc-wr2ch" in namespace "gc-3346"
    Sep  7 05:58:27.061: INFO: Deleting pod "simpletest.rc-xkdzm" in namespace "gc-3346"
    Sep  7 05:58:27.108: INFO: Deleting pod "simpletest.rc-z5ngw" in namespace "gc-3346"
    Sep  7 05:58:27.159: INFO: Deleting pod "simpletest.rc-z62q7" in namespace "gc-3346"
    Sep  7 05:58:27.209: INFO: Deleting pod "simpletest.rc-zlgcj" in namespace "gc-3346"
    Sep  7 05:58:27.260: INFO: Deleting pod "simpletest.rc-zs5cc" in namespace "gc-3346"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3346" for this suite. 09/07/23 05:58:27.355
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:27.407
Sep  7 05:58:27.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 05:58:27.408
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:27.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:27.419
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/07/23 05:58:27.421
STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:58:27.425
STEP: Creating a ResourceQuota with not best effort scope 09/07/23 05:58:29.428
STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:58:29.431
STEP: Creating a best-effort pod 09/07/23 05:58:31.434
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/07/23 05:58:31.444
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/07/23 05:58:33.446
STEP: Deleting the pod 09/07/23 05:58:35.45
STEP: Ensuring resource quota status released the pod usage 09/07/23 05:58:35.458
STEP: Creating a not best-effort pod 09/07/23 05:58:37.462
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/07/23 05:58:37.468
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/07/23 05:58:39.471
STEP: Deleting the pod 09/07/23 05:58:41.475
STEP: Ensuring resource quota status released the pod usage 09/07/23 05:58:41.481
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:43.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-660" for this suite. 09/07/23 05:58:43.486
------------------------------
â€¢ [SLOW TEST] [16.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:27.407
    Sep  7 05:58:27.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 05:58:27.408
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:27.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:27.419
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/07/23 05:58:27.421
    STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:58:27.425
    STEP: Creating a ResourceQuota with not best effort scope 09/07/23 05:58:29.428
    STEP: Ensuring ResourceQuota status is calculated 09/07/23 05:58:29.431
    STEP: Creating a best-effort pod 09/07/23 05:58:31.434
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/07/23 05:58:31.444
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/07/23 05:58:33.446
    STEP: Deleting the pod 09/07/23 05:58:35.45
    STEP: Ensuring resource quota status released the pod usage 09/07/23 05:58:35.458
    STEP: Creating a not best-effort pod 09/07/23 05:58:37.462
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/07/23 05:58:37.468
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/07/23 05:58:39.471
    STEP: Deleting the pod 09/07/23 05:58:41.475
    STEP: Ensuring resource quota status released the pod usage 09/07/23 05:58:41.481
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:43.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-660" for this suite. 09/07/23 05:58:43.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:43.492
Sep  7 05:58:43.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 05:58:43.493
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:43.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:43.502
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/07/23 05:58:43.504
Sep  7 05:58:43.509: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8952" to be "running and ready"
Sep  7 05:58:43.511: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38124ms
Sep  7 05:58:43.511: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  7 05:58:45.514: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465508s
Sep  7 05:58:45.514: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  7 05:58:45.514: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/07/23 05:58:45.516
STEP: Then the orphan pod is adopted 09/07/23 05:58:45.519
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8952" for this suite. 09/07/23 05:58:46.524
------------------------------
â€¢ [3.036 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:43.492
    Sep  7 05:58:43.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 05:58:43.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:43.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:43.502
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/07/23 05:58:43.504
    Sep  7 05:58:43.509: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8952" to be "running and ready"
    Sep  7 05:58:43.511: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38124ms
    Sep  7 05:58:43.511: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 05:58:45.514: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465508s
    Sep  7 05:58:45.514: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  7 05:58:45.514: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/07/23 05:58:45.516
    STEP: Then the orphan pod is adopted 09/07/23 05:58:45.519
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:46.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8952" for this suite. 09/07/23 05:58:46.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:46.529
Sep  7 05:58:46.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename podtemplate 09/07/23 05:58:46.53
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:46.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:46.541
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/07/23 05:58:46.543
STEP: Replace a pod template 09/07/23 05:58:46.547
Sep  7 05:58:46.551: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:46.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-1987" for this suite. 09/07/23 05:58:46.553
------------------------------
â€¢ [0.029 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:46.529
    Sep  7 05:58:46.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename podtemplate 09/07/23 05:58:46.53
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:46.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:46.541
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/07/23 05:58:46.543
    STEP: Replace a pod template 09/07/23 05:58:46.547
    Sep  7 05:58:46.551: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:46.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-1987" for this suite. 09/07/23 05:58:46.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:46.559
Sep  7 05:58:46.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 05:58:46.56
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:46.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:46.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 05:58:46.578
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:58:46.936
STEP: Deploying the webhook pod 09/07/23 05:58:46.941
STEP: Wait for the deployment to be ready 09/07/23 05:58:46.95
Sep  7 05:58:46.954: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 05:58:48.96
STEP: Verifying the service has paired with the endpoint 09/07/23 05:58:48.968
Sep  7 05:58:49.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/07/23 05:58:49.971
STEP: Creating a custom resource definition that should be denied by the webhook 09/07/23 05:58:49.982
Sep  7 05:58:49.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:49.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2463" for this suite. 09/07/23 05:58:50.015
STEP: Destroying namespace "webhook-2463-markers" for this suite. 09/07/23 05:58:50.018
------------------------------
â€¢ [3.463 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:46.559
    Sep  7 05:58:46.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 05:58:46.56
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:46.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:46.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 05:58:46.578
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 05:58:46.936
    STEP: Deploying the webhook pod 09/07/23 05:58:46.941
    STEP: Wait for the deployment to be ready 09/07/23 05:58:46.95
    Sep  7 05:58:46.954: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 05:58:48.96
    STEP: Verifying the service has paired with the endpoint 09/07/23 05:58:48.968
    Sep  7 05:58:49.969: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/07/23 05:58:49.971
    STEP: Creating a custom resource definition that should be denied by the webhook 09/07/23 05:58:49.982
    Sep  7 05:58:49.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:49.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2463" for this suite. 09/07/23 05:58:50.015
    STEP: Destroying namespace "webhook-2463-markers" for this suite. 09/07/23 05:58:50.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:50.023
Sep  7 05:58:50.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 05:58:50.024
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:50.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:50.034
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/07/23 05:58:50.036
Sep  7 05:58:50.044: INFO: Waiting up to 5m0s for pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b" in namespace "emptydir-2979" to be "Succeeded or Failed"
Sep  7 05:58:50.045: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601899ms
Sep  7 05:58:52.049: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473305s
Sep  7 05:58:54.048: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004277001s
STEP: Saw pod success 09/07/23 05:58:54.048
Sep  7 05:58:54.048: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b" satisfied condition "Succeeded or Failed"
Sep  7 05:58:54.050: INFO: Trying to get logs from node kind-worker pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b container test-container: <nil>
STEP: delete the pod 09/07/23 05:58:54.059
Sep  7 05:58:54.066: INFO: Waiting for pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b to disappear
Sep  7 05:58:54.068: INFO: Pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:54.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2979" for this suite. 09/07/23 05:58:54.069
------------------------------
â€¢ [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:50.023
    Sep  7 05:58:50.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 05:58:50.024
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:50.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:50.034
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/07/23 05:58:50.036
    Sep  7 05:58:50.044: INFO: Waiting up to 5m0s for pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b" in namespace "emptydir-2979" to be "Succeeded or Failed"
    Sep  7 05:58:50.045: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601899ms
    Sep  7 05:58:52.049: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473305s
    Sep  7 05:58:54.048: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004277001s
    STEP: Saw pod success 09/07/23 05:58:54.048
    Sep  7 05:58:54.048: INFO: Pod "pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b" satisfied condition "Succeeded or Failed"
    Sep  7 05:58:54.050: INFO: Trying to get logs from node kind-worker pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b container test-container: <nil>
    STEP: delete the pod 09/07/23 05:58:54.059
    Sep  7 05:58:54.066: INFO: Waiting for pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b to disappear
    Sep  7 05:58:54.068: INFO: Pod pod-2ca71e8e-a445-4e62-9c63-ca32dff9700b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:54.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2979" for this suite. 09/07/23 05:58:54.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:54.073
Sep  7 05:58:54.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename controllerrevisions 09/07/23 05:58:54.073
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:54.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:54.081
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-bzr72-daemon-set" 09/07/23 05:58:54.089
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:58:54.092
Sep  7 05:58:54.093: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:58:54.095: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 0
Sep  7 05:58:54.095: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:58:55.096: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:58:55.099: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 1
Sep  7 05:58:55.099: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 05:58:56.098: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 05:58:56.101: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 2
Sep  7 05:58:56.101: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-bzr72-daemon-set
STEP: Confirm DaemonSet "e2e-bzr72-daemon-set" successfully created with "daemonset-name=e2e-bzr72-daemon-set" label 09/07/23 05:58:56.102
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-bzr72-daemon-set" 09/07/23 05:58:56.106
Sep  7 05:58:56.108: INFO: Located ControllerRevision: "e2e-bzr72-daemon-set-96964cf84"
STEP: Patching ControllerRevision "e2e-bzr72-daemon-set-96964cf84" 09/07/23 05:58:56.109
Sep  7 05:58:56.113: INFO: e2e-bzr72-daemon-set-96964cf84 has been patched
STEP: Create a new ControllerRevision 09/07/23 05:58:56.113
Sep  7 05:58:56.117: INFO: Created ControllerRevision: e2e-bzr72-daemon-set-996d46dfd
STEP: Confirm that there are two ControllerRevisions 09/07/23 05:58:56.117
Sep  7 05:58:56.117: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  7 05:58:56.119: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-bzr72-daemon-set-96964cf84" 09/07/23 05:58:56.119
STEP: Confirm that there is only one ControllerRevision 09/07/23 05:58:56.124
Sep  7 05:58:56.124: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  7 05:58:56.126: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-bzr72-daemon-set-996d46dfd" 09/07/23 05:58:56.127
Sep  7 05:58:56.131: INFO: e2e-bzr72-daemon-set-996d46dfd has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/07/23 05:58:56.131
W0907 05:58:56.137185      29 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/07/23 05:58:56.137
Sep  7 05:58:56.137: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  7 05:58:57.138: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  7 05:58:57.141: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-bzr72-daemon-set-996d46dfd=updated" 09/07/23 05:58:57.141
STEP: Confirm that there is only one ControllerRevision 09/07/23 05:58:57.147
Sep  7 05:58:57.147: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  7 05:58:57.149: INFO: Found 1 ControllerRevisions
Sep  7 05:58:57.150: INFO: ControllerRevision "e2e-bzr72-daemon-set-6969f45679" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-bzr72-daemon-set" 09/07/23 05:58:57.152
STEP: deleting DaemonSet.extensions e2e-bzr72-daemon-set in namespace controllerrevisions-4772, will wait for the garbage collector to delete the pods 09/07/23 05:58:57.152
Sep  7 05:58:57.207: INFO: Deleting DaemonSet.extensions e2e-bzr72-daemon-set took: 3.15724ms
Sep  7 05:58:57.308: INFO: Terminating DaemonSet.extensions e2e-bzr72-daemon-set pods took: 101.058093ms
Sep  7 05:58:59.811: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 0
Sep  7 05:58:59.811: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-bzr72-daemon-set
Sep  7 05:58:59.812: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19343"},"items":null}

Sep  7 05:58:59.813: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19343"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 05:58:59.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4772" for this suite. 09/07/23 05:58:59.82
------------------------------
â€¢ [SLOW TEST] [5.752 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:54.073
    Sep  7 05:58:54.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename controllerrevisions 09/07/23 05:58:54.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:54.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:54.081
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-bzr72-daemon-set" 09/07/23 05:58:54.089
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 05:58:54.092
    Sep  7 05:58:54.093: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:58:54.095: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 0
    Sep  7 05:58:54.095: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:58:55.096: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:58:55.099: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 1
    Sep  7 05:58:55.099: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 05:58:56.098: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 05:58:56.101: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 2
    Sep  7 05:58:56.101: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-bzr72-daemon-set
    STEP: Confirm DaemonSet "e2e-bzr72-daemon-set" successfully created with "daemonset-name=e2e-bzr72-daemon-set" label 09/07/23 05:58:56.102
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-bzr72-daemon-set" 09/07/23 05:58:56.106
    Sep  7 05:58:56.108: INFO: Located ControllerRevision: "e2e-bzr72-daemon-set-96964cf84"
    STEP: Patching ControllerRevision "e2e-bzr72-daemon-set-96964cf84" 09/07/23 05:58:56.109
    Sep  7 05:58:56.113: INFO: e2e-bzr72-daemon-set-96964cf84 has been patched
    STEP: Create a new ControllerRevision 09/07/23 05:58:56.113
    Sep  7 05:58:56.117: INFO: Created ControllerRevision: e2e-bzr72-daemon-set-996d46dfd
    STEP: Confirm that there are two ControllerRevisions 09/07/23 05:58:56.117
    Sep  7 05:58:56.117: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  7 05:58:56.119: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-bzr72-daemon-set-96964cf84" 09/07/23 05:58:56.119
    STEP: Confirm that there is only one ControllerRevision 09/07/23 05:58:56.124
    Sep  7 05:58:56.124: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  7 05:58:56.126: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-bzr72-daemon-set-996d46dfd" 09/07/23 05:58:56.127
    Sep  7 05:58:56.131: INFO: e2e-bzr72-daemon-set-996d46dfd has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/07/23 05:58:56.131
    W0907 05:58:56.137185      29 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/07/23 05:58:56.137
    Sep  7 05:58:56.137: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  7 05:58:57.138: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  7 05:58:57.141: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-bzr72-daemon-set-996d46dfd=updated" 09/07/23 05:58:57.141
    STEP: Confirm that there is only one ControllerRevision 09/07/23 05:58:57.147
    Sep  7 05:58:57.147: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  7 05:58:57.149: INFO: Found 1 ControllerRevisions
    Sep  7 05:58:57.150: INFO: ControllerRevision "e2e-bzr72-daemon-set-6969f45679" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-bzr72-daemon-set" 09/07/23 05:58:57.152
    STEP: deleting DaemonSet.extensions e2e-bzr72-daemon-set in namespace controllerrevisions-4772, will wait for the garbage collector to delete the pods 09/07/23 05:58:57.152
    Sep  7 05:58:57.207: INFO: Deleting DaemonSet.extensions e2e-bzr72-daemon-set took: 3.15724ms
    Sep  7 05:58:57.308: INFO: Terminating DaemonSet.extensions e2e-bzr72-daemon-set pods took: 101.058093ms
    Sep  7 05:58:59.811: INFO: Number of nodes with available pods controlled by daemonset e2e-bzr72-daemon-set: 0
    Sep  7 05:58:59.811: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-bzr72-daemon-set
    Sep  7 05:58:59.812: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19343"},"items":null}

    Sep  7 05:58:59.813: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19343"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 05:58:59.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4772" for this suite. 09/07/23 05:58:59.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 05:58:59.825
Sep  7 05:58:59.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename taint-multiple-pods 09/07/23 05:58:59.826
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:59.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:59.836
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  7 05:58:59.839: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 05:59:59.852: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  7 05:59:59.854: INFO: Starting informer...
STEP: Starting pods... 09/07/23 05:59:59.854
Sep  7 06:00:00.065: INFO: Pod1 is running on kind-worker2. Tainting Node
Sep  7 06:00:00.271: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2775" to be "running"
Sep  7 06:00:00.273: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04496ms
Sep  7 06:00:02.276: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005197118s
Sep  7 06:00:02.276: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  7 06:00:02.276: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2775" to be "running"
Sep  7 06:00:02.278: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.83993ms
Sep  7 06:00:02.278: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  7 06:00:02.278: INFO: Pod2 is running on kind-worker2. Tainting Node
STEP: Trying to apply a taint on the Node 09/07/23 06:00:02.278
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 06:00:02.286
STEP: Waiting for Pod1 and Pod2 to be deleted 09/07/23 06:00:02.288
Sep  7 06:00:08.962: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  7 06:00:30.000: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 06:00:30.009
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:00:30.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-2775" for this suite. 09/07/23 06:00:30.014
------------------------------
â€¢ [SLOW TEST] [90.193 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 05:58:59.825
    Sep  7 05:58:59.825: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename taint-multiple-pods 09/07/23 05:58:59.826
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 05:58:59.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 05:58:59.836
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  7 05:58:59.839: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 05:59:59.852: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  7 05:59:59.854: INFO: Starting informer...
    STEP: Starting pods... 09/07/23 05:59:59.854
    Sep  7 06:00:00.065: INFO: Pod1 is running on kind-worker2. Tainting Node
    Sep  7 06:00:00.271: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2775" to be "running"
    Sep  7 06:00:00.273: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04496ms
    Sep  7 06:00:02.276: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005197118s
    Sep  7 06:00:02.276: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  7 06:00:02.276: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2775" to be "running"
    Sep  7 06:00:02.278: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.83993ms
    Sep  7 06:00:02.278: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  7 06:00:02.278: INFO: Pod2 is running on kind-worker2. Tainting Node
    STEP: Trying to apply a taint on the Node 09/07/23 06:00:02.278
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 06:00:02.286
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/07/23 06:00:02.288
    Sep  7 06:00:08.962: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  7 06:00:30.000: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/07/23 06:00:30.009
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:00:30.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-2775" for this suite. 09/07/23 06:00:30.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:00:30.019
Sep  7 06:00:30.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:00:30.02
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:00:30.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:00:30.031
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/07/23 06:00:30.033
Sep  7 06:00:30.038: INFO: created test-pod-1
Sep  7 06:00:30.043: INFO: created test-pod-2
Sep  7 06:00:30.049: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/07/23 06:00:30.049
Sep  7 06:00:30.049: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3469' to be running and ready
Sep  7 06:00:30.057: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  7 06:00:30.057: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  7 06:00:30.057: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  7 06:00:30.057: INFO: 0 / 3 pods in namespace 'pods-3469' are running and ready (0 seconds elapsed)
Sep  7 06:00:30.057: INFO: expected 0 pod replicas in namespace 'pods-3469', 0 are Running and Ready.
Sep  7 06:00:30.057: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Sep  7 06:00:30.057: INFO: test-pod-1  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
Sep  7 06:00:30.057: INFO: test-pod-2  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
Sep  7 06:00:30.057: INFO: test-pod-3  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
Sep  7 06:00:30.057: INFO: 
Sep  7 06:00:32.063: INFO: 3 / 3 pods in namespace 'pods-3469' are running and ready (2 seconds elapsed)
Sep  7 06:00:32.064: INFO: expected 0 pod replicas in namespace 'pods-3469', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/07/23 06:00:32.075
Sep  7 06:00:32.077: INFO: Pod quantity 3 is different from expected quantity 0
Sep  7 06:00:33.079: INFO: Pod quantity 3 is different from expected quantity 0
Sep  7 06:00:34.081: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:00:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3469" for this suite. 09/07/23 06:00:35.081
------------------------------
â€¢ [SLOW TEST] [5.065 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:00:30.019
    Sep  7 06:00:30.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:00:30.02
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:00:30.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:00:30.031
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/07/23 06:00:30.033
    Sep  7 06:00:30.038: INFO: created test-pod-1
    Sep  7 06:00:30.043: INFO: created test-pod-2
    Sep  7 06:00:30.049: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/07/23 06:00:30.049
    Sep  7 06:00:30.049: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3469' to be running and ready
    Sep  7 06:00:30.057: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  7 06:00:30.057: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  7 06:00:30.057: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  7 06:00:30.057: INFO: 0 / 3 pods in namespace 'pods-3469' are running and ready (0 seconds elapsed)
    Sep  7 06:00:30.057: INFO: expected 0 pod replicas in namespace 'pods-3469', 0 are Running and Ready.
    Sep  7 06:00:30.057: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Sep  7 06:00:30.057: INFO: test-pod-1  kind-worker2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
    Sep  7 06:00:30.057: INFO: test-pod-2  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
    Sep  7 06:00:30.057: INFO: test-pod-3  kind-worker2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:00:30 +0000 UTC  }]
    Sep  7 06:00:30.057: INFO: 
    Sep  7 06:00:32.063: INFO: 3 / 3 pods in namespace 'pods-3469' are running and ready (2 seconds elapsed)
    Sep  7 06:00:32.064: INFO: expected 0 pod replicas in namespace 'pods-3469', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/07/23 06:00:32.075
    Sep  7 06:00:32.077: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  7 06:00:33.079: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  7 06:00:34.081: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:00:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3469" for this suite. 09/07/23 06:00:35.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:00:35.085
Sep  7 06:00:35.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 06:00:35.086
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:00:35.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:00:35.096
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-629 09/07/23 06:00:35.097
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/07/23 06:00:35.1
Sep  7 06:00:35.106: INFO: Found 0 stateful pods, waiting for 3
Sep  7 06:00:45.110: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:00:45.110: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:00:45.110: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/07/23 06:00:45.115
Sep  7 06:00:45.130: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/07/23 06:00:45.131
STEP: Not applying an update when the partition is greater than the number of replicas 09/07/23 06:00:55.143
STEP: Performing a canary update 09/07/23 06:00:55.143
Sep  7 06:00:55.159: INFO: Updating stateful set ss2
Sep  7 06:00:55.162: INFO: Waiting for Pod statefulset-629/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/07/23 06:01:05.167
Sep  7 06:01:05.189: INFO: Found 1 stateful pods, waiting for 3
Sep  7 06:01:15.192: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:01:15.192: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:01:15.192: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/07/23 06:01:15.196
Sep  7 06:01:15.211: INFO: Updating stateful set ss2
Sep  7 06:01:15.215: INFO: Waiting for Pod statefulset-629/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  7 06:01:25.235: INFO: Updating stateful set ss2
Sep  7 06:01:25.239: INFO: Waiting for StatefulSet statefulset-629/ss2 to complete update
Sep  7 06:01:25.239: INFO: Waiting for Pod statefulset-629/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 06:01:35.244: INFO: Deleting all statefulset in ns statefulset-629
Sep  7 06:01:35.246: INFO: Scaling statefulset ss2 to 0
Sep  7 06:01:45.258: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:01:45.259: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:45.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-629" for this suite. 09/07/23 06:01:45.268
------------------------------
â€¢ [SLOW TEST] [70.187 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:00:35.085
    Sep  7 06:00:35.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 06:00:35.086
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:00:35.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:00:35.096
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-629 09/07/23 06:00:35.097
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/07/23 06:00:35.1
    Sep  7 06:00:35.106: INFO: Found 0 stateful pods, waiting for 3
    Sep  7 06:00:45.110: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:00:45.110: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:00:45.110: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/07/23 06:00:45.115
    Sep  7 06:00:45.130: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/07/23 06:00:45.131
    STEP: Not applying an update when the partition is greater than the number of replicas 09/07/23 06:00:55.143
    STEP: Performing a canary update 09/07/23 06:00:55.143
    Sep  7 06:00:55.159: INFO: Updating stateful set ss2
    Sep  7 06:00:55.162: INFO: Waiting for Pod statefulset-629/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/07/23 06:01:05.167
    Sep  7 06:01:05.189: INFO: Found 1 stateful pods, waiting for 3
    Sep  7 06:01:15.192: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:01:15.192: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:01:15.192: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/07/23 06:01:15.196
    Sep  7 06:01:15.211: INFO: Updating stateful set ss2
    Sep  7 06:01:15.215: INFO: Waiting for Pod statefulset-629/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  7 06:01:25.235: INFO: Updating stateful set ss2
    Sep  7 06:01:25.239: INFO: Waiting for StatefulSet statefulset-629/ss2 to complete update
    Sep  7 06:01:25.239: INFO: Waiting for Pod statefulset-629/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 06:01:35.244: INFO: Deleting all statefulset in ns statefulset-629
    Sep  7 06:01:35.246: INFO: Scaling statefulset ss2 to 0
    Sep  7 06:01:45.258: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:01:45.259: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:45.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-629" for this suite. 09/07/23 06:01:45.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:45.272
Sep  7 06:01:45.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 06:01:45.273
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:45.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:45.281
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-ff4762da-070c-437f-8408-a0d2ba0886b5 09/07/23 06:01:45.283
STEP: Creating a pod to test consume secrets 09/07/23 06:01:45.285
Sep  7 06:01:45.291: INFO: Waiting up to 5m0s for pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42" in namespace "secrets-4270" to be "Succeeded or Failed"
Sep  7 06:01:45.292: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23467ms
Sep  7 06:01:47.294: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003700197s
Sep  7 06:01:49.295: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004655975s
STEP: Saw pod success 09/07/23 06:01:49.295
Sep  7 06:01:49.296: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42" satisfied condition "Succeeded or Failed"
Sep  7 06:01:49.298: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 06:01:49.306
Sep  7 06:01:49.313: INFO: Waiting for pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 to disappear
Sep  7 06:01:49.314: INFO: Pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4270" for this suite. 09/07/23 06:01:49.316
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:45.272
    Sep  7 06:01:45.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 06:01:45.273
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:45.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:45.281
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-ff4762da-070c-437f-8408-a0d2ba0886b5 09/07/23 06:01:45.283
    STEP: Creating a pod to test consume secrets 09/07/23 06:01:45.285
    Sep  7 06:01:45.291: INFO: Waiting up to 5m0s for pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42" in namespace "secrets-4270" to be "Succeeded or Failed"
    Sep  7 06:01:45.292: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23467ms
    Sep  7 06:01:47.294: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003700197s
    Sep  7 06:01:49.295: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004655975s
    STEP: Saw pod success 09/07/23 06:01:49.295
    Sep  7 06:01:49.296: INFO: Pod "pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42" satisfied condition "Succeeded or Failed"
    Sep  7 06:01:49.298: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 06:01:49.306
    Sep  7 06:01:49.313: INFO: Waiting for pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 to disappear
    Sep  7 06:01:49.314: INFO: Pod pod-secrets-fc75cdb3-5ac9-4b56-a64a-69de733b6e42 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4270" for this suite. 09/07/23 06:01:49.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:49.322
Sep  7 06:01:49.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename cronjob 09/07/23 06:01:49.323
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:49.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:49.33
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/07/23 06:01:49.332
STEP: creating 09/07/23 06:01:49.332
STEP: getting 09/07/23 06:01:49.335
STEP: listing 09/07/23 06:01:49.336
STEP: watching 09/07/23 06:01:49.337
Sep  7 06:01:49.337: INFO: starting watch
STEP: cluster-wide listing 09/07/23 06:01:49.338
STEP: cluster-wide watching 09/07/23 06:01:49.339
Sep  7 06:01:49.339: INFO: starting watch
STEP: patching 09/07/23 06:01:49.34
STEP: updating 09/07/23 06:01:49.343
Sep  7 06:01:49.349: INFO: waiting for watch events with expected annotations
Sep  7 06:01:49.349: INFO: saw patched and updated annotations
STEP: patching /status 09/07/23 06:01:49.349
STEP: updating /status 09/07/23 06:01:49.353
STEP: get /status 09/07/23 06:01:49.357
STEP: deleting 09/07/23 06:01:49.358
STEP: deleting a collection 09/07/23 06:01:49.366
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:49.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5513" for this suite. 09/07/23 06:01:49.372
------------------------------
â€¢ [0.053 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:49.322
    Sep  7 06:01:49.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename cronjob 09/07/23 06:01:49.323
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:49.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:49.33
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/07/23 06:01:49.332
    STEP: creating 09/07/23 06:01:49.332
    STEP: getting 09/07/23 06:01:49.335
    STEP: listing 09/07/23 06:01:49.336
    STEP: watching 09/07/23 06:01:49.337
    Sep  7 06:01:49.337: INFO: starting watch
    STEP: cluster-wide listing 09/07/23 06:01:49.338
    STEP: cluster-wide watching 09/07/23 06:01:49.339
    Sep  7 06:01:49.339: INFO: starting watch
    STEP: patching 09/07/23 06:01:49.34
    STEP: updating 09/07/23 06:01:49.343
    Sep  7 06:01:49.349: INFO: waiting for watch events with expected annotations
    Sep  7 06:01:49.349: INFO: saw patched and updated annotations
    STEP: patching /status 09/07/23 06:01:49.349
    STEP: updating /status 09/07/23 06:01:49.353
    STEP: get /status 09/07/23 06:01:49.357
    STEP: deleting 09/07/23 06:01:49.358
    STEP: deleting a collection 09/07/23 06:01:49.366
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:49.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5513" for this suite. 09/07/23 06:01:49.372
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:49.376
Sep  7 06:01:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 06:01:49.376
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:49.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:49.384
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0 09/07/23 06:01:49.386
Sep  7 06:01:49.390: INFO: Pod name my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Found 0 pods out of 1
Sep  7 06:01:54.393: INFO: Pod name my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Found 1 pods out of 1
Sep  7 06:01:54.393: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0" are running
Sep  7 06:01:54.393: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" in namespace "replication-controller-3123" to be "running"
Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv": Phase="Running", Reason="", readiness=true. Elapsed: 1.40999ms
Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" satisfied condition "running"
Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:49 +0000 UTC Reason: Message:}])
Sep  7 06:01:54.394: INFO: Trying to dial the pod
Sep  7 06:01:59.401: INFO: Controller my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Got expected result from replica 1 [my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv]: "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3123" for this suite. 09/07/23 06:01:59.403
------------------------------
â€¢ [SLOW TEST] [10.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:49.376
    Sep  7 06:01:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 06:01:49.376
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:49.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:49.384
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0 09/07/23 06:01:49.386
    Sep  7 06:01:49.390: INFO: Pod name my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Found 0 pods out of 1
    Sep  7 06:01:54.393: INFO: Pod name my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Found 1 pods out of 1
    Sep  7 06:01:54.393: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0" are running
    Sep  7 06:01:54.393: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" in namespace "replication-controller-3123" to be "running"
    Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv": Phase="Running", Reason="", readiness=true. Elapsed: 1.40999ms
    Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" satisfied condition "running"
    Sep  7 06:01:54.394: INFO: Pod "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-07 06:01:49 +0000 UTC Reason: Message:}])
    Sep  7 06:01:54.394: INFO: Trying to dial the pod
    Sep  7 06:01:59.401: INFO: Controller my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0: Got expected result from replica 1 [my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv]: "my-hostname-basic-d4aa8ad1-6e37-479c-9999-453000d180e0-nb2nv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3123" for this suite. 09/07/23 06:01:59.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:59.407
Sep  7 06:01:59.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:01:59.407
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.416
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/07/23 06:01:59.418
Sep  7 06:01:59.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-400 api-versions'
Sep  7 06:01:59.479: INFO: stderr: ""
Sep  7 06:01:59.479: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:59.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-400" for this suite. 09/07/23 06:01:59.481
------------------------------
â€¢ [0.078 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:59.407
    Sep  7 06:01:59.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:01:59.407
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.416
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/07/23 06:01:59.418
    Sep  7 06:01:59.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-400 api-versions'
    Sep  7 06:01:59.479: INFO: stderr: ""
    Sep  7 06:01:59.479: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:59.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-400" for this suite. 09/07/23 06:01:59.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:59.486
Sep  7 06:01:59.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename watch 09/07/23 06:01:59.486
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.495
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/07/23 06:01:59.496
STEP: modifying the configmap once 09/07/23 06:01:59.501
STEP: modifying the configmap a second time 09/07/23 06:01:59.505
STEP: deleting the configmap 09/07/23 06:01:59.509
STEP: creating a watch on configmaps from the resource version returned by the first update 09/07/23 06:01:59.512
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/07/23 06:01:59.513
Sep  7 06:01:59.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1264  8285472f-8894-4e89-a3ba-3c90f8e394f0 20134 0 2023-09-07 06:01:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-07 06:01:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:01:59.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1264  8285472f-8894-4e89-a3ba-3c90f8e394f0 20135 0 2023-09-07 06:01:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-07 06:01:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  7 06:01:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1264" for this suite. 09/07/23 06:01:59.515
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:59.486
    Sep  7 06:01:59.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename watch 09/07/23 06:01:59.486
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.495
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/07/23 06:01:59.496
    STEP: modifying the configmap once 09/07/23 06:01:59.501
    STEP: modifying the configmap a second time 09/07/23 06:01:59.505
    STEP: deleting the configmap 09/07/23 06:01:59.509
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/07/23 06:01:59.512
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/07/23 06:01:59.513
    Sep  7 06:01:59.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1264  8285472f-8894-4e89-a3ba-3c90f8e394f0 20134 0 2023-09-07 06:01:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-07 06:01:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:01:59.513: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1264  8285472f-8894-4e89-a3ba-3c90f8e394f0 20135 0 2023-09-07 06:01:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-07 06:01:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:01:59.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1264" for this suite. 09/07/23 06:01:59.515
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:01:59.518
Sep  7 06:01:59.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:01:59.519
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.526
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-3ca62711-42b4-4d43-ab84-9651913431ec 09/07/23 06:01:59.528
STEP: Creating a pod to test consume configMaps 09/07/23 06:01:59.531
Sep  7 06:01:59.537: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d" in namespace "projected-578" to be "Succeeded or Failed"
Sep  7 06:01:59.538: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310819ms
Sep  7 06:02:01.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003976868s
Sep  7 06:02:03.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003831658s
STEP: Saw pod success 09/07/23 06:02:03.541
Sep  7 06:02:03.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d" satisfied condition "Succeeded or Failed"
Sep  7 06:02:03.542: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d container agnhost-container: <nil>
STEP: delete the pod 09/07/23 06:02:03.546
Sep  7 06:02:03.553: INFO: Waiting for pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d to disappear
Sep  7 06:02:03.554: INFO: Pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-578" for this suite. 09/07/23 06:02:03.556
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:01:59.518
    Sep  7 06:01:59.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:01:59.519
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:01:59.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:01:59.526
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-3ca62711-42b4-4d43-ab84-9651913431ec 09/07/23 06:01:59.528
    STEP: Creating a pod to test consume configMaps 09/07/23 06:01:59.531
    Sep  7 06:01:59.537: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d" in namespace "projected-578" to be "Succeeded or Failed"
    Sep  7 06:01:59.538: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.310819ms
    Sep  7 06:02:01.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003976868s
    Sep  7 06:02:03.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003831658s
    STEP: Saw pod success 09/07/23 06:02:03.541
    Sep  7 06:02:03.541: INFO: Pod "pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d" satisfied condition "Succeeded or Failed"
    Sep  7 06:02:03.542: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 06:02:03.546
    Sep  7 06:02:03.553: INFO: Waiting for pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d to disappear
    Sep  7 06:02:03.554: INFO: Pod pod-projected-configmaps-1fe97c28-cf55-49e1-b825-613ae93f505d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-578" for this suite. 09/07/23 06:02:03.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:03.561
Sep  7 06:02:03.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:02:03.561
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:03.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:03.57
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2914 09/07/23 06:02:03.572
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/07/23 06:02:03.579
STEP: creating service externalsvc in namespace services-2914 09/07/23 06:02:03.58
STEP: creating replication controller externalsvc in namespace services-2914 09/07/23 06:02:03.587
I0907 06:02:03.592223      29 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2914, replica count: 2
I0907 06:02:06.644175      29 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/07/23 06:02:06.646
Sep  7 06:02:06.657: INFO: Creating new exec pod
Sep  7 06:02:06.664: INFO: Waiting up to 5m0s for pod "execpod5tpvn" in namespace "services-2914" to be "running"
Sep  7 06:02:06.665: INFO: Pod "execpod5tpvn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80333ms
Sep  7 06:02:08.668: INFO: Pod "execpod5tpvn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004356919s
Sep  7 06:02:08.668: INFO: Pod "execpod5tpvn" satisfied condition "running"
Sep  7 06:02:08.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-2914 exec execpod5tpvn -- /bin/sh -x -c nslookup clusterip-service.services-2914.svc.cluster.local'
Sep  7 06:02:09.091: INFO: stderr: "+ nslookup clusterip-service.services-2914.svc.cluster.local\n"
Sep  7 06:02:09.091: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2914.svc.cluster.local\tcanonical name = externalsvc.services-2914.svc.cluster.local.\nName:\texternalsvc.services-2914.svc.cluster.local\nAddress: 10.96.35.148\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2914, will wait for the garbage collector to delete the pods 09/07/23 06:02:09.091
Sep  7 06:02:09.147: INFO: Deleting ReplicationController externalsvc took: 3.758449ms
Sep  7 06:02:09.248: INFO: Terminating ReplicationController externalsvc pods took: 101.018534ms
Sep  7 06:02:12.260: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:12.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2914" for this suite. 09/07/23 06:02:12.267
------------------------------
â€¢ [SLOW TEST] [8.710 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:03.561
    Sep  7 06:02:03.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:02:03.561
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:03.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:03.57
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2914 09/07/23 06:02:03.572
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/07/23 06:02:03.579
    STEP: creating service externalsvc in namespace services-2914 09/07/23 06:02:03.58
    STEP: creating replication controller externalsvc in namespace services-2914 09/07/23 06:02:03.587
    I0907 06:02:03.592223      29 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2914, replica count: 2
    I0907 06:02:06.644175      29 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/07/23 06:02:06.646
    Sep  7 06:02:06.657: INFO: Creating new exec pod
    Sep  7 06:02:06.664: INFO: Waiting up to 5m0s for pod "execpod5tpvn" in namespace "services-2914" to be "running"
    Sep  7 06:02:06.665: INFO: Pod "execpod5tpvn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.80333ms
    Sep  7 06:02:08.668: INFO: Pod "execpod5tpvn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004356919s
    Sep  7 06:02:08.668: INFO: Pod "execpod5tpvn" satisfied condition "running"
    Sep  7 06:02:08.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-2914 exec execpod5tpvn -- /bin/sh -x -c nslookup clusterip-service.services-2914.svc.cluster.local'
    Sep  7 06:02:09.091: INFO: stderr: "+ nslookup clusterip-service.services-2914.svc.cluster.local\n"
    Sep  7 06:02:09.091: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2914.svc.cluster.local\tcanonical name = externalsvc.services-2914.svc.cluster.local.\nName:\texternalsvc.services-2914.svc.cluster.local\nAddress: 10.96.35.148\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2914, will wait for the garbage collector to delete the pods 09/07/23 06:02:09.091
    Sep  7 06:02:09.147: INFO: Deleting ReplicationController externalsvc took: 3.758449ms
    Sep  7 06:02:09.248: INFO: Terminating ReplicationController externalsvc pods took: 101.018534ms
    Sep  7 06:02:12.260: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:12.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2914" for this suite. 09/07/23 06:02:12.267
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:12.271
Sep  7 06:02:12.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:12.271
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:12.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:12.282
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/07/23 06:02:12.284
Sep  7 06:02:12.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 create -f -'
Sep  7 06:02:12.634: INFO: stderr: ""
Sep  7 06:02:12.634: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/07/23 06:02:12.634
Sep  7 06:02:12.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 diff -f -'
Sep  7 06:02:12.757: INFO: rc: 1
Sep  7 06:02:12.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 delete -f -'
Sep  7 06:02:12.814: INFO: stderr: ""
Sep  7 06:02:12.814: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:12.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3642" for this suite. 09/07/23 06:02:12.817
------------------------------
â€¢ [0.550 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:12.271
    Sep  7 06:02:12.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:12.271
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:12.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:12.282
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/07/23 06:02:12.284
    Sep  7 06:02:12.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 create -f -'
    Sep  7 06:02:12.634: INFO: stderr: ""
    Sep  7 06:02:12.634: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/07/23 06:02:12.634
    Sep  7 06:02:12.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 diff -f -'
    Sep  7 06:02:12.757: INFO: rc: 1
    Sep  7 06:02:12.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3642 delete -f -'
    Sep  7 06:02:12.814: INFO: stderr: ""
    Sep  7 06:02:12.814: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:12.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3642" for this suite. 09/07/23 06:02:12.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:12.822
Sep  7 06:02:12.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:02:12.822
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:12.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:12.832
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:02:12.833
Sep  7 06:02:12.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45" in namespace "projected-6355" to be "Succeeded or Failed"
Sep  7 06:02:12.841: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47408ms
Sep  7 06:02:14.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00484757s
Sep  7 06:02:16.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00484624s
STEP: Saw pod success 09/07/23 06:02:16.844
Sep  7 06:02:16.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45" satisfied condition "Succeeded or Failed"
Sep  7 06:02:16.846: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 container client-container: <nil>
STEP: delete the pod 09/07/23 06:02:16.849
Sep  7 06:02:16.858: INFO: Waiting for pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 to disappear
Sep  7 06:02:16.859: INFO: Pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6355" for this suite. 09/07/23 06:02:16.861
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:12.822
    Sep  7 06:02:12.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:02:12.822
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:12.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:12.832
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:02:12.833
    Sep  7 06:02:12.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45" in namespace "projected-6355" to be "Succeeded or Failed"
    Sep  7 06:02:12.841: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47408ms
    Sep  7 06:02:14.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00484757s
    Sep  7 06:02:16.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00484624s
    STEP: Saw pod success 09/07/23 06:02:16.844
    Sep  7 06:02:16.844: INFO: Pod "downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45" satisfied condition "Succeeded or Failed"
    Sep  7 06:02:16.846: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:02:16.849
    Sep  7 06:02:16.858: INFO: Waiting for pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 to disappear
    Sep  7 06:02:16.859: INFO: Pod downwardapi-volume-db826b52-8d53-45d5-bc59-37b7a4511b45 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6355" for this suite. 09/07/23 06:02:16.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:16.866
Sep  7 06:02:16.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:02:16.866
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:16.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:16.874
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7088 09/07/23 06:02:16.876
STEP: creating replication controller nodeport-test in namespace services-7088 09/07/23 06:02:16.884
I0907 06:02:16.890318      29 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7088, replica count: 2
I0907 06:02:19.941217      29 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 06:02:19.941: INFO: Creating new exec pod
Sep  7 06:02:19.947: INFO: Waiting up to 5m0s for pod "execpodh6rxx" in namespace "services-7088" to be "running"
Sep  7 06:02:19.948: INFO: Pod "execpodh6rxx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52174ms
Sep  7 06:02:21.950: INFO: Pod "execpodh6rxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.0032836s
Sep  7 06:02:21.950: INFO: Pod "execpodh6rxx" satisfied condition "running"
Sep  7 06:02:22.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  7 06:02:23.108: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  7 06:02:23.108: INFO: stdout: ""
Sep  7 06:02:23.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 10.96.93.76 80'
Sep  7 06:02:23.271: INFO: stderr: "+ nc -v -z -w 2 10.96.93.76 80\nConnection to 10.96.93.76 80 port [tcp/http] succeeded!\n"
Sep  7 06:02:23.271: INFO: stdout: ""
Sep  7 06:02:23.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31343'
Sep  7 06:02:23.426: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31343\nConnection to 192.168.8.3 31343 port [tcp/*] succeeded!\n"
Sep  7 06:02:23.426: INFO: stdout: ""
Sep  7 06:02:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31343'
Sep  7 06:02:23.569: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31343\nConnection to 192.168.8.6 31343 port [tcp/*] succeeded!\n"
Sep  7 06:02:23.569: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:23.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7088" for this suite. 09/07/23 06:02:23.572
------------------------------
â€¢ [SLOW TEST] [6.710 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:16.866
    Sep  7 06:02:16.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:02:16.866
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:16.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:16.874
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7088 09/07/23 06:02:16.876
    STEP: creating replication controller nodeport-test in namespace services-7088 09/07/23 06:02:16.884
    I0907 06:02:16.890318      29 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7088, replica count: 2
    I0907 06:02:19.941217      29 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 06:02:19.941: INFO: Creating new exec pod
    Sep  7 06:02:19.947: INFO: Waiting up to 5m0s for pod "execpodh6rxx" in namespace "services-7088" to be "running"
    Sep  7 06:02:19.948: INFO: Pod "execpodh6rxx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52174ms
    Sep  7 06:02:21.950: INFO: Pod "execpodh6rxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.0032836s
    Sep  7 06:02:21.950: INFO: Pod "execpodh6rxx" satisfied condition "running"
    Sep  7 06:02:22.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  7 06:02:23.108: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  7 06:02:23.108: INFO: stdout: ""
    Sep  7 06:02:23.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 10.96.93.76 80'
    Sep  7 06:02:23.271: INFO: stderr: "+ nc -v -z -w 2 10.96.93.76 80\nConnection to 10.96.93.76 80 port [tcp/http] succeeded!\n"
    Sep  7 06:02:23.271: INFO: stdout: ""
    Sep  7 06:02:23.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31343'
    Sep  7 06:02:23.426: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31343\nConnection to 192.168.8.3 31343 port [tcp/*] succeeded!\n"
    Sep  7 06:02:23.426: INFO: stdout: ""
    Sep  7 06:02:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-7088 exec execpodh6rxx -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31343'
    Sep  7 06:02:23.569: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31343\nConnection to 192.168.8.6 31343 port [tcp/*] succeeded!\n"
    Sep  7 06:02:23.569: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:23.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7088" for this suite. 09/07/23 06:02:23.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:23.576
Sep  7 06:02:23.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 06:02:23.577
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:23.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:23.588
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-6994482a-23e8-4c4d-a11b-aa36244d4ba0 09/07/23 06:02:23.59
STEP: Creating a pod to test consume secrets 09/07/23 06:02:23.593
Sep  7 06:02:23.599: INFO: Waiting up to 5m0s for pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06" in namespace "secrets-1895" to be "Succeeded or Failed"
Sep  7 06:02:23.600: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Pending", Reason="", readiness=false. Elapsed: 1.382959ms
Sep  7 06:02:25.603: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00434368s
Sep  7 06:02:27.604: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004501691s
STEP: Saw pod success 09/07/23 06:02:27.604
Sep  7 06:02:27.604: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06" satisfied condition "Succeeded or Failed"
Sep  7 06:02:27.606: INFO: Trying to get logs from node kind-worker pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 container secret-volume-test: <nil>
STEP: delete the pod 09/07/23 06:02:27.614
Sep  7 06:02:27.621: INFO: Waiting for pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 to disappear
Sep  7 06:02:27.622: INFO: Pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1895" for this suite. 09/07/23 06:02:27.624
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:23.576
    Sep  7 06:02:23.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 06:02:23.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:23.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:23.588
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-6994482a-23e8-4c4d-a11b-aa36244d4ba0 09/07/23 06:02:23.59
    STEP: Creating a pod to test consume secrets 09/07/23 06:02:23.593
    Sep  7 06:02:23.599: INFO: Waiting up to 5m0s for pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06" in namespace "secrets-1895" to be "Succeeded or Failed"
    Sep  7 06:02:23.600: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Pending", Reason="", readiness=false. Elapsed: 1.382959ms
    Sep  7 06:02:25.603: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00434368s
    Sep  7 06:02:27.604: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004501691s
    STEP: Saw pod success 09/07/23 06:02:27.604
    Sep  7 06:02:27.604: INFO: Pod "pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06" satisfied condition "Succeeded or Failed"
    Sep  7 06:02:27.606: INFO: Trying to get logs from node kind-worker pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 container secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 06:02:27.614
    Sep  7 06:02:27.621: INFO: Waiting for pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 to disappear
    Sep  7 06:02:27.622: INFO: Pod pod-secrets-4d9207b7-b3a5-43f8-b3d0-6059a8fe0a06 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:27.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1895" for this suite. 09/07/23 06:02:27.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:27.628
Sep  7 06:02:27.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:02:27.628
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:27.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:27.636
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:02:27.638
Sep  7 06:02:27.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69" in namespace "downward-api-4445" to be "Succeeded or Failed"
Sep  7 06:02:27.645: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35218ms
Sep  7 06:02:29.648: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004214051s
Sep  7 06:02:31.649: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005135153s
STEP: Saw pod success 09/07/23 06:02:31.649
Sep  7 06:02:31.649: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69" satisfied condition "Succeeded or Failed"
Sep  7 06:02:31.650: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 container client-container: <nil>
STEP: delete the pod 09/07/23 06:02:31.653
Sep  7 06:02:31.660: INFO: Waiting for pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 to disappear
Sep  7 06:02:31.662: INFO: Pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:31.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4445" for this suite. 09/07/23 06:02:31.664
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:27.628
    Sep  7 06:02:27.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:02:27.628
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:27.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:27.636
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:02:27.638
    Sep  7 06:02:27.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69" in namespace "downward-api-4445" to be "Succeeded or Failed"
    Sep  7 06:02:27.645: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 1.35218ms
    Sep  7 06:02:29.648: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004214051s
    Sep  7 06:02:31.649: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005135153s
    STEP: Saw pod success 09/07/23 06:02:31.649
    Sep  7 06:02:31.649: INFO: Pod "downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69" satisfied condition "Succeeded or Failed"
    Sep  7 06:02:31.650: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:02:31.653
    Sep  7 06:02:31.660: INFO: Waiting for pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 to disappear
    Sep  7 06:02:31.662: INFO: Pod downwardapi-volume-524717fe-42b8-445d-83e3-cdbfdfaf8b69 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:31.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4445" for this suite. 09/07/23 06:02:31.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:31.669
Sep  7 06:02:31.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:31.669
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:31.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:31.678
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/07/23 06:02:31.68
Sep  7 06:02:31.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7103 cluster-info'
Sep  7 06:02:31.731: INFO: stderr: ""
Sep  7 06:02:31.731: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:31.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7103" for this suite. 09/07/23 06:02:31.733
------------------------------
â€¢ [0.067 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:31.669
    Sep  7 06:02:31.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:31.669
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:31.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:31.678
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/07/23 06:02:31.68
    Sep  7 06:02:31.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7103 cluster-info'
    Sep  7 06:02:31.731: INFO: stderr: ""
    Sep  7 06:02:31.731: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:31.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7103" for this suite. 09/07/23 06:02:31.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:31.736
Sep  7 06:02:31.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:31.737
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:31.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:31.746
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/07/23 06:02:31.748
Sep  7 06:02:31.748: INFO: namespace kubectl-105
Sep  7 06:02:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 create -f -'
Sep  7 06:02:31.867: INFO: stderr: ""
Sep  7 06:02:31.867: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/07/23 06:02:31.867
Sep  7 06:02:32.869: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 06:02:32.869: INFO: Found 0 / 1
Sep  7 06:02:33.870: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 06:02:33.870: INFO: Found 1 / 1
Sep  7 06:02:33.870: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  7 06:02:33.872: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  7 06:02:33.872: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  7 06:02:33.872: INFO: wait on agnhost-primary startup in kubectl-105 
Sep  7 06:02:33.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 logs agnhost-primary-rnkwb agnhost-primary'
Sep  7 06:02:33.926: INFO: stderr: ""
Sep  7 06:02:33.926: INFO: stdout: "Paused\n"
STEP: exposing RC 09/07/23 06:02:33.926
Sep  7 06:02:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  7 06:02:33.986: INFO: stderr: ""
Sep  7 06:02:33.986: INFO: stdout: "service/rm2 exposed\n"
Sep  7 06:02:33.989: INFO: Service rm2 in namespace kubectl-105 found.
STEP: exposing service 09/07/23 06:02:35.994
Sep  7 06:02:35.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  7 06:02:36.055: INFO: stderr: ""
Sep  7 06:02:36.055: INFO: stdout: "service/rm3 exposed\n"
Sep  7 06:02:36.059: INFO: Service rm3 in namespace kubectl-105 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:38.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-105" for this suite. 09/07/23 06:02:38.066
------------------------------
â€¢ [SLOW TEST] [6.336 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:31.736
    Sep  7 06:02:31.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:31.737
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:31.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:31.746
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/07/23 06:02:31.748
    Sep  7 06:02:31.748: INFO: namespace kubectl-105
    Sep  7 06:02:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 create -f -'
    Sep  7 06:02:31.867: INFO: stderr: ""
    Sep  7 06:02:31.867: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/07/23 06:02:31.867
    Sep  7 06:02:32.869: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 06:02:32.869: INFO: Found 0 / 1
    Sep  7 06:02:33.870: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 06:02:33.870: INFO: Found 1 / 1
    Sep  7 06:02:33.870: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  7 06:02:33.872: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  7 06:02:33.872: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  7 06:02:33.872: INFO: wait on agnhost-primary startup in kubectl-105 
    Sep  7 06:02:33.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 logs agnhost-primary-rnkwb agnhost-primary'
    Sep  7 06:02:33.926: INFO: stderr: ""
    Sep  7 06:02:33.926: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/07/23 06:02:33.926
    Sep  7 06:02:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  7 06:02:33.986: INFO: stderr: ""
    Sep  7 06:02:33.986: INFO: stdout: "service/rm2 exposed\n"
    Sep  7 06:02:33.989: INFO: Service rm2 in namespace kubectl-105 found.
    STEP: exposing service 09/07/23 06:02:35.994
    Sep  7 06:02:35.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-105 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  7 06:02:36.055: INFO: stderr: ""
    Sep  7 06:02:36.055: INFO: stdout: "service/rm3 exposed\n"
    Sep  7 06:02:36.059: INFO: Service rm3 in namespace kubectl-105 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:38.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-105" for this suite. 09/07/23 06:02:38.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:38.073
Sep  7 06:02:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:02:38.073
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:38.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:38.082
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/07/23 06:02:38.084
STEP: Counting existing ResourceQuota 09/07/23 06:02:43.087
STEP: Creating a ResourceQuota 09/07/23 06:02:48.09
STEP: Ensuring resource quota status is calculated 09/07/23 06:02:48.095
STEP: Creating a Secret 09/07/23 06:02:50.098
STEP: Ensuring resource quota status captures secret creation 09/07/23 06:02:50.108
STEP: Deleting a secret 09/07/23 06:02:52.111
STEP: Ensuring resource quota status released usage 09/07/23 06:02:52.114
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:54.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3786" for this suite. 09/07/23 06:02:54.119
------------------------------
â€¢ [SLOW TEST] [16.052 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:38.073
    Sep  7 06:02:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:02:38.073
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:38.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:38.082
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/07/23 06:02:38.084
    STEP: Counting existing ResourceQuota 09/07/23 06:02:43.087
    STEP: Creating a ResourceQuota 09/07/23 06:02:48.09
    STEP: Ensuring resource quota status is calculated 09/07/23 06:02:48.095
    STEP: Creating a Secret 09/07/23 06:02:50.098
    STEP: Ensuring resource quota status captures secret creation 09/07/23 06:02:50.108
    STEP: Deleting a secret 09/07/23 06:02:52.111
    STEP: Ensuring resource quota status released usage 09/07/23 06:02:52.114
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:54.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3786" for this suite. 09/07/23 06:02:54.119
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:54.124
Sep  7 06:02:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:54.125
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:54.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:54.135
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/07/23 06:02:54.137
Sep  7 06:02:54.137: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7705 proxy --unix-socket=/tmp/kubectl-proxy-unix3183556849/test'
STEP: retrieving proxy /api/ output 09/07/23 06:02:54.173
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:54.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7705" for this suite. 09/07/23 06:02:54.175
------------------------------
â€¢ [0.056 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:54.124
    Sep  7 06:02:54.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:02:54.125
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:54.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:54.135
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/07/23 06:02:54.137
    Sep  7 06:02:54.137: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-7705 proxy --unix-socket=/tmp/kubectl-proxy-unix3183556849/test'
    STEP: retrieving proxy /api/ output 09/07/23 06:02:54.173
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:54.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7705" for this suite. 09/07/23 06:02:54.175
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:54.18
Sep  7 06:02:54.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:02:54.181
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:54.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:54.19
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-a07f31be-f048-4e14-a382-0dcd4d4feca5 09/07/23 06:02:54.193
STEP: Creating the pod 09/07/23 06:02:54.195
Sep  7 06:02:54.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9" in namespace "configmap-3701" to be "running"
Sep  7 06:02:54.202: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2273ms
Sep  7 06:02:56.204: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9": Phase="Running", Reason="", readiness=false. Elapsed: 2.003625903s
Sep  7 06:02:56.204: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9" satisfied condition "running"
STEP: Waiting for pod with text data 09/07/23 06:02:56.204
STEP: Waiting for pod with binary data 09/07/23 06:02:56.208
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:56.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3701" for this suite. 09/07/23 06:02:56.213
------------------------------
â€¢ [2.038 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:54.18
    Sep  7 06:02:54.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:02:54.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:54.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:54.19
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-a07f31be-f048-4e14-a382-0dcd4d4feca5 09/07/23 06:02:54.193
    STEP: Creating the pod 09/07/23 06:02:54.195
    Sep  7 06:02:54.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9" in namespace "configmap-3701" to be "running"
    Sep  7 06:02:54.202: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2273ms
    Sep  7 06:02:56.204: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9": Phase="Running", Reason="", readiness=false. Elapsed: 2.003625903s
    Sep  7 06:02:56.204: INFO: Pod "pod-configmaps-eabf5d68-9884-4e09-ab7a-91ea99384da9" satisfied condition "running"
    STEP: Waiting for pod with text data 09/07/23 06:02:56.204
    STEP: Waiting for pod with binary data 09/07/23 06:02:56.208
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:56.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3701" for this suite. 09/07/23 06:02:56.213
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:56.218
Sep  7 06:02:56.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename csistoragecapacity 09/07/23 06:02:56.219
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:56.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:56.229
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/07/23 06:02:56.231
STEP: getting /apis/storage.k8s.io 09/07/23 06:02:56.233
STEP: getting /apis/storage.k8s.io/v1 09/07/23 06:02:56.233
STEP: creating 09/07/23 06:02:56.234
STEP: watching 09/07/23 06:02:56.244
Sep  7 06:02:56.244: INFO: starting watch
STEP: getting 09/07/23 06:02:56.247
STEP: listing in namespace 09/07/23 06:02:56.249
STEP: listing across namespaces 09/07/23 06:02:56.25
STEP: patching 09/07/23 06:02:56.251
STEP: updating 09/07/23 06:02:56.254
Sep  7 06:02:56.257: INFO: waiting for watch events with expected annotations in namespace
Sep  7 06:02:56.257: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/07/23 06:02:56.257
STEP: deleting a collection 09/07/23 06:02:56.263
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-2063" for this suite. 09/07/23 06:02:56.272
------------------------------
â€¢ [0.057 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:56.218
    Sep  7 06:02:56.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename csistoragecapacity 09/07/23 06:02:56.219
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:56.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:56.229
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/07/23 06:02:56.231
    STEP: getting /apis/storage.k8s.io 09/07/23 06:02:56.233
    STEP: getting /apis/storage.k8s.io/v1 09/07/23 06:02:56.233
    STEP: creating 09/07/23 06:02:56.234
    STEP: watching 09/07/23 06:02:56.244
    Sep  7 06:02:56.244: INFO: starting watch
    STEP: getting 09/07/23 06:02:56.247
    STEP: listing in namespace 09/07/23 06:02:56.249
    STEP: listing across namespaces 09/07/23 06:02:56.25
    STEP: patching 09/07/23 06:02:56.251
    STEP: updating 09/07/23 06:02:56.254
    Sep  7 06:02:56.257: INFO: waiting for watch events with expected annotations in namespace
    Sep  7 06:02:56.257: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/07/23 06:02:56.257
    STEP: deleting a collection 09/07/23 06:02:56.263
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-2063" for this suite. 09/07/23 06:02:56.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:56.275
Sep  7 06:02:56.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 06:02:56.276
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:56.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:56.287
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/07/23 06:02:56.289
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/07/23 06:02:56.291
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/07/23 06:02:56.291
STEP: creating a pod to probe DNS 09/07/23 06:02:56.292
STEP: submitting the pod to kubernetes 09/07/23 06:02:56.292
Sep  7 06:02:56.298: INFO: Waiting up to 15m0s for pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df" in namespace "dns-9029" to be "running"
Sep  7 06:02:56.300: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20103ms
Sep  7 06:02:58.303: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df": Phase="Running", Reason="", readiness=true. Elapsed: 2.004969002s
Sep  7 06:02:58.303: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df" satisfied condition "running"
STEP: retrieving the pod 09/07/23 06:02:58.303
STEP: looking for the results for each expected name from probers 09/07/23 06:02:58.305
Sep  7 06:02:58.315: INFO: DNS probes using dns-9029/dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df succeeded

STEP: deleting the pod 09/07/23 06:02:58.315
STEP: deleting the test headless service 09/07/23 06:02:58.325
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 06:02:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9029" for this suite. 09/07/23 06:02:58.334
------------------------------
â€¢ [2.062 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:56.275
    Sep  7 06:02:56.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 06:02:56.276
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:56.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:56.287
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/07/23 06:02:56.289
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/07/23 06:02:56.291
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9029.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/07/23 06:02:56.291
    STEP: creating a pod to probe DNS 09/07/23 06:02:56.292
    STEP: submitting the pod to kubernetes 09/07/23 06:02:56.292
    Sep  7 06:02:56.298: INFO: Waiting up to 15m0s for pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df" in namespace "dns-9029" to be "running"
    Sep  7 06:02:56.300: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.20103ms
    Sep  7 06:02:58.303: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df": Phase="Running", Reason="", readiness=true. Elapsed: 2.004969002s
    Sep  7 06:02:58.303: INFO: Pod "dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 06:02:58.303
    STEP: looking for the results for each expected name from probers 09/07/23 06:02:58.305
    Sep  7 06:02:58.315: INFO: DNS probes using dns-9029/dns-test-373deacd-cd3a-402b-9b9d-224d5f3c22df succeeded

    STEP: deleting the pod 09/07/23 06:02:58.315
    STEP: deleting the test headless service 09/07/23 06:02:58.325
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:02:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9029" for this suite. 09/07/23 06:02:58.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:02:58.34
Sep  7 06:02:58.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 06:02:58.34
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:58.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:58.349
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/07/23 06:02:58.354
Sep  7 06:02:58.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5137" to be "running and ready"
Sep  7 06:02:58.359: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34363ms
Sep  7 06:02:58.359: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:03:00.363: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004468063s
Sep  7 06:03:00.363: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  7 06:03:00.363: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/07/23 06:03:00.365
Sep  7 06:03:00.371: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5137" to be "running and ready"
Sep  7 06:03:00.372: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61619ms
Sep  7 06:03:00.372: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:03:02.375: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004150983s
Sep  7 06:03:02.375: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  7 06:03:02.375: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/07/23 06:03:02.377
Sep  7 06:03:02.383: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  7 06:03:02.384: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  7 06:03:04.385: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  7 06:03:04.387: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  7 06:03:06.385: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  7 06:03:06.388: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/07/23 06:03:06.388
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  7 06:03:06.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5137" for this suite. 09/07/23 06:03:06.394
------------------------------
â€¢ [SLOW TEST] [8.057 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:02:58.34
    Sep  7 06:02:58.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 06:02:58.34
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:02:58.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:02:58.349
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/07/23 06:02:58.354
    Sep  7 06:02:58.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5137" to be "running and ready"
    Sep  7 06:02:58.359: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34363ms
    Sep  7 06:02:58.359: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:03:00.363: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004468063s
    Sep  7 06:03:00.363: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  7 06:03:00.363: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/07/23 06:03:00.365
    Sep  7 06:03:00.371: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5137" to be "running and ready"
    Sep  7 06:03:00.372: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61619ms
    Sep  7 06:03:00.372: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:03:02.375: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004150983s
    Sep  7 06:03:02.375: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  7 06:03:02.375: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/07/23 06:03:02.377
    Sep  7 06:03:02.383: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  7 06:03:02.384: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  7 06:03:04.385: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  7 06:03:04.387: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  7 06:03:06.385: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  7 06:03:06.388: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/07/23 06:03:06.388
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:03:06.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5137" for this suite. 09/07/23 06:03:06.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:03:06.398
Sep  7 06:03:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:03:06.398
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:03:06.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:03:06.407
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/07/23 06:03:06.409
Sep  7 06:03:06.413: INFO: Waiting up to 5m0s for pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0" in namespace "downward-api-4763" to be "Succeeded or Failed"
Sep  7 06:03:06.415: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47012ms
Sep  7 06:03:08.417: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004043813s
Sep  7 06:03:10.418: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005253118s
STEP: Saw pod success 09/07/23 06:03:10.418
Sep  7 06:03:10.419: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0" satisfied condition "Succeeded or Failed"
Sep  7 06:03:10.420: INFO: Trying to get logs from node kind-worker pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:03:10.425
Sep  7 06:03:10.433: INFO: Waiting for pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 to disappear
Sep  7 06:03:10.435: INFO: Pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  7 06:03:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4763" for this suite. 09/07/23 06:03:10.437
------------------------------
â€¢ [4.042 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:03:06.398
    Sep  7 06:03:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:03:06.398
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:03:06.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:03:06.407
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/07/23 06:03:06.409
    Sep  7 06:03:06.413: INFO: Waiting up to 5m0s for pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0" in namespace "downward-api-4763" to be "Succeeded or Failed"
    Sep  7 06:03:06.415: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47012ms
    Sep  7 06:03:08.417: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004043813s
    Sep  7 06:03:10.418: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005253118s
    STEP: Saw pod success 09/07/23 06:03:10.418
    Sep  7 06:03:10.419: INFO: Pod "downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0" satisfied condition "Succeeded or Failed"
    Sep  7 06:03:10.420: INFO: Trying to get logs from node kind-worker pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:03:10.425
    Sep  7 06:03:10.433: INFO: Waiting for pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 to disappear
    Sep  7 06:03:10.435: INFO: Pod downward-api-0bad019d-a0ef-425f-a2cf-f8c7b7b6c1e0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:03:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4763" for this suite. 09/07/23 06:03:10.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:03:10.443
Sep  7 06:03:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:03:10.444
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:03:10.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:03:10.452
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1aace403-1c14-40ec-821d-a737f81fc284 09/07/23 06:03:10.456
STEP: Creating the pod 09/07/23 06:03:10.46
Sep  7 06:03:10.464: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270" in namespace "projected-4661" to be "running and ready"
Sep  7 06:03:10.465: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4128ms
Sep  7 06:03:10.465: INFO: The phase of Pod pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:03:12.470: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828313s
Sep  7 06:03:12.470: INFO: The phase of Pod pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 is Running (Ready = true)
Sep  7 06:03:12.470: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-1aace403-1c14-40ec-821d-a737f81fc284 09/07/23 06:03:12.474
STEP: waiting to observe update in volume 09/07/23 06:03:12.478
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:26.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4661" for this suite. 09/07/23 06:04:26.685
------------------------------
â€¢ [SLOW TEST] [76.247 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:03:10.443
    Sep  7 06:03:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:03:10.444
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:03:10.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:03:10.452
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-1aace403-1c14-40ec-821d-a737f81fc284 09/07/23 06:03:10.456
    STEP: Creating the pod 09/07/23 06:03:10.46
    Sep  7 06:03:10.464: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270" in namespace "projected-4661" to be "running and ready"
    Sep  7 06:03:10.465: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4128ms
    Sep  7 06:03:10.465: INFO: The phase of Pod pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:03:12.470: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270": Phase="Running", Reason="", readiness=true. Elapsed: 2.005828313s
    Sep  7 06:03:12.470: INFO: The phase of Pod pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 is Running (Ready = true)
    Sep  7 06:03:12.470: INFO: Pod "pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-1aace403-1c14-40ec-821d-a737f81fc284 09/07/23 06:03:12.474
    STEP: waiting to observe update in volume 09/07/23 06:03:12.478
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:26.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4661" for this suite. 09/07/23 06:04:26.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:26.69
Sep  7 06:04:26.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-pred 09/07/23 06:04:26.691
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:26.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:26.699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  7 06:04:26.701: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  7 06:04:26.704: INFO: Waiting for terminating namespaces to be deleted...
Sep  7 06:04:26.706: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  7 06:04:26.709: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.709: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 06:04:26.709: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.709: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 06:04:26.709: INFO: pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 from projected-4661 started at 2023-09-07 06:03:10 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.709: INFO: 	Container agnhost-container ready: true, restart count 0
Sep  7 06:04:26.709: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.709: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  7 06:04:26.709: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:26.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:26.709: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  7 06:04:26.709: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  7 06:04:26.712: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.712: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 06:04:26.712: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:26.712: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 06:04:26.712: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:26.712: INFO: 	Container e2e ready: true, restart count 0
Sep  7 06:04:26.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:26.712: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:26.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:26.712: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/07/23 06:04:26.712
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.178288a867ea6558], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/07/23 06:04:26.729
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:27.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8911" for this suite. 09/07/23 06:04:27.728
------------------------------
â€¢ [1.042 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:26.69
    Sep  7 06:04:26.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-pred 09/07/23 06:04:26.691
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:26.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:26.699
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  7 06:04:26.701: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  7 06:04:26.704: INFO: Waiting for terminating namespaces to be deleted...
    Sep  7 06:04:26.706: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  7 06:04:26.709: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.709: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.709: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: pod-projected-configmaps-928ed498-e61f-4234-9980-bac064d07270 from projected-4661 started at 2023-09-07 06:03:10 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.709: INFO: 	Container agnhost-container ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.709: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:26.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  7 06:04:26.709: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  7 06:04:26.712: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.712: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 06:04:26.712: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:26.712: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 06:04:26.712: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:26.712: INFO: 	Container e2e ready: true, restart count 0
    Sep  7 06:04:26.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:26.712: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:26.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:26.712: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/07/23 06:04:26.712
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.178288a867ea6558], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 09/07/23 06:04:26.729
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:27.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8911" for this suite. 09/07/23 06:04:27.728
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:27.733
Sep  7 06:04:27.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:04:27.733
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:27.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:27.744
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:04:27.752
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:04:28.04
STEP: Deploying the webhook pod 09/07/23 06:04:28.045
STEP: Wait for the deployment to be ready 09/07/23 06:04:28.055
Sep  7 06:04:28.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 06:04:30.066
STEP: Verifying the service has paired with the endpoint 09/07/23 06:04:30.074
Sep  7 06:04:31.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  7 06:04:31.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5352-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 06:04:31.585
STEP: Creating a custom resource that should be mutated by the webhook 09/07/23 06:04:31.597
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:34.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1036" for this suite. 09/07/23 06:04:34.17
STEP: Destroying namespace "webhook-1036-markers" for this suite. 09/07/23 06:04:34.174
------------------------------
â€¢ [SLOW TEST] [6.446 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:27.733
    Sep  7 06:04:27.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:04:27.733
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:27.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:27.744
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:04:27.752
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:04:28.04
    STEP: Deploying the webhook pod 09/07/23 06:04:28.045
    STEP: Wait for the deployment to be ready 09/07/23 06:04:28.055
    Sep  7 06:04:28.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 06:04:30.066
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:04:30.074
    Sep  7 06:04:31.074: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  7 06:04:31.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5352-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 06:04:31.585
    STEP: Creating a custom resource that should be mutated by the webhook 09/07/23 06:04:31.597
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:34.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1036" for this suite. 09/07/23 06:04:34.17
    STEP: Destroying namespace "webhook-1036-markers" for this suite. 09/07/23 06:04:34.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:34.179
Sep  7 06:04:34.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-pred 09/07/23 06:04:34.18
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:34.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:34.192
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  7 06:04:34.194: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  7 06:04:34.197: INFO: Waiting for terminating namespaces to be deleted...
Sep  7 06:04:34.199: INFO: 
Logging pods the apiserver thinks is on node kind-worker before test
Sep  7 06:04:34.202: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:34.202: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 06:04:34.202: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:34.202: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 06:04:34.202: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:34.202: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  7 06:04:34.202: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:34.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:34.202: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  7 06:04:34.202: INFO: 
Logging pods the apiserver thinks is on node kind-worker2 before test
Sep  7 06:04:34.204: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:34.204: INFO: 	Container kindnet-cni ready: true, restart count 0
Sep  7 06:04:34.204: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
Sep  7 06:04:34.204: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  7 06:04:34.204: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:34.204: INFO: 	Container e2e ready: true, restart count 0
Sep  7 06:04:34.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:34.204: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
Sep  7 06:04:34.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  7 06:04:34.204: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 06:04:34.204
Sep  7 06:04:34.209: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7814" to be "running"
Sep  7 06:04:34.210: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53417ms
Sep  7 06:04:36.213: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004428229s
Sep  7 06:04:36.213: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 06:04:36.215
STEP: Trying to apply a random label on the found node. 09/07/23 06:04:36.223
STEP: verifying the node has the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 42 09/07/23 06:04:36.228
STEP: Trying to relaunch the pod, now with labels. 09/07/23 06:04:36.23
Sep  7 06:04:36.235: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7814" to be "not pending"
Sep  7 06:04:36.236: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7127ms
Sep  7 06:04:38.239: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004402039s
Sep  7 06:04:38.239: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 off the node kind-worker 09/07/23 06:04:38.241
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 09/07/23 06:04:38.25
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7814" for this suite. 09/07/23 06:04:38.255
------------------------------
â€¢ [4.087 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:34.179
    Sep  7 06:04:34.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-pred 09/07/23 06:04:34.18
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:34.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:34.192
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  7 06:04:34.194: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  7 06:04:34.197: INFO: Waiting for terminating namespaces to be deleted...
    Sep  7 06:04:34.199: INFO: 
    Logging pods the apiserver thinks is on node kind-worker before test
    Sep  7 06:04:34.202: INFO: kindnet-x7hxm from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:34.202: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 06:04:34.202: INFO: kube-proxy-7blqf from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:34.202: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 06:04:34.202: INFO: sonobuoy from sonobuoy started at 2023-09-07 05:05:04 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:34.202: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  7 06:04:34.202: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-ffjxb from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:34.202: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:34.202: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  7 06:04:34.202: INFO: 
    Logging pods the apiserver thinks is on node kind-worker2 before test
    Sep  7 06:04:34.204: INFO: kindnet-69bgp from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:34.204: INFO: 	Container kindnet-cni ready: true, restart count 0
    Sep  7 06:04:34.204: INFO: kube-proxy-9w4bc from kube-system started at 2023-09-07 04:57:20 +0000 UTC (1 container statuses recorded)
    Sep  7 06:04:34.204: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  7 06:04:34.204: INFO: sonobuoy-e2e-job-ac4880036cda42a0 from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:34.204: INFO: 	Container e2e ready: true, restart count 0
    Sep  7 06:04:34.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:34.204: INFO: sonobuoy-systemd-logs-daemon-set-2484bdad5e054591-vpjpx from sonobuoy started at 2023-09-07 05:05:08 +0000 UTC (2 container statuses recorded)
    Sep  7 06:04:34.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  7 06:04:34.204: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 06:04:34.204
    Sep  7 06:04:34.209: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7814" to be "running"
    Sep  7 06:04:34.210: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53417ms
    Sep  7 06:04:36.213: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004428229s
    Sep  7 06:04:36.213: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 06:04:36.215
    STEP: Trying to apply a random label on the found node. 09/07/23 06:04:36.223
    STEP: verifying the node has the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 42 09/07/23 06:04:36.228
    STEP: Trying to relaunch the pod, now with labels. 09/07/23 06:04:36.23
    Sep  7 06:04:36.235: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7814" to be "not pending"
    Sep  7 06:04:36.236: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.7127ms
    Sep  7 06:04:38.239: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004402039s
    Sep  7 06:04:38.239: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 off the node kind-worker 09/07/23 06:04:38.241
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-b712f98b-3d0f-499b-81f5-d41baacea5e7 09/07/23 06:04:38.25
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7814" for this suite. 09/07/23 06:04:38.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:38.266
Sep  7 06:04:38.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:04:38.267
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:38.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:38.278
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/07/23 06:04:38.28
Sep  7 06:04:38.286: INFO: Waiting up to 5m0s for pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f" in namespace "downward-api-8948" to be "Succeeded or Failed"
Sep  7 06:04:38.288: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70865ms
Sep  7 06:04:40.292: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00616519s
Sep  7 06:04:42.290: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004403589s
STEP: Saw pod success 09/07/23 06:04:42.29
Sep  7 06:04:42.291: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f" satisfied condition "Succeeded or Failed"
Sep  7 06:04:42.292: INFO: Trying to get logs from node kind-worker pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:04:42.296
Sep  7 06:04:42.308: INFO: Waiting for pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f to disappear
Sep  7 06:04:42.309: INFO: Pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8948" for this suite. 09/07/23 06:04:42.311
------------------------------
â€¢ [4.048 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:38.266
    Sep  7 06:04:38.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:04:38.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:38.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:38.278
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/07/23 06:04:38.28
    Sep  7 06:04:38.286: INFO: Waiting up to 5m0s for pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f" in namespace "downward-api-8948" to be "Succeeded or Failed"
    Sep  7 06:04:38.288: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70865ms
    Sep  7 06:04:40.292: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00616519s
    Sep  7 06:04:42.290: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004403589s
    STEP: Saw pod success 09/07/23 06:04:42.29
    Sep  7 06:04:42.291: INFO: Pod "downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f" satisfied condition "Succeeded or Failed"
    Sep  7 06:04:42.292: INFO: Trying to get logs from node kind-worker pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:04:42.296
    Sep  7 06:04:42.308: INFO: Waiting for pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f to disappear
    Sep  7 06:04:42.309: INFO: Pod downward-api-770f3a29-3c56-4a1b-bcf1-3264ccba153f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:42.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8948" for this suite. 09/07/23 06:04:42.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:42.315
Sep  7 06:04:42.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:04:42.315
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:42.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:42.327
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 06:04:42.328
Sep  7 06:04:42.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  7 06:04:42.383: INFO: stderr: ""
Sep  7 06:04:42.383: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/07/23 06:04:42.383
STEP: verifying the pod e2e-test-httpd-pod was created 09/07/23 06:04:47.435
Sep  7 06:04:47.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 get pod e2e-test-httpd-pod -o json'
Sep  7 06:04:47.486: INFO: stderr: ""
Sep  7 06:04:47.486: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-07T06:04:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5713\",\n        \"resourceVersion\": \"21146\",\n        \"uid\": \"ffa5ea64-0fe7-4917-9fde-da8eaf2ef0c7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r9h7s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-worker\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r9h7s\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://30e5ebbaa65b1236664da6a9357fe5c86a5715ac21779d4411b3f1f75c848a18\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-07T06:04:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.8.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-07T06:04:42Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/07/23 06:04:47.487
Sep  7 06:04:47.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 replace -f -'
Sep  7 06:04:47.948: INFO: stderr: ""
Sep  7 06:04:47.948: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/07/23 06:04:47.948
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  7 06:04:47.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 delete pods e2e-test-httpd-pod'
Sep  7 06:04:49.427: INFO: stderr: ""
Sep  7 06:04:49.427: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:49.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5713" for this suite. 09/07/23 06:04:49.43
------------------------------
â€¢ [SLOW TEST] [7.119 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:42.315
    Sep  7 06:04:42.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:04:42.315
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:42.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:42.327
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 06:04:42.328
    Sep  7 06:04:42.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  7 06:04:42.383: INFO: stderr: ""
    Sep  7 06:04:42.383: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/07/23 06:04:42.383
    STEP: verifying the pod e2e-test-httpd-pod was created 09/07/23 06:04:47.435
    Sep  7 06:04:47.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 get pod e2e-test-httpd-pod -o json'
    Sep  7 06:04:47.486: INFO: stderr: ""
    Sep  7 06:04:47.486: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-09-07T06:04:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5713\",\n        \"resourceVersion\": \"21146\",\n        \"uid\": \"ffa5ea64-0fe7-4917-9fde-da8eaf2ef0c7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r9h7s\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kind-worker\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r9h7s\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-07T06:04:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://30e5ebbaa65b1236664da6a9357fe5c86a5715ac21779d4411b3f1f75c848a18\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-07T06:04:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.8.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.27\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.27\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-07T06:04:42Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/07/23 06:04:47.487
    Sep  7 06:04:47.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 replace -f -'
    Sep  7 06:04:47.948: INFO: stderr: ""
    Sep  7 06:04:47.948: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/07/23 06:04:47.948
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  7 06:04:47.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-5713 delete pods e2e-test-httpd-pod'
    Sep  7 06:04:49.427: INFO: stderr: ""
    Sep  7 06:04:49.427: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:49.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5713" for this suite. 09/07/23 06:04:49.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:49.436
Sep  7 06:04:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:04:49.438
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:49.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:49.447
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/07/23 06:04:49.449
Sep  7 06:04:49.453: INFO: Waiting up to 5m0s for pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400" in namespace "downward-api-6586" to be "Succeeded or Failed"
Sep  7 06:04:49.455: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66835ms
Sep  7 06:04:51.458: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411395s
Sep  7 06:04:53.459: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00500899s
STEP: Saw pod success 09/07/23 06:04:53.459
Sep  7 06:04:53.459: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400" satisfied condition "Succeeded or Failed"
Sep  7 06:04:53.461: INFO: Trying to get logs from node kind-worker pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:04:53.465
Sep  7 06:04:53.473: INFO: Waiting for pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 to disappear
Sep  7 06:04:53.474: INFO: Pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:53.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6586" for this suite. 09/07/23 06:04:53.477
------------------------------
â€¢ [4.044 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:49.436
    Sep  7 06:04:49.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:04:49.438
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:49.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:49.447
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/07/23 06:04:49.449
    Sep  7 06:04:49.453: INFO: Waiting up to 5m0s for pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400" in namespace "downward-api-6586" to be "Succeeded or Failed"
    Sep  7 06:04:49.455: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66835ms
    Sep  7 06:04:51.458: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411395s
    Sep  7 06:04:53.459: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00500899s
    STEP: Saw pod success 09/07/23 06:04:53.459
    Sep  7 06:04:53.459: INFO: Pod "downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400" satisfied condition "Succeeded or Failed"
    Sep  7 06:04:53.461: INFO: Trying to get logs from node kind-worker pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:04:53.465
    Sep  7 06:04:53.473: INFO: Waiting for pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 to disappear
    Sep  7 06:04:53.474: INFO: Pod downward-api-5c621a6a-7bd5-4bdd-a11e-37887d67c400 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:53.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6586" for this suite. 09/07/23 06:04:53.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:53.481
Sep  7 06:04:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename init-container 09/07/23 06:04:53.482
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:53.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:53.491
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/07/23 06:04:53.494
Sep  7 06:04:53.494: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:04:57.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4570" for this suite. 09/07/23 06:04:57.473
------------------------------
â€¢ [3.996 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:53.481
    Sep  7 06:04:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename init-container 09/07/23 06:04:53.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:53.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:53.491
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/07/23 06:04:53.494
    Sep  7 06:04:53.494: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:04:57.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4570" for this suite. 09/07/23 06:04:57.473
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:04:57.478
Sep  7 06:04:57.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:04:57.481
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:57.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:57.491
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/07/23 06:04:57.493
STEP: submitting the pod to kubernetes 09/07/23 06:04:57.493
Sep  7 06:04:57.499: INFO: Waiting up to 5m0s for pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" in namespace "pods-9517" to be "running and ready"
Sep  7 06:04:57.501: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512949ms
Sep  7 06:04:57.501: INFO: The phase of Pod pod-update-9618196a-321d-4416-a5b0-c08b41b661e8 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:04:59.504: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00466378s
Sep  7 06:04:59.504: INFO: The phase of Pod pod-update-9618196a-321d-4416-a5b0-c08b41b661e8 is Running (Ready = true)
Sep  7 06:04:59.504: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/07/23 06:04:59.505
STEP: updating the pod 09/07/23 06:04:59.507
Sep  7 06:05:00.015: INFO: Successfully updated pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8"
Sep  7 06:05:00.015: INFO: Waiting up to 5m0s for pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" in namespace "pods-9517" to be "running"
Sep  7 06:05:00.016: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Running", Reason="", readiness=true. Elapsed: 1.88232ms
Sep  7 06:05:00.017: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/07/23 06:05:00.017
Sep  7 06:05:00.018: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:00.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9517" for this suite. 09/07/23 06:05:00.023
------------------------------
â€¢ [2.549 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:04:57.478
    Sep  7 06:04:57.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:04:57.481
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:04:57.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:04:57.491
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/07/23 06:04:57.493
    STEP: submitting the pod to kubernetes 09/07/23 06:04:57.493
    Sep  7 06:04:57.499: INFO: Waiting up to 5m0s for pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" in namespace "pods-9517" to be "running and ready"
    Sep  7 06:04:57.501: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512949ms
    Sep  7 06:04:57.501: INFO: The phase of Pod pod-update-9618196a-321d-4416-a5b0-c08b41b661e8 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:04:59.504: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.00466378s
    Sep  7 06:04:59.504: INFO: The phase of Pod pod-update-9618196a-321d-4416-a5b0-c08b41b661e8 is Running (Ready = true)
    Sep  7 06:04:59.504: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/07/23 06:04:59.505
    STEP: updating the pod 09/07/23 06:04:59.507
    Sep  7 06:05:00.015: INFO: Successfully updated pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8"
    Sep  7 06:05:00.015: INFO: Waiting up to 5m0s for pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" in namespace "pods-9517" to be "running"
    Sep  7 06:05:00.016: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8": Phase="Running", Reason="", readiness=true. Elapsed: 1.88232ms
    Sep  7 06:05:00.017: INFO: Pod "pod-update-9618196a-321d-4416-a5b0-c08b41b661e8" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/07/23 06:05:00.017
    Sep  7 06:05:00.018: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:00.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9517" for this suite. 09/07/23 06:05:00.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:00.028
Sep  7 06:05:00.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename subpath 09/07/23 06:05:00.028
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:00.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:00.038
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/07/23 06:05:00.04
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-n6cg 09/07/23 06:05:00.045
STEP: Creating a pod to test atomic-volume-subpath 09/07/23 06:05:00.045
Sep  7 06:05:00.051: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n6cg" in namespace "subpath-6557" to be "Succeeded or Failed"
Sep  7 06:05:00.052: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36195ms
Sep  7 06:05:02.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 2.00455973s
Sep  7 06:05:04.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 4.004044922s
Sep  7 06:05:06.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 6.005050352s
Sep  7 06:05:08.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 8.004478954s
Sep  7 06:05:10.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 10.004014976s
Sep  7 06:05:12.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 12.004341677s
Sep  7 06:05:14.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 14.004772228s
Sep  7 06:05:16.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 16.00472865s
Sep  7 06:05:18.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 18.004240112s
Sep  7 06:05:20.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 20.004987844s
Sep  7 06:05:22.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=false. Elapsed: 22.004590695s
Sep  7 06:05:24.057: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006269147s
STEP: Saw pod success 09/07/23 06:05:24.057
Sep  7 06:05:24.057: INFO: Pod "pod-subpath-test-projected-n6cg" satisfied condition "Succeeded or Failed"
Sep  7 06:05:24.059: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-projected-n6cg container test-container-subpath-projected-n6cg: <nil>
STEP: delete the pod 09/07/23 06:05:24.064
Sep  7 06:05:24.073: INFO: Waiting for pod pod-subpath-test-projected-n6cg to disappear
Sep  7 06:05:24.074: INFO: Pod pod-subpath-test-projected-n6cg no longer exists
STEP: Deleting pod pod-subpath-test-projected-n6cg 09/07/23 06:05:24.075
Sep  7 06:05:24.075: INFO: Deleting pod "pod-subpath-test-projected-n6cg" in namespace "subpath-6557"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:24.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6557" for this suite. 09/07/23 06:05:24.079
------------------------------
â€¢ [SLOW TEST] [24.055 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:00.028
    Sep  7 06:05:00.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename subpath 09/07/23 06:05:00.028
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:00.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:00.038
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/07/23 06:05:00.04
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-n6cg 09/07/23 06:05:00.045
    STEP: Creating a pod to test atomic-volume-subpath 09/07/23 06:05:00.045
    Sep  7 06:05:00.051: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-n6cg" in namespace "subpath-6557" to be "Succeeded or Failed"
    Sep  7 06:05:00.052: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.36195ms
    Sep  7 06:05:02.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 2.00455973s
    Sep  7 06:05:04.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 4.004044922s
    Sep  7 06:05:06.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 6.005050352s
    Sep  7 06:05:08.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 8.004478954s
    Sep  7 06:05:10.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 10.004014976s
    Sep  7 06:05:12.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 12.004341677s
    Sep  7 06:05:14.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 14.004772228s
    Sep  7 06:05:16.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 16.00472865s
    Sep  7 06:05:18.055: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 18.004240112s
    Sep  7 06:05:20.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=true. Elapsed: 20.004987844s
    Sep  7 06:05:22.056: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Running", Reason="", readiness=false. Elapsed: 22.004590695s
    Sep  7 06:05:24.057: INFO: Pod "pod-subpath-test-projected-n6cg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006269147s
    STEP: Saw pod success 09/07/23 06:05:24.057
    Sep  7 06:05:24.057: INFO: Pod "pod-subpath-test-projected-n6cg" satisfied condition "Succeeded or Failed"
    Sep  7 06:05:24.059: INFO: Trying to get logs from node kind-worker pod pod-subpath-test-projected-n6cg container test-container-subpath-projected-n6cg: <nil>
    STEP: delete the pod 09/07/23 06:05:24.064
    Sep  7 06:05:24.073: INFO: Waiting for pod pod-subpath-test-projected-n6cg to disappear
    Sep  7 06:05:24.074: INFO: Pod pod-subpath-test-projected-n6cg no longer exists
    STEP: Deleting pod pod-subpath-test-projected-n6cg 09/07/23 06:05:24.075
    Sep  7 06:05:24.075: INFO: Deleting pod "pod-subpath-test-projected-n6cg" in namespace "subpath-6557"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:24.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6557" for this suite. 09/07/23 06:05:24.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:24.085
Sep  7 06:05:24.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pod-network-test 09/07/23 06:05:24.085
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:24.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:24.097
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-3354 09/07/23 06:05:24.098
STEP: creating a selector 09/07/23 06:05:24.098
STEP: Creating the service pods in kubernetes 09/07/23 06:05:24.098
Sep  7 06:05:24.099: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  7 06:05:24.112: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3354" to be "running and ready"
Sep  7 06:05:24.113: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361161ms
Sep  7 06:05:24.113: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:05:26.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004878682s
Sep  7 06:05:26.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:05:28.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004014284s
Sep  7 06:05:28.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:05:30.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005225567s
Sep  7 06:05:30.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:05:32.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00477659s
Sep  7 06:05:32.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:05:34.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004652482s
Sep  7 06:05:34.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:05:36.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.003731755s
Sep  7 06:05:36.116: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  7 06:05:36.116: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  7 06:05:36.117: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3354" to be "running and ready"
Sep  7 06:05:36.119: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.820439ms
Sep  7 06:05:36.119: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  7 06:05:36.119: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/07/23 06:05:36.121
Sep  7 06:05:36.127: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3354" to be "running"
Sep  7 06:05:36.130: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02158ms
Sep  7 06:05:38.133: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006223653s
Sep  7 06:05:38.133: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  7 06:05:38.135: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3354" to be "running"
Sep  7 06:05:38.136: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.550729ms
Sep  7 06:05:38.136: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  7 06:05:38.138: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  7 06:05:38.138: INFO: Going to poll 10.244.1.30 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  7 06:05:38.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.30:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:05:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:05:38.140: INFO: ExecWithOptions: Clientset creation
Sep  7 06:05:38.140: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.30%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  7 06:05:38.259: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  7 06:05:38.259: INFO: Going to poll 10.244.2.248 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  7 06:05:38.262: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.248:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:05:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:05:38.262: INFO: ExecWithOptions: Clientset creation
Sep  7 06:05:38.262: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.248%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  7 06:05:38.386: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3354" for this suite. 09/07/23 06:05:38.389
------------------------------
â€¢ [SLOW TEST] [14.308 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:24.085
    Sep  7 06:05:24.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pod-network-test 09/07/23 06:05:24.085
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:24.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:24.097
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-3354 09/07/23 06:05:24.098
    STEP: creating a selector 09/07/23 06:05:24.098
    STEP: Creating the service pods in kubernetes 09/07/23 06:05:24.098
    Sep  7 06:05:24.099: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  7 06:05:24.112: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3354" to be "running and ready"
    Sep  7 06:05:24.113: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361161ms
    Sep  7 06:05:24.113: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:05:26.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004878682s
    Sep  7 06:05:26.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:05:28.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004014284s
    Sep  7 06:05:28.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:05:30.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005225567s
    Sep  7 06:05:30.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:05:32.117: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00477659s
    Sep  7 06:05:32.117: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:05:34.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004652482s
    Sep  7 06:05:34.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:05:36.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.003731755s
    Sep  7 06:05:36.116: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  7 06:05:36.116: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  7 06:05:36.117: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3354" to be "running and ready"
    Sep  7 06:05:36.119: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.820439ms
    Sep  7 06:05:36.119: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  7 06:05:36.119: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/07/23 06:05:36.121
    Sep  7 06:05:36.127: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3354" to be "running"
    Sep  7 06:05:36.130: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.02158ms
    Sep  7 06:05:38.133: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006223653s
    Sep  7 06:05:38.133: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  7 06:05:38.135: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3354" to be "running"
    Sep  7 06:05:38.136: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.550729ms
    Sep  7 06:05:38.136: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  7 06:05:38.138: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  7 06:05:38.138: INFO: Going to poll 10.244.1.30 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  7 06:05:38.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.30:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:05:38.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:05:38.140: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:05:38.140: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.30%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  7 06:05:38.259: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  7 06:05:38.259: INFO: Going to poll 10.244.2.248 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  7 06:05:38.262: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.248:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:05:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:05:38.262: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:05:38.262: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.248%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  7 06:05:38.386: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3354" for this suite. 09/07/23 06:05:38.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:38.394
Sep  7 06:05:38.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 06:05:38.394
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:38.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:38.403
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-6982021f-8b1b-4cd9-8c8d-255ae48ef607 09/07/23 06:05:38.405
STEP: Creating a pod to test consume secrets 09/07/23 06:05:38.408
Sep  7 06:05:38.412: INFO: Waiting up to 5m0s for pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077" in namespace "secrets-7801" to be "Succeeded or Failed"
Sep  7 06:05:38.414: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Pending", Reason="", readiness=false. Elapsed: 1.68144ms
Sep  7 06:05:40.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005084013s
Sep  7 06:05:42.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005021066s
STEP: Saw pod success 09/07/23 06:05:42.417
Sep  7 06:05:42.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077" satisfied condition "Succeeded or Failed"
Sep  7 06:05:42.420: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 container secret-env-test: <nil>
STEP: delete the pod 09/07/23 06:05:42.429
Sep  7 06:05:42.436: INFO: Waiting for pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 to disappear
Sep  7 06:05:42.437: INFO: Pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:42.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7801" for this suite. 09/07/23 06:05:42.439
------------------------------
â€¢ [4.050 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:38.394
    Sep  7 06:05:38.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 06:05:38.394
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:38.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:38.403
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-6982021f-8b1b-4cd9-8c8d-255ae48ef607 09/07/23 06:05:38.405
    STEP: Creating a pod to test consume secrets 09/07/23 06:05:38.408
    Sep  7 06:05:38.412: INFO: Waiting up to 5m0s for pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077" in namespace "secrets-7801" to be "Succeeded or Failed"
    Sep  7 06:05:38.414: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Pending", Reason="", readiness=false. Elapsed: 1.68144ms
    Sep  7 06:05:40.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005084013s
    Sep  7 06:05:42.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005021066s
    STEP: Saw pod success 09/07/23 06:05:42.417
    Sep  7 06:05:42.417: INFO: Pod "pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077" satisfied condition "Succeeded or Failed"
    Sep  7 06:05:42.420: INFO: Trying to get logs from node kind-worker2 pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 container secret-env-test: <nil>
    STEP: delete the pod 09/07/23 06:05:42.429
    Sep  7 06:05:42.436: INFO: Waiting for pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 to disappear
    Sep  7 06:05:42.437: INFO: Pod pod-secrets-3b36dfb4-a6e1-49e0-a9bd-b8bce8c6a077 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:42.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7801" for this suite. 09/07/23 06:05:42.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:42.444
Sep  7 06:05:42.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 06:05:42.445
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:42.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:42.456
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  7 06:05:42.464: INFO: Waiting up to 5m0s for pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960" in namespace "svcaccounts-9636" to be "running"
Sep  7 06:05:42.466: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41529ms
Sep  7 06:05:44.469: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960": Phase="Running", Reason="", readiness=true. Elapsed: 2.005105843s
Sep  7 06:05:44.469: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960" satisfied condition "running"
STEP: reading a file in the container 09/07/23 06:05:44.469
Sep  7 06:05:44.469: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/07/23 06:05:44.667
Sep  7 06:05:44.668: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/07/23 06:05:44.834
Sep  7 06:05:44.834: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  7 06:05:44.992: INFO: Got root ca configmap in namespace "svcaccounts-9636"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9636" for this suite. 09/07/23 06:05:44.996
------------------------------
â€¢ [2.558 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:42.444
    Sep  7 06:05:42.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 06:05:42.445
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:42.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:42.456
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  7 06:05:42.464: INFO: Waiting up to 5m0s for pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960" in namespace "svcaccounts-9636" to be "running"
    Sep  7 06:05:42.466: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41529ms
    Sep  7 06:05:44.469: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960": Phase="Running", Reason="", readiness=true. Elapsed: 2.005105843s
    Sep  7 06:05:44.469: INFO: Pod "pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960" satisfied condition "running"
    STEP: reading a file in the container 09/07/23 06:05:44.469
    Sep  7 06:05:44.469: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/07/23 06:05:44.667
    Sep  7 06:05:44.668: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/07/23 06:05:44.834
    Sep  7 06:05:44.834: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9636 pod-service-account-7d5bbe6b-1449-4eac-820f-dc641ee16960 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  7 06:05:44.992: INFO: Got root ca configmap in namespace "svcaccounts-9636"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9636" for this suite. 09/07/23 06:05:44.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:45.003
Sep  7 06:05:45.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 06:05:45.004
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:45.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:45.015
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2133.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2133.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/07/23 06:05:45.016
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2133.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2133.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/07/23 06:05:45.017
STEP: creating a pod to probe /etc/hosts 09/07/23 06:05:45.017
STEP: submitting the pod to kubernetes 09/07/23 06:05:45.017
Sep  7 06:05:45.023: INFO: Waiting up to 15m0s for pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760" in namespace "dns-2133" to be "running"
Sep  7 06:05:45.025: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538609ms
Sep  7 06:05:47.028: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760": Phase="Running", Reason="", readiness=true. Elapsed: 2.004956873s
Sep  7 06:05:47.028: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760" satisfied condition "running"
STEP: retrieving the pod 09/07/23 06:05:47.028
STEP: looking for the results for each expected name from probers 09/07/23 06:05:47.03
Sep  7 06:05:47.038: INFO: DNS probes using dns-2133/dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760 succeeded

STEP: deleting the pod 09/07/23 06:05:47.038
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:47.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2133" for this suite. 09/07/23 06:05:47.049
------------------------------
â€¢ [2.049 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:45.003
    Sep  7 06:05:45.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 06:05:45.004
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:45.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:45.015
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2133.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2133.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/07/23 06:05:45.016
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2133.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2133.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/07/23 06:05:45.017
    STEP: creating a pod to probe /etc/hosts 09/07/23 06:05:45.017
    STEP: submitting the pod to kubernetes 09/07/23 06:05:45.017
    Sep  7 06:05:45.023: INFO: Waiting up to 15m0s for pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760" in namespace "dns-2133" to be "running"
    Sep  7 06:05:45.025: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538609ms
    Sep  7 06:05:47.028: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760": Phase="Running", Reason="", readiness=true. Elapsed: 2.004956873s
    Sep  7 06:05:47.028: INFO: Pod "dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 06:05:47.028
    STEP: looking for the results for each expected name from probers 09/07/23 06:05:47.03
    Sep  7 06:05:47.038: INFO: DNS probes using dns-2133/dns-test-d8dbba2b-ad13-454a-a68b-f41e8cdc5760 succeeded

    STEP: deleting the pod 09/07/23 06:05:47.038
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:47.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2133" for this suite. 09/07/23 06:05:47.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:47.053
Sep  7 06:05:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubelet-test 09/07/23 06:05:47.053
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:47.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:47.061
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  7 06:05:47.067: INFO: Waiting up to 5m0s for pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109" in namespace "kubelet-test-1760" to be "running and ready"
Sep  7 06:05:47.068: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32181ms
Sep  7 06:05:47.068: INFO: The phase of Pod busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:05:49.071: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109": Phase="Running", Reason="", readiness=true. Elapsed: 2.004008543s
Sep  7 06:05:49.071: INFO: The phase of Pod busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109 is Running (Ready = true)
Sep  7 06:05:49.071: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:49.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1760" for this suite. 09/07/23 06:05:49.079
------------------------------
â€¢ [2.030 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:47.053
    Sep  7 06:05:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubelet-test 09/07/23 06:05:47.053
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:47.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:47.061
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  7 06:05:47.067: INFO: Waiting up to 5m0s for pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109" in namespace "kubelet-test-1760" to be "running and ready"
    Sep  7 06:05:47.068: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109": Phase="Pending", Reason="", readiness=false. Elapsed: 1.32181ms
    Sep  7 06:05:47.068: INFO: The phase of Pod busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:05:49.071: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109": Phase="Running", Reason="", readiness=true. Elapsed: 2.004008543s
    Sep  7 06:05:49.071: INFO: The phase of Pod busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109 is Running (Ready = true)
    Sep  7 06:05:49.071: INFO: Pod "busybox-scheduling-b51d474c-6ca0-4173-b324-f57de6701109" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:49.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1760" for this suite. 09/07/23 06:05:49.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:49.083
Sep  7 06:05:49.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename limitrange 09/07/23 06:05:49.084
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:49.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:49.096
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-fqkjx" in namespace "limitrange-339" 09/07/23 06:05:49.098
STEP: Creating another limitRange in another namespace 09/07/23 06:05:49.101
Sep  7 06:05:49.108: INFO: Namespace "e2e-limitrange-fqkjx-3722" created
Sep  7 06:05:49.108: INFO: Creating LimitRange "e2e-limitrange-fqkjx" in namespace "e2e-limitrange-fqkjx-3722"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-fqkjx" 09/07/23 06:05:49.11
Sep  7 06:05:49.112: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-fqkjx" in "limitrange-339" namespace 09/07/23 06:05:49.112
Sep  7 06:05:49.118: INFO: LimitRange "e2e-limitrange-fqkjx" has been patched
STEP: Delete LimitRange "e2e-limitrange-fqkjx" by Collection with labelSelector: "e2e-limitrange-fqkjx=patched" 09/07/23 06:05:49.118
STEP: Confirm that the limitRange "e2e-limitrange-fqkjx" has been deleted 09/07/23 06:05:49.121
Sep  7 06:05:49.121: INFO: Requesting list of LimitRange to confirm quantity
Sep  7 06:05:49.123: INFO: Found 0 LimitRange with label "e2e-limitrange-fqkjx=patched"
Sep  7 06:05:49.123: INFO: LimitRange "e2e-limitrange-fqkjx" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-fqkjx" 09/07/23 06:05:49.123
Sep  7 06:05:49.124: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  7 06:05:49.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-339" for this suite. 09/07/23 06:05:49.126
STEP: Destroying namespace "e2e-limitrange-fqkjx-3722" for this suite. 09/07/23 06:05:49.13
------------------------------
â€¢ [0.050 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:49.083
    Sep  7 06:05:49.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename limitrange 09/07/23 06:05:49.084
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:49.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:49.096
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-fqkjx" in namespace "limitrange-339" 09/07/23 06:05:49.098
    STEP: Creating another limitRange in another namespace 09/07/23 06:05:49.101
    Sep  7 06:05:49.108: INFO: Namespace "e2e-limitrange-fqkjx-3722" created
    Sep  7 06:05:49.108: INFO: Creating LimitRange "e2e-limitrange-fqkjx" in namespace "e2e-limitrange-fqkjx-3722"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-fqkjx" 09/07/23 06:05:49.11
    Sep  7 06:05:49.112: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-fqkjx" in "limitrange-339" namespace 09/07/23 06:05:49.112
    Sep  7 06:05:49.118: INFO: LimitRange "e2e-limitrange-fqkjx" has been patched
    STEP: Delete LimitRange "e2e-limitrange-fqkjx" by Collection with labelSelector: "e2e-limitrange-fqkjx=patched" 09/07/23 06:05:49.118
    STEP: Confirm that the limitRange "e2e-limitrange-fqkjx" has been deleted 09/07/23 06:05:49.121
    Sep  7 06:05:49.121: INFO: Requesting list of LimitRange to confirm quantity
    Sep  7 06:05:49.123: INFO: Found 0 LimitRange with label "e2e-limitrange-fqkjx=patched"
    Sep  7 06:05:49.123: INFO: LimitRange "e2e-limitrange-fqkjx" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-fqkjx" 09/07/23 06:05:49.123
    Sep  7 06:05:49.124: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:05:49.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-339" for this suite. 09/07/23 06:05:49.126
    STEP: Destroying namespace "e2e-limitrange-fqkjx-3722" for this suite. 09/07/23 06:05:49.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:05:49.135
Sep  7 06:05:49.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename cronjob 09/07/23 06:05:49.136
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:49.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:49.145
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/07/23 06:05:49.147
STEP: Ensuring a job is scheduled 09/07/23 06:05:49.15
STEP: Ensuring exactly one is scheduled 09/07/23 06:06:01.153
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/07/23 06:06:01.155
STEP: Ensuring no more jobs are scheduled 09/07/23 06:06:01.157
STEP: Removing cronjob 09/07/23 06:11:01.163
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:01.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-953" for this suite. 09/07/23 06:11:01.17
------------------------------
â€¢ [SLOW TEST] [312.038 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:05:49.135
    Sep  7 06:05:49.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename cronjob 09/07/23 06:05:49.136
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:05:49.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:05:49.145
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/07/23 06:05:49.147
    STEP: Ensuring a job is scheduled 09/07/23 06:05:49.15
    STEP: Ensuring exactly one is scheduled 09/07/23 06:06:01.153
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/07/23 06:06:01.155
    STEP: Ensuring no more jobs are scheduled 09/07/23 06:06:01.157
    STEP: Removing cronjob 09/07/23 06:11:01.163
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:01.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-953" for this suite. 09/07/23 06:11:01.17
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:01.174
Sep  7 06:11:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 06:11:01.175
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:01.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:01.184
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/07/23 06:11:01.186
STEP: Ensure pods equal to parallelism count is attached to the job 09/07/23 06:11:01.192
STEP: patching /status 09/07/23 06:11:03.196
STEP: updating /status 09/07/23 06:11:03.203
STEP: get /status 09/07/23 06:11:03.207
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7945" for this suite. 09/07/23 06:11:03.211
------------------------------
â€¢ [2.040 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:01.174
    Sep  7 06:11:01.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 06:11:01.175
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:01.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:01.184
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/07/23 06:11:01.186
    STEP: Ensure pods equal to parallelism count is attached to the job 09/07/23 06:11:01.192
    STEP: patching /status 09/07/23 06:11:03.196
    STEP: updating /status 09/07/23 06:11:03.203
    STEP: get /status 09/07/23 06:11:03.207
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7945" for this suite. 09/07/23 06:11:03.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:03.215
Sep  7 06:11:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:11:03.215
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:03.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:03.225
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/07/23 06:11:03.226
Sep  7 06:11:03.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: mark a version not serverd 09/07/23 06:11:06.429
STEP: check the unserved version gets removed 09/07/23 06:11:06.441
STEP: check the other version is not changed 09/07/23 06:11:07.708
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:10.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4628" for this suite. 09/07/23 06:11:10.248
------------------------------
â€¢ [SLOW TEST] [7.037 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:03.215
    Sep  7 06:11:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:11:03.215
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:03.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:03.225
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/07/23 06:11:03.226
    Sep  7 06:11:03.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: mark a version not serverd 09/07/23 06:11:06.429
    STEP: check the unserved version gets removed 09/07/23 06:11:06.441
    STEP: check the other version is not changed 09/07/23 06:11:07.708
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:10.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4628" for this suite. 09/07/23 06:11:10.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:10.252
Sep  7 06:11:10.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replicaset 09/07/23 06:11:10.253
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:10.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:10.264
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/07/23 06:11:10.265
Sep  7 06:11:10.270: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6646" to be "running and ready"
Sep  7 06:11:10.272: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349099ms
Sep  7 06:11:10.272: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:11:12.274: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003819238s
Sep  7 06:11:12.274: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  7 06:11:12.274: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/07/23 06:11:12.276
STEP: Then the orphan pod is adopted 09/07/23 06:11:12.28
STEP: When the matched label of one of its pods change 09/07/23 06:11:13.284
Sep  7 06:11:13.286: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/07/23 06:11:13.295
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:14.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6646" for this suite. 09/07/23 06:11:14.301
------------------------------
â€¢ [4.052 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:10.252
    Sep  7 06:11:10.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replicaset 09/07/23 06:11:10.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:10.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:10.264
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/07/23 06:11:10.265
    Sep  7 06:11:10.270: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6646" to be "running and ready"
    Sep  7 06:11:10.272: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349099ms
    Sep  7 06:11:10.272: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:11:12.274: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.003819238s
    Sep  7 06:11:12.274: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  7 06:11:12.274: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/07/23 06:11:12.276
    STEP: Then the orphan pod is adopted 09/07/23 06:11:12.28
    STEP: When the matched label of one of its pods change 09/07/23 06:11:13.284
    Sep  7 06:11:13.286: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/07/23 06:11:13.295
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:14.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6646" for this suite. 09/07/23 06:11:14.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:14.305
Sep  7 06:11:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 06:11:14.306
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:14.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:14.317
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/07/23 06:11:14.319
Sep  7 06:11:14.323: INFO: Waiting up to 5m0s for pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63" in namespace "emptydir-7382" to be "Succeeded or Failed"
Sep  7 06:11:14.324: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34342ms
Sep  7 06:11:16.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Running", Reason="", readiness=false. Elapsed: 2.003592296s
Sep  7 06:11:18.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00399773s
STEP: Saw pod success 09/07/23 06:11:18.327
Sep  7 06:11:18.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63" satisfied condition "Succeeded or Failed"
Sep  7 06:11:18.329: INFO: Trying to get logs from node kind-worker2 pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 container test-container: <nil>
STEP: delete the pod 09/07/23 06:11:18.338
Sep  7 06:11:18.346: INFO: Waiting for pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 to disappear
Sep  7 06:11:18.347: INFO: Pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:18.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7382" for this suite. 09/07/23 06:11:18.349
------------------------------
â€¢ [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:14.305
    Sep  7 06:11:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 06:11:14.306
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:14.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:14.317
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/07/23 06:11:14.319
    Sep  7 06:11:14.323: INFO: Waiting up to 5m0s for pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63" in namespace "emptydir-7382" to be "Succeeded or Failed"
    Sep  7 06:11:14.324: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34342ms
    Sep  7 06:11:16.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Running", Reason="", readiness=false. Elapsed: 2.003592296s
    Sep  7 06:11:18.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00399773s
    STEP: Saw pod success 09/07/23 06:11:18.327
    Sep  7 06:11:18.327: INFO: Pod "pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63" satisfied condition "Succeeded or Failed"
    Sep  7 06:11:18.329: INFO: Trying to get logs from node kind-worker2 pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 container test-container: <nil>
    STEP: delete the pod 09/07/23 06:11:18.338
    Sep  7 06:11:18.346: INFO: Waiting for pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 to disappear
    Sep  7 06:11:18.347: INFO: Pod pod-d3e1cb5c-fe9e-4e8f-a33d-c39050087d63 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:18.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7382" for this suite. 09/07/23 06:11:18.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:18.355
Sep  7 06:11:18.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename conformance-tests 09/07/23 06:11:18.355
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:18.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:18.364
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/07/23 06:11:18.366
Sep  7 06:11:18.366: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-6135" for this suite. 09/07/23 06:11:18.37
------------------------------
â€¢ [0.019 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:18.355
    Sep  7 06:11:18.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename conformance-tests 09/07/23 06:11:18.355
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:18.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:18.364
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/07/23 06:11:18.366
    Sep  7 06:11:18.366: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:18.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-6135" for this suite. 09/07/23 06:11:18.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:18.374
Sep  7 06:11:18.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:11:18.374
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:18.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:18.386
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-25c1b2ff-203d-4e6c-b6bf-6964519133ce 09/07/23 06:11:18.388
STEP: Creating a pod to test consume configMaps 09/07/23 06:11:18.39
Sep  7 06:11:18.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b" in namespace "projected-3808" to be "Succeeded or Failed"
Sep  7 06:11:18.397: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428441ms
Sep  7 06:11:20.400: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004474694s
Sep  7 06:11:22.401: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952925s
STEP: Saw pod success 09/07/23 06:11:22.401
Sep  7 06:11:22.401: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b" satisfied condition "Succeeded or Failed"
Sep  7 06:11:22.402: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b container agnhost-container: <nil>
STEP: delete the pod 09/07/23 06:11:22.411
Sep  7 06:11:22.418: INFO: Waiting for pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b to disappear
Sep  7 06:11:22.420: INFO: Pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3808" for this suite. 09/07/23 06:11:22.422
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:18.374
    Sep  7 06:11:18.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:11:18.374
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:18.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:18.386
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-25c1b2ff-203d-4e6c-b6bf-6964519133ce 09/07/23 06:11:18.388
    STEP: Creating a pod to test consume configMaps 09/07/23 06:11:18.39
    Sep  7 06:11:18.396: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b" in namespace "projected-3808" to be "Succeeded or Failed"
    Sep  7 06:11:18.397: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428441ms
    Sep  7 06:11:20.400: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004474694s
    Sep  7 06:11:22.401: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952925s
    STEP: Saw pod success 09/07/23 06:11:22.401
    Sep  7 06:11:22.401: INFO: Pod "pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b" satisfied condition "Succeeded or Failed"
    Sep  7 06:11:22.402: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 06:11:22.411
    Sep  7 06:11:22.418: INFO: Waiting for pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b to disappear
    Sep  7 06:11:22.420: INFO: Pod pod-projected-configmaps-83eec160-958f-49ad-9790-c9bdf3861a5b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:22.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3808" for this suite. 09/07/23 06:11:22.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:22.426
Sep  7 06:11:22.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubelet-test 09/07/23 06:11:22.426
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.442
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:22.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4679" for this suite. 09/07/23 06:11:22.456
------------------------------
â€¢ [0.037 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:22.426
    Sep  7 06:11:22.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubelet-test 09/07/23 06:11:22.426
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.442
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:22.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4679" for this suite. 09/07/23 06:11:22.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:22.463
Sep  7 06:11:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename endpointslice 09/07/23 06:11:22.464
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.472
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  7 06:11:22.478: INFO: Endpoints addresses: [192.168.8.7] , ports: [6443]
Sep  7 06:11:22.478: INFO: EndpointSlices addresses: [192.168.8.7] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-154" for this suite. 09/07/23 06:11:22.48
------------------------------
â€¢ [0.021 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:22.463
    Sep  7 06:11:22.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename endpointslice 09/07/23 06:11:22.464
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.472
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  7 06:11:22.478: INFO: Endpoints addresses: [192.168.8.7] , ports: [6443]
    Sep  7 06:11:22.478: INFO: EndpointSlices addresses: [192.168.8.7] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:22.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-154" for this suite. 09/07/23 06:11:22.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:22.485
Sep  7 06:11:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:11:22.485
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.493
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/07/23 06:11:22.494
STEP: getting /apis/node.k8s.io 09/07/23 06:11:22.496
STEP: getting /apis/node.k8s.io/v1 09/07/23 06:11:22.496
STEP: creating 09/07/23 06:11:22.497
STEP: watching 09/07/23 06:11:22.506
Sep  7 06:11:22.506: INFO: starting watch
STEP: getting 09/07/23 06:11:22.51
STEP: listing 09/07/23 06:11:22.511
STEP: patching 09/07/23 06:11:22.513
STEP: updating 09/07/23 06:11:22.515
Sep  7 06:11:22.518: INFO: waiting for watch events with expected annotations
STEP: deleting 09/07/23 06:11:22.518
STEP: deleting a collection 09/07/23 06:11:22.524
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:22.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2886" for this suite. 09/07/23 06:11:22.532
------------------------------
â€¢ [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:22.485
    Sep  7 06:11:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:11:22.485
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.493
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/07/23 06:11:22.494
    STEP: getting /apis/node.k8s.io 09/07/23 06:11:22.496
    STEP: getting /apis/node.k8s.io/v1 09/07/23 06:11:22.496
    STEP: creating 09/07/23 06:11:22.497
    STEP: watching 09/07/23 06:11:22.506
    Sep  7 06:11:22.506: INFO: starting watch
    STEP: getting 09/07/23 06:11:22.51
    STEP: listing 09/07/23 06:11:22.511
    STEP: patching 09/07/23 06:11:22.513
    STEP: updating 09/07/23 06:11:22.515
    Sep  7 06:11:22.518: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/07/23 06:11:22.518
    STEP: deleting a collection 09/07/23 06:11:22.524
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:22.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2886" for this suite. 09/07/23 06:11:22.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:22.537
Sep  7 06:11:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 06:11:22.538
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.545
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1522 09/07/23 06:11:22.547
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-1522 09/07/23 06:11:22.552
Sep  7 06:11:22.556: INFO: Found 0 stateful pods, waiting for 1
Sep  7 06:11:32.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/07/23 06:11:32.563
STEP: Getting /status 09/07/23 06:11:32.567
Sep  7 06:11:32.569: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/07/23 06:11:32.569
Sep  7 06:11:32.576: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/07/23 06:11:32.576
Sep  7 06:11:32.577: INFO: Observed &StatefulSet event: ADDED
Sep  7 06:11:32.577: INFO: Found Statefulset ss in namespace statefulset-1522 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  7 06:11:32.577: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/07/23 06:11:32.577
Sep  7 06:11:32.577: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  7 06:11:32.581: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/07/23 06:11:32.581
Sep  7 06:11:32.582: INFO: Observed &StatefulSet event: ADDED
Sep  7 06:11:32.582: INFO: Observed Statefulset ss in namespace statefulset-1522 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  7 06:11:32.582: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 06:11:32.582: INFO: Deleting all statefulset in ns statefulset-1522
Sep  7 06:11:32.584: INFO: Scaling statefulset ss to 0
Sep  7 06:11:42.594: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:11:42.596: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:42.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1522" for this suite. 09/07/23 06:11:42.605
------------------------------
â€¢ [SLOW TEST] [20.071 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:22.537
    Sep  7 06:11:22.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 06:11:22.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:22.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:22.545
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1522 09/07/23 06:11:22.547
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-1522 09/07/23 06:11:22.552
    Sep  7 06:11:22.556: INFO: Found 0 stateful pods, waiting for 1
    Sep  7 06:11:32.559: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/07/23 06:11:32.563
    STEP: Getting /status 09/07/23 06:11:32.567
    Sep  7 06:11:32.569: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/07/23 06:11:32.569
    Sep  7 06:11:32.576: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/07/23 06:11:32.576
    Sep  7 06:11:32.577: INFO: Observed &StatefulSet event: ADDED
    Sep  7 06:11:32.577: INFO: Found Statefulset ss in namespace statefulset-1522 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  7 06:11:32.577: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/07/23 06:11:32.577
    Sep  7 06:11:32.577: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  7 06:11:32.581: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/07/23 06:11:32.581
    Sep  7 06:11:32.582: INFO: Observed &StatefulSet event: ADDED
    Sep  7 06:11:32.582: INFO: Observed Statefulset ss in namespace statefulset-1522 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  7 06:11:32.582: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 06:11:32.582: INFO: Deleting all statefulset in ns statefulset-1522
    Sep  7 06:11:32.584: INFO: Scaling statefulset ss to 0
    Sep  7 06:11:42.594: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:11:42.596: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:42.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1522" for this suite. 09/07/23 06:11:42.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:42.609
Sep  7 06:11:42.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 06:11:42.61
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:42.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:42.619
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  7 06:11:42.620: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  7 06:11:42.625: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  7 06:11:47.628: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/07/23 06:11:47.628
Sep  7 06:11:47.628: INFO: Creating deployment "test-rolling-update-deployment"
Sep  7 06:11:47.633: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  7 06:11:47.635: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  7 06:11:49.641: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  7 06:11:49.642: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 06:11:49.647: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2077  ef4b1b90-5a89-422c-b341-eb526830c6d2 22505 1 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005415608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 06:11:47 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-07 06:11:49 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  7 06:11:49.649: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2077  f91568af-6716-4694-ae9b-4eef7ba2afd3 22495 1 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ef4b1b90-5a89-422c-b341-eb526830c6d2 0xc005415ad7 0xc005415ad8}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4b1b90-5a89-422c-b341-eb526830c6d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005415b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  7 06:11:49.649: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  7 06:11:49.649: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2077  38ce74bc-92b5-43bd-858e-8a81e17832a0 22504 2 2023-09-07 06:11:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ef4b1b90-5a89-422c-b341-eb526830c6d2 0xc0054159a7 0xc0054159a8}] [] [{e2e.test Update apps/v1 2023-09-07 06:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4b1b90-5a89-422c-b341-eb526830c6d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005415a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 06:11:49.651: INFO: Pod "test-rolling-update-deployment-7549d9f46d-pbwtc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-pbwtc test-rolling-update-deployment-7549d9f46d- deployment-2077  60ec86d8-770a-4c60-9107-bcb0e88e9e6a 22494 0 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d f91568af-6716-4694-ae9b-4eef7ba2afd3 0xc005415fe7 0xc005415fe8}] [] [{kube-controller-manager Update v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f91568af-6716-4694-ae9b-4eef7ba2afd3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95lm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95lm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.5,StartTime:2023-09-07 06:11:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:11:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fd336be989aee4e606b97bd24fb7b4e30c7553ef3f9a22d8f1a960b74120399b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:49.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2077" for this suite. 09/07/23 06:11:49.652
------------------------------
â€¢ [SLOW TEST] [7.047 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:42.609
    Sep  7 06:11:42.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 06:11:42.61
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:42.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:42.619
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  7 06:11:42.620: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  7 06:11:42.625: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  7 06:11:47.628: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/07/23 06:11:47.628
    Sep  7 06:11:47.628: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  7 06:11:47.633: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  7 06:11:47.635: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep  7 06:11:49.641: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  7 06:11:49.642: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 06:11:49.647: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2077  ef4b1b90-5a89-422c-b341-eb526830c6d2 22505 1 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005415608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 06:11:47 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-07 06:11:49 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  7 06:11:49.649: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-2077  f91568af-6716-4694-ae9b-4eef7ba2afd3 22495 1 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ef4b1b90-5a89-422c-b341-eb526830c6d2 0xc005415ad7 0xc005415ad8}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4b1b90-5a89-422c-b341-eb526830c6d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005415b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 06:11:49.649: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  7 06:11:49.649: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2077  38ce74bc-92b5-43bd-858e-8a81e17832a0 22504 2 2023-09-07 06:11:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ef4b1b90-5a89-422c-b341-eb526830c6d2 0xc0054159a7 0xc0054159a8}] [] [{e2e.test Update apps/v1 2023-09-07 06:11:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4b1b90-5a89-422c-b341-eb526830c6d2\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005415a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 06:11:49.651: INFO: Pod "test-rolling-update-deployment-7549d9f46d-pbwtc" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-pbwtc test-rolling-update-deployment-7549d9f46d- deployment-2077  60ec86d8-770a-4c60-9107-bcb0e88e9e6a 22494 0 2023-09-07 06:11:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d f91568af-6716-4694-ae9b-4eef7ba2afd3 0xc005415fe7 0xc005415fe8}] [] [{kube-controller-manager Update v1 2023-09-07 06:11:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f91568af-6716-4694-ae9b-4eef7ba2afd3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:11:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95lm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95lm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:11:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.5,StartTime:2023-09-07 06:11:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:11:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://fd336be989aee4e606b97bd24fb7b4e30c7553ef3f9a22d8f1a960b74120399b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:49.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2077" for this suite. 09/07/23 06:11:49.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:49.657
Sep  7 06:11:49.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:11:49.657
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:49.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:49.668
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:11:49.669
Sep  7 06:11:49.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e" in namespace "projected-2325" to be "Succeeded or Failed"
Sep  7 06:11:49.677: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29993ms
Sep  7 06:11:51.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004016082s
Sep  7 06:11:53.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004449983s
STEP: Saw pod success 09/07/23 06:11:53.68
Sep  7 06:11:53.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e" satisfied condition "Succeeded or Failed"
Sep  7 06:11:53.682: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e container client-container: <nil>
STEP: delete the pod 09/07/23 06:11:53.685
Sep  7 06:11:53.694: INFO: Waiting for pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e to disappear
Sep  7 06:11:53.696: INFO: Pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:53.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2325" for this suite. 09/07/23 06:11:53.705
------------------------------
â€¢ [4.052 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:49.657
    Sep  7 06:11:49.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:11:49.657
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:49.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:49.668
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:11:49.669
    Sep  7 06:11:49.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e" in namespace "projected-2325" to be "Succeeded or Failed"
    Sep  7 06:11:49.677: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29993ms
    Sep  7 06:11:51.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004016082s
    Sep  7 06:11:53.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004449983s
    STEP: Saw pod success 09/07/23 06:11:53.68
    Sep  7 06:11:53.680: INFO: Pod "downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e" satisfied condition "Succeeded or Failed"
    Sep  7 06:11:53.682: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e container client-container: <nil>
    STEP: delete the pod 09/07/23 06:11:53.685
    Sep  7 06:11:53.694: INFO: Waiting for pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e to disappear
    Sep  7 06:11:53.696: INFO: Pod downwardapi-volume-6edfe0b3-90f3-46b0-80e2-150a2ba82f4e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:53.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2325" for this suite. 09/07/23 06:11:53.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:53.711
Sep  7 06:11:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 06:11:53.712
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:53.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:53.722
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/07/23 06:11:53.723
STEP: When the matched label of one of its pods change 09/07/23 06:11:53.726
Sep  7 06:11:53.727: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  7 06:11:58.731: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/07/23 06:11:58.736
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 06:11:59.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-309" for this suite. 09/07/23 06:11:59.741
------------------------------
â€¢ [SLOW TEST] [6.036 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:53.711
    Sep  7 06:11:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 06:11:53.712
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:53.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:53.722
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/07/23 06:11:53.723
    STEP: When the matched label of one of its pods change 09/07/23 06:11:53.726
    Sep  7 06:11:53.727: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  7 06:11:58.731: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/07/23 06:11:58.736
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:11:59.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-309" for this suite. 09/07/23 06:11:59.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:11:59.747
Sep  7 06:11:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 06:11:59.748
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:59.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:59.757
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/07/23 06:11:59.763
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:59.772
STEP: Creating a service in the namespace 09/07/23 06:11:59.773
STEP: Deleting the namespace 09/07/23 06:11:59.78
STEP: Waiting for the namespace to be removed. 09/07/23 06:11:59.783
STEP: Recreating the namespace 09/07/23 06:12:05.785
STEP: Verifying there is no service in the namespace 09/07/23 06:12:05.797
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:12:05.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-491" for this suite. 09/07/23 06:12:05.8
STEP: Destroying namespace "nsdeletetest-8" for this suite. 09/07/23 06:12:05.803
Sep  7 06:12:05.804: INFO: Namespace nsdeletetest-8 was already deleted
STEP: Destroying namespace "nsdeletetest-3352" for this suite. 09/07/23 06:12:05.804
------------------------------
â€¢ [SLOW TEST] [6.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:11:59.747
    Sep  7 06:11:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 06:11:59.748
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:59.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:11:59.757
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/07/23 06:11:59.763
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:11:59.772
    STEP: Creating a service in the namespace 09/07/23 06:11:59.773
    STEP: Deleting the namespace 09/07/23 06:11:59.78
    STEP: Waiting for the namespace to be removed. 09/07/23 06:11:59.783
    STEP: Recreating the namespace 09/07/23 06:12:05.785
    STEP: Verifying there is no service in the namespace 09/07/23 06:12:05.797
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:12:05.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-491" for this suite. 09/07/23 06:12:05.8
    STEP: Destroying namespace "nsdeletetest-8" for this suite. 09/07/23 06:12:05.803
    Sep  7 06:12:05.804: INFO: Namespace nsdeletetest-8 was already deleted
    STEP: Destroying namespace "nsdeletetest-3352" for this suite. 09/07/23 06:12:05.804
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:12:05.807
Sep  7 06:12:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:12:05.808
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:12:05.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:12:05.816
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/07/23 06:12:05.82
STEP: watching for Pod to be ready 09/07/23 06:12:05.823
Sep  7 06:12:05.824: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  7 06:12:05.828: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
Sep  7 06:12:05.834: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
Sep  7 06:12:07.169: INFO: Found Pod pod-test in namespace pods-9417 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/07/23 06:12:07.171
STEP: getting the Pod and ensuring that it's patched 09/07/23 06:12:07.178
STEP: replacing the Pod's status Ready condition to False 09/07/23 06:12:07.179
STEP: check the Pod again to ensure its Ready conditions are False 09/07/23 06:12:07.185
STEP: deleting the Pod via a Collection with a LabelSelector 09/07/23 06:12:07.185
STEP: watching for the Pod to be deleted 09/07/23 06:12:07.19
Sep  7 06:12:07.192: INFO: observed event type MODIFIED
Sep  7 06:12:09.172: INFO: observed event type MODIFIED
Sep  7 06:12:11.175: INFO: observed event type MODIFIED
Sep  7 06:12:11.180: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:12:11.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9417" for this suite. 09/07/23 06:12:11.187
------------------------------
â€¢ [SLOW TEST] [5.383 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:12:05.807
    Sep  7 06:12:05.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:12:05.808
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:12:05.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:12:05.816
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/07/23 06:12:05.82
    STEP: watching for Pod to be ready 09/07/23 06:12:05.823
    Sep  7 06:12:05.824: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  7 06:12:05.828: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
    Sep  7 06:12:05.834: INFO: observed Pod pod-test in namespace pods-9417 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
    Sep  7 06:12:07.169: INFO: Found Pod pod-test in namespace pods-9417 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:12:05 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/07/23 06:12:07.171
    STEP: getting the Pod and ensuring that it's patched 09/07/23 06:12:07.178
    STEP: replacing the Pod's status Ready condition to False 09/07/23 06:12:07.179
    STEP: check the Pod again to ensure its Ready conditions are False 09/07/23 06:12:07.185
    STEP: deleting the Pod via a Collection with a LabelSelector 09/07/23 06:12:07.185
    STEP: watching for the Pod to be deleted 09/07/23 06:12:07.19
    Sep  7 06:12:07.192: INFO: observed event type MODIFIED
    Sep  7 06:12:09.172: INFO: observed event type MODIFIED
    Sep  7 06:12:11.175: INFO: observed event type MODIFIED
    Sep  7 06:12:11.180: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:12:11.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9417" for this suite. 09/07/23 06:12:11.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:12:11.19
Sep  7 06:12:11.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 06:12:11.191
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:12:11.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:12:11.201
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c in namespace container-probe-8181 09/07/23 06:12:11.202
Sep  7 06:12:11.208: INFO: Waiting up to 5m0s for pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c" in namespace "container-probe-8181" to be "not pending"
Sep  7 06:12:11.209: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.27607ms
Sep  7 06:12:13.212: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004344488s
Sep  7 06:12:13.212: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c" satisfied condition "not pending"
Sep  7 06:12:13.212: INFO: Started pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c in namespace container-probe-8181
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:12:13.212
Sep  7 06:12:13.214: INFO: Initial restart count of pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c is 0
STEP: deleting the pod 09/07/23 06:16:13.589
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 06:16:13.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8181" for this suite. 09/07/23 06:16:13.601
------------------------------
â€¢ [SLOW TEST] [242.415 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:12:11.19
    Sep  7 06:12:11.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 06:12:11.191
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:12:11.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:12:11.201
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c in namespace container-probe-8181 09/07/23 06:12:11.202
    Sep  7 06:12:11.208: INFO: Waiting up to 5m0s for pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c" in namespace "container-probe-8181" to be "not pending"
    Sep  7 06:12:11.209: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.27607ms
    Sep  7 06:12:13.212: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004344488s
    Sep  7 06:12:13.212: INFO: Pod "liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c" satisfied condition "not pending"
    Sep  7 06:12:13.212: INFO: Started pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c in namespace container-probe-8181
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:12:13.212
    Sep  7 06:12:13.214: INFO: Initial restart count of pod liveness-0b42a1ee-971d-4526-a83f-a877d7d3478c is 0
    STEP: deleting the pod 09/07/23 06:16:13.589
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:16:13.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8181" for this suite. 09/07/23 06:16:13.601
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:16:13.606
Sep  7 06:16:13.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:16:13.606
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:13.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:13.615
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:16:13.627
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:16:13.962
STEP: Deploying the webhook pod 09/07/23 06:16:13.968
STEP: Wait for the deployment to be ready 09/07/23 06:16:13.978
Sep  7 06:16:13.982: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/07/23 06:16:15.989
STEP: Verifying the service has paired with the endpoint 09/07/23 06:16:15.996
Sep  7 06:16:16.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  7 06:16:16.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6724-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 06:16:17.507
STEP: Creating a custom resource while v1 is storage version 09/07/23 06:16:17.518
STEP: Patching Custom Resource Definition to set v2 as storage 09/07/23 06:16:19.528
STEP: Patching the custom resource while v2 is storage version 09/07/23 06:16:19.542
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:16:20.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9610" for this suite. 09/07/23 06:16:20.111
STEP: Destroying namespace "webhook-9610-markers" for this suite. 09/07/23 06:16:20.117
------------------------------
â€¢ [SLOW TEST] [6.516 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:16:13.606
    Sep  7 06:16:13.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:16:13.606
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:13.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:13.615
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:16:13.627
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:16:13.962
    STEP: Deploying the webhook pod 09/07/23 06:16:13.968
    STEP: Wait for the deployment to be ready 09/07/23 06:16:13.978
    Sep  7 06:16:13.982: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/07/23 06:16:15.989
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:16:15.996
    Sep  7 06:16:16.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  7 06:16:16.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6724-crds.webhook.example.com via the AdmissionRegistration API 09/07/23 06:16:17.507
    STEP: Creating a custom resource while v1 is storage version 09/07/23 06:16:17.518
    STEP: Patching Custom Resource Definition to set v2 as storage 09/07/23 06:16:19.528
    STEP: Patching the custom resource while v2 is storage version 09/07/23 06:16:19.542
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:16:20.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9610" for this suite. 09/07/23 06:16:20.111
    STEP: Destroying namespace "webhook-9610-markers" for this suite. 09/07/23 06:16:20.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:16:20.124
Sep  7 06:16:20.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svc-latency 09/07/23 06:16:20.125
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:20.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:20.135
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  7 06:16:20.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: creating replication controller svc-latency-rc in namespace svc-latency-575 09/07/23 06:16:20.138
I0907 06:16:20.142836      29 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-575, replica count: 1
I0907 06:16:21.194043      29 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0907 06:16:22.194799      29 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 06:16:22.304: INFO: Created: latency-svc-kvf85
Sep  7 06:16:22.307: INFO: Got endpoints: latency-svc-kvf85 [12.407651ms]
Sep  7 06:16:22.316: INFO: Created: latency-svc-ltlp7
Sep  7 06:16:22.321: INFO: Got endpoints: latency-svc-ltlp7 [13.3121ms]
Sep  7 06:16:22.322: INFO: Created: latency-svc-qfzjd
Sep  7 06:16:22.326: INFO: Got endpoints: latency-svc-qfzjd [18.617041ms]
Sep  7 06:16:22.327: INFO: Created: latency-svc-4tjx8
Sep  7 06:16:22.330: INFO: Got endpoints: latency-svc-4tjx8 [22.949711ms]
Sep  7 06:16:22.332: INFO: Created: latency-svc-ncwfl
Sep  7 06:16:22.335: INFO: Got endpoints: latency-svc-ncwfl [27.964782ms]
Sep  7 06:16:22.336: INFO: Created: latency-svc-tldvt
Sep  7 06:16:22.340: INFO: Got endpoints: latency-svc-tldvt [32.861182ms]
Sep  7 06:16:22.341: INFO: Created: latency-svc-2gv2s
Sep  7 06:16:22.344: INFO: Got endpoints: latency-svc-2gv2s [36.854482ms]
Sep  7 06:16:22.345: INFO: Created: latency-svc-f4nvr
Sep  7 06:16:22.350: INFO: Got endpoints: latency-svc-f4nvr [42.025223ms]
Sep  7 06:16:22.350: INFO: Created: latency-svc-cccsn
Sep  7 06:16:22.355: INFO: Got endpoints: latency-svc-cccsn [47.170113ms]
Sep  7 06:16:22.355: INFO: Created: latency-svc-ld6dv
Sep  7 06:16:22.358: INFO: Got endpoints: latency-svc-ld6dv [50.397653ms]
Sep  7 06:16:22.359: INFO: Created: latency-svc-bzpwh
Sep  7 06:16:22.363: INFO: Got endpoints: latency-svc-bzpwh [55.066504ms]
Sep  7 06:16:22.364: INFO: Created: latency-svc-lxhgc
Sep  7 06:16:22.368: INFO: Got endpoints: latency-svc-lxhgc [60.215335ms]
Sep  7 06:16:22.369: INFO: Created: latency-svc-6pcsv
Sep  7 06:16:22.371: INFO: Got endpoints: latency-svc-6pcsv [63.359575ms]
Sep  7 06:16:22.372: INFO: Created: latency-svc-bhbcj
Sep  7 06:16:22.377: INFO: Got endpoints: latency-svc-bhbcj [69.301474ms]
Sep  7 06:16:22.378: INFO: Created: latency-svc-bw455
Sep  7 06:16:22.382: INFO: Got endpoints: latency-svc-bw455 [74.094425ms]
Sep  7 06:16:22.383: INFO: Created: latency-svc-gkt8l
Sep  7 06:16:22.385: INFO: Got endpoints: latency-svc-gkt8l [77.260656ms]
Sep  7 06:16:22.386: INFO: Created: latency-svc-xfdjj
Sep  7 06:16:22.390: INFO: Got endpoints: latency-svc-xfdjj [69.558426ms]
Sep  7 06:16:22.391: INFO: Created: latency-svc-9cdgd
Sep  7 06:16:22.394: INFO: Got endpoints: latency-svc-9cdgd [68.429795ms]
Sep  7 06:16:22.396: INFO: Created: latency-svc-5gjdf
Sep  7 06:16:22.399: INFO: Got endpoints: latency-svc-5gjdf [68.277446ms]
Sep  7 06:16:22.400: INFO: Created: latency-svc-5bzff
Sep  7 06:16:22.403: INFO: Got endpoints: latency-svc-5bzff [67.781035ms]
Sep  7 06:16:22.404: INFO: Created: latency-svc-d7rcp
Sep  7 06:16:22.407: INFO: Got endpoints: latency-svc-d7rcp [66.175385ms]
Sep  7 06:16:22.409: INFO: Created: latency-svc-gbr82
Sep  7 06:16:22.411: INFO: Got endpoints: latency-svc-gbr82 [66.934366ms]
Sep  7 06:16:22.412: INFO: Created: latency-svc-24w8r
Sep  7 06:16:22.419: INFO: Got endpoints: latency-svc-24w8r [68.933315ms]
Sep  7 06:16:22.419: INFO: Created: latency-svc-kmmg7
Sep  7 06:16:22.422: INFO: Got endpoints: latency-svc-kmmg7 [67.560285ms]
Sep  7 06:16:22.425: INFO: Created: latency-svc-r7294
Sep  7 06:16:22.427: INFO: Got endpoints: latency-svc-r7294 [69.368286ms]
Sep  7 06:16:22.428: INFO: Created: latency-svc-8ffnh
Sep  7 06:16:22.433: INFO: Got endpoints: latency-svc-8ffnh [69.804585ms]
Sep  7 06:16:22.434: INFO: Created: latency-svc-h6wdh
Sep  7 06:16:22.439: INFO: Got endpoints: latency-svc-h6wdh [70.723186ms]
Sep  7 06:16:22.439: INFO: Created: latency-svc-s64n5
Sep  7 06:16:22.443: INFO: Got endpoints: latency-svc-s64n5 [71.486376ms]
Sep  7 06:16:22.443: INFO: Created: latency-svc-bs7zd
Sep  7 06:16:22.447: INFO: Got endpoints: latency-svc-bs7zd [70.525425ms]
Sep  7 06:16:22.448: INFO: Created: latency-svc-m8twq
Sep  7 06:16:22.453: INFO: Got endpoints: latency-svc-m8twq [70.974736ms]
Sep  7 06:16:22.454: INFO: Created: latency-svc-qtp5l
Sep  7 06:16:22.456: INFO: Got endpoints: latency-svc-qtp5l [70.909275ms]
Sep  7 06:16:22.457: INFO: Created: latency-svc-9lj68
Sep  7 06:16:22.461: INFO: Got endpoints: latency-svc-9lj68 [70.183645ms]
Sep  7 06:16:22.462: INFO: Created: latency-svc-fsvjc
Sep  7 06:16:22.464: INFO: Got endpoints: latency-svc-fsvjc [69.518296ms]
Sep  7 06:16:22.466: INFO: Created: latency-svc-wgc46
Sep  7 06:16:22.470: INFO: Created: latency-svc-drx6z
Sep  7 06:16:22.473: INFO: Created: latency-svc-ts5kz
Sep  7 06:16:22.478: INFO: Created: latency-svc-km9bg
Sep  7 06:16:22.481: INFO: Created: latency-svc-zpg28
Sep  7 06:16:22.484: INFO: Created: latency-svc-qx2qz
Sep  7 06:16:22.487: INFO: Created: latency-svc-l9rgk
Sep  7 06:16:22.490: INFO: Created: latency-svc-2cqf9
Sep  7 06:16:22.495: INFO: Created: latency-svc-fqkwc
Sep  7 06:16:22.498: INFO: Created: latency-svc-6p9j4
Sep  7 06:16:22.502: INFO: Created: latency-svc-qb8qm
Sep  7 06:16:22.505: INFO: Created: latency-svc-2jwc4
Sep  7 06:16:22.509: INFO: Got endpoints: latency-svc-wgc46 [109.788419ms]
Sep  7 06:16:22.509: INFO: Created: latency-svc-hllbm
Sep  7 06:16:22.513: INFO: Created: latency-svc-8r9dt
Sep  7 06:16:22.516: INFO: Created: latency-svc-spdpl
Sep  7 06:16:22.525: INFO: Created: latency-svc-st4xh
Sep  7 06:16:22.558: INFO: Got endpoints: latency-svc-drx6z [154.710502ms]
Sep  7 06:16:22.566: INFO: Created: latency-svc-h6fld
Sep  7 06:16:22.610: INFO: Got endpoints: latency-svc-ts5kz [203.601936ms]
Sep  7 06:16:22.617: INFO: Created: latency-svc-997bh
Sep  7 06:16:22.660: INFO: Got endpoints: latency-svc-km9bg [248.822139ms]
Sep  7 06:16:22.667: INFO: Created: latency-svc-t5t4n
Sep  7 06:16:22.708: INFO: Got endpoints: latency-svc-zpg28 [288.954063ms]
Sep  7 06:16:22.716: INFO: Created: latency-svc-t8pvr
Sep  7 06:16:22.759: INFO: Got endpoints: latency-svc-qx2qz [336.557605ms]
Sep  7 06:16:22.767: INFO: Created: latency-svc-pwkv5
Sep  7 06:16:22.810: INFO: Got endpoints: latency-svc-l9rgk [382.472789ms]
Sep  7 06:16:22.817: INFO: Created: latency-svc-qdjz7
Sep  7 06:16:22.857: INFO: Got endpoints: latency-svc-2cqf9 [424.818923ms]
Sep  7 06:16:22.865: INFO: Created: latency-svc-jpsgf
Sep  7 06:16:22.907: INFO: Got endpoints: latency-svc-fqkwc [468.710587ms]
Sep  7 06:16:22.916: INFO: Created: latency-svc-6mrb7
Sep  7 06:16:22.957: INFO: Got endpoints: latency-svc-6p9j4 [514.46951ms]
Sep  7 06:16:22.964: INFO: Created: latency-svc-c4plc
Sep  7 06:16:23.010: INFO: Got endpoints: latency-svc-qb8qm [562.406894ms]
Sep  7 06:16:23.017: INFO: Created: latency-svc-8xwbs
Sep  7 06:16:23.057: INFO: Got endpoints: latency-svc-2jwc4 [603.957756ms]
Sep  7 06:16:23.063: INFO: Created: latency-svc-75dtr
Sep  7 06:16:23.110: INFO: Got endpoints: latency-svc-hllbm [653.87247ms]
Sep  7 06:16:23.115: INFO: Created: latency-svc-72zrj
Sep  7 06:16:23.157: INFO: Got endpoints: latency-svc-8r9dt [696.411943ms]
Sep  7 06:16:23.164: INFO: Created: latency-svc-2zqng
Sep  7 06:16:23.207: INFO: Got endpoints: latency-svc-spdpl [742.740977ms]
Sep  7 06:16:23.215: INFO: Created: latency-svc-xrghw
Sep  7 06:16:23.257: INFO: Got endpoints: latency-svc-st4xh [748.471428ms]
Sep  7 06:16:23.264: INFO: Created: latency-svc-lpnvw
Sep  7 06:16:23.309: INFO: Got endpoints: latency-svc-h6fld [750.531138ms]
Sep  7 06:16:23.315: INFO: Created: latency-svc-kbqz5
Sep  7 06:16:23.358: INFO: Got endpoints: latency-svc-997bh [747.495628ms]
Sep  7 06:16:23.364: INFO: Created: latency-svc-5xxcl
Sep  7 06:16:23.407: INFO: Got endpoints: latency-svc-t5t4n [746.569097ms]
Sep  7 06:16:23.415: INFO: Created: latency-svc-77zbj
Sep  7 06:16:23.458: INFO: Got endpoints: latency-svc-t8pvr [750.513108ms]
Sep  7 06:16:23.466: INFO: Created: latency-svc-m8hzd
Sep  7 06:16:23.509: INFO: Got endpoints: latency-svc-pwkv5 [750.396548ms]
Sep  7 06:16:23.516: INFO: Created: latency-svc-jwwsz
Sep  7 06:16:23.557: INFO: Got endpoints: latency-svc-qdjz7 [747.227368ms]
Sep  7 06:16:23.564: INFO: Created: latency-svc-6zc78
Sep  7 06:16:23.607: INFO: Got endpoints: latency-svc-jpsgf [749.805668ms]
Sep  7 06:16:23.614: INFO: Created: latency-svc-p68vm
Sep  7 06:16:23.658: INFO: Got endpoints: latency-svc-6mrb7 [750.530648ms]
Sep  7 06:16:23.665: INFO: Created: latency-svc-967jp
Sep  7 06:16:23.709: INFO: Got endpoints: latency-svc-c4plc [751.511677ms]
Sep  7 06:16:23.715: INFO: Created: latency-svc-d929w
Sep  7 06:16:23.759: INFO: Got endpoints: latency-svc-8xwbs [748.712967ms]
Sep  7 06:16:23.765: INFO: Created: latency-svc-56s8s
Sep  7 06:16:23.808: INFO: Got endpoints: latency-svc-75dtr [750.764718ms]
Sep  7 06:16:23.814: INFO: Created: latency-svc-7fzrn
Sep  7 06:16:23.857: INFO: Got endpoints: latency-svc-72zrj [746.991718ms]
Sep  7 06:16:23.864: INFO: Created: latency-svc-mdnqp
Sep  7 06:16:23.911: INFO: Got endpoints: latency-svc-2zqng [753.554218ms]
Sep  7 06:16:23.917: INFO: Created: latency-svc-vbhr8
Sep  7 06:16:23.957: INFO: Got endpoints: latency-svc-xrghw [750.160288ms]
Sep  7 06:16:23.964: INFO: Created: latency-svc-tpkdq
Sep  7 06:16:24.008: INFO: Got endpoints: latency-svc-lpnvw [750.632557ms]
Sep  7 06:16:24.014: INFO: Created: latency-svc-fhcl4
Sep  7 06:16:24.057: INFO: Got endpoints: latency-svc-kbqz5 [748.181807ms]
Sep  7 06:16:24.066: INFO: Created: latency-svc-4f95t
Sep  7 06:16:24.110: INFO: Got endpoints: latency-svc-5xxcl [751.995748ms]
Sep  7 06:16:24.116: INFO: Created: latency-svc-zrrps
Sep  7 06:16:24.157: INFO: Got endpoints: latency-svc-77zbj [750.135868ms]
Sep  7 06:16:24.164: INFO: Created: latency-svc-t4db6
Sep  7 06:16:24.207: INFO: Got endpoints: latency-svc-m8hzd [749.030588ms]
Sep  7 06:16:24.214: INFO: Created: latency-svc-8qhs4
Sep  7 06:16:24.258: INFO: Got endpoints: latency-svc-jwwsz [748.380238ms]
Sep  7 06:16:24.264: INFO: Created: latency-svc-7vdhh
Sep  7 06:16:24.309: INFO: Got endpoints: latency-svc-6zc78 [751.998958ms]
Sep  7 06:16:24.316: INFO: Created: latency-svc-67f4s
Sep  7 06:16:24.358: INFO: Got endpoints: latency-svc-p68vm [750.643247ms]
Sep  7 06:16:24.366: INFO: Created: latency-svc-wsfl4
Sep  7 06:16:24.408: INFO: Got endpoints: latency-svc-967jp [750.309597ms]
Sep  7 06:16:24.415: INFO: Created: latency-svc-7vrf2
Sep  7 06:16:24.458: INFO: Got endpoints: latency-svc-d929w [749.331448ms]
Sep  7 06:16:24.467: INFO: Created: latency-svc-lc5xq
Sep  7 06:16:24.507: INFO: Got endpoints: latency-svc-56s8s [748.424048ms]
Sep  7 06:16:24.514: INFO: Created: latency-svc-cl647
Sep  7 06:16:24.557: INFO: Got endpoints: latency-svc-7fzrn [749.570928ms]
Sep  7 06:16:24.564: INFO: Created: latency-svc-txsgg
Sep  7 06:16:24.607: INFO: Got endpoints: latency-svc-mdnqp [750.488448ms]
Sep  7 06:16:24.615: INFO: Created: latency-svc-b2jpm
Sep  7 06:16:24.657: INFO: Got endpoints: latency-svc-vbhr8 [746.207167ms]
Sep  7 06:16:24.666: INFO: Created: latency-svc-rkw2v
Sep  7 06:16:24.707: INFO: Got endpoints: latency-svc-tpkdq [750.203287ms]
Sep  7 06:16:24.715: INFO: Created: latency-svc-97wr6
Sep  7 06:16:24.759: INFO: Got endpoints: latency-svc-fhcl4 [751.177107ms]
Sep  7 06:16:24.766: INFO: Created: latency-svc-fw8hh
Sep  7 06:16:24.810: INFO: Got endpoints: latency-svc-4f95t [752.722678ms]
Sep  7 06:16:24.816: INFO: Created: latency-svc-xdwfw
Sep  7 06:16:24.858: INFO: Got endpoints: latency-svc-zrrps [747.799098ms]
Sep  7 06:16:24.866: INFO: Created: latency-svc-49dx7
Sep  7 06:16:24.907: INFO: Got endpoints: latency-svc-t4db6 [750.141287ms]
Sep  7 06:16:24.915: INFO: Created: latency-svc-6wrrn
Sep  7 06:16:24.959: INFO: Got endpoints: latency-svc-8qhs4 [751.963307ms]
Sep  7 06:16:24.966: INFO: Created: latency-svc-8v4d7
Sep  7 06:16:25.010: INFO: Got endpoints: latency-svc-7vdhh [751.902998ms]
Sep  7 06:16:25.019: INFO: Created: latency-svc-k42gz
Sep  7 06:16:25.058: INFO: Got endpoints: latency-svc-67f4s [749.158737ms]
Sep  7 06:16:25.066: INFO: Created: latency-svc-vrj6j
Sep  7 06:16:25.108: INFO: Got endpoints: latency-svc-wsfl4 [750.334478ms]
Sep  7 06:16:25.116: INFO: Created: latency-svc-dkrz9
Sep  7 06:16:25.158: INFO: Got endpoints: latency-svc-7vrf2 [749.668378ms]
Sep  7 06:16:25.164: INFO: Created: latency-svc-qvlkc
Sep  7 06:16:25.210: INFO: Got endpoints: latency-svc-lc5xq [751.832808ms]
Sep  7 06:16:25.216: INFO: Created: latency-svc-82fmk
Sep  7 06:16:25.259: INFO: Got endpoints: latency-svc-cl647 [751.592287ms]
Sep  7 06:16:25.265: INFO: Created: latency-svc-sjj8c
Sep  7 06:16:25.310: INFO: Got endpoints: latency-svc-txsgg [752.380187ms]
Sep  7 06:16:25.316: INFO: Created: latency-svc-t48dt
Sep  7 06:16:25.358: INFO: Got endpoints: latency-svc-b2jpm [751.130337ms]
Sep  7 06:16:25.366: INFO: Created: latency-svc-xpxwk
Sep  7 06:16:25.408: INFO: Got endpoints: latency-svc-rkw2v [751.017738ms]
Sep  7 06:16:25.416: INFO: Created: latency-svc-8cvbn
Sep  7 06:16:25.458: INFO: Got endpoints: latency-svc-97wr6 [751.001288ms]
Sep  7 06:16:25.466: INFO: Created: latency-svc-wjdvk
Sep  7 06:16:25.509: INFO: Got endpoints: latency-svc-fw8hh [750.114428ms]
Sep  7 06:16:25.516: INFO: Created: latency-svc-cbqdv
Sep  7 06:16:25.557: INFO: Got endpoints: latency-svc-xdwfw [747.236877ms]
Sep  7 06:16:25.565: INFO: Created: latency-svc-hznhd
Sep  7 06:16:25.607: INFO: Got endpoints: latency-svc-49dx7 [749.263468ms]
Sep  7 06:16:25.614: INFO: Created: latency-svc-mdhsc
Sep  7 06:16:25.658: INFO: Got endpoints: latency-svc-6wrrn [750.641287ms]
Sep  7 06:16:25.666: INFO: Created: latency-svc-8gm29
Sep  7 06:16:25.707: INFO: Got endpoints: latency-svc-8v4d7 [747.729236ms]
Sep  7 06:16:25.714: INFO: Created: latency-svc-sksjv
Sep  7 06:16:25.758: INFO: Got endpoints: latency-svc-k42gz [748.298267ms]
Sep  7 06:16:25.765: INFO: Created: latency-svc-kkkdv
Sep  7 06:16:25.807: INFO: Got endpoints: latency-svc-vrj6j [748.576108ms]
Sep  7 06:16:25.814: INFO: Created: latency-svc-6x5td
Sep  7 06:16:25.858: INFO: Got endpoints: latency-svc-dkrz9 [749.815298ms]
Sep  7 06:16:25.866: INFO: Created: latency-svc-vnncg
Sep  7 06:16:25.909: INFO: Got endpoints: latency-svc-qvlkc [751.218417ms]
Sep  7 06:16:25.916: INFO: Created: latency-svc-mz6w8
Sep  7 06:16:25.959: INFO: Got endpoints: latency-svc-82fmk [749.506857ms]
Sep  7 06:16:25.967: INFO: Created: latency-svc-hsg7w
Sep  7 06:16:26.008: INFO: Got endpoints: latency-svc-sjj8c [749.552118ms]
Sep  7 06:16:26.016: INFO: Created: latency-svc-mcd7r
Sep  7 06:16:26.058: INFO: Got endpoints: latency-svc-t48dt [748.516648ms]
Sep  7 06:16:26.066: INFO: Created: latency-svc-z97w9
Sep  7 06:16:26.108: INFO: Got endpoints: latency-svc-xpxwk [749.618497ms]
Sep  7 06:16:26.117: INFO: Created: latency-svc-d92td
Sep  7 06:16:26.157: INFO: Got endpoints: latency-svc-8cvbn [749.095437ms]
Sep  7 06:16:26.164: INFO: Created: latency-svc-tb7rz
Sep  7 06:16:26.210: INFO: Got endpoints: latency-svc-wjdvk [752.023917ms]
Sep  7 06:16:26.218: INFO: Created: latency-svc-gprnk
Sep  7 06:16:26.257: INFO: Got endpoints: latency-svc-cbqdv [747.463147ms]
Sep  7 06:16:26.264: INFO: Created: latency-svc-bwwts
Sep  7 06:16:26.308: INFO: Got endpoints: latency-svc-hznhd [751.257498ms]
Sep  7 06:16:26.316: INFO: Created: latency-svc-vsrmv
Sep  7 06:16:26.358: INFO: Got endpoints: latency-svc-mdhsc [750.708978ms]
Sep  7 06:16:26.366: INFO: Created: latency-svc-khbgg
Sep  7 06:16:26.410: INFO: Got endpoints: latency-svc-8gm29 [751.701428ms]
Sep  7 06:16:26.416: INFO: Created: latency-svc-dxqnj
Sep  7 06:16:26.459: INFO: Got endpoints: latency-svc-sksjv [752.440507ms]
Sep  7 06:16:26.466: INFO: Created: latency-svc-gk2z9
Sep  7 06:16:26.509: INFO: Got endpoints: latency-svc-kkkdv [750.770837ms]
Sep  7 06:16:26.515: INFO: Created: latency-svc-7lvdl
Sep  7 06:16:26.558: INFO: Got endpoints: latency-svc-6x5td [750.883257ms]
Sep  7 06:16:26.566: INFO: Created: latency-svc-v7dng
Sep  7 06:16:26.608: INFO: Got endpoints: latency-svc-vnncg [750.037767ms]
Sep  7 06:16:26.615: INFO: Created: latency-svc-dj9gd
Sep  7 06:16:26.658: INFO: Got endpoints: latency-svc-mz6w8 [749.004907ms]
Sep  7 06:16:26.665: INFO: Created: latency-svc-vpjfj
Sep  7 06:16:26.707: INFO: Got endpoints: latency-svc-hsg7w [747.718728ms]
Sep  7 06:16:26.714: INFO: Created: latency-svc-8zh4r
Sep  7 06:16:26.757: INFO: Got endpoints: latency-svc-mcd7r [748.619127ms]
Sep  7 06:16:26.764: INFO: Created: latency-svc-mm9f6
Sep  7 06:16:26.807: INFO: Got endpoints: latency-svc-z97w9 [749.248277ms]
Sep  7 06:16:26.818: INFO: Created: latency-svc-xmbcz
Sep  7 06:16:26.857: INFO: Got endpoints: latency-svc-d92td [749.332078ms]
Sep  7 06:16:26.864: INFO: Created: latency-svc-t4pf7
Sep  7 06:16:26.909: INFO: Got endpoints: latency-svc-tb7rz [752.216568ms]
Sep  7 06:16:26.916: INFO: Created: latency-svc-mgck5
Sep  7 06:16:26.957: INFO: Got endpoints: latency-svc-gprnk [746.196487ms]
Sep  7 06:16:26.963: INFO: Created: latency-svc-tlsm9
Sep  7 06:16:27.009: INFO: Got endpoints: latency-svc-bwwts [752.462937ms]
Sep  7 06:16:27.016: INFO: Created: latency-svc-829g4
Sep  7 06:16:27.058: INFO: Got endpoints: latency-svc-vsrmv [749.825357ms]
Sep  7 06:16:27.065: INFO: Created: latency-svc-pbxhg
Sep  7 06:16:27.109: INFO: Got endpoints: latency-svc-khbgg [751.337958ms]
Sep  7 06:16:27.115: INFO: Created: latency-svc-r9hv2
Sep  7 06:16:27.158: INFO: Got endpoints: latency-svc-dxqnj [747.845756ms]
Sep  7 06:16:27.164: INFO: Created: latency-svc-nc9t9
Sep  7 06:16:27.209: INFO: Got endpoints: latency-svc-gk2z9 [749.262907ms]
Sep  7 06:16:27.215: INFO: Created: latency-svc-fc8f2
Sep  7 06:16:27.259: INFO: Got endpoints: latency-svc-7lvdl [750.164797ms]
Sep  7 06:16:27.266: INFO: Created: latency-svc-lfnjb
Sep  7 06:16:27.310: INFO: Got endpoints: latency-svc-v7dng [751.897477ms]
Sep  7 06:16:27.317: INFO: Created: latency-svc-5bv6r
Sep  7 06:16:27.357: INFO: Got endpoints: latency-svc-dj9gd [749.196407ms]
Sep  7 06:16:27.366: INFO: Created: latency-svc-tg4sz
Sep  7 06:16:27.408: INFO: Got endpoints: latency-svc-vpjfj [749.463637ms]
Sep  7 06:16:27.416: INFO: Created: latency-svc-dlrbt
Sep  7 06:16:27.457: INFO: Got endpoints: latency-svc-8zh4r [749.833647ms]
Sep  7 06:16:27.464: INFO: Created: latency-svc-4xs5q
Sep  7 06:16:27.510: INFO: Got endpoints: latency-svc-mm9f6 [752.574187ms]
Sep  7 06:16:27.516: INFO: Created: latency-svc-4vtt6
Sep  7 06:16:27.558: INFO: Got endpoints: latency-svc-xmbcz [750.232277ms]
Sep  7 06:16:27.565: INFO: Created: latency-svc-pfkdd
Sep  7 06:16:27.610: INFO: Got endpoints: latency-svc-t4pf7 [752.583727ms]
Sep  7 06:16:27.617: INFO: Created: latency-svc-xd77q
Sep  7 06:16:27.657: INFO: Got endpoints: latency-svc-mgck5 [747.711667ms]
Sep  7 06:16:27.665: INFO: Created: latency-svc-2fftz
Sep  7 06:16:27.708: INFO: Got endpoints: latency-svc-tlsm9 [751.115627ms]
Sep  7 06:16:27.714: INFO: Created: latency-svc-9wqq8
Sep  7 06:16:27.759: INFO: Got endpoints: latency-svc-829g4 [749.740677ms]
Sep  7 06:16:27.766: INFO: Created: latency-svc-jrm4c
Sep  7 06:16:27.809: INFO: Got endpoints: latency-svc-pbxhg [751.336607ms]
Sep  7 06:16:27.815: INFO: Created: latency-svc-8pmtz
Sep  7 06:16:27.858: INFO: Got endpoints: latency-svc-r9hv2 [748.540388ms]
Sep  7 06:16:27.864: INFO: Created: latency-svc-29qwt
Sep  7 06:16:27.908: INFO: Got endpoints: latency-svc-nc9t9 [750.080668ms]
Sep  7 06:16:27.916: INFO: Created: latency-svc-kdmhh
Sep  7 06:16:27.958: INFO: Got endpoints: latency-svc-fc8f2 [748.851948ms]
Sep  7 06:16:27.964: INFO: Created: latency-svc-wwcqq
Sep  7 06:16:28.010: INFO: Got endpoints: latency-svc-lfnjb [750.873307ms]
Sep  7 06:16:28.016: INFO: Created: latency-svc-dbmhh
Sep  7 06:16:28.059: INFO: Got endpoints: latency-svc-5bv6r [749.371347ms]
Sep  7 06:16:28.066: INFO: Created: latency-svc-wlf5r
Sep  7 06:16:28.108: INFO: Got endpoints: latency-svc-tg4sz [750.150627ms]
Sep  7 06:16:28.117: INFO: Created: latency-svc-nd2gz
Sep  7 06:16:28.158: INFO: Got endpoints: latency-svc-dlrbt [750.161397ms]
Sep  7 06:16:28.165: INFO: Created: latency-svc-zct5s
Sep  7 06:16:28.210: INFO: Got endpoints: latency-svc-4xs5q [752.755638ms]
Sep  7 06:16:28.217: INFO: Created: latency-svc-ctbgf
Sep  7 06:16:28.257: INFO: Got endpoints: latency-svc-4vtt6 [747.250997ms]
Sep  7 06:16:28.265: INFO: Created: latency-svc-mgl2r
Sep  7 06:16:28.308: INFO: Got endpoints: latency-svc-pfkdd [750.130727ms]
Sep  7 06:16:28.315: INFO: Created: latency-svc-npx7h
Sep  7 06:16:28.358: INFO: Got endpoints: latency-svc-xd77q [748.030047ms]
Sep  7 06:16:28.367: INFO: Created: latency-svc-zzx8q
Sep  7 06:16:28.408: INFO: Got endpoints: latency-svc-2fftz [751.192896ms]
Sep  7 06:16:28.415: INFO: Created: latency-svc-pskpc
Sep  7 06:16:28.460: INFO: Got endpoints: latency-svc-9wqq8 [752.108568ms]
Sep  7 06:16:28.467: INFO: Created: latency-svc-4w4tt
Sep  7 06:16:28.510: INFO: Got endpoints: latency-svc-jrm4c [751.468927ms]
Sep  7 06:16:28.517: INFO: Created: latency-svc-vrcvm
Sep  7 06:16:28.558: INFO: Got endpoints: latency-svc-8pmtz [748.373417ms]
Sep  7 06:16:28.565: INFO: Created: latency-svc-mfqcv
Sep  7 06:16:28.608: INFO: Got endpoints: latency-svc-29qwt [750.292967ms]
Sep  7 06:16:28.616: INFO: Created: latency-svc-fwd5v
Sep  7 06:16:28.658: INFO: Got endpoints: latency-svc-kdmhh [750.651257ms]
Sep  7 06:16:28.665: INFO: Created: latency-svc-9nj4t
Sep  7 06:16:28.711: INFO: Got endpoints: latency-svc-wwcqq [752.994677ms]
Sep  7 06:16:28.717: INFO: Created: latency-svc-c9p9v
Sep  7 06:16:28.757: INFO: Got endpoints: latency-svc-dbmhh [746.906557ms]
Sep  7 06:16:28.764: INFO: Created: latency-svc-h5w2n
Sep  7 06:16:28.807: INFO: Got endpoints: latency-svc-wlf5r [747.783647ms]
Sep  7 06:16:28.815: INFO: Created: latency-svc-f44v7
Sep  7 06:16:28.858: INFO: Got endpoints: latency-svc-nd2gz [750.288557ms]
Sep  7 06:16:28.865: INFO: Created: latency-svc-l28zw
Sep  7 06:16:28.910: INFO: Got endpoints: latency-svc-zct5s [751.876297ms]
Sep  7 06:16:28.917: INFO: Created: latency-svc-xn2ss
Sep  7 06:16:28.957: INFO: Got endpoints: latency-svc-ctbgf [747.655906ms]
Sep  7 06:16:28.965: INFO: Created: latency-svc-spww2
Sep  7 06:16:29.007: INFO: Got endpoints: latency-svc-mgl2r [750.465797ms]
Sep  7 06:16:29.015: INFO: Created: latency-svc-dnk9w
Sep  7 06:16:29.058: INFO: Got endpoints: latency-svc-npx7h [750.291317ms]
Sep  7 06:16:29.066: INFO: Created: latency-svc-hnsxm
Sep  7 06:16:29.107: INFO: Got endpoints: latency-svc-zzx8q [748.713847ms]
Sep  7 06:16:29.114: INFO: Created: latency-svc-qtt2m
Sep  7 06:16:29.157: INFO: Got endpoints: latency-svc-pskpc [748.659226ms]
Sep  7 06:16:29.165: INFO: Created: latency-svc-swrvm
Sep  7 06:16:29.208: INFO: Got endpoints: latency-svc-4w4tt [747.869456ms]
Sep  7 06:16:29.215: INFO: Created: latency-svc-hmzgg
Sep  7 06:16:29.262: INFO: Got endpoints: latency-svc-vrcvm [750.963697ms]
Sep  7 06:16:29.268: INFO: Created: latency-svc-bnw86
Sep  7 06:16:29.307: INFO: Got endpoints: latency-svc-mfqcv [749.043297ms]
Sep  7 06:16:29.316: INFO: Created: latency-svc-qxd8s
Sep  7 06:16:29.359: INFO: Got endpoints: latency-svc-fwd5v [750.569447ms]
Sep  7 06:16:29.367: INFO: Created: latency-svc-vttnv
Sep  7 06:16:29.407: INFO: Got endpoints: latency-svc-9nj4t [749.071056ms]
Sep  7 06:16:29.415: INFO: Created: latency-svc-s2gr9
Sep  7 06:16:29.463: INFO: Got endpoints: latency-svc-c9p9v [752.070447ms]
Sep  7 06:16:29.472: INFO: Created: latency-svc-rswdb
Sep  7 06:16:29.508: INFO: Got endpoints: latency-svc-h5w2n [751.326838ms]
Sep  7 06:16:29.515: INFO: Created: latency-svc-n5969
Sep  7 06:16:29.557: INFO: Got endpoints: latency-svc-f44v7 [749.888577ms]
Sep  7 06:16:29.564: INFO: Created: latency-svc-76scx
Sep  7 06:16:29.608: INFO: Got endpoints: latency-svc-l28zw [750.440847ms]
Sep  7 06:16:29.617: INFO: Created: latency-svc-hmhpn
Sep  7 06:16:29.658: INFO: Got endpoints: latency-svc-xn2ss [748.442427ms]
Sep  7 06:16:29.668: INFO: Created: latency-svc-zg7mr
Sep  7 06:16:29.710: INFO: Got endpoints: latency-svc-spww2 [752.306798ms]
Sep  7 06:16:29.717: INFO: Created: latency-svc-vdbts
Sep  7 06:16:29.757: INFO: Got endpoints: latency-svc-dnk9w [749.922027ms]
Sep  7 06:16:29.764: INFO: Created: latency-svc-sq5rr
Sep  7 06:16:29.808: INFO: Got endpoints: latency-svc-hnsxm [749.736467ms]
Sep  7 06:16:29.816: INFO: Created: latency-svc-md7fk
Sep  7 06:16:29.858: INFO: Got endpoints: latency-svc-qtt2m [750.841547ms]
Sep  7 06:16:29.864: INFO: Created: latency-svc-vm9bq
Sep  7 06:16:29.910: INFO: Got endpoints: latency-svc-swrvm [752.586097ms]
Sep  7 06:16:29.917: INFO: Created: latency-svc-jpvgw
Sep  7 06:16:29.960: INFO: Got endpoints: latency-svc-hmzgg [751.759768ms]
Sep  7 06:16:29.967: INFO: Created: latency-svc-rczw5
Sep  7 06:16:30.009: INFO: Got endpoints: latency-svc-bnw86 [747.202876ms]
Sep  7 06:16:30.017: INFO: Created: latency-svc-qlhsg
Sep  7 06:16:30.058: INFO: Got endpoints: latency-svc-qxd8s [751.333997ms]
Sep  7 06:16:30.066: INFO: Created: latency-svc-wrnwk
Sep  7 06:16:30.107: INFO: Got endpoints: latency-svc-vttnv [748.007366ms]
Sep  7 06:16:30.113: INFO: Created: latency-svc-8mh24
Sep  7 06:16:30.160: INFO: Got endpoints: latency-svc-s2gr9 [752.015357ms]
Sep  7 06:16:30.207: INFO: Got endpoints: latency-svc-rswdb [744.305287ms]
Sep  7 06:16:30.259: INFO: Got endpoints: latency-svc-n5969 [750.775236ms]
Sep  7 06:16:30.308: INFO: Got endpoints: latency-svc-76scx [750.754807ms]
Sep  7 06:16:30.360: INFO: Got endpoints: latency-svc-hmhpn [751.401837ms]
Sep  7 06:16:30.410: INFO: Got endpoints: latency-svc-zg7mr [751.137356ms]
Sep  7 06:16:30.458: INFO: Got endpoints: latency-svc-vdbts [747.993497ms]
Sep  7 06:16:30.508: INFO: Got endpoints: latency-svc-sq5rr [750.701917ms]
Sep  7 06:16:30.559: INFO: Got endpoints: latency-svc-md7fk [750.680016ms]
Sep  7 06:16:30.608: INFO: Got endpoints: latency-svc-vm9bq [750.405436ms]
Sep  7 06:16:30.658: INFO: Got endpoints: latency-svc-jpvgw [748.632666ms]
Sep  7 06:16:30.710: INFO: Got endpoints: latency-svc-rczw5 [750.354957ms]
Sep  7 06:16:30.758: INFO: Got endpoints: latency-svc-qlhsg [749.672427ms]
Sep  7 06:16:30.810: INFO: Got endpoints: latency-svc-wrnwk [751.908017ms]
Sep  7 06:16:30.857: INFO: Got endpoints: latency-svc-8mh24 [750.635857ms]
Sep  7 06:16:30.857: INFO: Latencies: [13.3121ms 18.617041ms 22.949711ms 27.964782ms 32.861182ms 36.854482ms 42.025223ms 47.170113ms 50.397653ms 55.066504ms 60.215335ms 63.359575ms 66.175385ms 66.934366ms 67.560285ms 67.781035ms 68.277446ms 68.429795ms 68.933315ms 69.301474ms 69.368286ms 69.518296ms 69.558426ms 69.804585ms 70.183645ms 70.525425ms 70.723186ms 70.909275ms 70.974736ms 71.486376ms 74.094425ms 77.260656ms 109.788419ms 154.710502ms 203.601936ms 248.822139ms 288.954063ms 336.557605ms 382.472789ms 424.818923ms 468.710587ms 514.46951ms 562.406894ms 603.957756ms 653.87247ms 696.411943ms 742.740977ms 744.305287ms 746.196487ms 746.207167ms 746.569097ms 746.906557ms 746.991718ms 747.202876ms 747.227368ms 747.236877ms 747.250997ms 747.463147ms 747.495628ms 747.655906ms 747.711667ms 747.718728ms 747.729236ms 747.783647ms 747.799098ms 747.845756ms 747.869456ms 747.993497ms 748.007366ms 748.030047ms 748.181807ms 748.298267ms 748.373417ms 748.380238ms 748.424048ms 748.442427ms 748.471428ms 748.516648ms 748.540388ms 748.576108ms 748.619127ms 748.632666ms 748.659226ms 748.712967ms 748.713847ms 748.851948ms 749.004907ms 749.030588ms 749.043297ms 749.071056ms 749.095437ms 749.158737ms 749.196407ms 749.248277ms 749.262907ms 749.263468ms 749.331448ms 749.332078ms 749.371347ms 749.463637ms 749.506857ms 749.552118ms 749.570928ms 749.618497ms 749.668378ms 749.672427ms 749.736467ms 749.740677ms 749.805668ms 749.815298ms 749.825357ms 749.833647ms 749.888577ms 749.922027ms 750.037767ms 750.080668ms 750.114428ms 750.130727ms 750.135868ms 750.141287ms 750.150627ms 750.160288ms 750.161397ms 750.164797ms 750.203287ms 750.232277ms 750.288557ms 750.291317ms 750.292967ms 750.309597ms 750.334478ms 750.354957ms 750.396548ms 750.405436ms 750.440847ms 750.465797ms 750.488448ms 750.513108ms 750.530648ms 750.531138ms 750.569447ms 750.632557ms 750.635857ms 750.641287ms 750.643247ms 750.651257ms 750.680016ms 750.701917ms 750.708978ms 750.754807ms 750.764718ms 750.770837ms 750.775236ms 750.841547ms 750.873307ms 750.883257ms 750.963697ms 751.001288ms 751.017738ms 751.115627ms 751.130337ms 751.137356ms 751.177107ms 751.192896ms 751.218417ms 751.257498ms 751.326838ms 751.333997ms 751.336607ms 751.337958ms 751.401837ms 751.468927ms 751.511677ms 751.592287ms 751.701428ms 751.759768ms 751.832808ms 751.876297ms 751.897477ms 751.902998ms 751.908017ms 751.963307ms 751.995748ms 751.998958ms 752.015357ms 752.023917ms 752.070447ms 752.108568ms 752.216568ms 752.306798ms 752.380187ms 752.440507ms 752.462937ms 752.574187ms 752.583727ms 752.586097ms 752.722678ms 752.755638ms 752.994677ms 753.554218ms]
Sep  7 06:16:30.857: INFO: 50 %ile: 749.506857ms
Sep  7 06:16:30.857: INFO: 90 %ile: 751.908017ms
Sep  7 06:16:30.857: INFO: 99 %ile: 752.994677ms
Sep  7 06:16:30.857: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  7 06:16:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-575" for this suite. 09/07/23 06:16:30.861
------------------------------
â€¢ [SLOW TEST] [10.740 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:16:20.124
    Sep  7 06:16:20.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svc-latency 09/07/23 06:16:20.125
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:20.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:20.135
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  7 06:16:20.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-575 09/07/23 06:16:20.138
    I0907 06:16:20.142836      29 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-575, replica count: 1
    I0907 06:16:21.194043      29 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0907 06:16:22.194799      29 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 06:16:22.304: INFO: Created: latency-svc-kvf85
    Sep  7 06:16:22.307: INFO: Got endpoints: latency-svc-kvf85 [12.407651ms]
    Sep  7 06:16:22.316: INFO: Created: latency-svc-ltlp7
    Sep  7 06:16:22.321: INFO: Got endpoints: latency-svc-ltlp7 [13.3121ms]
    Sep  7 06:16:22.322: INFO: Created: latency-svc-qfzjd
    Sep  7 06:16:22.326: INFO: Got endpoints: latency-svc-qfzjd [18.617041ms]
    Sep  7 06:16:22.327: INFO: Created: latency-svc-4tjx8
    Sep  7 06:16:22.330: INFO: Got endpoints: latency-svc-4tjx8 [22.949711ms]
    Sep  7 06:16:22.332: INFO: Created: latency-svc-ncwfl
    Sep  7 06:16:22.335: INFO: Got endpoints: latency-svc-ncwfl [27.964782ms]
    Sep  7 06:16:22.336: INFO: Created: latency-svc-tldvt
    Sep  7 06:16:22.340: INFO: Got endpoints: latency-svc-tldvt [32.861182ms]
    Sep  7 06:16:22.341: INFO: Created: latency-svc-2gv2s
    Sep  7 06:16:22.344: INFO: Got endpoints: latency-svc-2gv2s [36.854482ms]
    Sep  7 06:16:22.345: INFO: Created: latency-svc-f4nvr
    Sep  7 06:16:22.350: INFO: Got endpoints: latency-svc-f4nvr [42.025223ms]
    Sep  7 06:16:22.350: INFO: Created: latency-svc-cccsn
    Sep  7 06:16:22.355: INFO: Got endpoints: latency-svc-cccsn [47.170113ms]
    Sep  7 06:16:22.355: INFO: Created: latency-svc-ld6dv
    Sep  7 06:16:22.358: INFO: Got endpoints: latency-svc-ld6dv [50.397653ms]
    Sep  7 06:16:22.359: INFO: Created: latency-svc-bzpwh
    Sep  7 06:16:22.363: INFO: Got endpoints: latency-svc-bzpwh [55.066504ms]
    Sep  7 06:16:22.364: INFO: Created: latency-svc-lxhgc
    Sep  7 06:16:22.368: INFO: Got endpoints: latency-svc-lxhgc [60.215335ms]
    Sep  7 06:16:22.369: INFO: Created: latency-svc-6pcsv
    Sep  7 06:16:22.371: INFO: Got endpoints: latency-svc-6pcsv [63.359575ms]
    Sep  7 06:16:22.372: INFO: Created: latency-svc-bhbcj
    Sep  7 06:16:22.377: INFO: Got endpoints: latency-svc-bhbcj [69.301474ms]
    Sep  7 06:16:22.378: INFO: Created: latency-svc-bw455
    Sep  7 06:16:22.382: INFO: Got endpoints: latency-svc-bw455 [74.094425ms]
    Sep  7 06:16:22.383: INFO: Created: latency-svc-gkt8l
    Sep  7 06:16:22.385: INFO: Got endpoints: latency-svc-gkt8l [77.260656ms]
    Sep  7 06:16:22.386: INFO: Created: latency-svc-xfdjj
    Sep  7 06:16:22.390: INFO: Got endpoints: latency-svc-xfdjj [69.558426ms]
    Sep  7 06:16:22.391: INFO: Created: latency-svc-9cdgd
    Sep  7 06:16:22.394: INFO: Got endpoints: latency-svc-9cdgd [68.429795ms]
    Sep  7 06:16:22.396: INFO: Created: latency-svc-5gjdf
    Sep  7 06:16:22.399: INFO: Got endpoints: latency-svc-5gjdf [68.277446ms]
    Sep  7 06:16:22.400: INFO: Created: latency-svc-5bzff
    Sep  7 06:16:22.403: INFO: Got endpoints: latency-svc-5bzff [67.781035ms]
    Sep  7 06:16:22.404: INFO: Created: latency-svc-d7rcp
    Sep  7 06:16:22.407: INFO: Got endpoints: latency-svc-d7rcp [66.175385ms]
    Sep  7 06:16:22.409: INFO: Created: latency-svc-gbr82
    Sep  7 06:16:22.411: INFO: Got endpoints: latency-svc-gbr82 [66.934366ms]
    Sep  7 06:16:22.412: INFO: Created: latency-svc-24w8r
    Sep  7 06:16:22.419: INFO: Got endpoints: latency-svc-24w8r [68.933315ms]
    Sep  7 06:16:22.419: INFO: Created: latency-svc-kmmg7
    Sep  7 06:16:22.422: INFO: Got endpoints: latency-svc-kmmg7 [67.560285ms]
    Sep  7 06:16:22.425: INFO: Created: latency-svc-r7294
    Sep  7 06:16:22.427: INFO: Got endpoints: latency-svc-r7294 [69.368286ms]
    Sep  7 06:16:22.428: INFO: Created: latency-svc-8ffnh
    Sep  7 06:16:22.433: INFO: Got endpoints: latency-svc-8ffnh [69.804585ms]
    Sep  7 06:16:22.434: INFO: Created: latency-svc-h6wdh
    Sep  7 06:16:22.439: INFO: Got endpoints: latency-svc-h6wdh [70.723186ms]
    Sep  7 06:16:22.439: INFO: Created: latency-svc-s64n5
    Sep  7 06:16:22.443: INFO: Got endpoints: latency-svc-s64n5 [71.486376ms]
    Sep  7 06:16:22.443: INFO: Created: latency-svc-bs7zd
    Sep  7 06:16:22.447: INFO: Got endpoints: latency-svc-bs7zd [70.525425ms]
    Sep  7 06:16:22.448: INFO: Created: latency-svc-m8twq
    Sep  7 06:16:22.453: INFO: Got endpoints: latency-svc-m8twq [70.974736ms]
    Sep  7 06:16:22.454: INFO: Created: latency-svc-qtp5l
    Sep  7 06:16:22.456: INFO: Got endpoints: latency-svc-qtp5l [70.909275ms]
    Sep  7 06:16:22.457: INFO: Created: latency-svc-9lj68
    Sep  7 06:16:22.461: INFO: Got endpoints: latency-svc-9lj68 [70.183645ms]
    Sep  7 06:16:22.462: INFO: Created: latency-svc-fsvjc
    Sep  7 06:16:22.464: INFO: Got endpoints: latency-svc-fsvjc [69.518296ms]
    Sep  7 06:16:22.466: INFO: Created: latency-svc-wgc46
    Sep  7 06:16:22.470: INFO: Created: latency-svc-drx6z
    Sep  7 06:16:22.473: INFO: Created: latency-svc-ts5kz
    Sep  7 06:16:22.478: INFO: Created: latency-svc-km9bg
    Sep  7 06:16:22.481: INFO: Created: latency-svc-zpg28
    Sep  7 06:16:22.484: INFO: Created: latency-svc-qx2qz
    Sep  7 06:16:22.487: INFO: Created: latency-svc-l9rgk
    Sep  7 06:16:22.490: INFO: Created: latency-svc-2cqf9
    Sep  7 06:16:22.495: INFO: Created: latency-svc-fqkwc
    Sep  7 06:16:22.498: INFO: Created: latency-svc-6p9j4
    Sep  7 06:16:22.502: INFO: Created: latency-svc-qb8qm
    Sep  7 06:16:22.505: INFO: Created: latency-svc-2jwc4
    Sep  7 06:16:22.509: INFO: Got endpoints: latency-svc-wgc46 [109.788419ms]
    Sep  7 06:16:22.509: INFO: Created: latency-svc-hllbm
    Sep  7 06:16:22.513: INFO: Created: latency-svc-8r9dt
    Sep  7 06:16:22.516: INFO: Created: latency-svc-spdpl
    Sep  7 06:16:22.525: INFO: Created: latency-svc-st4xh
    Sep  7 06:16:22.558: INFO: Got endpoints: latency-svc-drx6z [154.710502ms]
    Sep  7 06:16:22.566: INFO: Created: latency-svc-h6fld
    Sep  7 06:16:22.610: INFO: Got endpoints: latency-svc-ts5kz [203.601936ms]
    Sep  7 06:16:22.617: INFO: Created: latency-svc-997bh
    Sep  7 06:16:22.660: INFO: Got endpoints: latency-svc-km9bg [248.822139ms]
    Sep  7 06:16:22.667: INFO: Created: latency-svc-t5t4n
    Sep  7 06:16:22.708: INFO: Got endpoints: latency-svc-zpg28 [288.954063ms]
    Sep  7 06:16:22.716: INFO: Created: latency-svc-t8pvr
    Sep  7 06:16:22.759: INFO: Got endpoints: latency-svc-qx2qz [336.557605ms]
    Sep  7 06:16:22.767: INFO: Created: latency-svc-pwkv5
    Sep  7 06:16:22.810: INFO: Got endpoints: latency-svc-l9rgk [382.472789ms]
    Sep  7 06:16:22.817: INFO: Created: latency-svc-qdjz7
    Sep  7 06:16:22.857: INFO: Got endpoints: latency-svc-2cqf9 [424.818923ms]
    Sep  7 06:16:22.865: INFO: Created: latency-svc-jpsgf
    Sep  7 06:16:22.907: INFO: Got endpoints: latency-svc-fqkwc [468.710587ms]
    Sep  7 06:16:22.916: INFO: Created: latency-svc-6mrb7
    Sep  7 06:16:22.957: INFO: Got endpoints: latency-svc-6p9j4 [514.46951ms]
    Sep  7 06:16:22.964: INFO: Created: latency-svc-c4plc
    Sep  7 06:16:23.010: INFO: Got endpoints: latency-svc-qb8qm [562.406894ms]
    Sep  7 06:16:23.017: INFO: Created: latency-svc-8xwbs
    Sep  7 06:16:23.057: INFO: Got endpoints: latency-svc-2jwc4 [603.957756ms]
    Sep  7 06:16:23.063: INFO: Created: latency-svc-75dtr
    Sep  7 06:16:23.110: INFO: Got endpoints: latency-svc-hllbm [653.87247ms]
    Sep  7 06:16:23.115: INFO: Created: latency-svc-72zrj
    Sep  7 06:16:23.157: INFO: Got endpoints: latency-svc-8r9dt [696.411943ms]
    Sep  7 06:16:23.164: INFO: Created: latency-svc-2zqng
    Sep  7 06:16:23.207: INFO: Got endpoints: latency-svc-spdpl [742.740977ms]
    Sep  7 06:16:23.215: INFO: Created: latency-svc-xrghw
    Sep  7 06:16:23.257: INFO: Got endpoints: latency-svc-st4xh [748.471428ms]
    Sep  7 06:16:23.264: INFO: Created: latency-svc-lpnvw
    Sep  7 06:16:23.309: INFO: Got endpoints: latency-svc-h6fld [750.531138ms]
    Sep  7 06:16:23.315: INFO: Created: latency-svc-kbqz5
    Sep  7 06:16:23.358: INFO: Got endpoints: latency-svc-997bh [747.495628ms]
    Sep  7 06:16:23.364: INFO: Created: latency-svc-5xxcl
    Sep  7 06:16:23.407: INFO: Got endpoints: latency-svc-t5t4n [746.569097ms]
    Sep  7 06:16:23.415: INFO: Created: latency-svc-77zbj
    Sep  7 06:16:23.458: INFO: Got endpoints: latency-svc-t8pvr [750.513108ms]
    Sep  7 06:16:23.466: INFO: Created: latency-svc-m8hzd
    Sep  7 06:16:23.509: INFO: Got endpoints: latency-svc-pwkv5 [750.396548ms]
    Sep  7 06:16:23.516: INFO: Created: latency-svc-jwwsz
    Sep  7 06:16:23.557: INFO: Got endpoints: latency-svc-qdjz7 [747.227368ms]
    Sep  7 06:16:23.564: INFO: Created: latency-svc-6zc78
    Sep  7 06:16:23.607: INFO: Got endpoints: latency-svc-jpsgf [749.805668ms]
    Sep  7 06:16:23.614: INFO: Created: latency-svc-p68vm
    Sep  7 06:16:23.658: INFO: Got endpoints: latency-svc-6mrb7 [750.530648ms]
    Sep  7 06:16:23.665: INFO: Created: latency-svc-967jp
    Sep  7 06:16:23.709: INFO: Got endpoints: latency-svc-c4plc [751.511677ms]
    Sep  7 06:16:23.715: INFO: Created: latency-svc-d929w
    Sep  7 06:16:23.759: INFO: Got endpoints: latency-svc-8xwbs [748.712967ms]
    Sep  7 06:16:23.765: INFO: Created: latency-svc-56s8s
    Sep  7 06:16:23.808: INFO: Got endpoints: latency-svc-75dtr [750.764718ms]
    Sep  7 06:16:23.814: INFO: Created: latency-svc-7fzrn
    Sep  7 06:16:23.857: INFO: Got endpoints: latency-svc-72zrj [746.991718ms]
    Sep  7 06:16:23.864: INFO: Created: latency-svc-mdnqp
    Sep  7 06:16:23.911: INFO: Got endpoints: latency-svc-2zqng [753.554218ms]
    Sep  7 06:16:23.917: INFO: Created: latency-svc-vbhr8
    Sep  7 06:16:23.957: INFO: Got endpoints: latency-svc-xrghw [750.160288ms]
    Sep  7 06:16:23.964: INFO: Created: latency-svc-tpkdq
    Sep  7 06:16:24.008: INFO: Got endpoints: latency-svc-lpnvw [750.632557ms]
    Sep  7 06:16:24.014: INFO: Created: latency-svc-fhcl4
    Sep  7 06:16:24.057: INFO: Got endpoints: latency-svc-kbqz5 [748.181807ms]
    Sep  7 06:16:24.066: INFO: Created: latency-svc-4f95t
    Sep  7 06:16:24.110: INFO: Got endpoints: latency-svc-5xxcl [751.995748ms]
    Sep  7 06:16:24.116: INFO: Created: latency-svc-zrrps
    Sep  7 06:16:24.157: INFO: Got endpoints: latency-svc-77zbj [750.135868ms]
    Sep  7 06:16:24.164: INFO: Created: latency-svc-t4db6
    Sep  7 06:16:24.207: INFO: Got endpoints: latency-svc-m8hzd [749.030588ms]
    Sep  7 06:16:24.214: INFO: Created: latency-svc-8qhs4
    Sep  7 06:16:24.258: INFO: Got endpoints: latency-svc-jwwsz [748.380238ms]
    Sep  7 06:16:24.264: INFO: Created: latency-svc-7vdhh
    Sep  7 06:16:24.309: INFO: Got endpoints: latency-svc-6zc78 [751.998958ms]
    Sep  7 06:16:24.316: INFO: Created: latency-svc-67f4s
    Sep  7 06:16:24.358: INFO: Got endpoints: latency-svc-p68vm [750.643247ms]
    Sep  7 06:16:24.366: INFO: Created: latency-svc-wsfl4
    Sep  7 06:16:24.408: INFO: Got endpoints: latency-svc-967jp [750.309597ms]
    Sep  7 06:16:24.415: INFO: Created: latency-svc-7vrf2
    Sep  7 06:16:24.458: INFO: Got endpoints: latency-svc-d929w [749.331448ms]
    Sep  7 06:16:24.467: INFO: Created: latency-svc-lc5xq
    Sep  7 06:16:24.507: INFO: Got endpoints: latency-svc-56s8s [748.424048ms]
    Sep  7 06:16:24.514: INFO: Created: latency-svc-cl647
    Sep  7 06:16:24.557: INFO: Got endpoints: latency-svc-7fzrn [749.570928ms]
    Sep  7 06:16:24.564: INFO: Created: latency-svc-txsgg
    Sep  7 06:16:24.607: INFO: Got endpoints: latency-svc-mdnqp [750.488448ms]
    Sep  7 06:16:24.615: INFO: Created: latency-svc-b2jpm
    Sep  7 06:16:24.657: INFO: Got endpoints: latency-svc-vbhr8 [746.207167ms]
    Sep  7 06:16:24.666: INFO: Created: latency-svc-rkw2v
    Sep  7 06:16:24.707: INFO: Got endpoints: latency-svc-tpkdq [750.203287ms]
    Sep  7 06:16:24.715: INFO: Created: latency-svc-97wr6
    Sep  7 06:16:24.759: INFO: Got endpoints: latency-svc-fhcl4 [751.177107ms]
    Sep  7 06:16:24.766: INFO: Created: latency-svc-fw8hh
    Sep  7 06:16:24.810: INFO: Got endpoints: latency-svc-4f95t [752.722678ms]
    Sep  7 06:16:24.816: INFO: Created: latency-svc-xdwfw
    Sep  7 06:16:24.858: INFO: Got endpoints: latency-svc-zrrps [747.799098ms]
    Sep  7 06:16:24.866: INFO: Created: latency-svc-49dx7
    Sep  7 06:16:24.907: INFO: Got endpoints: latency-svc-t4db6 [750.141287ms]
    Sep  7 06:16:24.915: INFO: Created: latency-svc-6wrrn
    Sep  7 06:16:24.959: INFO: Got endpoints: latency-svc-8qhs4 [751.963307ms]
    Sep  7 06:16:24.966: INFO: Created: latency-svc-8v4d7
    Sep  7 06:16:25.010: INFO: Got endpoints: latency-svc-7vdhh [751.902998ms]
    Sep  7 06:16:25.019: INFO: Created: latency-svc-k42gz
    Sep  7 06:16:25.058: INFO: Got endpoints: latency-svc-67f4s [749.158737ms]
    Sep  7 06:16:25.066: INFO: Created: latency-svc-vrj6j
    Sep  7 06:16:25.108: INFO: Got endpoints: latency-svc-wsfl4 [750.334478ms]
    Sep  7 06:16:25.116: INFO: Created: latency-svc-dkrz9
    Sep  7 06:16:25.158: INFO: Got endpoints: latency-svc-7vrf2 [749.668378ms]
    Sep  7 06:16:25.164: INFO: Created: latency-svc-qvlkc
    Sep  7 06:16:25.210: INFO: Got endpoints: latency-svc-lc5xq [751.832808ms]
    Sep  7 06:16:25.216: INFO: Created: latency-svc-82fmk
    Sep  7 06:16:25.259: INFO: Got endpoints: latency-svc-cl647 [751.592287ms]
    Sep  7 06:16:25.265: INFO: Created: latency-svc-sjj8c
    Sep  7 06:16:25.310: INFO: Got endpoints: latency-svc-txsgg [752.380187ms]
    Sep  7 06:16:25.316: INFO: Created: latency-svc-t48dt
    Sep  7 06:16:25.358: INFO: Got endpoints: latency-svc-b2jpm [751.130337ms]
    Sep  7 06:16:25.366: INFO: Created: latency-svc-xpxwk
    Sep  7 06:16:25.408: INFO: Got endpoints: latency-svc-rkw2v [751.017738ms]
    Sep  7 06:16:25.416: INFO: Created: latency-svc-8cvbn
    Sep  7 06:16:25.458: INFO: Got endpoints: latency-svc-97wr6 [751.001288ms]
    Sep  7 06:16:25.466: INFO: Created: latency-svc-wjdvk
    Sep  7 06:16:25.509: INFO: Got endpoints: latency-svc-fw8hh [750.114428ms]
    Sep  7 06:16:25.516: INFO: Created: latency-svc-cbqdv
    Sep  7 06:16:25.557: INFO: Got endpoints: latency-svc-xdwfw [747.236877ms]
    Sep  7 06:16:25.565: INFO: Created: latency-svc-hznhd
    Sep  7 06:16:25.607: INFO: Got endpoints: latency-svc-49dx7 [749.263468ms]
    Sep  7 06:16:25.614: INFO: Created: latency-svc-mdhsc
    Sep  7 06:16:25.658: INFO: Got endpoints: latency-svc-6wrrn [750.641287ms]
    Sep  7 06:16:25.666: INFO: Created: latency-svc-8gm29
    Sep  7 06:16:25.707: INFO: Got endpoints: latency-svc-8v4d7 [747.729236ms]
    Sep  7 06:16:25.714: INFO: Created: latency-svc-sksjv
    Sep  7 06:16:25.758: INFO: Got endpoints: latency-svc-k42gz [748.298267ms]
    Sep  7 06:16:25.765: INFO: Created: latency-svc-kkkdv
    Sep  7 06:16:25.807: INFO: Got endpoints: latency-svc-vrj6j [748.576108ms]
    Sep  7 06:16:25.814: INFO: Created: latency-svc-6x5td
    Sep  7 06:16:25.858: INFO: Got endpoints: latency-svc-dkrz9 [749.815298ms]
    Sep  7 06:16:25.866: INFO: Created: latency-svc-vnncg
    Sep  7 06:16:25.909: INFO: Got endpoints: latency-svc-qvlkc [751.218417ms]
    Sep  7 06:16:25.916: INFO: Created: latency-svc-mz6w8
    Sep  7 06:16:25.959: INFO: Got endpoints: latency-svc-82fmk [749.506857ms]
    Sep  7 06:16:25.967: INFO: Created: latency-svc-hsg7w
    Sep  7 06:16:26.008: INFO: Got endpoints: latency-svc-sjj8c [749.552118ms]
    Sep  7 06:16:26.016: INFO: Created: latency-svc-mcd7r
    Sep  7 06:16:26.058: INFO: Got endpoints: latency-svc-t48dt [748.516648ms]
    Sep  7 06:16:26.066: INFO: Created: latency-svc-z97w9
    Sep  7 06:16:26.108: INFO: Got endpoints: latency-svc-xpxwk [749.618497ms]
    Sep  7 06:16:26.117: INFO: Created: latency-svc-d92td
    Sep  7 06:16:26.157: INFO: Got endpoints: latency-svc-8cvbn [749.095437ms]
    Sep  7 06:16:26.164: INFO: Created: latency-svc-tb7rz
    Sep  7 06:16:26.210: INFO: Got endpoints: latency-svc-wjdvk [752.023917ms]
    Sep  7 06:16:26.218: INFO: Created: latency-svc-gprnk
    Sep  7 06:16:26.257: INFO: Got endpoints: latency-svc-cbqdv [747.463147ms]
    Sep  7 06:16:26.264: INFO: Created: latency-svc-bwwts
    Sep  7 06:16:26.308: INFO: Got endpoints: latency-svc-hznhd [751.257498ms]
    Sep  7 06:16:26.316: INFO: Created: latency-svc-vsrmv
    Sep  7 06:16:26.358: INFO: Got endpoints: latency-svc-mdhsc [750.708978ms]
    Sep  7 06:16:26.366: INFO: Created: latency-svc-khbgg
    Sep  7 06:16:26.410: INFO: Got endpoints: latency-svc-8gm29 [751.701428ms]
    Sep  7 06:16:26.416: INFO: Created: latency-svc-dxqnj
    Sep  7 06:16:26.459: INFO: Got endpoints: latency-svc-sksjv [752.440507ms]
    Sep  7 06:16:26.466: INFO: Created: latency-svc-gk2z9
    Sep  7 06:16:26.509: INFO: Got endpoints: latency-svc-kkkdv [750.770837ms]
    Sep  7 06:16:26.515: INFO: Created: latency-svc-7lvdl
    Sep  7 06:16:26.558: INFO: Got endpoints: latency-svc-6x5td [750.883257ms]
    Sep  7 06:16:26.566: INFO: Created: latency-svc-v7dng
    Sep  7 06:16:26.608: INFO: Got endpoints: latency-svc-vnncg [750.037767ms]
    Sep  7 06:16:26.615: INFO: Created: latency-svc-dj9gd
    Sep  7 06:16:26.658: INFO: Got endpoints: latency-svc-mz6w8 [749.004907ms]
    Sep  7 06:16:26.665: INFO: Created: latency-svc-vpjfj
    Sep  7 06:16:26.707: INFO: Got endpoints: latency-svc-hsg7w [747.718728ms]
    Sep  7 06:16:26.714: INFO: Created: latency-svc-8zh4r
    Sep  7 06:16:26.757: INFO: Got endpoints: latency-svc-mcd7r [748.619127ms]
    Sep  7 06:16:26.764: INFO: Created: latency-svc-mm9f6
    Sep  7 06:16:26.807: INFO: Got endpoints: latency-svc-z97w9 [749.248277ms]
    Sep  7 06:16:26.818: INFO: Created: latency-svc-xmbcz
    Sep  7 06:16:26.857: INFO: Got endpoints: latency-svc-d92td [749.332078ms]
    Sep  7 06:16:26.864: INFO: Created: latency-svc-t4pf7
    Sep  7 06:16:26.909: INFO: Got endpoints: latency-svc-tb7rz [752.216568ms]
    Sep  7 06:16:26.916: INFO: Created: latency-svc-mgck5
    Sep  7 06:16:26.957: INFO: Got endpoints: latency-svc-gprnk [746.196487ms]
    Sep  7 06:16:26.963: INFO: Created: latency-svc-tlsm9
    Sep  7 06:16:27.009: INFO: Got endpoints: latency-svc-bwwts [752.462937ms]
    Sep  7 06:16:27.016: INFO: Created: latency-svc-829g4
    Sep  7 06:16:27.058: INFO: Got endpoints: latency-svc-vsrmv [749.825357ms]
    Sep  7 06:16:27.065: INFO: Created: latency-svc-pbxhg
    Sep  7 06:16:27.109: INFO: Got endpoints: latency-svc-khbgg [751.337958ms]
    Sep  7 06:16:27.115: INFO: Created: latency-svc-r9hv2
    Sep  7 06:16:27.158: INFO: Got endpoints: latency-svc-dxqnj [747.845756ms]
    Sep  7 06:16:27.164: INFO: Created: latency-svc-nc9t9
    Sep  7 06:16:27.209: INFO: Got endpoints: latency-svc-gk2z9 [749.262907ms]
    Sep  7 06:16:27.215: INFO: Created: latency-svc-fc8f2
    Sep  7 06:16:27.259: INFO: Got endpoints: latency-svc-7lvdl [750.164797ms]
    Sep  7 06:16:27.266: INFO: Created: latency-svc-lfnjb
    Sep  7 06:16:27.310: INFO: Got endpoints: latency-svc-v7dng [751.897477ms]
    Sep  7 06:16:27.317: INFO: Created: latency-svc-5bv6r
    Sep  7 06:16:27.357: INFO: Got endpoints: latency-svc-dj9gd [749.196407ms]
    Sep  7 06:16:27.366: INFO: Created: latency-svc-tg4sz
    Sep  7 06:16:27.408: INFO: Got endpoints: latency-svc-vpjfj [749.463637ms]
    Sep  7 06:16:27.416: INFO: Created: latency-svc-dlrbt
    Sep  7 06:16:27.457: INFO: Got endpoints: latency-svc-8zh4r [749.833647ms]
    Sep  7 06:16:27.464: INFO: Created: latency-svc-4xs5q
    Sep  7 06:16:27.510: INFO: Got endpoints: latency-svc-mm9f6 [752.574187ms]
    Sep  7 06:16:27.516: INFO: Created: latency-svc-4vtt6
    Sep  7 06:16:27.558: INFO: Got endpoints: latency-svc-xmbcz [750.232277ms]
    Sep  7 06:16:27.565: INFO: Created: latency-svc-pfkdd
    Sep  7 06:16:27.610: INFO: Got endpoints: latency-svc-t4pf7 [752.583727ms]
    Sep  7 06:16:27.617: INFO: Created: latency-svc-xd77q
    Sep  7 06:16:27.657: INFO: Got endpoints: latency-svc-mgck5 [747.711667ms]
    Sep  7 06:16:27.665: INFO: Created: latency-svc-2fftz
    Sep  7 06:16:27.708: INFO: Got endpoints: latency-svc-tlsm9 [751.115627ms]
    Sep  7 06:16:27.714: INFO: Created: latency-svc-9wqq8
    Sep  7 06:16:27.759: INFO: Got endpoints: latency-svc-829g4 [749.740677ms]
    Sep  7 06:16:27.766: INFO: Created: latency-svc-jrm4c
    Sep  7 06:16:27.809: INFO: Got endpoints: latency-svc-pbxhg [751.336607ms]
    Sep  7 06:16:27.815: INFO: Created: latency-svc-8pmtz
    Sep  7 06:16:27.858: INFO: Got endpoints: latency-svc-r9hv2 [748.540388ms]
    Sep  7 06:16:27.864: INFO: Created: latency-svc-29qwt
    Sep  7 06:16:27.908: INFO: Got endpoints: latency-svc-nc9t9 [750.080668ms]
    Sep  7 06:16:27.916: INFO: Created: latency-svc-kdmhh
    Sep  7 06:16:27.958: INFO: Got endpoints: latency-svc-fc8f2 [748.851948ms]
    Sep  7 06:16:27.964: INFO: Created: latency-svc-wwcqq
    Sep  7 06:16:28.010: INFO: Got endpoints: latency-svc-lfnjb [750.873307ms]
    Sep  7 06:16:28.016: INFO: Created: latency-svc-dbmhh
    Sep  7 06:16:28.059: INFO: Got endpoints: latency-svc-5bv6r [749.371347ms]
    Sep  7 06:16:28.066: INFO: Created: latency-svc-wlf5r
    Sep  7 06:16:28.108: INFO: Got endpoints: latency-svc-tg4sz [750.150627ms]
    Sep  7 06:16:28.117: INFO: Created: latency-svc-nd2gz
    Sep  7 06:16:28.158: INFO: Got endpoints: latency-svc-dlrbt [750.161397ms]
    Sep  7 06:16:28.165: INFO: Created: latency-svc-zct5s
    Sep  7 06:16:28.210: INFO: Got endpoints: latency-svc-4xs5q [752.755638ms]
    Sep  7 06:16:28.217: INFO: Created: latency-svc-ctbgf
    Sep  7 06:16:28.257: INFO: Got endpoints: latency-svc-4vtt6 [747.250997ms]
    Sep  7 06:16:28.265: INFO: Created: latency-svc-mgl2r
    Sep  7 06:16:28.308: INFO: Got endpoints: latency-svc-pfkdd [750.130727ms]
    Sep  7 06:16:28.315: INFO: Created: latency-svc-npx7h
    Sep  7 06:16:28.358: INFO: Got endpoints: latency-svc-xd77q [748.030047ms]
    Sep  7 06:16:28.367: INFO: Created: latency-svc-zzx8q
    Sep  7 06:16:28.408: INFO: Got endpoints: latency-svc-2fftz [751.192896ms]
    Sep  7 06:16:28.415: INFO: Created: latency-svc-pskpc
    Sep  7 06:16:28.460: INFO: Got endpoints: latency-svc-9wqq8 [752.108568ms]
    Sep  7 06:16:28.467: INFO: Created: latency-svc-4w4tt
    Sep  7 06:16:28.510: INFO: Got endpoints: latency-svc-jrm4c [751.468927ms]
    Sep  7 06:16:28.517: INFO: Created: latency-svc-vrcvm
    Sep  7 06:16:28.558: INFO: Got endpoints: latency-svc-8pmtz [748.373417ms]
    Sep  7 06:16:28.565: INFO: Created: latency-svc-mfqcv
    Sep  7 06:16:28.608: INFO: Got endpoints: latency-svc-29qwt [750.292967ms]
    Sep  7 06:16:28.616: INFO: Created: latency-svc-fwd5v
    Sep  7 06:16:28.658: INFO: Got endpoints: latency-svc-kdmhh [750.651257ms]
    Sep  7 06:16:28.665: INFO: Created: latency-svc-9nj4t
    Sep  7 06:16:28.711: INFO: Got endpoints: latency-svc-wwcqq [752.994677ms]
    Sep  7 06:16:28.717: INFO: Created: latency-svc-c9p9v
    Sep  7 06:16:28.757: INFO: Got endpoints: latency-svc-dbmhh [746.906557ms]
    Sep  7 06:16:28.764: INFO: Created: latency-svc-h5w2n
    Sep  7 06:16:28.807: INFO: Got endpoints: latency-svc-wlf5r [747.783647ms]
    Sep  7 06:16:28.815: INFO: Created: latency-svc-f44v7
    Sep  7 06:16:28.858: INFO: Got endpoints: latency-svc-nd2gz [750.288557ms]
    Sep  7 06:16:28.865: INFO: Created: latency-svc-l28zw
    Sep  7 06:16:28.910: INFO: Got endpoints: latency-svc-zct5s [751.876297ms]
    Sep  7 06:16:28.917: INFO: Created: latency-svc-xn2ss
    Sep  7 06:16:28.957: INFO: Got endpoints: latency-svc-ctbgf [747.655906ms]
    Sep  7 06:16:28.965: INFO: Created: latency-svc-spww2
    Sep  7 06:16:29.007: INFO: Got endpoints: latency-svc-mgl2r [750.465797ms]
    Sep  7 06:16:29.015: INFO: Created: latency-svc-dnk9w
    Sep  7 06:16:29.058: INFO: Got endpoints: latency-svc-npx7h [750.291317ms]
    Sep  7 06:16:29.066: INFO: Created: latency-svc-hnsxm
    Sep  7 06:16:29.107: INFO: Got endpoints: latency-svc-zzx8q [748.713847ms]
    Sep  7 06:16:29.114: INFO: Created: latency-svc-qtt2m
    Sep  7 06:16:29.157: INFO: Got endpoints: latency-svc-pskpc [748.659226ms]
    Sep  7 06:16:29.165: INFO: Created: latency-svc-swrvm
    Sep  7 06:16:29.208: INFO: Got endpoints: latency-svc-4w4tt [747.869456ms]
    Sep  7 06:16:29.215: INFO: Created: latency-svc-hmzgg
    Sep  7 06:16:29.262: INFO: Got endpoints: latency-svc-vrcvm [750.963697ms]
    Sep  7 06:16:29.268: INFO: Created: latency-svc-bnw86
    Sep  7 06:16:29.307: INFO: Got endpoints: latency-svc-mfqcv [749.043297ms]
    Sep  7 06:16:29.316: INFO: Created: latency-svc-qxd8s
    Sep  7 06:16:29.359: INFO: Got endpoints: latency-svc-fwd5v [750.569447ms]
    Sep  7 06:16:29.367: INFO: Created: latency-svc-vttnv
    Sep  7 06:16:29.407: INFO: Got endpoints: latency-svc-9nj4t [749.071056ms]
    Sep  7 06:16:29.415: INFO: Created: latency-svc-s2gr9
    Sep  7 06:16:29.463: INFO: Got endpoints: latency-svc-c9p9v [752.070447ms]
    Sep  7 06:16:29.472: INFO: Created: latency-svc-rswdb
    Sep  7 06:16:29.508: INFO: Got endpoints: latency-svc-h5w2n [751.326838ms]
    Sep  7 06:16:29.515: INFO: Created: latency-svc-n5969
    Sep  7 06:16:29.557: INFO: Got endpoints: latency-svc-f44v7 [749.888577ms]
    Sep  7 06:16:29.564: INFO: Created: latency-svc-76scx
    Sep  7 06:16:29.608: INFO: Got endpoints: latency-svc-l28zw [750.440847ms]
    Sep  7 06:16:29.617: INFO: Created: latency-svc-hmhpn
    Sep  7 06:16:29.658: INFO: Got endpoints: latency-svc-xn2ss [748.442427ms]
    Sep  7 06:16:29.668: INFO: Created: latency-svc-zg7mr
    Sep  7 06:16:29.710: INFO: Got endpoints: latency-svc-spww2 [752.306798ms]
    Sep  7 06:16:29.717: INFO: Created: latency-svc-vdbts
    Sep  7 06:16:29.757: INFO: Got endpoints: latency-svc-dnk9w [749.922027ms]
    Sep  7 06:16:29.764: INFO: Created: latency-svc-sq5rr
    Sep  7 06:16:29.808: INFO: Got endpoints: latency-svc-hnsxm [749.736467ms]
    Sep  7 06:16:29.816: INFO: Created: latency-svc-md7fk
    Sep  7 06:16:29.858: INFO: Got endpoints: latency-svc-qtt2m [750.841547ms]
    Sep  7 06:16:29.864: INFO: Created: latency-svc-vm9bq
    Sep  7 06:16:29.910: INFO: Got endpoints: latency-svc-swrvm [752.586097ms]
    Sep  7 06:16:29.917: INFO: Created: latency-svc-jpvgw
    Sep  7 06:16:29.960: INFO: Got endpoints: latency-svc-hmzgg [751.759768ms]
    Sep  7 06:16:29.967: INFO: Created: latency-svc-rczw5
    Sep  7 06:16:30.009: INFO: Got endpoints: latency-svc-bnw86 [747.202876ms]
    Sep  7 06:16:30.017: INFO: Created: latency-svc-qlhsg
    Sep  7 06:16:30.058: INFO: Got endpoints: latency-svc-qxd8s [751.333997ms]
    Sep  7 06:16:30.066: INFO: Created: latency-svc-wrnwk
    Sep  7 06:16:30.107: INFO: Got endpoints: latency-svc-vttnv [748.007366ms]
    Sep  7 06:16:30.113: INFO: Created: latency-svc-8mh24
    Sep  7 06:16:30.160: INFO: Got endpoints: latency-svc-s2gr9 [752.015357ms]
    Sep  7 06:16:30.207: INFO: Got endpoints: latency-svc-rswdb [744.305287ms]
    Sep  7 06:16:30.259: INFO: Got endpoints: latency-svc-n5969 [750.775236ms]
    Sep  7 06:16:30.308: INFO: Got endpoints: latency-svc-76scx [750.754807ms]
    Sep  7 06:16:30.360: INFO: Got endpoints: latency-svc-hmhpn [751.401837ms]
    Sep  7 06:16:30.410: INFO: Got endpoints: latency-svc-zg7mr [751.137356ms]
    Sep  7 06:16:30.458: INFO: Got endpoints: latency-svc-vdbts [747.993497ms]
    Sep  7 06:16:30.508: INFO: Got endpoints: latency-svc-sq5rr [750.701917ms]
    Sep  7 06:16:30.559: INFO: Got endpoints: latency-svc-md7fk [750.680016ms]
    Sep  7 06:16:30.608: INFO: Got endpoints: latency-svc-vm9bq [750.405436ms]
    Sep  7 06:16:30.658: INFO: Got endpoints: latency-svc-jpvgw [748.632666ms]
    Sep  7 06:16:30.710: INFO: Got endpoints: latency-svc-rczw5 [750.354957ms]
    Sep  7 06:16:30.758: INFO: Got endpoints: latency-svc-qlhsg [749.672427ms]
    Sep  7 06:16:30.810: INFO: Got endpoints: latency-svc-wrnwk [751.908017ms]
    Sep  7 06:16:30.857: INFO: Got endpoints: latency-svc-8mh24 [750.635857ms]
    Sep  7 06:16:30.857: INFO: Latencies: [13.3121ms 18.617041ms 22.949711ms 27.964782ms 32.861182ms 36.854482ms 42.025223ms 47.170113ms 50.397653ms 55.066504ms 60.215335ms 63.359575ms 66.175385ms 66.934366ms 67.560285ms 67.781035ms 68.277446ms 68.429795ms 68.933315ms 69.301474ms 69.368286ms 69.518296ms 69.558426ms 69.804585ms 70.183645ms 70.525425ms 70.723186ms 70.909275ms 70.974736ms 71.486376ms 74.094425ms 77.260656ms 109.788419ms 154.710502ms 203.601936ms 248.822139ms 288.954063ms 336.557605ms 382.472789ms 424.818923ms 468.710587ms 514.46951ms 562.406894ms 603.957756ms 653.87247ms 696.411943ms 742.740977ms 744.305287ms 746.196487ms 746.207167ms 746.569097ms 746.906557ms 746.991718ms 747.202876ms 747.227368ms 747.236877ms 747.250997ms 747.463147ms 747.495628ms 747.655906ms 747.711667ms 747.718728ms 747.729236ms 747.783647ms 747.799098ms 747.845756ms 747.869456ms 747.993497ms 748.007366ms 748.030047ms 748.181807ms 748.298267ms 748.373417ms 748.380238ms 748.424048ms 748.442427ms 748.471428ms 748.516648ms 748.540388ms 748.576108ms 748.619127ms 748.632666ms 748.659226ms 748.712967ms 748.713847ms 748.851948ms 749.004907ms 749.030588ms 749.043297ms 749.071056ms 749.095437ms 749.158737ms 749.196407ms 749.248277ms 749.262907ms 749.263468ms 749.331448ms 749.332078ms 749.371347ms 749.463637ms 749.506857ms 749.552118ms 749.570928ms 749.618497ms 749.668378ms 749.672427ms 749.736467ms 749.740677ms 749.805668ms 749.815298ms 749.825357ms 749.833647ms 749.888577ms 749.922027ms 750.037767ms 750.080668ms 750.114428ms 750.130727ms 750.135868ms 750.141287ms 750.150627ms 750.160288ms 750.161397ms 750.164797ms 750.203287ms 750.232277ms 750.288557ms 750.291317ms 750.292967ms 750.309597ms 750.334478ms 750.354957ms 750.396548ms 750.405436ms 750.440847ms 750.465797ms 750.488448ms 750.513108ms 750.530648ms 750.531138ms 750.569447ms 750.632557ms 750.635857ms 750.641287ms 750.643247ms 750.651257ms 750.680016ms 750.701917ms 750.708978ms 750.754807ms 750.764718ms 750.770837ms 750.775236ms 750.841547ms 750.873307ms 750.883257ms 750.963697ms 751.001288ms 751.017738ms 751.115627ms 751.130337ms 751.137356ms 751.177107ms 751.192896ms 751.218417ms 751.257498ms 751.326838ms 751.333997ms 751.336607ms 751.337958ms 751.401837ms 751.468927ms 751.511677ms 751.592287ms 751.701428ms 751.759768ms 751.832808ms 751.876297ms 751.897477ms 751.902998ms 751.908017ms 751.963307ms 751.995748ms 751.998958ms 752.015357ms 752.023917ms 752.070447ms 752.108568ms 752.216568ms 752.306798ms 752.380187ms 752.440507ms 752.462937ms 752.574187ms 752.583727ms 752.586097ms 752.722678ms 752.755638ms 752.994677ms 753.554218ms]
    Sep  7 06:16:30.857: INFO: 50 %ile: 749.506857ms
    Sep  7 06:16:30.857: INFO: 90 %ile: 751.908017ms
    Sep  7 06:16:30.857: INFO: 99 %ile: 752.994677ms
    Sep  7 06:16:30.857: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:16:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-575" for this suite. 09/07/23 06:16:30.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:16:30.865
Sep  7 06:16:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:16:30.865
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:30.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:30.876
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-280/configmap-test-beb37b54-c52f-44fe-b2b0-af535285a882 09/07/23 06:16:30.878
STEP: Creating a pod to test consume configMaps 09/07/23 06:16:30.881
Sep  7 06:16:30.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974" in namespace "configmap-280" to be "Succeeded or Failed"
Sep  7 06:16:30.889: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515131ms
Sep  7 06:16:32.892: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003855521s
Sep  7 06:16:34.893: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005206932s
STEP: Saw pod success 09/07/23 06:16:34.893
Sep  7 06:16:34.893: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974" satisfied condition "Succeeded or Failed"
Sep  7 06:16:34.895: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 container env-test: <nil>
STEP: delete the pod 09/07/23 06:16:34.904
Sep  7 06:16:34.913: INFO: Waiting for pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 to disappear
Sep  7 06:16:34.915: INFO: Pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:16:34.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-280" for this suite. 09/07/23 06:16:34.916
------------------------------
â€¢ [4.056 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:16:30.865
    Sep  7 06:16:30.865: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:16:30.865
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:30.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:30.876
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-280/configmap-test-beb37b54-c52f-44fe-b2b0-af535285a882 09/07/23 06:16:30.878
    STEP: Creating a pod to test consume configMaps 09/07/23 06:16:30.881
    Sep  7 06:16:30.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974" in namespace "configmap-280" to be "Succeeded or Failed"
    Sep  7 06:16:30.889: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515131ms
    Sep  7 06:16:32.892: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003855521s
    Sep  7 06:16:34.893: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005206932s
    STEP: Saw pod success 09/07/23 06:16:34.893
    Sep  7 06:16:34.893: INFO: Pod "pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974" satisfied condition "Succeeded or Failed"
    Sep  7 06:16:34.895: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 container env-test: <nil>
    STEP: delete the pod 09/07/23 06:16:34.904
    Sep  7 06:16:34.913: INFO: Waiting for pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 to disappear
    Sep  7 06:16:34.915: INFO: Pod pod-configmaps-8f27a670-76fa-4de7-83e1-72debe5b9974 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:16:34.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-280" for this suite. 09/07/23 06:16:34.916
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:16:34.921
Sep  7 06:16:34.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:16:34.922
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:34.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:34.931
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-7c851e19-6174-4dfd-b681-52c4f7f71b89 09/07/23 06:16:34.932
STEP: Creating a pod to test consume configMaps 09/07/23 06:16:34.935
Sep  7 06:16:34.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca" in namespace "configmap-7623" to be "Succeeded or Failed"
Sep  7 06:16:34.942: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51023ms
Sep  7 06:16:36.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004825779s
Sep  7 06:16:38.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004536088s
STEP: Saw pod success 09/07/23 06:16:38.945
Sep  7 06:16:38.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca" satisfied condition "Succeeded or Failed"
Sep  7 06:16:38.947: INFO: Trying to get logs from node kind-worker pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca container agnhost-container: <nil>
STEP: delete the pod 09/07/23 06:16:38.956
Sep  7 06:16:38.962: INFO: Waiting for pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca to disappear
Sep  7 06:16:38.963: INFO: Pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:16:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7623" for this suite. 09/07/23 06:16:38.966
------------------------------
â€¢ [4.049 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:16:34.921
    Sep  7 06:16:34.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:16:34.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:34.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:34.931
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-7c851e19-6174-4dfd-b681-52c4f7f71b89 09/07/23 06:16:34.932
    STEP: Creating a pod to test consume configMaps 09/07/23 06:16:34.935
    Sep  7 06:16:34.941: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca" in namespace "configmap-7623" to be "Succeeded or Failed"
    Sep  7 06:16:34.942: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.51023ms
    Sep  7 06:16:36.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004825779s
    Sep  7 06:16:38.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004536088s
    STEP: Saw pod success 09/07/23 06:16:38.945
    Sep  7 06:16:38.945: INFO: Pod "pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca" satisfied condition "Succeeded or Failed"
    Sep  7 06:16:38.947: INFO: Trying to get logs from node kind-worker pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 06:16:38.956
    Sep  7 06:16:38.962: INFO: Waiting for pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca to disappear
    Sep  7 06:16:38.963: INFO: Pod pod-configmaps-9dc32d94-bc56-47fe-8cdf-4b6dcb1ddfca no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:16:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7623" for this suite. 09/07/23 06:16:38.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:16:38.97
Sep  7 06:16:38.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename init-container 09/07/23 06:16:38.971
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:38.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:38.98
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/07/23 06:16:38.981
Sep  7 06:16:38.982: INFO: PodSpec: initContainers in spec.initContainers
Sep  7 06:17:25.629: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9f461449-af4b-4ad7-a930-5f41ac493924", GenerateName:"", Namespace:"init-container-1054", SelfLink:"", UID:"3ac3ce7a-30cb-4c3f-8e6e-ad7249b3b3b8", ResourceVersion:"25105", Generation:0, CreationTimestamp:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"982081339"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0084938d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 7, 6, 17, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc008493908), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-s5t9m", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005671020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0056529c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-worker", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000679ce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005652a50)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005652a70)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005652a78), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005652a7c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00155e5b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.8.3", PodIP:"10.244.1.43", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.43"}}, StartTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000679dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000679e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://2ede224509235045d0578bd4932f3304ff64d48aa2f86df469e5d5c2ea65d86d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0056710a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005671080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc005652aff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:25.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1054" for this suite. 09/07/23 06:17:25.632
------------------------------
â€¢ [SLOW TEST] [46.666 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:16:38.97
    Sep  7 06:16:38.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename init-container 09/07/23 06:16:38.971
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:16:38.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:16:38.98
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/07/23 06:16:38.981
    Sep  7 06:16:38.982: INFO: PodSpec: initContainers in spec.initContainers
    Sep  7 06:17:25.629: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9f461449-af4b-4ad7-a930-5f41ac493924", GenerateName:"", Namespace:"init-container-1054", SelfLink:"", UID:"3ac3ce7a-30cb-4c3f-8e6e-ad7249b3b3b8", ResourceVersion:"25105", Generation:0, CreationTimestamp:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"982081339"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0084938d8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 7, 6, 17, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc008493908), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-s5t9m", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc005671020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-s5t9m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0056529c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kind-worker", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000679ce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005652a50)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005652a70)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005652a78), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005652a7c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00155e5b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.8.3", PodIP:"10.244.1.43", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.43"}}, StartTime:time.Date(2023, time.September, 7, 6, 16, 38, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000679dc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000679e30)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://2ede224509235045d0578bd4932f3304ff64d48aa2f86df469e5d5c2ea65d86d", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0056710a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005671080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc005652aff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:25.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1054" for this suite. 09/07/23 06:17:25.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:25.637
Sep  7 06:17:25.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename svcaccounts 09/07/23 06:17:25.637
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:25.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:25.647
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/07/23 06:17:25.649
STEP: watching for the ServiceAccount to be added 09/07/23 06:17:25.653
STEP: patching the ServiceAccount 09/07/23 06:17:25.654
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/07/23 06:17:25.658
STEP: deleting the ServiceAccount 09/07/23 06:17:25.66
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:25.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4808" for this suite. 09/07/23 06:17:25.667
------------------------------
â€¢ [0.033 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:25.637
    Sep  7 06:17:25.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename svcaccounts 09/07/23 06:17:25.637
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:25.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:25.647
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/07/23 06:17:25.649
    STEP: watching for the ServiceAccount to be added 09/07/23 06:17:25.653
    STEP: patching the ServiceAccount 09/07/23 06:17:25.654
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/07/23 06:17:25.658
    STEP: deleting the ServiceAccount 09/07/23 06:17:25.66
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:25.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4808" for this suite. 09/07/23 06:17:25.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:25.671
Sep  7 06:17:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 06:17:25.671
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:25.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:25.682
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 09/07/23 06:17:25.692
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 06:17:25.695
Sep  7 06:17:25.697: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:17:25.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 06:17:25.700: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 06:17:26.703: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:17:26.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 06:17:26.704: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/07/23 06:17:26.705
Sep  7 06:17:26.707: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/07/23 06:17:26.707
Sep  7 06:17:26.712: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/07/23 06:17:26.712
Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: ADDED
Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.714: INFO: Found daemon set daemon-set in namespace daemonsets-2662 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  7 06:17:26.714: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/07/23 06:17:26.714
STEP: watching for the daemon set status to be patched 09/07/23 06:17:26.719
Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: ADDED
Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.721: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.721: INFO: Observed daemon set daemon-set in namespace daemonsets-2662 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  7 06:17:26.721: INFO: Observed &DaemonSet event: MODIFIED
Sep  7 06:17:26.721: INFO: Found daemon set daemon-set in namespace daemonsets-2662 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  7 06:17:26.721: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 06:17:26.723
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2662, will wait for the garbage collector to delete the pods 09/07/23 06:17:26.723
Sep  7 06:17:26.780: INFO: Deleting DaemonSet.extensions daemon-set took: 4.695211ms
Sep  7 06:17:26.882: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.095286ms
Sep  7 06:17:30.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 06:17:30.984: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 06:17:30.986: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25179"},"items":null}

Sep  7 06:17:30.988: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25179"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:30.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2662" for this suite. 09/07/23 06:17:30.997
------------------------------
â€¢ [SLOW TEST] [5.330 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:25.671
    Sep  7 06:17:25.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 06:17:25.671
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:25.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:25.682
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 09/07/23 06:17:25.692
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 06:17:25.695
    Sep  7 06:17:25.697: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:17:25.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 06:17:25.700: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 06:17:26.703: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:17:26.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 06:17:26.704: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/07/23 06:17:26.705
    Sep  7 06:17:26.707: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/07/23 06:17:26.707
    Sep  7 06:17:26.712: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/07/23 06:17:26.712
    Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: ADDED
    Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.714: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.714: INFO: Found daemon set daemon-set in namespace daemonsets-2662 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  7 06:17:26.714: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/07/23 06:17:26.714
    STEP: watching for the daemon set status to be patched 09/07/23 06:17:26.719
    Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: ADDED
    Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.720: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.721: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.721: INFO: Observed daemon set daemon-set in namespace daemonsets-2662 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  7 06:17:26.721: INFO: Observed &DaemonSet event: MODIFIED
    Sep  7 06:17:26.721: INFO: Found daemon set daemon-set in namespace daemonsets-2662 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  7 06:17:26.721: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 06:17:26.723
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2662, will wait for the garbage collector to delete the pods 09/07/23 06:17:26.723
    Sep  7 06:17:26.780: INFO: Deleting DaemonSet.extensions daemon-set took: 4.695211ms
    Sep  7 06:17:26.882: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.095286ms
    Sep  7 06:17:30.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 06:17:30.984: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 06:17:30.986: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25179"},"items":null}

    Sep  7 06:17:30.988: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25179"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:30.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2662" for this suite. 09/07/23 06:17:30.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:31.005
Sep  7 06:17:31.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:17:31.006
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:31.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:31.015
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-8ec3e5d9-5c29-4315-a862-63b8aed08695 09/07/23 06:17:31.017
STEP: Creating a pod to test consume configMaps 09/07/23 06:17:31.02
Sep  7 06:17:31.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386" in namespace "projected-3645" to be "Succeeded or Failed"
Sep  7 06:17:31.027: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Pending", Reason="", readiness=false. Elapsed: 1.218629ms
Sep  7 06:17:33.029: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003638339s
Sep  7 06:17:35.030: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004466748s
STEP: Saw pod success 09/07/23 06:17:35.03
Sep  7 06:17:35.030: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386" satisfied condition "Succeeded or Failed"
Sep  7 06:17:35.031: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 container agnhost-container: <nil>
STEP: delete the pod 09/07/23 06:17:35.034
Sep  7 06:17:35.040: INFO: Waiting for pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 to disappear
Sep  7 06:17:35.041: INFO: Pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:35.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3645" for this suite. 09/07/23 06:17:35.043
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:31.005
    Sep  7 06:17:31.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:17:31.006
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:31.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:31.015
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-8ec3e5d9-5c29-4315-a862-63b8aed08695 09/07/23 06:17:31.017
    STEP: Creating a pod to test consume configMaps 09/07/23 06:17:31.02
    Sep  7 06:17:31.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386" in namespace "projected-3645" to be "Succeeded or Failed"
    Sep  7 06:17:31.027: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Pending", Reason="", readiness=false. Elapsed: 1.218629ms
    Sep  7 06:17:33.029: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003638339s
    Sep  7 06:17:35.030: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004466748s
    STEP: Saw pod success 09/07/23 06:17:35.03
    Sep  7 06:17:35.030: INFO: Pod "pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386" satisfied condition "Succeeded or Failed"
    Sep  7 06:17:35.031: INFO: Trying to get logs from node kind-worker pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 06:17:35.034
    Sep  7 06:17:35.040: INFO: Waiting for pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 to disappear
    Sep  7 06:17:35.041: INFO: Pod pod-projected-configmaps-25cbc78f-33f4-42f4-9b30-f30cb1b37386 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:35.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3645" for this suite. 09/07/23 06:17:35.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:35.049
Sep  7 06:17:35.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename watch 09/07/23 06:17:35.05
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:35.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:35.06
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/07/23 06:17:35.062
STEP: creating a watch on configmaps with label B 09/07/23 06:17:35.063
STEP: creating a watch on configmaps with label A or B 09/07/23 06:17:35.063
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.064
Sep  7 06:17:35.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25210 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:35.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25210 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.066
Sep  7 06:17:35.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25211 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:35.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25211 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/07/23 06:17:35.07
Sep  7 06:17:35.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25212 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:35.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25212 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.075
Sep  7 06:17:35.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25213 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:35.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25213 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/07/23 06:17:35.077
Sep  7 06:17:35.080: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25214 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:35.080: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25214 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/07/23 06:17:45.081
Sep  7 06:17:45.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25257 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  7 06:17:45.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25257 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:55.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9845" for this suite. 09/07/23 06:17:55.091
------------------------------
â€¢ [SLOW TEST] [20.045 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:35.049
    Sep  7 06:17:35.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename watch 09/07/23 06:17:35.05
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:35.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:35.06
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/07/23 06:17:35.062
    STEP: creating a watch on configmaps with label B 09/07/23 06:17:35.063
    STEP: creating a watch on configmaps with label A or B 09/07/23 06:17:35.063
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.064
    Sep  7 06:17:35.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25210 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:35.066: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25210 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.066
    Sep  7 06:17:35.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25211 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:35.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25211 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/07/23 06:17:35.07
    Sep  7 06:17:35.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25212 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:35.075: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25212 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/07/23 06:17:35.075
    Sep  7 06:17:35.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25213 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:35.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9845  a77b3ba1-448e-4b09-af82-c84e18ffb1d3 25213 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/07/23 06:17:35.077
    Sep  7 06:17:35.080: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25214 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:35.080: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25214 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/07/23 06:17:45.081
    Sep  7 06:17:45.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25257 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  7 06:17:45.085: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9845  fc111097-8af4-4cf4-8efb-99d33ba24c98 25257 0 2023-09-07 06:17:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-07 06:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:55.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9845" for this suite. 09/07/23 06:17:55.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:55.095
Sep  7 06:17:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 06:17:55.096
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:55.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:55.105
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  7 06:17:55.111: INFO: Waiting up to 2m0s for pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" in namespace "var-expansion-1794" to be "container 0 failed with reason CreateContainerConfigError"
Sep  7 06:17:55.112: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25593ms
Sep  7 06:17:57.116: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004785452s
Sep  7 06:17:57.116: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  7 06:17:57.116: INFO: Deleting pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" in namespace "var-expansion-1794"
Sep  7 06:17:57.121: INFO: Wait up to 5m0s for pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 06:17:59.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1794" for this suite. 09/07/23 06:17:59.128
------------------------------
â€¢ [4.036 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:55.095
    Sep  7 06:17:55.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 06:17:55.096
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:55.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:55.105
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  7 06:17:55.111: INFO: Waiting up to 2m0s for pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" in namespace "var-expansion-1794" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  7 06:17:55.112: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25593ms
    Sep  7 06:17:57.116: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004785452s
    Sep  7 06:17:57.116: INFO: Pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  7 06:17:57.116: INFO: Deleting pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" in namespace "var-expansion-1794"
    Sep  7 06:17:57.121: INFO: Wait up to 5m0s for pod "var-expansion-5cd82ea2-6460-420d-ae16-26f2dfaacef8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:17:59.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1794" for this suite. 09/07/23 06:17:59.128
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:17:59.131
Sep  7 06:17:59.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:17:59.132
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:59.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:59.143
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/07/23 06:17:59.145
STEP: Creating a ResourceQuota 09/07/23 06:18:04.147
STEP: Ensuring resource quota status is calculated 09/07/23 06:18:04.15
STEP: Creating a ReplicaSet 09/07/23 06:18:06.154
STEP: Ensuring resource quota status captures replicaset creation 09/07/23 06:18:06.163
STEP: Deleting a ReplicaSet 09/07/23 06:18:08.166
STEP: Ensuring resource quota status released usage 09/07/23 06:18:08.17
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:10.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2958" for this suite. 09/07/23 06:18:10.177
------------------------------
â€¢ [SLOW TEST] [11.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:17:59.131
    Sep  7 06:17:59.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:17:59.132
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:17:59.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:17:59.143
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/07/23 06:17:59.145
    STEP: Creating a ResourceQuota 09/07/23 06:18:04.147
    STEP: Ensuring resource quota status is calculated 09/07/23 06:18:04.15
    STEP: Creating a ReplicaSet 09/07/23 06:18:06.154
    STEP: Ensuring resource quota status captures replicaset creation 09/07/23 06:18:06.163
    STEP: Deleting a ReplicaSet 09/07/23 06:18:08.166
    STEP: Ensuring resource quota status released usage 09/07/23 06:18:08.17
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:10.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2958" for this suite. 09/07/23 06:18:10.177
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:10.182
Sep  7 06:18:10.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 06:18:10.183
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:10.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:10.192
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/07/23 06:18:10.195
Sep  7 06:18:10.195: INFO: Creating simple deployment test-deployment-7htjd
Sep  7 06:18:10.203: INFO: deployment "test-deployment-7htjd" doesn't have the required revision set
STEP: Getting /status 09/07/23 06:18:12.21
Sep  7 06:18:12.212: INFO: Deployment test-deployment-7htjd has Conditions: [{Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/07/23 06:18:12.212
Sep  7 06:18:12.219: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 6, 18, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7htjd-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/07/23 06:18:12.219
Sep  7 06:18:12.220: INFO: Observed &Deployment event: ADDED
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7htjd-54bc444df" is progressing.}
Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
Sep  7 06:18:12.220: INFO: Found Deployment test-deployment-7htjd in namespace deployment-8654 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  7 06:18:12.220: INFO: Deployment test-deployment-7htjd has an updated status
STEP: patching the Statefulset Status 09/07/23 06:18:12.22
Sep  7 06:18:12.220: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  7 06:18:12.226: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/07/23 06:18:12.226
Sep  7 06:18:12.228: INFO: Observed &Deployment event: ADDED
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7htjd-54bc444df" is progressing.}
Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
Sep  7 06:18:12.228: INFO: Found deployment test-deployment-7htjd in namespace deployment-8654 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  7 06:18:12.228: INFO: Deployment test-deployment-7htjd has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 06:18:12.230: INFO: Deployment "test-deployment-7htjd":
&Deployment{ObjectMeta:{test-deployment-7htjd  deployment-8654  9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41 25367 1 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-07 06:18:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:18:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003f9bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7htjd-54bc444df",LastUpdateTime:2023-09-07 06:18:12 +0000 UTC,LastTransitionTime:2023-09-07 06:18:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  7 06:18:12.232: INFO: New ReplicaSet "test-deployment-7htjd-54bc444df" of Deployment "test-deployment-7htjd":
&ReplicaSet{ObjectMeta:{test-deployment-7htjd-54bc444df  deployment-8654  85c67032-1f0b-4d6d-9965-f9a38a4b975d 25363 1 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7htjd 9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41 0xc000e56570 0xc000e56571}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:18:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e56778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  7 06:18:12.234: INFO: Pod "test-deployment-7htjd-54bc444df-qbtd7" is available:
&Pod{ObjectMeta:{test-deployment-7htjd-54bc444df-qbtd7 test-deployment-7htjd-54bc444df- deployment-8654  6c2b7686-45c1-46a2-8a10-5737b2660d6f 25362 0 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-7htjd-54bc444df 85c67032-1f0b-4d6d-9965-f9a38a4b975d 0xc004712c50 0xc004712c51}] [] [{kube-controller-manager Update v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85c67032-1f0b-4d6d-9965-f9a38a4b975d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:18:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxtvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxtvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.47,StartTime:2023-09-07 06:18:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:18:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5faef8d52e7bb677f75776ed22150b1d86a0a51e4fab83881b98ffe059fe7793,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8654" for this suite. 09/07/23 06:18:12.238
------------------------------
â€¢ [2.059 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:10.182
    Sep  7 06:18:10.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 06:18:10.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:10.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:10.192
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/07/23 06:18:10.195
    Sep  7 06:18:10.195: INFO: Creating simple deployment test-deployment-7htjd
    Sep  7 06:18:10.203: INFO: deployment "test-deployment-7htjd" doesn't have the required revision set
    STEP: Getting /status 09/07/23 06:18:12.21
    Sep  7 06:18:12.212: INFO: Deployment test-deployment-7htjd has Conditions: [{Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/07/23 06:18:12.212
    Sep  7 06:18:12.219: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 7, 6, 18, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 7, 6, 18, 10, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7htjd-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/07/23 06:18:12.219
    Sep  7 06:18:12.220: INFO: Observed &Deployment event: ADDED
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
    Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7htjd-54bc444df" is progressing.}
    Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
    Sep  7 06:18:12.220: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  7 06:18:12.220: INFO: Observed Deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
    Sep  7 06:18:12.220: INFO: Found Deployment test-deployment-7htjd in namespace deployment-8654 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  7 06:18:12.220: INFO: Deployment test-deployment-7htjd has an updated status
    STEP: patching the Statefulset Status 09/07/23 06:18:12.22
    Sep  7 06:18:12.220: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  7 06:18:12.226: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/07/23 06:18:12.226
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: ADDED
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7htjd-54bc444df"}
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:10 +0000 UTC 2023-09-07 06:18:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7htjd-54bc444df" is progressing.}
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-07 06:18:11 +0000 UTC 2023-09-07 06:18:10 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7htjd-54bc444df" has successfully progressed.}
    Sep  7 06:18:12.228: INFO: Observed deployment test-deployment-7htjd in namespace deployment-8654 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  7 06:18:12.228: INFO: Observed &Deployment event: MODIFIED
    Sep  7 06:18:12.228: INFO: Found deployment test-deployment-7htjd in namespace deployment-8654 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  7 06:18:12.228: INFO: Deployment test-deployment-7htjd has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 06:18:12.230: INFO: Deployment "test-deployment-7htjd":
    &Deployment{ObjectMeta:{test-deployment-7htjd  deployment-8654  9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41 25367 1 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-09-07 06:18:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:18:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003f9bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7htjd-54bc444df",LastUpdateTime:2023-09-07 06:18:12 +0000 UTC,LastTransitionTime:2023-09-07 06:18:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  7 06:18:12.232: INFO: New ReplicaSet "test-deployment-7htjd-54bc444df" of Deployment "test-deployment-7htjd":
    &ReplicaSet{ObjectMeta:{test-deployment-7htjd-54bc444df  deployment-8654  85c67032-1f0b-4d6d-9965-f9a38a4b975d 25363 1 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7htjd 9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41 0xc000e56570 0xc000e56571}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f2e2c12-45c6-4234-a5f2-7b89d7cf1b41\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-07 06:18:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e56778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 06:18:12.234: INFO: Pod "test-deployment-7htjd-54bc444df-qbtd7" is available:
    &Pod{ObjectMeta:{test-deployment-7htjd-54bc444df-qbtd7 test-deployment-7htjd-54bc444df- deployment-8654  6c2b7686-45c1-46a2-8a10-5737b2660d6f 25362 0 2023-09-07 06:18:10 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-7htjd-54bc444df 85c67032-1f0b-4d6d-9965-f9a38a4b975d 0xc004712c50 0xc004712c51}] [] [{kube-controller-manager Update v1 2023-09-07 06:18:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85c67032-1f0b-4d6d-9965-f9a38a4b975d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:18:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxtvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxtvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:18:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.47,StartTime:2023-09-07 06:18:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:18:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5faef8d52e7bb677f75776ed22150b1d86a0a51e4fab83881b98ffe059fe7793,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8654" for this suite. 09/07/23 06:18:12.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:12.242
Sep  7 06:18:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 06:18:12.242
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:12.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:12.25
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/07/23 06:18:12.252
Sep  7 06:18:12.257: INFO: Waiting up to 5m0s for pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464" in namespace "var-expansion-2335" to be "Succeeded or Failed"
Sep  7 06:18:12.259: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03993ms
Sep  7 06:18:14.262: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004904967s
Sep  7 06:18:16.263: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006146162s
STEP: Saw pod success 09/07/23 06:18:16.263
Sep  7 06:18:16.263: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464" satisfied condition "Succeeded or Failed"
Sep  7 06:18:16.265: INFO: Trying to get logs from node kind-worker pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:18:16.269
Sep  7 06:18:16.278: INFO: Waiting for pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 to disappear
Sep  7 06:18:16.279: INFO: Pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:16.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2335" for this suite. 09/07/23 06:18:16.281
------------------------------
â€¢ [4.043 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:12.242
    Sep  7 06:18:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 06:18:12.242
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:12.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:12.25
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/07/23 06:18:12.252
    Sep  7 06:18:12.257: INFO: Waiting up to 5m0s for pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464" in namespace "var-expansion-2335" to be "Succeeded or Failed"
    Sep  7 06:18:12.259: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03993ms
    Sep  7 06:18:14.262: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004904967s
    Sep  7 06:18:16.263: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006146162s
    STEP: Saw pod success 09/07/23 06:18:16.263
    Sep  7 06:18:16.263: INFO: Pod "var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464" satisfied condition "Succeeded or Failed"
    Sep  7 06:18:16.265: INFO: Trying to get logs from node kind-worker pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:18:16.269
    Sep  7 06:18:16.278: INFO: Waiting for pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 to disappear
    Sep  7 06:18:16.279: INFO: Pod var-expansion-225495ff-1477-410a-ad28-ec55ac3f7464 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:16.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2335" for this suite. 09/07/23 06:18:16.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:16.286
Sep  7 06:18:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:18:16.286
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:16.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:16.294
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/07/23 06:18:16.296
Sep  7 06:18:16.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 create -f -'
Sep  7 06:18:16.766: INFO: stderr: ""
Sep  7 06:18:16.766: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:16.766
Sep  7 06:18:16.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:16.823: INFO: stderr: ""
Sep  7 06:18:16.823: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
Sep  7 06:18:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:16.877: INFO: stderr: ""
Sep  7 06:18:16.877: INFO: stdout: ""
Sep  7 06:18:16.877: INFO: update-demo-nautilus-dmv7q is created but not running
Sep  7 06:18:21.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:21.928: INFO: stderr: ""
Sep  7 06:18:21.928: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
Sep  7 06:18:21.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:21.980: INFO: stderr: ""
Sep  7 06:18:21.980: INFO: stdout: "true"
Sep  7 06:18:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 06:18:22.032: INFO: stderr: ""
Sep  7 06:18:22.032: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 06:18:22.032: INFO: validating pod update-demo-nautilus-dmv7q
Sep  7 06:18:22.035: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 06:18:22.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 06:18:22.035: INFO: update-demo-nautilus-dmv7q is verified up and running
Sep  7 06:18:22.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:22.090: INFO: stderr: ""
Sep  7 06:18:22.090: INFO: stdout: "true"
Sep  7 06:18:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 06:18:22.142: INFO: stderr: ""
Sep  7 06:18:22.142: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 06:18:22.142: INFO: validating pod update-demo-nautilus-qcnb4
Sep  7 06:18:22.144: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 06:18:22.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 06:18:22.144: INFO: update-demo-nautilus-qcnb4 is verified up and running
STEP: scaling down the replication controller 09/07/23 06:18:22.144
Sep  7 06:18:22.145: INFO: scanned /root for discovery docs: <nil>
Sep  7 06:18:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  7 06:18:23.207: INFO: stderr: ""
Sep  7 06:18:23.207: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:23.207
Sep  7 06:18:23.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:23.265: INFO: stderr: ""
Sep  7 06:18:23.265: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/07/23 06:18:23.265
Sep  7 06:18:28.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:28.318: INFO: stderr: ""
Sep  7 06:18:28.318: INFO: stdout: "update-demo-nautilus-qcnb4 "
Sep  7 06:18:28.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:28.370: INFO: stderr: ""
Sep  7 06:18:28.370: INFO: stdout: "true"
Sep  7 06:18:28.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 06:18:28.422: INFO: stderr: ""
Sep  7 06:18:28.422: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 06:18:28.422: INFO: validating pod update-demo-nautilus-qcnb4
Sep  7 06:18:28.424: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 06:18:28.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 06:18:28.424: INFO: update-demo-nautilus-qcnb4 is verified up and running
STEP: scaling up the replication controller 09/07/23 06:18:28.424
Sep  7 06:18:28.425: INFO: scanned /root for discovery docs: <nil>
Sep  7 06:18:28.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  7 06:18:29.489: INFO: stderr: ""
Sep  7 06:18:29.489: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:29.489
Sep  7 06:18:29.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:29.542: INFO: stderr: ""
Sep  7 06:18:29.542: INFO: stdout: "update-demo-nautilus-9cbvg update-demo-nautilus-qcnb4 "
Sep  7 06:18:29.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:29.595: INFO: stderr: ""
Sep  7 06:18:29.595: INFO: stdout: ""
Sep  7 06:18:29.595: INFO: update-demo-nautilus-9cbvg is created but not running
Sep  7 06:18:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  7 06:18:34.645: INFO: stderr: ""
Sep  7 06:18:34.645: INFO: stdout: "update-demo-nautilus-9cbvg update-demo-nautilus-qcnb4 "
Sep  7 06:18:34.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:34.697: INFO: stderr: ""
Sep  7 06:18:34.697: INFO: stdout: "true"
Sep  7 06:18:34.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 06:18:34.749: INFO: stderr: ""
Sep  7 06:18:34.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 06:18:34.749: INFO: validating pod update-demo-nautilus-9cbvg
Sep  7 06:18:34.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 06:18:34.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 06:18:34.752: INFO: update-demo-nautilus-9cbvg is verified up and running
Sep  7 06:18:34.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  7 06:18:34.800: INFO: stderr: ""
Sep  7 06:18:34.800: INFO: stdout: "true"
Sep  7 06:18:34.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  7 06:18:34.847: INFO: stderr: ""
Sep  7 06:18:34.847: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  7 06:18:34.847: INFO: validating pod update-demo-nautilus-qcnb4
Sep  7 06:18:34.850: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  7 06:18:34.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  7 06:18:34.850: INFO: update-demo-nautilus-qcnb4 is verified up and running
STEP: using delete to clean up resources 09/07/23 06:18:34.85
Sep  7 06:18:34.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
Sep  7 06:18:34.902: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  7 06:18:34.902: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  7 06:18:34.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get rc,svc -l name=update-demo --no-headers'
Sep  7 06:18:34.964: INFO: stderr: "No resources found in kubectl-6553 namespace.\n"
Sep  7 06:18:34.964: INFO: stdout: ""
Sep  7 06:18:34.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  7 06:18:35.019: INFO: stderr: ""
Sep  7 06:18:35.019: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6553" for this suite. 09/07/23 06:18:35.021
------------------------------
â€¢ [SLOW TEST] [18.740 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:16.286
    Sep  7 06:18:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:18:16.286
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:16.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:16.294
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/07/23 06:18:16.296
    Sep  7 06:18:16.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 create -f -'
    Sep  7 06:18:16.766: INFO: stderr: ""
    Sep  7 06:18:16.766: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:16.766
    Sep  7 06:18:16.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:16.823: INFO: stderr: ""
    Sep  7 06:18:16.823: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
    Sep  7 06:18:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:16.877: INFO: stderr: ""
    Sep  7 06:18:16.877: INFO: stdout: ""
    Sep  7 06:18:16.877: INFO: update-demo-nautilus-dmv7q is created but not running
    Sep  7 06:18:21.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:21.928: INFO: stderr: ""
    Sep  7 06:18:21.928: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
    Sep  7 06:18:21.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:21.980: INFO: stderr: ""
    Sep  7 06:18:21.980: INFO: stdout: "true"
    Sep  7 06:18:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-dmv7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 06:18:22.032: INFO: stderr: ""
    Sep  7 06:18:22.032: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 06:18:22.032: INFO: validating pod update-demo-nautilus-dmv7q
    Sep  7 06:18:22.035: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 06:18:22.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 06:18:22.035: INFO: update-demo-nautilus-dmv7q is verified up and running
    Sep  7 06:18:22.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:22.090: INFO: stderr: ""
    Sep  7 06:18:22.090: INFO: stdout: "true"
    Sep  7 06:18:22.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 06:18:22.142: INFO: stderr: ""
    Sep  7 06:18:22.142: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 06:18:22.142: INFO: validating pod update-demo-nautilus-qcnb4
    Sep  7 06:18:22.144: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 06:18:22.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 06:18:22.144: INFO: update-demo-nautilus-qcnb4 is verified up and running
    STEP: scaling down the replication controller 09/07/23 06:18:22.144
    Sep  7 06:18:22.145: INFO: scanned /root for discovery docs: <nil>
    Sep  7 06:18:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  7 06:18:23.207: INFO: stderr: ""
    Sep  7 06:18:23.207: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:23.207
    Sep  7 06:18:23.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:23.265: INFO: stderr: ""
    Sep  7 06:18:23.265: INFO: stdout: "update-demo-nautilus-dmv7q update-demo-nautilus-qcnb4 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/07/23 06:18:23.265
    Sep  7 06:18:28.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:28.318: INFO: stderr: ""
    Sep  7 06:18:28.318: INFO: stdout: "update-demo-nautilus-qcnb4 "
    Sep  7 06:18:28.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:28.370: INFO: stderr: ""
    Sep  7 06:18:28.370: INFO: stdout: "true"
    Sep  7 06:18:28.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 06:18:28.422: INFO: stderr: ""
    Sep  7 06:18:28.422: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 06:18:28.422: INFO: validating pod update-demo-nautilus-qcnb4
    Sep  7 06:18:28.424: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 06:18:28.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 06:18:28.424: INFO: update-demo-nautilus-qcnb4 is verified up and running
    STEP: scaling up the replication controller 09/07/23 06:18:28.424
    Sep  7 06:18:28.425: INFO: scanned /root for discovery docs: <nil>
    Sep  7 06:18:28.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  7 06:18:29.489: INFO: stderr: ""
    Sep  7 06:18:29.489: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/07/23 06:18:29.489
    Sep  7 06:18:29.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:29.542: INFO: stderr: ""
    Sep  7 06:18:29.542: INFO: stdout: "update-demo-nautilus-9cbvg update-demo-nautilus-qcnb4 "
    Sep  7 06:18:29.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:29.595: INFO: stderr: ""
    Sep  7 06:18:29.595: INFO: stdout: ""
    Sep  7 06:18:29.595: INFO: update-demo-nautilus-9cbvg is created but not running
    Sep  7 06:18:34.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  7 06:18:34.645: INFO: stderr: ""
    Sep  7 06:18:34.645: INFO: stdout: "update-demo-nautilus-9cbvg update-demo-nautilus-qcnb4 "
    Sep  7 06:18:34.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:34.697: INFO: stderr: ""
    Sep  7 06:18:34.697: INFO: stdout: "true"
    Sep  7 06:18:34.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-9cbvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 06:18:34.749: INFO: stderr: ""
    Sep  7 06:18:34.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 06:18:34.749: INFO: validating pod update-demo-nautilus-9cbvg
    Sep  7 06:18:34.752: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 06:18:34.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 06:18:34.752: INFO: update-demo-nautilus-9cbvg is verified up and running
    Sep  7 06:18:34.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  7 06:18:34.800: INFO: stderr: ""
    Sep  7 06:18:34.800: INFO: stdout: "true"
    Sep  7 06:18:34.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods update-demo-nautilus-qcnb4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  7 06:18:34.847: INFO: stderr: ""
    Sep  7 06:18:34.847: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  7 06:18:34.847: INFO: validating pod update-demo-nautilus-qcnb4
    Sep  7 06:18:34.850: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  7 06:18:34.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  7 06:18:34.850: INFO: update-demo-nautilus-qcnb4 is verified up and running
    STEP: using delete to clean up resources 09/07/23 06:18:34.85
    Sep  7 06:18:34.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 delete --grace-period=0 --force -f -'
    Sep  7 06:18:34.902: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  7 06:18:34.902: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  7 06:18:34.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get rc,svc -l name=update-demo --no-headers'
    Sep  7 06:18:34.964: INFO: stderr: "No resources found in kubectl-6553 namespace.\n"
    Sep  7 06:18:34.964: INFO: stdout: ""
    Sep  7 06:18:34.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6553 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  7 06:18:35.019: INFO: stderr: ""
    Sep  7 06:18:35.019: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6553" for this suite. 09/07/23 06:18:35.021
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:35.025
Sep  7 06:18:35.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:18:35.026
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:35.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:35.037
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/07/23 06:18:35.038
Sep  7 06:18:35.044: INFO: Waiting up to 5m0s for pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96" in namespace "downward-api-5730" to be "Succeeded or Failed"
Sep  7 06:18:35.045: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37586ms
Sep  7 06:18:37.049: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00463822s
Sep  7 06:18:39.048: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004010538s
STEP: Saw pod success 09/07/23 06:18:39.048
Sep  7 06:18:39.048: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96" satisfied condition "Succeeded or Failed"
Sep  7 06:18:39.050: INFO: Trying to get logs from node kind-worker2 pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:18:39.058
Sep  7 06:18:39.066: INFO: Waiting for pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 to disappear
Sep  7 06:18:39.068: INFO: Pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:39.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5730" for this suite. 09/07/23 06:18:39.069
------------------------------
â€¢ [4.047 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:35.025
    Sep  7 06:18:35.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:18:35.026
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:35.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:35.037
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/07/23 06:18:35.038
    Sep  7 06:18:35.044: INFO: Waiting up to 5m0s for pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96" in namespace "downward-api-5730" to be "Succeeded or Failed"
    Sep  7 06:18:35.045: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.37586ms
    Sep  7 06:18:37.049: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00463822s
    Sep  7 06:18:39.048: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004010538s
    STEP: Saw pod success 09/07/23 06:18:39.048
    Sep  7 06:18:39.048: INFO: Pod "downward-api-5ea178db-e325-4542-bcef-f65179a64c96" satisfied condition "Succeeded or Failed"
    Sep  7 06:18:39.050: INFO: Trying to get logs from node kind-worker2 pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:18:39.058
    Sep  7 06:18:39.066: INFO: Waiting for pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 to disappear
    Sep  7 06:18:39.068: INFO: Pod downward-api-5ea178db-e325-4542-bcef-f65179a64c96 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:39.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5730" for this suite. 09/07/23 06:18:39.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:39.073
Sep  7 06:18:39.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:18:39.074
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:39.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:39.082
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-69334775-9897-4c0f-92ea-7a56c3a2a201 09/07/23 06:18:39.083
STEP: Creating a pod to test consume configMaps 09/07/23 06:18:39.086
Sep  7 06:18:39.090: INFO: Waiting up to 5m0s for pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d" in namespace "configmap-1822" to be "Succeeded or Failed"
Sep  7 06:18:39.091: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38046ms
Sep  7 06:18:41.094: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004837379s
Sep  7 06:18:43.093: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003903036s
STEP: Saw pod success 09/07/23 06:18:43.093
Sep  7 06:18:43.094: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d" satisfied condition "Succeeded or Failed"
Sep  7 06:18:43.095: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d container agnhost-container: <nil>
STEP: delete the pod 09/07/23 06:18:43.099
Sep  7 06:18:43.107: INFO: Waiting for pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d to disappear
Sep  7 06:18:43.108: INFO: Pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:43.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1822" for this suite. 09/07/23 06:18:43.11
------------------------------
â€¢ [4.040 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:39.073
    Sep  7 06:18:39.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:18:39.074
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:39.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:39.082
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-69334775-9897-4c0f-92ea-7a56c3a2a201 09/07/23 06:18:39.083
    STEP: Creating a pod to test consume configMaps 09/07/23 06:18:39.086
    Sep  7 06:18:39.090: INFO: Waiting up to 5m0s for pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d" in namespace "configmap-1822" to be "Succeeded or Failed"
    Sep  7 06:18:39.091: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38046ms
    Sep  7 06:18:41.094: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004837379s
    Sep  7 06:18:43.093: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003903036s
    STEP: Saw pod success 09/07/23 06:18:43.093
    Sep  7 06:18:43.094: INFO: Pod "pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d" satisfied condition "Succeeded or Failed"
    Sep  7 06:18:43.095: INFO: Trying to get logs from node kind-worker2 pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d container agnhost-container: <nil>
    STEP: delete the pod 09/07/23 06:18:43.099
    Sep  7 06:18:43.107: INFO: Waiting for pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d to disappear
    Sep  7 06:18:43.108: INFO: Pod pod-configmaps-70460c92-0105-40b4-9687-b8020f7a651d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:43.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1822" for this suite. 09/07/23 06:18:43.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:43.114
Sep  7 06:18:43.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:18:43.114
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:43.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:43.123
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  7 06:18:43.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6755 version'
Sep  7 06:18:43.171: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  7 06:18:43.171: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T00:45:36Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:43.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6755" for this suite. 09/07/23 06:18:43.173
------------------------------
â€¢ [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:43.114
    Sep  7 06:18:43.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:18:43.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:43.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:43.123
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  7 06:18:43.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-6755 version'
    Sep  7 06:18:43.171: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  7 06:18:43.171: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-15T00:45:36Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:43.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6755" for this suite. 09/07/23 06:18:43.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:43.177
Sep  7 06:18:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:18:43.177
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:43.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:43.188
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:18:43.196
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:18:43.384
STEP: Deploying the webhook pod 09/07/23 06:18:43.389
STEP: Wait for the deployment to be ready 09/07/23 06:18:43.398
Sep  7 06:18:43.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 06:18:45.409
STEP: Verifying the service has paired with the endpoint 09/07/23 06:18:45.417
Sep  7 06:18:46.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/07/23 06:18:46.419
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/07/23 06:18:46.434
STEP: Creating a configMap that should not be mutated 09/07/23 06:18:46.439
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/07/23 06:18:46.444
STEP: Creating a configMap that should be mutated 09/07/23 06:18:46.45
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:18:46.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-145" for this suite. 09/07/23 06:18:46.484
STEP: Destroying namespace "webhook-145-markers" for this suite. 09/07/23 06:18:46.487
------------------------------
â€¢ [3.315 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:43.177
    Sep  7 06:18:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:18:43.177
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:43.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:43.188
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:18:43.196
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:18:43.384
    STEP: Deploying the webhook pod 09/07/23 06:18:43.389
    STEP: Wait for the deployment to be ready 09/07/23 06:18:43.398
    Sep  7 06:18:43.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 06:18:45.409
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:18:45.417
    Sep  7 06:18:46.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/07/23 06:18:46.419
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/07/23 06:18:46.434
    STEP: Creating a configMap that should not be mutated 09/07/23 06:18:46.439
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/07/23 06:18:46.444
    STEP: Creating a configMap that should be mutated 09/07/23 06:18:46.45
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:18:46.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-145" for this suite. 09/07/23 06:18:46.484
    STEP: Destroying namespace "webhook-145-markers" for this suite. 09/07/23 06:18:46.487
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:18:46.492
Sep  7 06:18:46.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:18:46.493
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:46.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:46.503
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  7 06:18:46.514: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 06:19:46.528: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:19:46.53
Sep  7 06:19:46.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption-path 09/07/23 06:19:46.531
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:19:46.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:19:46.539
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/07/23 06:19:46.541
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 06:19:46.541
Sep  7 06:19:46.547: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-506" to be "running"
Sep  7 06:19:46.548: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19291ms
Sep  7 06:19:48.552: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319439s
Sep  7 06:19:48.552: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 06:19:48.553
Sep  7 06:19:48.562: INFO: found a healthy node: kind-worker
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  7 06:19:54.603: INFO: pods created so far: [1 1 1]
Sep  7 06:19:54.603: INFO: length of pods created so far: 3
Sep  7 06:19:58.612: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:05.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:05.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-506" for this suite. 09/07/23 06:20:05.664
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6905" for this suite. 09/07/23 06:20:05.667
------------------------------
â€¢ [SLOW TEST] [79.180 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:18:46.492
    Sep  7 06:18:46.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:18:46.493
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:18:46.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:18:46.503
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  7 06:18:46.514: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 06:19:46.528: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:19:46.53
    Sep  7 06:19:46.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption-path 09/07/23 06:19:46.531
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:19:46.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:19:46.539
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/07/23 06:19:46.541
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/07/23 06:19:46.541
    Sep  7 06:19:46.547: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-506" to be "running"
    Sep  7 06:19:46.548: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.19291ms
    Sep  7 06:19:48.552: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319439s
    Sep  7 06:19:48.552: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/07/23 06:19:48.553
    Sep  7 06:19:48.562: INFO: found a healthy node: kind-worker
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  7 06:19:54.603: INFO: pods created so far: [1 1 1]
    Sep  7 06:19:54.603: INFO: length of pods created so far: 3
    Sep  7 06:19:58.612: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:05.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:05.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-506" for this suite. 09/07/23 06:20:05.664
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6905" for this suite. 09/07/23 06:20:05.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:05.672
Sep  7 06:20:05.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:20:05.673
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:05.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:05.684
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:05.686
Sep  7 06:20:05.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29" in namespace "downward-api-6978" to be "Succeeded or Failed"
Sep  7 06:20:05.694: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47235ms
Sep  7 06:20:07.697: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004851726s
Sep  7 06:20:09.697: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004833729s
STEP: Saw pod success 09/07/23 06:20:09.698
Sep  7 06:20:09.698: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29" satisfied condition "Succeeded or Failed"
Sep  7 06:20:09.700: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 container client-container: <nil>
STEP: delete the pod 09/07/23 06:20:09.703
Sep  7 06:20:09.711: INFO: Waiting for pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 to disappear
Sep  7 06:20:09.712: INFO: Pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:09.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6978" for this suite. 09/07/23 06:20:09.714
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:05.672
    Sep  7 06:20:05.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:20:05.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:05.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:05.684
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:05.686
    Sep  7 06:20:05.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29" in namespace "downward-api-6978" to be "Succeeded or Failed"
    Sep  7 06:20:05.694: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47235ms
    Sep  7 06:20:07.697: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004851726s
    Sep  7 06:20:09.697: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004833729s
    STEP: Saw pod success 09/07/23 06:20:09.698
    Sep  7 06:20:09.698: INFO: Pod "downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29" satisfied condition "Succeeded or Failed"
    Sep  7 06:20:09.700: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:20:09.703
    Sep  7 06:20:09.711: INFO: Waiting for pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 to disappear
    Sep  7 06:20:09.712: INFO: Pod downwardapi-volume-39fd504a-cfe5-4881-9922-94ba0e71bf29 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:09.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6978" for this suite. 09/07/23 06:20:09.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:09.719
Sep  7 06:20:09.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:20:09.72
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:09.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:09.731
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:09.733
Sep  7 06:20:09.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3" in namespace "projected-555" to be "Succeeded or Failed"
Sep  7 06:20:09.738: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363941ms
Sep  7 06:20:11.741: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003837664s
Sep  7 06:20:13.742: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005438288s
STEP: Saw pod success 09/07/23 06:20:13.742
Sep  7 06:20:13.743: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3" satisfied condition "Succeeded or Failed"
Sep  7 06:20:13.744: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 container client-container: <nil>
STEP: delete the pod 09/07/23 06:20:13.748
Sep  7 06:20:13.758: INFO: Waiting for pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 to disappear
Sep  7 06:20:13.759: INFO: Pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:13.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-555" for this suite. 09/07/23 06:20:13.761
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:09.719
    Sep  7 06:20:09.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:20:09.72
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:09.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:09.731
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:09.733
    Sep  7 06:20:09.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3" in namespace "projected-555" to be "Succeeded or Failed"
    Sep  7 06:20:09.738: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363941ms
    Sep  7 06:20:11.741: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003837664s
    Sep  7 06:20:13.742: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005438288s
    STEP: Saw pod success 09/07/23 06:20:13.742
    Sep  7 06:20:13.743: INFO: Pod "downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3" satisfied condition "Succeeded or Failed"
    Sep  7 06:20:13.744: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:20:13.748
    Sep  7 06:20:13.758: INFO: Waiting for pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 to disappear
    Sep  7 06:20:13.759: INFO: Pod downwardapi-volume-5f229f2a-2ab6-4c54-a9ca-c346cb4986a3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:13.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-555" for this suite. 09/07/23 06:20:13.761
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:13.765
Sep  7 06:20:13.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename security-context-test 09/07/23 06:20:13.766
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:13.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:13.774
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  7 06:20:13.779: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357" in namespace "security-context-test-3973" to be "Succeeded or Failed"
Sep  7 06:20:13.780: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23557ms
Sep  7 06:20:15.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Running", Reason="", readiness=true. Elapsed: 2.004008513s
Sep  7 06:20:17.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Running", Reason="", readiness=false. Elapsed: 4.003770025s
Sep  7 06:20:19.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003964457s
Sep  7 06:20:19.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:19.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3973" for this suite. 09/07/23 06:20:19.793
------------------------------
â€¢ [SLOW TEST] [6.032 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:13.765
    Sep  7 06:20:13.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename security-context-test 09/07/23 06:20:13.766
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:13.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:13.774
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  7 06:20:13.779: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357" in namespace "security-context-test-3973" to be "Succeeded or Failed"
    Sep  7 06:20:13.780: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Pending", Reason="", readiness=false. Elapsed: 1.23557ms
    Sep  7 06:20:15.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Running", Reason="", readiness=true. Elapsed: 2.004008513s
    Sep  7 06:20:17.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Running", Reason="", readiness=false. Elapsed: 4.003770025s
    Sep  7 06:20:19.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.003964457s
    Sep  7 06:20:19.783: INFO: Pod "alpine-nnp-false-8f230287-530c-4e09-a191-a826c4e10357" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:19.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3973" for this suite. 09/07/23 06:20:19.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:19.797
Sep  7 06:20:19.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:20:19.798
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:19.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:19.807
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/07/23 06:20:19.809
Sep  7 06:20:19.815: INFO: Waiting up to 5m0s for pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44" in namespace "projected-1442" to be "running and ready"
Sep  7 06:20:19.817: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.408761ms
Sep  7 06:20:19.817: INFO: The phase of Pod labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:20:21.820: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44": Phase="Running", Reason="", readiness=true. Elapsed: 2.004690532s
Sep  7 06:20:21.820: INFO: The phase of Pod labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44 is Running (Ready = true)
Sep  7 06:20:21.820: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44" satisfied condition "running and ready"
Sep  7 06:20:22.334: INFO: Successfully updated pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:26.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1442" for this suite. 09/07/23 06:20:26.35
------------------------------
â€¢ [SLOW TEST] [6.557 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:19.797
    Sep  7 06:20:19.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:20:19.798
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:19.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:19.807
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/07/23 06:20:19.809
    Sep  7 06:20:19.815: INFO: Waiting up to 5m0s for pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44" in namespace "projected-1442" to be "running and ready"
    Sep  7 06:20:19.817: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.408761ms
    Sep  7 06:20:19.817: INFO: The phase of Pod labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:20:21.820: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44": Phase="Running", Reason="", readiness=true. Elapsed: 2.004690532s
    Sep  7 06:20:21.820: INFO: The phase of Pod labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44 is Running (Ready = true)
    Sep  7 06:20:21.820: INFO: Pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44" satisfied condition "running and ready"
    Sep  7 06:20:22.334: INFO: Successfully updated pod "labelsupdate970fc6bd-0fc7-42b1-83e9-193de1c32d44"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:26.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1442" for this suite. 09/07/23 06:20:26.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:26.355
Sep  7 06:20:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:20:26.355
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:26.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:26.364
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-6cfa3732-ffad-4b88-8853-7fbd4ae26f56 09/07/23 06:20:26.367
STEP: Creating the pod 09/07/23 06:20:26.369
Sep  7 06:20:26.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927" in namespace "configmap-4503" to be "running and ready"
Sep  7 06:20:26.374: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4512ms
Sep  7 06:20:26.374: INFO: The phase of Pod pod-configmaps-21b38455-36f1-4193-b408-af806d745927 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:20:28.377: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927": Phase="Running", Reason="", readiness=true. Elapsed: 2.00397693s
Sep  7 06:20:28.377: INFO: The phase of Pod pod-configmaps-21b38455-36f1-4193-b408-af806d745927 is Running (Ready = true)
Sep  7 06:20:28.377: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-6cfa3732-ffad-4b88-8853-7fbd4ae26f56 09/07/23 06:20:28.382
STEP: waiting to observe update in volume 09/07/23 06:20:28.384
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:32.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4503" for this suite. 09/07/23 06:20:32.403
------------------------------
â€¢ [SLOW TEST] [6.053 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:26.355
    Sep  7 06:20:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:20:26.355
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:26.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:26.364
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-6cfa3732-ffad-4b88-8853-7fbd4ae26f56 09/07/23 06:20:26.367
    STEP: Creating the pod 09/07/23 06:20:26.369
    Sep  7 06:20:26.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927" in namespace "configmap-4503" to be "running and ready"
    Sep  7 06:20:26.374: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4512ms
    Sep  7 06:20:26.374: INFO: The phase of Pod pod-configmaps-21b38455-36f1-4193-b408-af806d745927 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:20:28.377: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927": Phase="Running", Reason="", readiness=true. Elapsed: 2.00397693s
    Sep  7 06:20:28.377: INFO: The phase of Pod pod-configmaps-21b38455-36f1-4193-b408-af806d745927 is Running (Ready = true)
    Sep  7 06:20:28.377: INFO: Pod "pod-configmaps-21b38455-36f1-4193-b408-af806d745927" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-6cfa3732-ffad-4b88-8853-7fbd4ae26f56 09/07/23 06:20:28.382
    STEP: waiting to observe update in volume 09/07/23 06:20:28.384
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:32.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4503" for this suite. 09/07/23 06:20:32.403
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:32.408
Sep  7 06:20:32.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename namespaces 09/07/23 06:20:32.409
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:32.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:32.417
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/07/23 06:20:32.419
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:32.427
STEP: Creating a pod in the namespace 09/07/23 06:20:32.428
STEP: Waiting for the pod to have running status 09/07/23 06:20:32.432
Sep  7 06:20:32.432: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9908" to be "running"
Sep  7 06:20:32.433: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179431ms
Sep  7 06:20:34.435: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003233979s
Sep  7 06:20:34.435: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/07/23 06:20:34.435
STEP: Waiting for the namespace to be removed. 09/07/23 06:20:34.439
STEP: Recreating the namespace 09/07/23 06:20:45.441
STEP: Verifying there are no pods in the namespace 09/07/23 06:20:45.449
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:45.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-142" for this suite. 09/07/23 06:20:45.453
STEP: Destroying namespace "nsdeletetest-9908" for this suite. 09/07/23 06:20:45.457
Sep  7 06:20:45.459: INFO: Namespace nsdeletetest-9908 was already deleted
STEP: Destroying namespace "nsdeletetest-2257" for this suite. 09/07/23 06:20:45.459
------------------------------
â€¢ [SLOW TEST] [13.053 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:32.408
    Sep  7 06:20:32.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename namespaces 09/07/23 06:20:32.409
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:32.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:32.417
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/07/23 06:20:32.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:32.427
    STEP: Creating a pod in the namespace 09/07/23 06:20:32.428
    STEP: Waiting for the pod to have running status 09/07/23 06:20:32.432
    Sep  7 06:20:32.432: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9908" to be "running"
    Sep  7 06:20:32.433: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.179431ms
    Sep  7 06:20:34.435: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003233979s
    Sep  7 06:20:34.435: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/07/23 06:20:34.435
    STEP: Waiting for the namespace to be removed. 09/07/23 06:20:34.439
    STEP: Recreating the namespace 09/07/23 06:20:45.441
    STEP: Verifying there are no pods in the namespace 09/07/23 06:20:45.449
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:45.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-142" for this suite. 09/07/23 06:20:45.453
    STEP: Destroying namespace "nsdeletetest-9908" for this suite. 09/07/23 06:20:45.457
    Sep  7 06:20:45.459: INFO: Namespace nsdeletetest-9908 was already deleted
    STEP: Destroying namespace "nsdeletetest-2257" for this suite. 09/07/23 06:20:45.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:45.462
Sep  7 06:20:45.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:20:45.463
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:45.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:45.47
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  7 06:20:45.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: creating the pod 09/07/23 06:20:45.472
STEP: submitting the pod to kubernetes 09/07/23 06:20:45.472
Sep  7 06:20:45.477: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2" in namespace "pods-6412" to be "running and ready"
Sep  7 06:20:45.479: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33661ms
Sep  7 06:20:45.479: INFO: The phase of Pod pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:20:47.482: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319546s
Sep  7 06:20:47.482: INFO: The phase of Pod pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2 is Running (Ready = true)
Sep  7 06:20:47.482: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:47.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6412" for this suite. 09/07/23 06:20:47.491
------------------------------
â€¢ [2.032 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:45.462
    Sep  7 06:20:45.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:20:45.463
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:45.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:45.47
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  7 06:20:45.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: creating the pod 09/07/23 06:20:45.472
    STEP: submitting the pod to kubernetes 09/07/23 06:20:45.472
    Sep  7 06:20:45.477: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2" in namespace "pods-6412" to be "running and ready"
    Sep  7 06:20:45.479: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33661ms
    Sep  7 06:20:45.479: INFO: The phase of Pod pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:20:47.482: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004319546s
    Sep  7 06:20:47.482: INFO: The phase of Pod pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2 is Running (Ready = true)
    Sep  7 06:20:47.482: INFO: Pod "pod-logs-websocket-bd2de175-4dcb-4b9b-9ed1-8850d38f6af2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:47.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6412" for this suite. 09/07/23 06:20:47.491
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:47.494
Sep  7 06:20:47.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:20:47.495
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:47.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:47.503
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:47.505
Sep  7 06:20:47.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca" in namespace "downward-api-6343" to be "Succeeded or Failed"
Sep  7 06:20:47.511: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47605ms
Sep  7 06:20:49.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005066236s
Sep  7 06:20:51.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00524631s
STEP: Saw pod success 09/07/23 06:20:51.514
Sep  7 06:20:51.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca" satisfied condition "Succeeded or Failed"
Sep  7 06:20:51.516: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca container client-container: <nil>
STEP: delete the pod 09/07/23 06:20:51.52
Sep  7 06:20:51.529: INFO: Waiting for pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca to disappear
Sep  7 06:20:51.531: INFO: Pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:51.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6343" for this suite. 09/07/23 06:20:51.533
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:47.494
    Sep  7 06:20:47.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:20:47.495
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:47.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:47.503
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:20:47.505
    Sep  7 06:20:47.509: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca" in namespace "downward-api-6343" to be "Succeeded or Failed"
    Sep  7 06:20:47.511: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.47605ms
    Sep  7 06:20:49.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005066236s
    Sep  7 06:20:51.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00524631s
    STEP: Saw pod success 09/07/23 06:20:51.514
    Sep  7 06:20:51.514: INFO: Pod "downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca" satisfied condition "Succeeded or Failed"
    Sep  7 06:20:51.516: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca container client-container: <nil>
    STEP: delete the pod 09/07/23 06:20:51.52
    Sep  7 06:20:51.529: INFO: Waiting for pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca to disappear
    Sep  7 06:20:51.531: INFO: Pod downwardapi-volume-3674358a-00e2-4332-95cf-156c3204dfca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:51.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6343" for this suite. 09/07/23 06:20:51.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:51.536
Sep  7 06:20:51.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:20:51.536
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:51.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:51.545
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  7 06:20:51.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8470" for this suite. 09/07/23 06:20:51.552
------------------------------
â€¢ [0.020 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:51.536
    Sep  7 06:20:51.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:20:51.536
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:51.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:51.545
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:20:51.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8470" for this suite. 09/07/23 06:20:51.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:20:51.556
Sep  7 06:20:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pod-network-test 09/07/23 06:20:51.557
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:51.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:51.565
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9440 09/07/23 06:20:51.567
STEP: creating a selector 09/07/23 06:20:51.567
STEP: Creating the service pods in kubernetes 09/07/23 06:20:51.567
Sep  7 06:20:51.567: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  7 06:20:51.583: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9440" to be "running and ready"
Sep  7 06:20:51.584: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259741ms
Sep  7 06:20:51.584: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:20:53.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004120065s
Sep  7 06:20:53.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:20:55.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004540968s
Sep  7 06:20:55.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:20:57.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004407921s
Sep  7 06:20:57.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:20:59.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004584994s
Sep  7 06:20:59.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:01.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004312566s
Sep  7 06:21:01.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:03.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005070918s
Sep  7 06:21:03.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:05.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004949489s
Sep  7 06:21:05.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:07.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.003917769s
Sep  7 06:21:07.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:09.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00445627s
Sep  7 06:21:09.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:11.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00466515s
Sep  7 06:21:11.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  7 06:21:13.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005287409s
Sep  7 06:21:13.588: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  7 06:21:13.588: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  7 06:21:13.590: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9440" to be "running and ready"
Sep  7 06:21:13.592: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.632789ms
Sep  7 06:21:13.592: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  7 06:21:13.592: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/07/23 06:21:13.593
Sep  7 06:21:13.598: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9440" to be "running"
Sep  7 06:21:13.600: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53296ms
Sep  7 06:21:15.603: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004801539s
Sep  7 06:21:15.603: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  7 06:21:15.604: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  7 06:21:15.605: INFO: Breadth first check of 10.244.1.62 on host 192.168.8.3...
Sep  7 06:21:15.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.17:9080/dial?request=hostname&protocol=http&host=10.244.1.62&port=8083&tries=1'] Namespace:pod-network-test-9440 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:21:15.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:21:15.606: INFO: ExecWithOptions: Clientset creation
Sep  7 06:21:15.607: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9440/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.17%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.62%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  7 06:21:15.741: INFO: Waiting for responses: map[]
Sep  7 06:21:15.741: INFO: reached 10.244.1.62 after 0/1 tries
Sep  7 06:21:15.741: INFO: Breadth first check of 10.244.2.16 on host 192.168.8.6...
Sep  7 06:21:15.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.17:9080/dial?request=hostname&protocol=http&host=10.244.2.16&port=8083&tries=1'] Namespace:pod-network-test-9440 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:21:15.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:21:15.744: INFO: ExecWithOptions: Clientset creation
Sep  7 06:21:15.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9440/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.17%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  7 06:21:15.838: INFO: Waiting for responses: map[]
Sep  7 06:21:15.838: INFO: reached 10.244.2.16 after 0/1 tries
Sep  7 06:21:15.838: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:15.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9440" for this suite. 09/07/23 06:21:15.84
------------------------------
â€¢ [SLOW TEST] [24.288 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:20:51.556
    Sep  7 06:20:51.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pod-network-test 09/07/23 06:20:51.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:20:51.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:20:51.565
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9440 09/07/23 06:20:51.567
    STEP: creating a selector 09/07/23 06:20:51.567
    STEP: Creating the service pods in kubernetes 09/07/23 06:20:51.567
    Sep  7 06:20:51.567: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  7 06:20:51.583: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9440" to be "running and ready"
    Sep  7 06:20:51.584: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.259741ms
    Sep  7 06:20:51.584: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:20:53.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004120065s
    Sep  7 06:20:53.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:20:55.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.004540968s
    Sep  7 06:20:55.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:20:57.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004407921s
    Sep  7 06:20:57.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:20:59.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004584994s
    Sep  7 06:20:59.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:01.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004312566s
    Sep  7 06:21:01.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:03.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005070918s
    Sep  7 06:21:03.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:05.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.004949489s
    Sep  7 06:21:05.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:07.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.003917769s
    Sep  7 06:21:07.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:09.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.00445627s
    Sep  7 06:21:09.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:11.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00466515s
    Sep  7 06:21:11.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  7 06:21:13.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005287409s
    Sep  7 06:21:13.588: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  7 06:21:13.588: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  7 06:21:13.590: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9440" to be "running and ready"
    Sep  7 06:21:13.592: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.632789ms
    Sep  7 06:21:13.592: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  7 06:21:13.592: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/07/23 06:21:13.593
    Sep  7 06:21:13.598: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9440" to be "running"
    Sep  7 06:21:13.600: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53296ms
    Sep  7 06:21:15.603: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004801539s
    Sep  7 06:21:15.603: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  7 06:21:15.604: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  7 06:21:15.605: INFO: Breadth first check of 10.244.1.62 on host 192.168.8.3...
    Sep  7 06:21:15.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.17:9080/dial?request=hostname&protocol=http&host=10.244.1.62&port=8083&tries=1'] Namespace:pod-network-test-9440 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:21:15.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:21:15.606: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:21:15.607: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9440/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.17%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.62%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  7 06:21:15.741: INFO: Waiting for responses: map[]
    Sep  7 06:21:15.741: INFO: reached 10.244.1.62 after 0/1 tries
    Sep  7 06:21:15.741: INFO: Breadth first check of 10.244.2.16 on host 192.168.8.6...
    Sep  7 06:21:15.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.17:9080/dial?request=hostname&protocol=http&host=10.244.2.16&port=8083&tries=1'] Namespace:pod-network-test-9440 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:21:15.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:21:15.744: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:21:15.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9440/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.2.17%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.16%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  7 06:21:15.838: INFO: Waiting for responses: map[]
    Sep  7 06:21:15.838: INFO: reached 10.244.2.16 after 0/1 tries
    Sep  7 06:21:15.838: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:15.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9440" for this suite. 09/07/23 06:21:15.84
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:15.845
Sep  7 06:21:15.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 06:21:15.845
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:15.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:15.857
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/07/23 06:21:15.858
STEP: Wait for the Deployment to create new ReplicaSet 09/07/23 06:21:15.862
STEP: delete the deployment 09/07/23 06:21:16.366
STEP: wait for all rs to be garbage collected 09/07/23 06:21:16.369
STEP: expected 0 rs, got 1 rs 09/07/23 06:21:16.374
STEP: expected 0 pods, got 2 pods 09/07/23 06:21:16.376
STEP: Gathering metrics 09/07/23 06:21:16.882
Sep  7 06:21:16.894: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 06:21:16.896: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.65374ms
Sep  7 06:21:16.896: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 06:21:16.896: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 06:21:16.934: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:16.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8327" for this suite. 09/07/23 06:21:16.936
------------------------------
â€¢ [1.095 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:15.845
    Sep  7 06:21:15.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 06:21:15.845
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:15.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:15.857
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/07/23 06:21:15.858
    STEP: Wait for the Deployment to create new ReplicaSet 09/07/23 06:21:15.862
    STEP: delete the deployment 09/07/23 06:21:16.366
    STEP: wait for all rs to be garbage collected 09/07/23 06:21:16.369
    STEP: expected 0 rs, got 1 rs 09/07/23 06:21:16.374
    STEP: expected 0 pods, got 2 pods 09/07/23 06:21:16.376
    STEP: Gathering metrics 09/07/23 06:21:16.882
    Sep  7 06:21:16.894: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 06:21:16.896: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.65374ms
    Sep  7 06:21:16.896: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 06:21:16.896: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 06:21:16.934: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:16.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8327" for this suite. 09/07/23 06:21:16.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:16.94
Sep  7 06:21:16.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename lease-test 09/07/23 06:21:16.941
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:16.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:16.952
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-3732" for this suite. 09/07/23 06:21:16.983
------------------------------
â€¢ [0.046 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:16.94
    Sep  7 06:21:16.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename lease-test 09/07/23 06:21:16.941
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:16.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:16.952
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-3732" for this suite. 09/07/23 06:21:16.983
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:16.986
Sep  7 06:21:16.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:21:16.987
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:16.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:16.996
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/07/23 06:21:16.999
STEP: watching for the Service to be added 09/07/23 06:21:17.006
Sep  7 06:21:17.007: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  7 06:21:17.007: INFO: Service test-service-6wjqq created
STEP: Getting /status 09/07/23 06:21:17.007
Sep  7 06:21:17.009: INFO: Service test-service-6wjqq has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/07/23 06:21:17.009
STEP: watching for the Service to be patched 09/07/23 06:21:17.013
Sep  7 06:21:17.014: INFO: observed Service test-service-6wjqq in namespace services-2791 with annotations: map[] & LoadBalancer: {[]}
Sep  7 06:21:17.014: INFO: Found Service test-service-6wjqq in namespace services-2791 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  7 06:21:17.014: INFO: Service test-service-6wjqq has service status patched
STEP: updating the ServiceStatus 09/07/23 06:21:17.014
Sep  7 06:21:17.019: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/07/23 06:21:17.019
Sep  7 06:21:17.019: INFO: Observed Service test-service-6wjqq in namespace services-2791 with annotations: map[] & Conditions: {[]}
Sep  7 06:21:17.019: INFO: Observed event: &Service{ObjectMeta:{test-service-6wjqq  services-2791  83cd6815-f1dd-41cb-ac42-6c6cddc7ae11 26383 0 2023-09-07 06:21:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-07 06:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-07 06:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.8.27,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.8.27],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  7 06:21:17.020: INFO: Found Service test-service-6wjqq in namespace services-2791 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  7 06:21:17.020: INFO: Service test-service-6wjqq has service status updated
STEP: patching the service 09/07/23 06:21:17.02
STEP: watching for the Service to be patched 09/07/23 06:21:17.029
Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
Sep  7 06:21:17.030: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service:patched test-service-static:true]
Sep  7 06:21:17.030: INFO: Service test-service-6wjqq patched
STEP: deleting the service 09/07/23 06:21:17.03
STEP: watching for the Service to be deleted 09/07/23 06:21:17.041
Sep  7 06:21:17.042: INFO: Observed event: ADDED
Sep  7 06:21:17.042: INFO: Observed event: MODIFIED
Sep  7 06:21:17.042: INFO: Observed event: MODIFIED
Sep  7 06:21:17.043: INFO: Observed event: MODIFIED
Sep  7 06:21:17.043: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  7 06:21:17.043: INFO: Service test-service-6wjqq deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2791" for this suite. 09/07/23 06:21:17.045
------------------------------
â€¢ [0.062 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:16.986
    Sep  7 06:21:16.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:21:16.987
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:16.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:16.996
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/07/23 06:21:16.999
    STEP: watching for the Service to be added 09/07/23 06:21:17.006
    Sep  7 06:21:17.007: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  7 06:21:17.007: INFO: Service test-service-6wjqq created
    STEP: Getting /status 09/07/23 06:21:17.007
    Sep  7 06:21:17.009: INFO: Service test-service-6wjqq has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/07/23 06:21:17.009
    STEP: watching for the Service to be patched 09/07/23 06:21:17.013
    Sep  7 06:21:17.014: INFO: observed Service test-service-6wjqq in namespace services-2791 with annotations: map[] & LoadBalancer: {[]}
    Sep  7 06:21:17.014: INFO: Found Service test-service-6wjqq in namespace services-2791 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  7 06:21:17.014: INFO: Service test-service-6wjqq has service status patched
    STEP: updating the ServiceStatus 09/07/23 06:21:17.014
    Sep  7 06:21:17.019: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/07/23 06:21:17.019
    Sep  7 06:21:17.019: INFO: Observed Service test-service-6wjqq in namespace services-2791 with annotations: map[] & Conditions: {[]}
    Sep  7 06:21:17.019: INFO: Observed event: &Service{ObjectMeta:{test-service-6wjqq  services-2791  83cd6815-f1dd-41cb-ac42-6c6cddc7ae11 26383 0 2023-09-07 06:21:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-07 06:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-07 06:21:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.8.27,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.8.27],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  7 06:21:17.020: INFO: Found Service test-service-6wjqq in namespace services-2791 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  7 06:21:17.020: INFO: Service test-service-6wjqq has service status updated
    STEP: patching the service 09/07/23 06:21:17.02
    STEP: watching for the Service to be patched 09/07/23 06:21:17.029
    Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
    Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
    Sep  7 06:21:17.030: INFO: observed Service test-service-6wjqq in namespace services-2791 with labels: map[test-service-static:true]
    Sep  7 06:21:17.030: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service:patched test-service-static:true]
    Sep  7 06:21:17.030: INFO: Service test-service-6wjqq patched
    STEP: deleting the service 09/07/23 06:21:17.03
    STEP: watching for the Service to be deleted 09/07/23 06:21:17.041
    Sep  7 06:21:17.042: INFO: Observed event: ADDED
    Sep  7 06:21:17.042: INFO: Observed event: MODIFIED
    Sep  7 06:21:17.042: INFO: Observed event: MODIFIED
    Sep  7 06:21:17.043: INFO: Observed event: MODIFIED
    Sep  7 06:21:17.043: INFO: Found Service test-service-6wjqq in namespace services-2791 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  7 06:21:17.043: INFO: Service test-service-6wjqq deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2791" for this suite. 09/07/23 06:21:17.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:17.05
Sep  7 06:21:17.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption 09/07/23 06:21:17.051
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:17.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:17.061
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:17.063
Sep  7 06:21:17.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename disruption-2 09/07/23 06:21:17.063
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:17.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:17.072
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/07/23 06:21:17.078
STEP: Waiting for the pdb to be processed 09/07/23 06:21:19.085
STEP: Waiting for the pdb to be processed 09/07/23 06:21:21.093
STEP: listing a collection of PDBs across all namespaces 09/07/23 06:21:23.098
STEP: listing a collection of PDBs in namespace disruption-4467 09/07/23 06:21:23.1
STEP: deleting a collection of PDBs 09/07/23 06:21:23.102
STEP: Waiting for the PDB collection to be deleted 09/07/23 06:21:23.11
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:23.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2100" for this suite. 09/07/23 06:21:23.117
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4467" for this suite. 09/07/23 06:21:23.122
------------------------------
â€¢ [SLOW TEST] [6.075 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:17.05
    Sep  7 06:21:17.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption 09/07/23 06:21:17.051
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:17.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:17.061
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:17.063
    Sep  7 06:21:17.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename disruption-2 09/07/23 06:21:17.063
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:17.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:17.072
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/07/23 06:21:17.078
    STEP: Waiting for the pdb to be processed 09/07/23 06:21:19.085
    STEP: Waiting for the pdb to be processed 09/07/23 06:21:21.093
    STEP: listing a collection of PDBs across all namespaces 09/07/23 06:21:23.098
    STEP: listing a collection of PDBs in namespace disruption-4467 09/07/23 06:21:23.1
    STEP: deleting a collection of PDBs 09/07/23 06:21:23.102
    STEP: Waiting for the PDB collection to be deleted 09/07/23 06:21:23.11
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:23.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2100" for this suite. 09/07/23 06:21:23.117
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4467" for this suite. 09/07/23 06:21:23.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:23.126
Sep  7 06:21:23.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:21:23.126
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:23.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:23.135
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:21:23.145
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:21:23.363
STEP: Deploying the webhook pod 09/07/23 06:21:23.375
STEP: Wait for the deployment to be ready 09/07/23 06:21:23.382
Sep  7 06:21:23.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 06:21:25.394
STEP: Verifying the service has paired with the endpoint 09/07/23 06:21:25.401
Sep  7 06:21:26.402: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/07/23 06:21:26.444
STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 06:21:26.465
STEP: Deleting the collection of validation webhooks 09/07/23 06:21:26.481
STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 06:21:26.505
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:26.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4574" for this suite. 09/07/23 06:21:26.53
STEP: Destroying namespace "webhook-4574-markers" for this suite. 09/07/23 06:21:26.534
------------------------------
â€¢ [3.412 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:23.126
    Sep  7 06:21:23.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:21:23.126
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:23.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:23.135
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:21:23.145
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:21:23.363
    STEP: Deploying the webhook pod 09/07/23 06:21:23.375
    STEP: Wait for the deployment to be ready 09/07/23 06:21:23.382
    Sep  7 06:21:23.387: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 06:21:25.394
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:21:25.401
    Sep  7 06:21:26.402: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/07/23 06:21:26.444
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 06:21:26.465
    STEP: Deleting the collection of validation webhooks 09/07/23 06:21:26.481
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/07/23 06:21:26.505
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:26.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4574" for this suite. 09/07/23 06:21:26.53
    STEP: Destroying namespace "webhook-4574-markers" for this suite. 09/07/23 06:21:26.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:26.538
Sep  7 06:21:26.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:21:26.539
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:26.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:26.549
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  7 06:21:26.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 06:21:27.841
Sep  7 06:21:27.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 create -f -'
Sep  7 06:21:28.204: INFO: stderr: ""
Sep  7 06:21:28.204: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  7 06:21:28.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 delete e2e-test-crd-publish-openapi-2980-crds test-cr'
Sep  7 06:21:28.261: INFO: stderr: ""
Sep  7 06:21:28.261: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  7 06:21:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 apply -f -'
Sep  7 06:21:28.381: INFO: stderr: ""
Sep  7 06:21:28.381: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  7 06:21:28.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 delete e2e-test-crd-publish-openapi-2980-crds test-cr'
Sep  7 06:21:28.437: INFO: stderr: ""
Sep  7 06:21:28.437: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/07/23 06:21:28.437
Sep  7 06:21:28.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 explain e2e-test-crd-publish-openapi-2980-crds'
Sep  7 06:21:28.557: INFO: stderr: ""
Sep  7 06:21:28.557: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2980-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:29.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-89" for this suite. 09/07/23 06:21:29.847
------------------------------
â€¢ [3.312 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:26.538
    Sep  7 06:21:26.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:21:26.539
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:26.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:26.549
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  7 06:21:26.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 06:21:27.841
    Sep  7 06:21:27.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 create -f -'
    Sep  7 06:21:28.204: INFO: stderr: ""
    Sep  7 06:21:28.204: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  7 06:21:28.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 delete e2e-test-crd-publish-openapi-2980-crds test-cr'
    Sep  7 06:21:28.261: INFO: stderr: ""
    Sep  7 06:21:28.261: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  7 06:21:28.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 apply -f -'
    Sep  7 06:21:28.381: INFO: stderr: ""
    Sep  7 06:21:28.381: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  7 06:21:28.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 --namespace=crd-publish-openapi-89 delete e2e-test-crd-publish-openapi-2980-crds test-cr'
    Sep  7 06:21:28.437: INFO: stderr: ""
    Sep  7 06:21:28.437: INFO: stdout: "e2e-test-crd-publish-openapi-2980-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/07/23 06:21:28.437
    Sep  7 06:21:28.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-89 explain e2e-test-crd-publish-openapi-2980-crds'
    Sep  7 06:21:28.557: INFO: stderr: ""
    Sep  7 06:21:28.557: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2980-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:29.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-89" for this suite. 09/07/23 06:21:29.847
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:29.85
Sep  7 06:21:29.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:21:29.851
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:29.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:29.862
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:21:29.864
Sep  7 06:21:29.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed" in namespace "projected-5269" to be "Succeeded or Failed"
Sep  7 06:21:29.869: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4059ms
Sep  7 06:21:31.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004606946s
Sep  7 06:21:33.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004757031s
STEP: Saw pod success 09/07/23 06:21:33.873
Sep  7 06:21:33.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed" satisfied condition "Succeeded or Failed"
Sep  7 06:21:33.875: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed container client-container: <nil>
STEP: delete the pod 09/07/23 06:21:33.878
Sep  7 06:21:33.885: INFO: Waiting for pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed to disappear
Sep  7 06:21:33.887: INFO: Pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:33.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5269" for this suite. 09/07/23 06:21:33.889
------------------------------
â€¢ [4.041 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:29.85
    Sep  7 06:21:29.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:21:29.851
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:29.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:29.862
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:21:29.864
    Sep  7 06:21:29.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed" in namespace "projected-5269" to be "Succeeded or Failed"
    Sep  7 06:21:29.869: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4059ms
    Sep  7 06:21:31.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004606946s
    Sep  7 06:21:33.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004757031s
    STEP: Saw pod success 09/07/23 06:21:33.873
    Sep  7 06:21:33.873: INFO: Pod "downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed" satisfied condition "Succeeded or Failed"
    Sep  7 06:21:33.875: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed container client-container: <nil>
    STEP: delete the pod 09/07/23 06:21:33.878
    Sep  7 06:21:33.885: INFO: Waiting for pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed to disappear
    Sep  7 06:21:33.887: INFO: Pod downwardapi-volume-88fa78e3-cad9-4bb6-8565-d8378206d5ed no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:33.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5269" for this suite. 09/07/23 06:21:33.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:33.892
Sep  7 06:21:33.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename gc 09/07/23 06:21:33.892
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:33.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:33.9
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/07/23 06:21:33.903
STEP: delete the rc 09/07/23 06:21:38.91
STEP: wait for the rc to be deleted 09/07/23 06:21:38.917
Sep  7 06:21:39.924: INFO: 80 pods remaining
Sep  7 06:21:39.924: INFO: 80 pods has nil DeletionTimestamp
Sep  7 06:21:39.924: INFO: 
Sep  7 06:21:40.925: INFO: 71 pods remaining
Sep  7 06:21:40.925: INFO: 71 pods has nil DeletionTimestamp
Sep  7 06:21:40.925: INFO: 
Sep  7 06:21:41.925: INFO: 60 pods remaining
Sep  7 06:21:41.925: INFO: 60 pods has nil DeletionTimestamp
Sep  7 06:21:41.925: INFO: 
Sep  7 06:21:42.923: INFO: 40 pods remaining
Sep  7 06:21:42.923: INFO: 40 pods has nil DeletionTimestamp
Sep  7 06:21:42.923: INFO: 
Sep  7 06:21:43.923: INFO: 31 pods remaining
Sep  7 06:21:43.923: INFO: 31 pods has nil DeletionTimestamp
Sep  7 06:21:43.923: INFO: 
Sep  7 06:21:44.922: INFO: 20 pods remaining
Sep  7 06:21:44.922: INFO: 20 pods has nil DeletionTimestamp
Sep  7 06:21:44.922: INFO: 
STEP: Gathering metrics 09/07/23 06:21:45.921
Sep  7 06:21:45.927: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
Sep  7 06:21:45.929: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.5071ms
Sep  7 06:21:45.929: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
Sep  7 06:21:45.929: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
Sep  7 06:21:45.963: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  7 06:21:45.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9900" for this suite. 09/07/23 06:21:45.966
------------------------------
â€¢ [SLOW TEST] [12.079 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:33.892
    Sep  7 06:21:33.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename gc 09/07/23 06:21:33.892
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:33.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:33.9
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/07/23 06:21:33.903
    STEP: delete the rc 09/07/23 06:21:38.91
    STEP: wait for the rc to be deleted 09/07/23 06:21:38.917
    Sep  7 06:21:39.924: INFO: 80 pods remaining
    Sep  7 06:21:39.924: INFO: 80 pods has nil DeletionTimestamp
    Sep  7 06:21:39.924: INFO: 
    Sep  7 06:21:40.925: INFO: 71 pods remaining
    Sep  7 06:21:40.925: INFO: 71 pods has nil DeletionTimestamp
    Sep  7 06:21:40.925: INFO: 
    Sep  7 06:21:41.925: INFO: 60 pods remaining
    Sep  7 06:21:41.925: INFO: 60 pods has nil DeletionTimestamp
    Sep  7 06:21:41.925: INFO: 
    Sep  7 06:21:42.923: INFO: 40 pods remaining
    Sep  7 06:21:42.923: INFO: 40 pods has nil DeletionTimestamp
    Sep  7 06:21:42.923: INFO: 
    Sep  7 06:21:43.923: INFO: 31 pods remaining
    Sep  7 06:21:43.923: INFO: 31 pods has nil DeletionTimestamp
    Sep  7 06:21:43.923: INFO: 
    Sep  7 06:21:44.922: INFO: 20 pods remaining
    Sep  7 06:21:44.922: INFO: 20 pods has nil DeletionTimestamp
    Sep  7 06:21:44.922: INFO: 
    STEP: Gathering metrics 09/07/23 06:21:45.921
    Sep  7 06:21:45.927: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kind-control-plane" in namespace "kube-system" to be "running and ready"
    Sep  7 06:21:45.929: INFO: Pod "kube-controller-manager-kind-control-plane": Phase="Running", Reason="", readiness=true. Elapsed: 1.5071ms
    Sep  7 06:21:45.929: INFO: The phase of Pod kube-controller-manager-kind-control-plane is Running (Ready = true)
    Sep  7 06:21:45.929: INFO: Pod "kube-controller-manager-kind-control-plane" satisfied condition "running and ready"
    Sep  7 06:21:45.963: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:21:45.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9900" for this suite. 09/07/23 06:21:45.966
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:21:45.972
Sep  7 06:21:45.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:21:45.972
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:45.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:45.981
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  7 06:21:45.992: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 06:22:46.007: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/07/23 06:22:46.009
Sep  7 06:22:46.023: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  7 06:22:46.028: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  7 06:22:46.039: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  7 06:22:46.045: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/07/23 06:22:46.045
Sep  7 06:22:46.045: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5100" to be "running"
Sep  7 06:22:46.049: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.872661ms
Sep  7 06:22:48.052: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007102391s
Sep  7 06:22:48.052: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  7 06:22:48.052: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
Sep  7 06:22:48.054: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.73966ms
Sep  7 06:22:48.054: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  7 06:22:48.054: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
Sep  7 06:22:48.056: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.710989ms
Sep  7 06:22:48.056: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  7 06:22:48.056: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
Sep  7 06:22:48.057: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.49456ms
Sep  7 06:22:48.057: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/07/23 06:22:48.057
Sep  7 06:22:48.060: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5100" to be "running"
Sep  7 06:22:48.062: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33846ms
Sep  7 06:22:50.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00456269s
Sep  7 06:22:52.064: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00372693s
Sep  7 06:22:54.064: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00427487s
Sep  7 06:22:54.064: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:22:54.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5100" for this suite. 09/07/23 06:22:54.089
------------------------------
â€¢ [SLOW TEST] [68.121 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:21:45.972
    Sep  7 06:21:45.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:21:45.972
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:21:45.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:21:45.981
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  7 06:21:45.992: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 06:22:46.007: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/07/23 06:22:46.009
    Sep  7 06:22:46.023: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  7 06:22:46.028: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  7 06:22:46.039: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  7 06:22:46.045: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/07/23 06:22:46.045
    Sep  7 06:22:46.045: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5100" to be "running"
    Sep  7 06:22:46.049: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.872661ms
    Sep  7 06:22:48.052: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007102391s
    Sep  7 06:22:48.052: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  7 06:22:48.052: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
    Sep  7 06:22:48.054: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.73966ms
    Sep  7 06:22:48.054: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  7 06:22:48.054: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
    Sep  7 06:22:48.056: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.710989ms
    Sep  7 06:22:48.056: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  7 06:22:48.056: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5100" to be "running"
    Sep  7 06:22:48.057: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.49456ms
    Sep  7 06:22:48.057: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/07/23 06:22:48.057
    Sep  7 06:22:48.060: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5100" to be "running"
    Sep  7 06:22:48.062: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.33846ms
    Sep  7 06:22:50.065: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00456269s
    Sep  7 06:22:52.064: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00372693s
    Sep  7 06:22:54.064: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00427487s
    Sep  7 06:22:54.064: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:22:54.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5100" for this suite. 09/07/23 06:22:54.089
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:22:54.093
Sep  7 06:22:54.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:22:54.093
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:54.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:54.106
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-464a518b-cbe1-4772-9e5d-ef605e42d280 09/07/23 06:22:54.107
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:22:54.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5877" for this suite. 09/07/23 06:22:54.11
------------------------------
â€¢ [0.020 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:22:54.093
    Sep  7 06:22:54.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:22:54.093
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:54.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:54.106
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-464a518b-cbe1-4772-9e5d-ef605e42d280 09/07/23 06:22:54.107
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:22:54.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5877" for this suite. 09/07/23 06:22:54.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:22:54.114
Sep  7 06:22:54.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:22:54.114
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:54.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:54.122
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-d9sr9" 09/07/23 06:22:54.125
Sep  7 06:22:54.129: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard cpu limit of 500m
Sep  7 06:22:54.129: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.129
STEP: Confirm /status for "e2e-rq-status-d9sr9" resourceQuota via watch 09/07/23 06:22:54.133
Sep  7 06:22:54.134: INFO: observed resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList(nil)
Sep  7 06:22:54.134: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  7 06:22:54.134: INFO: ResourceQuota "e2e-rq-status-d9sr9" /status was updated
STEP: Patching hard spec values for cpu & memory 09/07/23 06:22:54.135
Sep  7 06:22:54.138: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard cpu limit of 1
Sep  7 06:22:54.138: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.138
STEP: Confirm /status for "e2e-rq-status-d9sr9" resourceQuota via watch 09/07/23 06:22:54.143
Sep  7 06:22:54.143: INFO: observed resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  7 06:22:54.143: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  7 06:22:54.143: INFO: ResourceQuota "e2e-rq-status-d9sr9" /status was patched
STEP: Get "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.143
Sep  7 06:22:54.145: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard cpu of 1
Sep  7 06:22:54.145: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-d9sr9" /status before checking Spec is unchanged 09/07/23 06:22:54.147
Sep  7 06:22:54.150: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard cpu of 2
Sep  7 06:22:54.150: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard memory of 2Gi
Sep  7 06:22:54.150: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep  7 06:22:59.154: INFO: ResourceQuota "e2e-rq-status-d9sr9" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:22:59.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8885" for this suite. 09/07/23 06:22:59.157
------------------------------
â€¢ [SLOW TEST] [5.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:22:54.114
    Sep  7 06:22:54.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:22:54.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:54.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:54.122
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-d9sr9" 09/07/23 06:22:54.125
    Sep  7 06:22:54.129: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard cpu limit of 500m
    Sep  7 06:22:54.129: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.129
    STEP: Confirm /status for "e2e-rq-status-d9sr9" resourceQuota via watch 09/07/23 06:22:54.133
    Sep  7 06:22:54.134: INFO: observed resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList(nil)
    Sep  7 06:22:54.134: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  7 06:22:54.134: INFO: ResourceQuota "e2e-rq-status-d9sr9" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/07/23 06:22:54.135
    Sep  7 06:22:54.138: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard cpu limit of 1
    Sep  7 06:22:54.138: INFO: Resource quota "e2e-rq-status-d9sr9" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.138
    STEP: Confirm /status for "e2e-rq-status-d9sr9" resourceQuota via watch 09/07/23 06:22:54.143
    Sep  7 06:22:54.143: INFO: observed resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  7 06:22:54.143: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  7 06:22:54.143: INFO: ResourceQuota "e2e-rq-status-d9sr9" /status was patched
    STEP: Get "e2e-rq-status-d9sr9" /status 09/07/23 06:22:54.143
    Sep  7 06:22:54.145: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard cpu of 1
    Sep  7 06:22:54.145: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-d9sr9" /status before checking Spec is unchanged 09/07/23 06:22:54.147
    Sep  7 06:22:54.150: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard cpu of 2
    Sep  7 06:22:54.150: INFO: Resourcequota "e2e-rq-status-d9sr9" reports status: hard memory of 2Gi
    Sep  7 06:22:54.150: INFO: Found resourceQuota "e2e-rq-status-d9sr9" in namespace "resourcequota-8885" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  7 06:22:59.154: INFO: ResourceQuota "e2e-rq-status-d9sr9" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:22:59.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8885" for this suite. 09/07/23 06:22:59.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:22:59.161
Sep  7 06:22:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:22:59.162
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:59.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:59.171
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  7 06:22:59.178: INFO: Waiting up to 5m0s for pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288" in namespace "pods-5769" to be "running and ready"
Sep  7 06:22:59.179: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54484ms
Sep  7 06:22:59.179: INFO: The phase of Pod server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:23:01.183: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288": Phase="Running", Reason="", readiness=true. Elapsed: 2.004822858s
Sep  7 06:23:01.183: INFO: The phase of Pod server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288 is Running (Ready = true)
Sep  7 06:23:01.183: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288" satisfied condition "running and ready"
Sep  7 06:23:01.197: INFO: Waiting up to 5m0s for pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb" in namespace "pods-5769" to be "Succeeded or Failed"
Sep  7 06:23:01.200: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50983ms
Sep  7 06:23:03.203: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005582388s
Sep  7 06:23:05.202: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005353636s
STEP: Saw pod success 09/07/23 06:23:05.202
Sep  7 06:23:05.203: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb" satisfied condition "Succeeded or Failed"
Sep  7 06:23:05.204: INFO: Trying to get logs from node kind-worker pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb container env3cont: <nil>
STEP: delete the pod 09/07/23 06:23:05.214
Sep  7 06:23:05.220: INFO: Waiting for pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb to disappear
Sep  7 06:23:05.222: INFO: Pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5769" for this suite. 09/07/23 06:23:05.224
------------------------------
â€¢ [SLOW TEST] [6.065 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:22:59.161
    Sep  7 06:22:59.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:22:59.162
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:22:59.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:22:59.171
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  7 06:22:59.178: INFO: Waiting up to 5m0s for pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288" in namespace "pods-5769" to be "running and ready"
    Sep  7 06:22:59.179: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54484ms
    Sep  7 06:22:59.179: INFO: The phase of Pod server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:23:01.183: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288": Phase="Running", Reason="", readiness=true. Elapsed: 2.004822858s
    Sep  7 06:23:01.183: INFO: The phase of Pod server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288 is Running (Ready = true)
    Sep  7 06:23:01.183: INFO: Pod "server-envvars-66b95447-ad5a-4d0b-a610-2ed5a8755288" satisfied condition "running and ready"
    Sep  7 06:23:01.197: INFO: Waiting up to 5m0s for pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb" in namespace "pods-5769" to be "Succeeded or Failed"
    Sep  7 06:23:01.200: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50983ms
    Sep  7 06:23:03.203: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005582388s
    Sep  7 06:23:05.202: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005353636s
    STEP: Saw pod success 09/07/23 06:23:05.202
    Sep  7 06:23:05.203: INFO: Pod "client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb" satisfied condition "Succeeded or Failed"
    Sep  7 06:23:05.204: INFO: Trying to get logs from node kind-worker pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb container env3cont: <nil>
    STEP: delete the pod 09/07/23 06:23:05.214
    Sep  7 06:23:05.220: INFO: Waiting for pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb to disappear
    Sep  7 06:23:05.222: INFO: Pod client-envvars-c7777cb2-6a28-4a6b-a29e-9e9611eb68fb no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5769" for this suite. 09/07/23 06:23:05.224
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:05.227
Sep  7 06:23:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 06:23:05.228
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:05.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:05.237
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/07/23 06:23:05.239
Sep  7 06:23:05.245: INFO: Waiting up to 5m0s for pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe" in namespace "var-expansion-3520" to be "Succeeded or Failed"
Sep  7 06:23:05.247: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60951ms
Sep  7 06:23:07.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898008s
Sep  7 06:23:09.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004807265s
STEP: Saw pod success 09/07/23 06:23:09.25
Sep  7 06:23:09.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe" satisfied condition "Succeeded or Failed"
Sep  7 06:23:09.252: INFO: Trying to get logs from node kind-worker2 pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:23:09.262
Sep  7 06:23:09.270: INFO: Waiting for pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe to disappear
Sep  7 06:23:09.272: INFO: Pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3520" for this suite. 09/07/23 06:23:09.274
------------------------------
â€¢ [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:05.227
    Sep  7 06:23:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 06:23:05.228
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:05.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:05.237
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/07/23 06:23:05.239
    Sep  7 06:23:05.245: INFO: Waiting up to 5m0s for pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe" in namespace "var-expansion-3520" to be "Succeeded or Failed"
    Sep  7 06:23:05.247: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.60951ms
    Sep  7 06:23:07.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004898008s
    Sep  7 06:23:09.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004807265s
    STEP: Saw pod success 09/07/23 06:23:09.25
    Sep  7 06:23:09.250: INFO: Pod "var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe" satisfied condition "Succeeded or Failed"
    Sep  7 06:23:09.252: INFO: Trying to get logs from node kind-worker2 pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:23:09.262
    Sep  7 06:23:09.270: INFO: Waiting for pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe to disappear
    Sep  7 06:23:09.272: INFO: Pod var-expansion-965efe75-7d22-4b7a-9784-93606c0bbbbe no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3520" for this suite. 09/07/23 06:23:09.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:09.278
Sep  7 06:23:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 06:23:09.279
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:09.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:09.289
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/07/23 06:23:09.29
Sep  7 06:23:09.295: INFO: Waiting up to 5m0s for pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6" in namespace "var-expansion-110" to be "Succeeded or Failed"
Sep  7 06:23:09.297: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53304ms
Sep  7 06:23:11.299: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003937946s
Sep  7 06:23:13.300: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004874113s
STEP: Saw pod success 09/07/23 06:23:13.3
Sep  7 06:23:13.300: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6" satisfied condition "Succeeded or Failed"
Sep  7 06:23:13.302: INFO: Trying to get logs from node kind-worker2 pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 container dapi-container: <nil>
STEP: delete the pod 09/07/23 06:23:13.305
Sep  7 06:23:13.314: INFO: Waiting for pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 to disappear
Sep  7 06:23:13.316: INFO: Pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:13.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-110" for this suite. 09/07/23 06:23:13.318
------------------------------
â€¢ [4.043 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:09.278
    Sep  7 06:23:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 06:23:09.279
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:09.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:09.289
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/07/23 06:23:09.29
    Sep  7 06:23:09.295: INFO: Waiting up to 5m0s for pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6" in namespace "var-expansion-110" to be "Succeeded or Failed"
    Sep  7 06:23:09.297: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53304ms
    Sep  7 06:23:11.299: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003937946s
    Sep  7 06:23:13.300: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004874113s
    STEP: Saw pod success 09/07/23 06:23:13.3
    Sep  7 06:23:13.300: INFO: Pod "var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6" satisfied condition "Succeeded or Failed"
    Sep  7 06:23:13.302: INFO: Trying to get logs from node kind-worker2 pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 container dapi-container: <nil>
    STEP: delete the pod 09/07/23 06:23:13.305
    Sep  7 06:23:13.314: INFO: Waiting for pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 to disappear
    Sep  7 06:23:13.316: INFO: Pod var-expansion-5a610ae6-6403-4025-b57c-2cda305378f6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:13.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-110" for this suite. 09/07/23 06:23:13.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:13.322
Sep  7 06:23:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename proxy 09/07/23 06:23:13.323
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:13.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:13.335
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/07/23 06:23:13.345
STEP: creating replication controller proxy-service-2clfj in namespace proxy-8704 09/07/23 06:23:13.345
I0907 06:23:13.352320      29 runners.go:193] Created replication controller with name: proxy-service-2clfj, namespace: proxy-8704, replica count: 1
I0907 06:23:14.403443      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0907 06:23:15.403614      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0907 06:23:16.404158      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 06:23:16.406: INFO: setup took 3.0693144s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/07/23 06:23:16.406
Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.99125ms)
Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.12436ms)
Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.25074ms)
Sep  7 06:23:16.410: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 3.41745ms)
Sep  7 06:23:16.410: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.59497ms)
Sep  7 06:23:16.411: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 4.75831ms)
Sep  7 06:23:16.413: INFO: (0) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 7.16813ms)
Sep  7 06:23:16.413: INFO: (0) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 7.06634ms)
Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 7.52175ms)
Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 7.54588ms)
Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 8.02249ms)
Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 9.19089ms)
Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 9.26699ms)
Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 9.22887ms)
Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 9.30791ms)
Sep  7 06:23:16.416: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 9.82216ms)
Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.552311ms)
Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.5928ms)
Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.55987ms)
Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.51003ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.470901ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.5389ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.594ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.78886ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.8707ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.024511ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.12687ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.147951ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.125121ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.27324ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.316791ms)
Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.262341ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.14409ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.198369ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.152169ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.445239ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.77593ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.020421ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.8434ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.866979ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.896959ms)
Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.878251ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.06048ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.024ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07196ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.135031ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.018041ms)
Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.13212ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.810319ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.0026ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.05706ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.296279ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.23493ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.501459ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.60416ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.58853ms)
Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.61953ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.82669ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.766479ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.77079ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.02673ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.10356ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.149739ms)
Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.315511ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.881321ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.84046ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.009ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.188751ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.161271ms)
Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.24254ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.61155ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.59248ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.642121ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.62843ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.78071ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.80506ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.762761ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.88528ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.84748ms)
Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.93395ms)
Sep  7 06:23:16.431: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.66046ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.53912ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.61932ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.799699ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.822ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.98862ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04068ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.05299ms)
Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.18186ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 4.377389ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 4.5212ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 4.40038ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 4.98609ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 5.0544ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 5.07828ms)
Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 5.07202ms)
Sep  7 06:23:16.436: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 1.98616ms)
Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.45267ms)
Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.45467ms)
Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.45516ms)
Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.4323ms)
Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.56242ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.98569ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.24784ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.0168ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.15267ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 3.05236ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 3.33254ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07393ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.10084ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.31114ms)
Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.28305ms)
Sep  7 06:23:16.440: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 1.9651ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.63124ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.65006ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.70943ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.76976ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.79032ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.8472ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.81211ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.7755ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.86743ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.86447ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.90594ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.82163ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.9245ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.89013ms)
Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.20105ms)
Sep  7 06:23:16.443: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.2329ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.15413ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.35701ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.21088ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.47597ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.57214ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.62118ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.61116ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.77511ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.83085ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.84049ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.85322ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.83003ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.88507ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.94168ms)
Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.91191ms)
Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.79354ms)
Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.7842ms)
Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.08618ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.16533ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.21982ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.22216ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.36758ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.37528ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.71902ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.7639ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.78781ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.74885ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.78343ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.85044ms)
Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.90568ms)
Sep  7 06:23:16.448: INFO: (9) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.10106ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.93849ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.97367ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.05665ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.26622ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.35497ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.3775ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.36876ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.499ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.60549ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.63719ms)
Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.9089ms)
Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.88165ms)
Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.03148ms)
Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07273ms)
Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.90711ms)
Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.9345ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.84879ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.9799ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.04002ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.09276ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.30879ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.41294ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.40821ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.47368ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.53679ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.46956ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.51397ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.52071ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.67231ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.65754ms)
Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.72784ms)
Sep  7 06:23:16.454: INFO: (11) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.85255ms)
Sep  7 06:23:16.455: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.65396ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.99397ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.02982ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.3502ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.36076ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.43321ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.48925ms)
Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.45161ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.80442ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.88537ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.87316ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.89469ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.89101ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.97151ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.99715ms)
Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04486ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 1.8234ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.07876ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.15795ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.11461ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.11301ms)
Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.27462ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.78302ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.86553ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.90218ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.82061ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.90809ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.84781ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.95752ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04112ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07811ms)
Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.17632ms)
Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.848519ms)
Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 1.92537ms)
Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.09738ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.411189ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.38399ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.431301ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.73909ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.67039ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.75627ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.78194ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.94232ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.80312ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.9183ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.84351ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.84711ms)
Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.83788ms)
Sep  7 06:23:16.465: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.680689ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.31661ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.33397ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.35333ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.41898ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.46923ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.50731ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.918611ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.90382ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.91637ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.00849ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.9749ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.9864ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.96022ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.97135ms)
Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.96857ms)
Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.59343ms)
Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.765881ms)
Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.71696ms)
Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.792321ms)
Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.95893ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 3.05903ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.139981ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.107619ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 3.19426ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.133881ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.173101ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.179581ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.267061ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.19749ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.277981ms)
Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.3055ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.031749ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.22438ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.21754ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.175399ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.28401ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.315439ms)
Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.28153ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.553409ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.565969ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.593489ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.69538ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.670069ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.949679ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.96332ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.98982ms)
Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.017329ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.016459ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.07436ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.99879ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.01897ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 1.997679ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.04631ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.106141ms)
Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.243571ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.46663ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.41619ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.5191ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.58149ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.671799ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.589981ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.646261ms)
Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.59443ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.01511ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.20647ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.404941ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.41005ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.4895ms)
Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.55132ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.66551ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.70487ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.71779ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 3.0525ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.926271ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.02655ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.97333ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.015839ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.13893ms)
Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.17511ms)
STEP: deleting ReplicationController proxy-service-2clfj in namespace proxy-8704, will wait for the garbage collector to delete the pods 09/07/23 06:23:16.479
Sep  7 06:23:16.536: INFO: Deleting ReplicationController proxy-service-2clfj took: 4.34799ms
Sep  7 06:23:16.637: INFO: Terminating ReplicationController proxy-service-2clfj pods took: 100.629072ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:20.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8704" for this suite. 09/07/23 06:23:20.44
------------------------------
â€¢ [SLOW TEST] [7.122 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:13.322
    Sep  7 06:23:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename proxy 09/07/23 06:23:13.323
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:13.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:13.335
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/07/23 06:23:13.345
    STEP: creating replication controller proxy-service-2clfj in namespace proxy-8704 09/07/23 06:23:13.345
    I0907 06:23:13.352320      29 runners.go:193] Created replication controller with name: proxy-service-2clfj, namespace: proxy-8704, replica count: 1
    I0907 06:23:14.403443      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0907 06:23:15.403614      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0907 06:23:16.404158      29 runners.go:193] proxy-service-2clfj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 06:23:16.406: INFO: setup took 3.0693144s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/07/23 06:23:16.406
    Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.99125ms)
    Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.12436ms)
    Sep  7 06:23:16.409: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.25074ms)
    Sep  7 06:23:16.410: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 3.41745ms)
    Sep  7 06:23:16.410: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.59497ms)
    Sep  7 06:23:16.411: INFO: (0) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 4.75831ms)
    Sep  7 06:23:16.413: INFO: (0) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 7.16813ms)
    Sep  7 06:23:16.413: INFO: (0) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 7.06634ms)
    Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 7.52175ms)
    Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 7.54588ms)
    Sep  7 06:23:16.414: INFO: (0) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 8.02249ms)
    Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 9.19089ms)
    Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 9.26699ms)
    Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 9.22887ms)
    Sep  7 06:23:16.415: INFO: (0) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 9.30791ms)
    Sep  7 06:23:16.416: INFO: (0) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 9.82216ms)
    Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.552311ms)
    Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.5928ms)
    Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.55987ms)
    Sep  7 06:23:16.418: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.51003ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.470901ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.5389ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.594ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.78886ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.8707ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.024511ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.12687ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.147951ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.125121ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.27324ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.316791ms)
    Sep  7 06:23:16.419: INFO: (1) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.262341ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.14409ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.198369ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.152169ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.445239ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.77593ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.020421ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.8434ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.866979ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.896959ms)
    Sep  7 06:23:16.422: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.878251ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.06048ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.024ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07196ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.135031ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.018041ms)
    Sep  7 06:23:16.423: INFO: (2) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.13212ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.810319ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.0026ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.05706ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.296279ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.23493ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.501459ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.60416ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.58853ms)
    Sep  7 06:23:16.425: INFO: (3) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.61953ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.82669ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.766479ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.77079ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.02673ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.10356ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.149739ms)
    Sep  7 06:23:16.426: INFO: (3) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.315511ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.881321ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.84046ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.009ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.188751ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.161271ms)
    Sep  7 06:23:16.428: INFO: (4) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.24254ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.61155ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.59248ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.642121ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.62843ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.78071ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.80506ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.762761ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.88528ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.84748ms)
    Sep  7 06:23:16.429: INFO: (4) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.93395ms)
    Sep  7 06:23:16.431: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.66046ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.53912ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.61932ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.799699ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.822ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.98862ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04068ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.05299ms)
    Sep  7 06:23:16.432: INFO: (5) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.18186ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 4.377389ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 4.5212ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 4.40038ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 4.98609ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 5.0544ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 5.07828ms)
    Sep  7 06:23:16.434: INFO: (5) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 5.07202ms)
    Sep  7 06:23:16.436: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 1.98616ms)
    Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.45267ms)
    Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.45467ms)
    Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.45516ms)
    Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.4323ms)
    Sep  7 06:23:16.437: INFO: (6) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.56242ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.98569ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.24784ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.0168ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.15267ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 3.05236ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 3.33254ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07393ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.10084ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.31114ms)
    Sep  7 06:23:16.438: INFO: (6) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.28305ms)
    Sep  7 06:23:16.440: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 1.9651ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.63124ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.65006ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.70943ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.76976ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.79032ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.8472ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.81211ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.7755ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.86743ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.86447ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.90594ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.82163ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.9245ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.89013ms)
    Sep  7 06:23:16.441: INFO: (7) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.20105ms)
    Sep  7 06:23:16.443: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.2329ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.15413ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.35701ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.21088ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.47597ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.57214ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.62118ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.61116ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.77511ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.83085ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.84049ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.85322ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.83003ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.88507ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.94168ms)
    Sep  7 06:23:16.444: INFO: (8) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.91191ms)
    Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.79354ms)
    Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.7842ms)
    Sep  7 06:23:16.446: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.08618ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.16533ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.21982ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.22216ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.36758ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.37528ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.71902ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.7639ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.78781ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.74885ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.78343ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.85044ms)
    Sep  7 06:23:16.447: INFO: (9) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.90568ms)
    Sep  7 06:23:16.448: INFO: (9) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 3.10106ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.93849ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.97367ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.05665ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.26622ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.35497ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.3775ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.36876ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.499ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.60549ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.63719ms)
    Sep  7 06:23:16.450: INFO: (10) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.9089ms)
    Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.88165ms)
    Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.03148ms)
    Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07273ms)
    Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.90711ms)
    Sep  7 06:23:16.451: INFO: (10) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.9345ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.84879ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.9799ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.04002ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.09276ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.30879ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.41294ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.40821ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.47368ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.53679ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.46956ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.51397ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.52071ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.67231ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.65754ms)
    Sep  7 06:23:16.453: INFO: (11) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.72784ms)
    Sep  7 06:23:16.454: INFO: (11) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.85255ms)
    Sep  7 06:23:16.455: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.65396ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 1.99397ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.02982ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.3502ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.36076ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.43321ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.48925ms)
    Sep  7 06:23:16.456: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.45161ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.80442ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.88537ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.87316ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.89469ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.89101ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.97151ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.99715ms)
    Sep  7 06:23:16.457: INFO: (12) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04486ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 1.8234ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.07876ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.15795ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.11461ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.11301ms)
    Sep  7 06:23:16.459: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.27462ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.78302ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.86553ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.90218ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.82061ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.90809ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.84781ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.95752ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.04112ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.07811ms)
    Sep  7 06:23:16.460: INFO: (13) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.17632ms)
    Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.848519ms)
    Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 1.92537ms)
    Sep  7 06:23:16.462: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.09738ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.411189ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.38399ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.431301ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.73909ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.67039ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.75627ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.78194ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.94232ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.80312ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.9183ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.84351ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.84711ms)
    Sep  7 06:23:16.463: INFO: (14) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.83788ms)
    Sep  7 06:23:16.465: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 1.680689ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.31661ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.33397ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.35333ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.41898ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.46923ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.50731ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.918611ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.90382ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.91637ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.00849ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.9749ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.9864ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.96022ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.97135ms)
    Sep  7 06:23:16.466: INFO: (15) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.96857ms)
    Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.59343ms)
    Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.765881ms)
    Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.71696ms)
    Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.792321ms)
    Sep  7 06:23:16.469: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.95893ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 3.05903ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.139981ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.107619ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 3.19426ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 3.133881ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.173101ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.179581ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 3.267061ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 3.19749ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.277981ms)
    Sep  7 06:23:16.470: INFO: (16) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.3055ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.031749ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.22438ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.21754ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.175399ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.28401ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.315439ms)
    Sep  7 06:23:16.472: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.28153ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.553409ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.565969ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.593489ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.69538ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.670069ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.949679ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.96332ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.98982ms)
    Sep  7 06:23:16.473: INFO: (17) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 3.017329ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.016459ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.07436ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 1.99879ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.01897ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 1.997679ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.04631ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.106141ms)
    Sep  7 06:23:16.475: INFO: (18) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.243571ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.46663ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.41619ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 2.5191ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 2.58149ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 2.671799ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 2.589981ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.646261ms)
    Sep  7 06:23:16.476: INFO: (18) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.59443ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:460/proxy/: tls baz (200; 2.01511ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:1080/proxy/rewriteme">... (200; 2.20647ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx/proxy/rewriteme">test</a> (200; 2.404941ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:443/proxy/tlsrewritem... (200; 2.41005ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:160/proxy/: foo (200; 2.4895ms)
    Sep  7 06:23:16.478: INFO: (19) /api/v1/namespaces/proxy-8704/pods/https:proxy-service-2clfj-d58fx:462/proxy/: tls qux (200; 2.55132ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/http:proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.66551ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname2/proxy/: bar (200; 2.70487ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname2/proxy/: tls qux (200; 2.71779ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/: <a href="/api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:1080/proxy/rewriteme">test<... (200; 3.0525ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:162/proxy/: bar (200; 2.926271ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/pods/proxy-service-2clfj-d58fx:160/proxy/: foo (200; 3.02655ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/https:proxy-service-2clfj:tlsportname1/proxy/: tls baz (200; 2.97333ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname1/proxy/: foo (200; 3.015839ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/proxy-service-2clfj:portname2/proxy/: bar (200; 3.13893ms)
    Sep  7 06:23:16.479: INFO: (19) /api/v1/namespaces/proxy-8704/services/http:proxy-service-2clfj:portname1/proxy/: foo (200; 3.17511ms)
    STEP: deleting ReplicationController proxy-service-2clfj in namespace proxy-8704, will wait for the garbage collector to delete the pods 09/07/23 06:23:16.479
    Sep  7 06:23:16.536: INFO: Deleting ReplicationController proxy-service-2clfj took: 4.34799ms
    Sep  7 06:23:16.637: INFO: Terminating ReplicationController proxy-service-2clfj pods took: 100.629072ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:20.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8704" for this suite. 09/07/23 06:23:20.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:20.445
Sep  7 06:23:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename events 09/07/23 06:23:20.446
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.458
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/07/23 06:23:20.459
STEP: listing all events in all namespaces 09/07/23 06:23:20.462
STEP: patching the test event 09/07/23 06:23:20.464
STEP: fetching the test event 09/07/23 06:23:20.468
STEP: updating the test event 09/07/23 06:23:20.471
STEP: getting the test event 09/07/23 06:23:20.476
STEP: deleting the test event 09/07/23 06:23:20.477
STEP: listing all events in all namespaces 09/07/23 06:23:20.48
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:20.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1836" for this suite. 09/07/23 06:23:20.484
------------------------------
â€¢ [0.043 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:20.445
    Sep  7 06:23:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename events 09/07/23 06:23:20.446
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.458
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/07/23 06:23:20.459
    STEP: listing all events in all namespaces 09/07/23 06:23:20.462
    STEP: patching the test event 09/07/23 06:23:20.464
    STEP: fetching the test event 09/07/23 06:23:20.468
    STEP: updating the test event 09/07/23 06:23:20.471
    STEP: getting the test event 09/07/23 06:23:20.476
    STEP: deleting the test event 09/07/23 06:23:20.477
    STEP: listing all events in all namespaces 09/07/23 06:23:20.48
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:20.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1836" for this suite. 09/07/23 06:23:20.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:20.488
Sep  7 06:23:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:23:20.489
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.497
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-286-delete-me 09/07/23 06:23:20.501
STEP: Waiting for the RuntimeClass to disappear 09/07/23 06:23:20.505
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-286" for this suite. 09/07/23 06:23:20.512
------------------------------
â€¢ [0.026 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:20.488
    Sep  7 06:23:20.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename runtimeclass 09/07/23 06:23:20.489
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.497
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-286-delete-me 09/07/23 06:23:20.501
    STEP: Waiting for the RuntimeClass to disappear 09/07/23 06:23:20.505
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:20.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-286" for this suite. 09/07/23 06:23:20.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:20.515
Sep  7 06:23:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 06:23:20.516
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.523
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/07/23 06:23:20.525
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/07/23 06:23:20.53
STEP: patching the secret 09/07/23 06:23:20.531
STEP: deleting the secret using a LabelSelector 09/07/23 06:23:20.536
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/07/23 06:23:20.539
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:20.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-979" for this suite. 09/07/23 06:23:20.543
------------------------------
â€¢ [0.030 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:20.515
    Sep  7 06:23:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 06:23:20.516
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.523
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/07/23 06:23:20.525
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/07/23 06:23:20.53
    STEP: patching the secret 09/07/23 06:23:20.531
    STEP: deleting the secret using a LabelSelector 09/07/23 06:23:20.536
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/07/23 06:23:20.539
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:20.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-979" for this suite. 09/07/23 06:23:20.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:20.546
Sep  7 06:23:20.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:23:20.546
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.556
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/07/23 06:23:37.561
STEP: Creating a ResourceQuota 09/07/23 06:23:42.563
STEP: Ensuring resource quota status is calculated 09/07/23 06:23:42.566
STEP: Creating a ConfigMap 09/07/23 06:23:44.57
STEP: Ensuring resource quota status captures configMap creation 09/07/23 06:23:44.578
STEP: Deleting a ConfigMap 09/07/23 06:23:46.58
STEP: Ensuring resource quota status released usage 09/07/23 06:23:46.584
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:48.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1679" for this suite. 09/07/23 06:23:48.589
------------------------------
â€¢ [SLOW TEST] [28.048 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:20.546
    Sep  7 06:23:20.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:23:20.546
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:20.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:20.556
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/07/23 06:23:37.561
    STEP: Creating a ResourceQuota 09/07/23 06:23:42.563
    STEP: Ensuring resource quota status is calculated 09/07/23 06:23:42.566
    STEP: Creating a ConfigMap 09/07/23 06:23:44.57
    STEP: Ensuring resource quota status captures configMap creation 09/07/23 06:23:44.578
    STEP: Deleting a ConfigMap 09/07/23 06:23:46.58
    STEP: Ensuring resource quota status released usage 09/07/23 06:23:46.584
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:48.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1679" for this suite. 09/07/23 06:23:48.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:48.594
Sep  7 06:23:48.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 06:23:48.595
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:48.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:48.605
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/07/23 06:23:48.609
Sep  7 06:23:48.614: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6455" to be "running and ready"
Sep  7 06:23:48.616: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34482ms
Sep  7 06:23:48.616: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:23:50.618: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003736711s
Sep  7 06:23:50.618: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  7 06:23:50.618: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/07/23 06:23:50.62
Sep  7 06:23:50.623: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6455" to be "running and ready"
Sep  7 06:23:50.624: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40668ms
Sep  7 06:23:50.624: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:23:52.627: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00405836s
Sep  7 06:23:52.627: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  7 06:23:52.627: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/07/23 06:23:52.629
STEP: delete the pod with lifecycle hook 09/07/23 06:23:52.632
Sep  7 06:23:52.637: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  7 06:23:52.639: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  7 06:23:54.640: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  7 06:23:54.643: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  7 06:23:56.640: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  7 06:23:56.642: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  7 06:23:56.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6455" for this suite. 09/07/23 06:23:56.644
------------------------------
â€¢ [SLOW TEST] [8.054 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:48.594
    Sep  7 06:23:48.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/07/23 06:23:48.595
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:48.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:48.605
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/07/23 06:23:48.609
    Sep  7 06:23:48.614: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6455" to be "running and ready"
    Sep  7 06:23:48.616: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.34482ms
    Sep  7 06:23:48.616: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:23:50.618: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003736711s
    Sep  7 06:23:50.618: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  7 06:23:50.618: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/07/23 06:23:50.62
    Sep  7 06:23:50.623: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6455" to be "running and ready"
    Sep  7 06:23:50.624: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.40668ms
    Sep  7 06:23:50.624: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:23:52.627: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00405836s
    Sep  7 06:23:52.627: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  7 06:23:52.627: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/07/23 06:23:52.629
    STEP: delete the pod with lifecycle hook 09/07/23 06:23:52.632
    Sep  7 06:23:52.637: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  7 06:23:52.639: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  7 06:23:54.640: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  7 06:23:54.643: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  7 06:23:56.640: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  7 06:23:56.642: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:23:56.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6455" for this suite. 09/07/23 06:23:56.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:23:56.649
Sep  7 06:23:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:23:56.65
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:56.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:56.66
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/07/23 06:23:56.662
Sep  7 06:23:56.666: INFO: Waiting up to 5m0s for pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d" in namespace "downward-api-4303" to be "running and ready"
Sep  7 06:23:56.668: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.14591ms
Sep  7 06:23:56.668: INFO: The phase of Pod labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:23:58.671: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623409s
Sep  7 06:23:58.671: INFO: The phase of Pod labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d is Running (Ready = true)
Sep  7 06:23:58.671: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d" satisfied condition "running and ready"
Sep  7 06:23:59.185: INFO: Successfully updated pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:24:03.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4303" for this suite. 09/07/23 06:24:03.199
------------------------------
â€¢ [SLOW TEST] [6.555 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:23:56.649
    Sep  7 06:23:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:23:56.65
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:23:56.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:23:56.66
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/07/23 06:23:56.662
    Sep  7 06:23:56.666: INFO: Waiting up to 5m0s for pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d" in namespace "downward-api-4303" to be "running and ready"
    Sep  7 06:23:56.668: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.14591ms
    Sep  7 06:23:56.668: INFO: The phase of Pod labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:23:58.671: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623409s
    Sep  7 06:23:58.671: INFO: The phase of Pod labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d is Running (Ready = true)
    Sep  7 06:23:58.671: INFO: Pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d" satisfied condition "running and ready"
    Sep  7 06:23:59.185: INFO: Successfully updated pod "labelsupdate94e25dbc-749d-4a3c-af94-de54e884969d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:24:03.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4303" for this suite. 09/07/23 06:24:03.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:24:03.205
Sep  7 06:24:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename job 09/07/23 06:24:03.206
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:03.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:03.215
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/07/23 06:24:03.217
STEP: Ensuring active pods == parallelism 09/07/23 06:24:03.221
STEP: delete a job 09/07/23 06:24:05.225
STEP: deleting Job.batch foo in namespace job-3574, will wait for the garbage collector to delete the pods 09/07/23 06:24:05.225
Sep  7 06:24:05.280: INFO: Deleting Job.batch foo took: 3.473919ms
Sep  7 06:24:05.381: INFO: Terminating Job.batch foo pods took: 101.081643ms
STEP: Ensuring job was deleted 09/07/23 06:24:38.882
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  7 06:24:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3574" for this suite. 09/07/23 06:24:38.885
------------------------------
â€¢ [SLOW TEST] [35.683 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:24:03.205
    Sep  7 06:24:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename job 09/07/23 06:24:03.206
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:03.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:03.215
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/07/23 06:24:03.217
    STEP: Ensuring active pods == parallelism 09/07/23 06:24:03.221
    STEP: delete a job 09/07/23 06:24:05.225
    STEP: deleting Job.batch foo in namespace job-3574, will wait for the garbage collector to delete the pods 09/07/23 06:24:05.225
    Sep  7 06:24:05.280: INFO: Deleting Job.batch foo took: 3.473919ms
    Sep  7 06:24:05.381: INFO: Terminating Job.batch foo pods took: 101.081643ms
    STEP: Ensuring job was deleted 09/07/23 06:24:38.882
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:24:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3574" for this suite. 09/07/23 06:24:38.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:24:38.889
Sep  7 06:24:38.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename configmap 09/07/23 06:24:38.89
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:38.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:38.901
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/07/23 06:24:38.903
STEP: fetching the ConfigMap 09/07/23 06:24:38.905
STEP: patching the ConfigMap 09/07/23 06:24:38.907
STEP: listing all ConfigMaps in all namespaces with a label selector 09/07/23 06:24:38.91
STEP: deleting the ConfigMap by collection with a label selector 09/07/23 06:24:38.912
STEP: listing all ConfigMaps in test namespace 09/07/23 06:24:38.915
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:24:38.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4340" for this suite. 09/07/23 06:24:38.918
------------------------------
â€¢ [0.031 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:24:38.889
    Sep  7 06:24:38.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename configmap 09/07/23 06:24:38.89
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:38.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:38.901
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/07/23 06:24:38.903
    STEP: fetching the ConfigMap 09/07/23 06:24:38.905
    STEP: patching the ConfigMap 09/07/23 06:24:38.907
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/07/23 06:24:38.91
    STEP: deleting the ConfigMap by collection with a label selector 09/07/23 06:24:38.912
    STEP: listing all ConfigMaps in test namespace 09/07/23 06:24:38.915
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:24:38.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4340" for this suite. 09/07/23 06:24:38.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:24:38.921
Sep  7 06:24:38.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:24:38.922
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:38.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:38.931
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/07/23 06:24:38.933
Sep  7 06:24:38.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:24:40.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:24:45.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5708" for this suite. 09/07/23 06:24:45.338
------------------------------
â€¢ [SLOW TEST] [6.421 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:24:38.921
    Sep  7 06:24:38.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:24:38.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:38.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:38.931
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/07/23 06:24:38.933
    Sep  7 06:24:38.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:24:40.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:24:45.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5708" for this suite. 09/07/23 06:24:45.338
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:24:45.342
Sep  7 06:24:45.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 06:24:45.343
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:45.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:45.354
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 in namespace container-probe-9042 09/07/23 06:24:45.356
Sep  7 06:24:45.360: INFO: Waiting up to 5m0s for pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0" in namespace "container-probe-9042" to be "not pending"
Sep  7 06:24:45.361: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25542ms
Sep  7 06:24:47.364: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004449822s
Sep  7 06:24:47.365: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0" satisfied condition "not pending"
Sep  7 06:24:47.365: INFO: Started pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 in namespace container-probe-9042
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:24:47.365
Sep  7 06:24:47.366: INFO: Initial restart count of pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is 0
Sep  7 06:25:07.398: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 1 (20.03139401s elapsed)
Sep  7 06:25:27.426: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 2 (40.060085053s elapsed)
Sep  7 06:25:47.459: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 3 (1m0.09308487s elapsed)
Sep  7 06:26:07.492: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 4 (1m20.126118861s elapsed)
Sep  7 06:27:19.606: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 5 (2m32.239600542s elapsed)
STEP: deleting the pod 09/07/23 06:27:19.606
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:19.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9042" for this suite. 09/07/23 06:27:19.616
------------------------------
â€¢ [SLOW TEST] [154.277 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:24:45.342
    Sep  7 06:24:45.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 06:24:45.343
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:24:45.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:24:45.354
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 in namespace container-probe-9042 09/07/23 06:24:45.356
    Sep  7 06:24:45.360: INFO: Waiting up to 5m0s for pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0" in namespace "container-probe-9042" to be "not pending"
    Sep  7 06:24:45.361: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.25542ms
    Sep  7 06:24:47.364: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.004449822s
    Sep  7 06:24:47.365: INFO: Pod "liveness-433f6834-ba23-456f-8800-0fd8d125bbd0" satisfied condition "not pending"
    Sep  7 06:24:47.365: INFO: Started pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 in namespace container-probe-9042
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:24:47.365
    Sep  7 06:24:47.366: INFO: Initial restart count of pod liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is 0
    Sep  7 06:25:07.398: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 1 (20.03139401s elapsed)
    Sep  7 06:25:27.426: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 2 (40.060085053s elapsed)
    Sep  7 06:25:47.459: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 3 (1m0.09308487s elapsed)
    Sep  7 06:26:07.492: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 4 (1m20.126118861s elapsed)
    Sep  7 06:27:19.606: INFO: Restart count of pod container-probe-9042/liveness-433f6834-ba23-456f-8800-0fd8d125bbd0 is now 5 (2m32.239600542s elapsed)
    STEP: deleting the pod 09/07/23 06:27:19.606
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:19.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9042" for this suite. 09/07/23 06:27:19.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:19.62
Sep  7 06:27:19.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:27:19.62
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:19.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:19.629
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-6263 09/07/23 06:27:19.631
STEP: creating service affinity-nodeport in namespace services-6263 09/07/23 06:27:19.631
STEP: creating replication controller affinity-nodeport in namespace services-6263 09/07/23 06:27:19.642
I0907 06:27:19.650964      29 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6263, replica count: 3
I0907 06:27:22.701711      29 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 06:27:22.707: INFO: Creating new exec pod
Sep  7 06:27:22.710: INFO: Waiting up to 5m0s for pod "execpod-affinity9jqw6" in namespace "services-6263" to be "running"
Sep  7 06:27:22.712: INFO: Pod "execpod-affinity9jqw6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65743ms
Sep  7 06:27:24.714: INFO: Pod "execpod-affinity9jqw6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004162953s
Sep  7 06:27:24.714: INFO: Pod "execpod-affinity9jqw6" satisfied condition "running"
Sep  7 06:27:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  7 06:27:25.899: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  7 06:27:25.899: INFO: stdout: ""
Sep  7 06:27:25.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 10.96.161.77 80'
Sep  7 06:27:26.063: INFO: stderr: "+ nc -v -z -w 2 10.96.161.77 80\nConnection to 10.96.161.77 80 port [tcp/http] succeeded!\n"
Sep  7 06:27:26.063: INFO: stdout: ""
Sep  7 06:27:26.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31767'
Sep  7 06:27:26.230: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31767\nConnection to 192.168.8.3 31767 port [tcp/*] succeeded!\n"
Sep  7 06:27:26.230: INFO: stdout: ""
Sep  7 06:27:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31767'
Sep  7 06:27:26.393: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31767\nConnection to 192.168.8.6 31767 port [tcp/*] succeeded!\n"
Sep  7 06:27:26.393: INFO: stdout: ""
Sep  7 06:27:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:31767/ ; done'
Sep  7 06:27:26.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n"
Sep  7 06:27:26.602: INFO: stdout: "\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m"
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
Sep  7 06:27:26.602: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6263, will wait for the garbage collector to delete the pods 09/07/23 06:27:26.612
Sep  7 06:27:26.668: INFO: Deleting ReplicationController affinity-nodeport took: 4.15507ms
Sep  7 06:27:26.768: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.482991ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:28.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6263" for this suite. 09/07/23 06:27:28.985
------------------------------
â€¢ [SLOW TEST] [9.370 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:19.62
    Sep  7 06:27:19.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:27:19.62
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:19.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:19.629
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-6263 09/07/23 06:27:19.631
    STEP: creating service affinity-nodeport in namespace services-6263 09/07/23 06:27:19.631
    STEP: creating replication controller affinity-nodeport in namespace services-6263 09/07/23 06:27:19.642
    I0907 06:27:19.650964      29 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6263, replica count: 3
    I0907 06:27:22.701711      29 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 06:27:22.707: INFO: Creating new exec pod
    Sep  7 06:27:22.710: INFO: Waiting up to 5m0s for pod "execpod-affinity9jqw6" in namespace "services-6263" to be "running"
    Sep  7 06:27:22.712: INFO: Pod "execpod-affinity9jqw6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.65743ms
    Sep  7 06:27:24.714: INFO: Pod "execpod-affinity9jqw6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004162953s
    Sep  7 06:27:24.714: INFO: Pod "execpod-affinity9jqw6" satisfied condition "running"
    Sep  7 06:27:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  7 06:27:25.899: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  7 06:27:25.899: INFO: stdout: ""
    Sep  7 06:27:25.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 10.96.161.77 80'
    Sep  7 06:27:26.063: INFO: stderr: "+ nc -v -z -w 2 10.96.161.77 80\nConnection to 10.96.161.77 80 port [tcp/http] succeeded!\n"
    Sep  7 06:27:26.063: INFO: stdout: ""
    Sep  7 06:27:26.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.3 31767'
    Sep  7 06:27:26.230: INFO: stderr: "+ nc -v -z -w 2 192.168.8.3 31767\nConnection to 192.168.8.3 31767 port [tcp/*] succeeded!\n"
    Sep  7 06:27:26.230: INFO: stdout: ""
    Sep  7 06:27:26.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c nc -v -z -w 2 192.168.8.6 31767'
    Sep  7 06:27:26.393: INFO: stderr: "+ nc -v -z -w 2 192.168.8.6 31767\nConnection to 192.168.8.6 31767 port [tcp/*] succeeded!\n"
    Sep  7 06:27:26.393: INFO: stdout: ""
    Sep  7 06:27:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-6263 exec execpod-affinity9jqw6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.8.3:31767/ ; done'
    Sep  7 06:27:26.602: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.8.3:31767/\n"
    Sep  7 06:27:26.602: INFO: stdout: "\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m\naffinity-nodeport-hvw7m"
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Received response from host: affinity-nodeport-hvw7m
    Sep  7 06:27:26.602: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6263, will wait for the garbage collector to delete the pods 09/07/23 06:27:26.612
    Sep  7 06:27:26.668: INFO: Deleting ReplicationController affinity-nodeport took: 4.15507ms
    Sep  7 06:27:26.768: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.482991ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:28.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6263" for this suite. 09/07/23 06:27:28.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:28.99
Sep  7 06:27:28.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 06:27:28.99
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:28.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:29
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/07/23 06:27:29.003
Sep  7 06:27:29.011: INFO: Waiting up to 5m0s for pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c" in namespace "emptydir-9549" to be "Succeeded or Failed"
Sep  7 06:27:29.012: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26698ms
Sep  7 06:27:31.016: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004883822s
Sep  7 06:27:33.015: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004539193s
STEP: Saw pod success 09/07/23 06:27:33.015
Sep  7 06:27:33.015: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c" satisfied condition "Succeeded or Failed"
Sep  7 06:27:33.017: INFO: Trying to get logs from node kind-worker2 pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c container test-container: <nil>
STEP: delete the pod 09/07/23 06:27:33.026
Sep  7 06:27:33.035: INFO: Waiting for pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c to disappear
Sep  7 06:27:33.036: INFO: Pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:33.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9549" for this suite. 09/07/23 06:27:33.038
------------------------------
â€¢ [4.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:28.99
    Sep  7 06:27:28.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 06:27:28.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:28.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:29
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/07/23 06:27:29.003
    Sep  7 06:27:29.011: INFO: Waiting up to 5m0s for pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c" in namespace "emptydir-9549" to be "Succeeded or Failed"
    Sep  7 06:27:29.012: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26698ms
    Sep  7 06:27:31.016: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004883822s
    Sep  7 06:27:33.015: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004539193s
    STEP: Saw pod success 09/07/23 06:27:33.015
    Sep  7 06:27:33.015: INFO: Pod "pod-a59f32cc-76ee-4959-bdba-b3a222fd725c" satisfied condition "Succeeded or Failed"
    Sep  7 06:27:33.017: INFO: Trying to get logs from node kind-worker2 pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c container test-container: <nil>
    STEP: delete the pod 09/07/23 06:27:33.026
    Sep  7 06:27:33.035: INFO: Waiting for pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c to disappear
    Sep  7 06:27:33.036: INFO: Pod pod-a59f32cc-76ee-4959-bdba-b3a222fd725c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:33.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9549" for this suite. 09/07/23 06:27:33.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:33.041
Sep  7 06:27:33.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/07/23 06:27:33.042
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:33.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:33.052
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/07/23 06:27:33.054
STEP: Creating hostNetwork=false pod 09/07/23 06:27:33.054
Sep  7 06:27:33.061: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9692" to be "running and ready"
Sep  7 06:27:33.062: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48528ms
Sep  7 06:27:33.062: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:27:35.065: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003608582s
Sep  7 06:27:35.065: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  7 06:27:35.065: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/07/23 06:27:35.066
Sep  7 06:27:35.070: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9692" to be "running and ready"
Sep  7 06:27:35.072: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5251ms
Sep  7 06:27:35.072: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:27:37.075: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004616901s
Sep  7 06:27:37.075: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  7 06:27:37.075: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/07/23 06:27:37.076
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/07/23 06:27:37.077
Sep  7 06:27:37.077: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.077: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.077: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  7 06:27:37.189: INFO: Exec stderr: ""
Sep  7 06:27:37.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.190: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  7 06:27:37.221: INFO: Exec stderr: ""
Sep  7 06:27:37.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.221: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.221: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  7 06:27:37.253: INFO: Exec stderr: ""
Sep  7 06:27:37.253: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.253: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  7 06:27:37.283: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/07/23 06:27:37.283
Sep  7 06:27:37.283: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.283: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.283: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  7 06:27:37.315: INFO: Exec stderr: ""
Sep  7 06:27:37.315: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.316: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.316: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  7 06:27:37.348: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/07/23 06:27:37.348
Sep  7 06:27:37.348: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.348: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.348: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  7 06:27:37.385: INFO: Exec stderr: ""
Sep  7 06:27:37.385: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.386: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  7 06:27:37.418: INFO: Exec stderr: ""
Sep  7 06:27:37.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.419: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  7 06:27:37.455: INFO: Exec stderr: ""
Sep  7 06:27:37.455: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:27:37.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:27:37.455: INFO: ExecWithOptions: Clientset creation
Sep  7 06:27:37.455: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  7 06:27:37.488: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:37.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9692" for this suite. 09/07/23 06:27:37.491
------------------------------
â€¢ [4.453 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:33.041
    Sep  7 06:27:33.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/07/23 06:27:33.042
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:33.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:33.052
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/07/23 06:27:33.054
    STEP: Creating hostNetwork=false pod 09/07/23 06:27:33.054
    Sep  7 06:27:33.061: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9692" to be "running and ready"
    Sep  7 06:27:33.062: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48528ms
    Sep  7 06:27:33.062: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:27:35.065: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003608582s
    Sep  7 06:27:35.065: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  7 06:27:35.065: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/07/23 06:27:35.066
    Sep  7 06:27:35.070: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9692" to be "running and ready"
    Sep  7 06:27:35.072: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5251ms
    Sep  7 06:27:35.072: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:27:37.075: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004616901s
    Sep  7 06:27:37.075: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  7 06:27:37.075: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/07/23 06:27:37.076
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/07/23 06:27:37.077
    Sep  7 06:27:37.077: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.077: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.077: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  7 06:27:37.189: INFO: Exec stderr: ""
    Sep  7 06:27:37.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.190: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  7 06:27:37.221: INFO: Exec stderr: ""
    Sep  7 06:27:37.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.221: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.221: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  7 06:27:37.253: INFO: Exec stderr: ""
    Sep  7 06:27:37.253: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.253: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  7 06:27:37.283: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/07/23 06:27:37.283
    Sep  7 06:27:37.283: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.283: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.283: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  7 06:27:37.315: INFO: Exec stderr: ""
    Sep  7 06:27:37.315: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.316: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.316: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  7 06:27:37.348: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/07/23 06:27:37.348
    Sep  7 06:27:37.348: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.348: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.348: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  7 06:27:37.385: INFO: Exec stderr: ""
    Sep  7 06:27:37.385: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.386: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  7 06:27:37.418: INFO: Exec stderr: ""
    Sep  7 06:27:37.418: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.419: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  7 06:27:37.455: INFO: Exec stderr: ""
    Sep  7 06:27:37.455: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9692 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:27:37.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:27:37.455: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:27:37.455: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9692/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  7 06:27:37.488: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:37.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9692" for this suite. 09/07/23 06:27:37.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:37.497
Sep  7 06:27:37.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:27:37.498
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:37.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:37.51
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/07/23 06:27:37.511
Sep  7 06:27:37.511: INFO: Creating e2e-svc-a-c2x4r
Sep  7 06:27:37.519: INFO: Creating e2e-svc-b-8nvmd
Sep  7 06:27:37.529: INFO: Creating e2e-svc-c-7dj7w
STEP: deleting service collection 09/07/23 06:27:37.541
Sep  7 06:27:37.562: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:37.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8474" for this suite. 09/07/23 06:27:37.565
------------------------------
â€¢ [0.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:37.497
    Sep  7 06:27:37.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:27:37.498
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:37.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:37.51
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/07/23 06:27:37.511
    Sep  7 06:27:37.511: INFO: Creating e2e-svc-a-c2x4r
    Sep  7 06:27:37.519: INFO: Creating e2e-svc-b-8nvmd
    Sep  7 06:27:37.529: INFO: Creating e2e-svc-c-7dj7w
    STEP: deleting service collection 09/07/23 06:27:37.541
    Sep  7 06:27:37.562: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:37.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8474" for this suite. 09/07/23 06:27:37.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:37.57
Sep  7 06:27:37.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:27:37.571
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:37.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:37.58
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/07/23 06:27:37.582
STEP: Creating a ResourceQuota 09/07/23 06:27:42.584
STEP: Ensuring resource quota status is calculated 09/07/23 06:27:42.587
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:44.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6632" for this suite. 09/07/23 06:27:44.592
------------------------------
â€¢ [SLOW TEST] [7.029 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:37.57
    Sep  7 06:27:37.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:27:37.571
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:37.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:37.58
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/07/23 06:27:37.582
    STEP: Creating a ResourceQuota 09/07/23 06:27:42.584
    STEP: Ensuring resource quota status is calculated 09/07/23 06:27:42.587
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:44.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6632" for this suite. 09/07/23 06:27:44.592
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:44.598
Sep  7 06:27:44.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:27:44.599
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:44.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:44.608
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:27:44.61
Sep  7 06:27:44.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c" in namespace "downward-api-9641" to be "Succeeded or Failed"
Sep  7 06:27:44.618: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41712ms
Sep  7 06:27:46.621: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00447934s
Sep  7 06:27:48.620: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004307259s
STEP: Saw pod success 09/07/23 06:27:48.62
Sep  7 06:27:48.621: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c" satisfied condition "Succeeded or Failed"
Sep  7 06:27:48.622: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c container client-container: <nil>
STEP: delete the pod 09/07/23 06:27:48.626
Sep  7 06:27:48.632: INFO: Waiting for pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c to disappear
Sep  7 06:27:48.634: INFO: Pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:48.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9641" for this suite. 09/07/23 06:27:48.636
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:44.598
    Sep  7 06:27:44.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:27:44.599
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:44.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:44.608
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:27:44.61
    Sep  7 06:27:44.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c" in namespace "downward-api-9641" to be "Succeeded or Failed"
    Sep  7 06:27:44.618: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41712ms
    Sep  7 06:27:46.621: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00447934s
    Sep  7 06:27:48.620: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004307259s
    STEP: Saw pod success 09/07/23 06:27:48.62
    Sep  7 06:27:48.621: INFO: Pod "downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c" satisfied condition "Succeeded or Failed"
    Sep  7 06:27:48.622: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c container client-container: <nil>
    STEP: delete the pod 09/07/23 06:27:48.626
    Sep  7 06:27:48.632: INFO: Waiting for pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c to disappear
    Sep  7 06:27:48.634: INFO: Pod downwardapi-volume-0d9bac65-0b9f-40a7-8978-af47f4fa533c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:48.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9641" for this suite. 09/07/23 06:27:48.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:48.642
Sep  7 06:27:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename kubectl 09/07/23 06:27:48.643
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:48.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:48.654
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 06:27:48.656
Sep  7 06:27:48.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3212 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  7 06:27:48.708: INFO: stderr: ""
Sep  7 06:27:48.708: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/07/23 06:27:48.708
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  7 06:27:48.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3212 delete pods e2e-test-httpd-pod'
Sep  7 06:27:51.895: INFO: stderr: ""
Sep  7 06:27:51.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:51.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3212" for this suite. 09/07/23 06:27:51.897
------------------------------
â€¢ [3.258 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:48.642
    Sep  7 06:27:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename kubectl 09/07/23 06:27:48.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:48.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:48.654
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/07/23 06:27:48.656
    Sep  7 06:27:48.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3212 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  7 06:27:48.708: INFO: stderr: ""
    Sep  7 06:27:48.708: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/07/23 06:27:48.708
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  7 06:27:48.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=kubectl-3212 delete pods e2e-test-httpd-pod'
    Sep  7 06:27:51.895: INFO: stderr: ""
    Sep  7 06:27:51.895: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:51.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3212" for this suite. 09/07/23 06:27:51.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:51.901
Sep  7 06:27:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:27:51.902
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:51.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:51.911
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:27:51.913
Sep  7 06:27:51.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8" in namespace "projected-4395" to be "Succeeded or Failed"
Sep  7 06:27:51.920: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4108ms
Sep  7 06:27:53.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143979s
Sep  7 06:27:55.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004453278s
STEP: Saw pod success 09/07/23 06:27:55.923
Sep  7 06:27:55.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8" satisfied condition "Succeeded or Failed"
Sep  7 06:27:55.925: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 container client-container: <nil>
STEP: delete the pod 09/07/23 06:27:55.933
Sep  7 06:27:55.939: INFO: Waiting for pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 to disappear
Sep  7 06:27:55.941: INFO: Pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:55.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4395" for this suite. 09/07/23 06:27:55.943
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:51.901
    Sep  7 06:27:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:27:51.902
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:51.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:51.911
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:27:51.913
    Sep  7 06:27:51.919: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8" in namespace "projected-4395" to be "Succeeded or Failed"
    Sep  7 06:27:51.920: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.4108ms
    Sep  7 06:27:53.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004143979s
    Sep  7 06:27:55.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004453278s
    STEP: Saw pod success 09/07/23 06:27:55.923
    Sep  7 06:27:55.923: INFO: Pod "downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8" satisfied condition "Succeeded or Failed"
    Sep  7 06:27:55.925: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:27:55.933
    Sep  7 06:27:55.939: INFO: Waiting for pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 to disappear
    Sep  7 06:27:55.941: INFO: Pod downwardapi-volume-64df9d0f-9fd2-4133-b739-712d390d5ed8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:55.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4395" for this suite. 09/07/23 06:27:55.943
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:55.946
Sep  7 06:27:55.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:27:55.947
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:55.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:55.957
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-bf9c3c42-f6e1-4efa-8815-28b10bdcf914 09/07/23 06:27:55.959
STEP: Creating a pod to test consume secrets 09/07/23 06:27:55.961
Sep  7 06:27:55.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11" in namespace "projected-9348" to be "Succeeded or Failed"
Sep  7 06:27:55.968: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29792ms
Sep  7 06:27:57.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004103528s
Sep  7 06:27:59.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004366107s
STEP: Saw pod success 09/07/23 06:27:59.971
Sep  7 06:27:59.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11" satisfied condition "Succeeded or Failed"
Sep  7 06:27:59.973: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/07/23 06:27:59.977
Sep  7 06:27:59.984: INFO: Waiting for pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 to disappear
Sep  7 06:27:59.985: INFO: Pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  7 06:27:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9348" for this suite. 09/07/23 06:27:59.987
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:55.946
    Sep  7 06:27:55.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:27:55.947
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:55.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:55.957
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-bf9c3c42-f6e1-4efa-8815-28b10bdcf914 09/07/23 06:27:55.959
    STEP: Creating a pod to test consume secrets 09/07/23 06:27:55.961
    Sep  7 06:27:55.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11" in namespace "projected-9348" to be "Succeeded or Failed"
    Sep  7 06:27:55.968: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Pending", Reason="", readiness=false. Elapsed: 1.29792ms
    Sep  7 06:27:57.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004103528s
    Sep  7 06:27:59.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004366107s
    STEP: Saw pod success 09/07/23 06:27:59.971
    Sep  7 06:27:59.971: INFO: Pod "pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11" satisfied condition "Succeeded or Failed"
    Sep  7 06:27:59.973: INFO: Trying to get logs from node kind-worker pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/07/23 06:27:59.977
    Sep  7 06:27:59.984: INFO: Waiting for pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 to disappear
    Sep  7 06:27:59.985: INFO: Pod pod-projected-secrets-b571c2d6-2d50-4005-8d66-8b8695e73b11 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:27:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9348" for this suite. 09/07/23 06:27:59.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:27:59.99
Sep  7 06:27:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 06:27:59.991
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:59.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:59.999
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/07/23 06:28:00.001
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8348;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8348;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +notcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_tcp@PTR;sleep 1; done
 09/07/23 06:28:00.013
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8348;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8348;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +notcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_tcp@PTR;sleep 1; done
 09/07/23 06:28:00.013
STEP: creating a pod to probe DNS 09/07/23 06:28:00.013
STEP: submitting the pod to kubernetes 09/07/23 06:28:00.013
Sep  7 06:28:00.020: INFO: Waiting up to 15m0s for pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218" in namespace "dns-8348" to be "running"
Sep  7 06:28:00.022: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849811ms
Sep  7 06:28:02.025: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218": Phase="Running", Reason="", readiness=true. Elapsed: 2.004993099s
Sep  7 06:28:02.025: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218" satisfied condition "running"
STEP: retrieving the pod 09/07/23 06:28:02.025
STEP: looking for the results for each expected name from probers 09/07/23 06:28:02.027
Sep  7 06:28:02.030: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.032: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.033: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.035: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.046: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.047: INFO: Unable to read 10.96.56.219_udp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.049: INFO: Unable to read 10.96.56.219_tcp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.051: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.052: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.054: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.057: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.065: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.067: INFO: Unable to read 10.96.56.219_udp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.068: INFO: Unable to read 10.96.56.219_tcp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:02.068: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc 10.96.56.219_udp@PTR 10.96.56.219_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc jessie_tcp@_http._tcp.test-service-2.dns-8348.svc 10.96.56.219_udp@PTR 10.96.56.219_tcp@PTR]

Sep  7 06:28:07.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.094: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.096: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.097: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.099: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:07.111: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

Sep  7 06:28:12.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.090: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.092: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.093: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.096: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:12.106: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

Sep  7 06:28:17.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.091: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.093: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.094: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.097: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:17.108: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

Sep  7 06:28:22.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.091: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.092: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.094: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.097: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:22.107: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

Sep  7 06:28:27.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.081: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.093: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.095: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.098: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:27.108: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

Sep  7 06:28:32.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:32.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
Sep  7 06:28:32.109: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service jessie_udp@dns-test-service]

Sep  7 06:28:37.107: INFO: DNS probes using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 succeeded

STEP: deleting the pod 09/07/23 06:28:37.107
STEP: deleting the test service 09/07/23 06:28:37.116
STEP: deleting the test headless service 09/07/23 06:28:37.131
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 06:28:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8348" for this suite. 09/07/23 06:28:37.139
------------------------------
â€¢ [SLOW TEST] [37.154 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:27:59.99
    Sep  7 06:27:59.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 06:27:59.991
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:27:59.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:27:59.999
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/07/23 06:28:00.001
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8348;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8348;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +notcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_tcp@PTR;sleep 1; done
     09/07/23 06:28:00.013
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8348;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8348;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8348.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8348.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8348.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8348.svc;check="$$(dig +notcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_udp@PTR;check="$$(dig +tcp +noall +answer +search 219.56.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.56.219_tcp@PTR;sleep 1; done
     09/07/23 06:28:00.013
    STEP: creating a pod to probe DNS 09/07/23 06:28:00.013
    STEP: submitting the pod to kubernetes 09/07/23 06:28:00.013
    Sep  7 06:28:00.020: INFO: Waiting up to 15m0s for pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218" in namespace "dns-8348" to be "running"
    Sep  7 06:28:00.022: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849811ms
    Sep  7 06:28:02.025: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218": Phase="Running", Reason="", readiness=true. Elapsed: 2.004993099s
    Sep  7 06:28:02.025: INFO: Pod "dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218" satisfied condition "running"
    STEP: retrieving the pod 09/07/23 06:28:02.025
    STEP: looking for the results for each expected name from probers 09/07/23 06:28:02.027
    Sep  7 06:28:02.030: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.032: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.033: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.035: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.037: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.039: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.046: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.047: INFO: Unable to read 10.96.56.219_udp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.049: INFO: Unable to read 10.96.56.219_tcp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.051: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.052: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.054: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.056: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.057: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.065: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.067: INFO: Unable to read 10.96.56.219_udp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.068: INFO: Unable to read 10.96.56.219_tcp@PTR from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:02.068: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc wheezy_tcp@_http._tcp.test-service-2.dns-8348.svc 10.96.56.219_udp@PTR 10.96.56.219_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc jessie_tcp@_http._tcp.test-service-2.dns-8348.svc 10.96.56.219_udp@PTR 10.96.56.219_tcp@PTR]

    Sep  7 06:28:07.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.094: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.096: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.097: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.099: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.100: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:07.111: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

    Sep  7 06:28:12.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.076: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.090: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.092: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.093: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.096: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:12.106: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

    Sep  7 06:28:17.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.091: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.093: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.094: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.097: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:17.108: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

    Sep  7 06:28:22.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.073: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.075: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.080: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.091: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.092: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.094: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.097: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:22.107: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

    Sep  7 06:28:27.072: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.074: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.076: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.079: INFO: Unable to read wheezy_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.081: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.093: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.095: INFO: Unable to read jessie_udp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.096: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348 from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.098: INFO: Unable to read jessie_udp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-8348.svc from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:27.108: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8348 wheezy_tcp@dns-test-service.dns-8348 wheezy_udp@dns-test-service.dns-8348.svc wheezy_tcp@dns-test-service.dns-8348.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8348 jessie_tcp@dns-test-service.dns-8348 jessie_udp@dns-test-service.dns-8348.svc jessie_tcp@dns-test-service.dns-8348.svc]

    Sep  7 06:28:32.071: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:32.092: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218: the server could not find the requested resource (get pods dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218)
    Sep  7 06:28:32.109: INFO: Lookups using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 failed for: [wheezy_udp@dns-test-service jessie_udp@dns-test-service]

    Sep  7 06:28:37.107: INFO: DNS probes using dns-8348/dns-test-b9e355a3-103d-48aa-a0f5-473a0d59c218 succeeded

    STEP: deleting the pod 09/07/23 06:28:37.107
    STEP: deleting the test service 09/07/23 06:28:37.116
    STEP: deleting the test headless service 09/07/23 06:28:37.131
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:28:37.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8348" for this suite. 09/07/23 06:28:37.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:28:37.145
Sep  7 06:28:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename server-version 09/07/23 06:28:37.146
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.154
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/07/23 06:28:37.156
STEP: Confirm major version 09/07/23 06:28:37.157
Sep  7 06:28:37.157: INFO: Major version: 1
STEP: Confirm minor version 09/07/23 06:28:37.157
Sep  7 06:28:37.157: INFO: cleanMinorVersion: 26
Sep  7 06:28:37.157: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  7 06:28:37.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-5872" for this suite. 09/07/23 06:28:37.159
------------------------------
â€¢ [0.016 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:28:37.145
    Sep  7 06:28:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename server-version 09/07/23 06:28:37.146
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.154
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/07/23 06:28:37.156
    STEP: Confirm major version 09/07/23 06:28:37.157
    Sep  7 06:28:37.157: INFO: Major version: 1
    STEP: Confirm minor version 09/07/23 06:28:37.157
    Sep  7 06:28:37.157: INFO: cleanMinorVersion: 26
    Sep  7 06:28:37.157: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:28:37.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-5872" for this suite. 09/07/23 06:28:37.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:28:37.163
Sep  7 06:28:37.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename podtemplate 09/07/23 06:28:37.164
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.171
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  7 06:28:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4453" for this suite. 09/07/23 06:28:37.189
------------------------------
â€¢ [0.031 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:28:37.163
    Sep  7 06:28:37.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename podtemplate 09/07/23 06:28:37.164
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.171
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:28:37.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4453" for this suite. 09/07/23 06:28:37.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:28:37.195
Sep  7 06:28:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:28:37.195
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.203
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  7 06:28:37.214: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  7 06:29:37.226: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:29:37.228
Sep  7 06:29:37.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename sched-preemption-path 09/07/23 06:29:37.228
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:29:37.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:29:37.239
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  7 06:29:37.248: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  7 06:29:37.249: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  7 06:29:37.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:29:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9457" for this suite. 09/07/23 06:29:37.293
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5842" for this suite. 09/07/23 06:29:37.296
------------------------------
â€¢ [SLOW TEST] [60.105 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:28:37.195
    Sep  7 06:28:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption 09/07/23 06:28:37.195
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:28:37.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:28:37.203
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  7 06:28:37.214: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  7 06:29:37.226: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:29:37.228
    Sep  7 06:29:37.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename sched-preemption-path 09/07/23 06:29:37.228
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:29:37.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:29:37.239
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  7 06:29:37.248: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  7 06:29:37.249: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:29:37.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:29:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9457" for this suite. 09/07/23 06:29:37.293
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5842" for this suite. 09/07/23 06:29:37.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:29:37.3
Sep  7 06:29:37.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 06:29:37.301
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:29:37.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:29:37.31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2314 09/07/23 06:29:37.312
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-2314 09/07/23 06:29:37.315
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2314 09/07/23 06:29:37.32
Sep  7 06:29:37.322: INFO: Found 0 stateful pods, waiting for 1
Sep  7 06:29:47.325: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/07/23 06:29:47.325
Sep  7 06:29:47.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 06:29:47.513: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 06:29:47.513: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 06:29:47.513: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 06:29:47.515: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  7 06:29:57.518: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 06:29:57.518: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:29:57.526: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  7 06:29:57.526: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  }]
Sep  7 06:29:57.526: INFO: 
Sep  7 06:29:57.526: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  7 06:29:58.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998171708s
Sep  7 06:29:59.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9957339s
Sep  7 06:30:00.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993222852s
Sep  7 06:30:01.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990838471s
Sep  7 06:30:02.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987364563s
Sep  7 06:30:03.542: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985025703s
Sep  7 06:30:04.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982066105s
Sep  7 06:30:05.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979730093s
Sep  7 06:30:06.550: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.861965ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2314 09/07/23 06:30:07.551
Sep  7 06:30:07.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 06:30:07.694: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  7 06:30:07.694: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 06:30:07.694: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 06:30:07.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 06:30:07.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  7 06:30:07.881: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 06:30:07.881: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 06:30:07.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  7 06:30:08.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  7 06:30:08.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  7 06:30:08.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  7 06:30:08.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  7 06:30:18.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:30:18.049: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  7 06:30:18.049: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/07/23 06:30:18.049
Sep  7 06:30:18.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 06:30:18.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 06:30:18.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 06:30:18.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 06:30:18.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 06:30:18.376: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 06:30:18.376: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 06:30:18.376: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 06:30:18.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  7 06:30:18.561: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  7 06:30:18.561: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  7 06:30:18.561: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  7 06:30:18.561: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:30:18.563: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  7 06:30:28.570: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 06:30:28.570: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 06:30:28.570: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  7 06:30:28.579: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  7 06:30:28.579: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  }]
Sep  7 06:30:28.579: INFO: ss-1  kind-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:28.579: INFO: ss-2  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:28.579: INFO: 
Sep  7 06:30:28.579: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  7 06:30:29.582: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  7 06:30:29.582: INFO: ss-1  kind-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:29.582: INFO: ss-2  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:29.582: INFO: 
Sep  7 06:30:29.582: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  7 06:30:30.585: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep  7 06:30:30.585: INFO: ss-1  kind-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:30.585: INFO: ss-2  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
Sep  7 06:30:30.585: INFO: 
Sep  7 06:30:30.585: INFO: StatefulSet ss has not reached scale 0, at 2
Sep  7 06:30:31.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992117595s
Sep  7 06:30:32.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.989993258s
Sep  7 06:30:33.592: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.98763991s
Sep  7 06:30:34.594: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985464672s
Sep  7 06:30:35.597: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.982968975s
Sep  7 06:30:36.599: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980232077s
Sep  7 06:30:37.602: INFO: Verifying statefulset ss doesn't scale past 0 for another 977.592229ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2314 09/07/23 06:30:38.602
Sep  7 06:30:38.605: INFO: Scaling statefulset ss to 0
Sep  7 06:30:38.611: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 06:30:38.612: INFO: Deleting all statefulset in ns statefulset-2314
Sep  7 06:30:38.614: INFO: Scaling statefulset ss to 0
Sep  7 06:30:38.619: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:30:38.620: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:30:38.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2314" for this suite. 09/07/23 06:30:38.629
------------------------------
â€¢ [SLOW TEST] [61.332 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:29:37.3
    Sep  7 06:29:37.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 06:29:37.301
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:29:37.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:29:37.31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2314 09/07/23 06:29:37.312
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-2314 09/07/23 06:29:37.315
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2314 09/07/23 06:29:37.32
    Sep  7 06:29:37.322: INFO: Found 0 stateful pods, waiting for 1
    Sep  7 06:29:47.325: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/07/23 06:29:47.325
    Sep  7 06:29:47.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 06:29:47.513: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 06:29:47.513: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 06:29:47.513: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 06:29:47.515: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  7 06:29:57.518: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 06:29:57.518: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:29:57.526: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  7 06:29:57.526: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  }]
    Sep  7 06:29:57.526: INFO: 
    Sep  7 06:29:57.526: INFO: StatefulSet ss has not reached scale 3, at 1
    Sep  7 06:29:58.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998171708s
    Sep  7 06:29:59.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9957339s
    Sep  7 06:30:00.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993222852s
    Sep  7 06:30:01.536: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990838471s
    Sep  7 06:30:02.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987364563s
    Sep  7 06:30:03.542: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985025703s
    Sep  7 06:30:04.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982066105s
    Sep  7 06:30:05.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979730093s
    Sep  7 06:30:06.550: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.861965ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2314 09/07/23 06:30:07.551
    Sep  7 06:30:07.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 06:30:07.694: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  7 06:30:07.694: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 06:30:07.694: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 06:30:07.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 06:30:07.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  7 06:30:07.881: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 06:30:07.881: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 06:30:07.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  7 06:30:08.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  7 06:30:08.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  7 06:30:08.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  7 06:30:08.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Sep  7 06:30:18.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:30:18.049: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  7 06:30:18.049: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/07/23 06:30:18.049
    Sep  7 06:30:18.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 06:30:18.227: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 06:30:18.227: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 06:30:18.227: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 06:30:18.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 06:30:18.376: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 06:30:18.376: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 06:30:18.376: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 06:30:18.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=statefulset-2314 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  7 06:30:18.561: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  7 06:30:18.561: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  7 06:30:18.561: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  7 06:30:18.561: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:30:18.563: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  7 06:30:28.570: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 06:30:28.570: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 06:30:28.570: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  7 06:30:28.579: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  7 06:30:28.579: INFO: ss-0  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:37 +0000 UTC  }]
    Sep  7 06:30:28.579: INFO: ss-1  kind-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:28.579: INFO: ss-2  kind-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:28.579: INFO: 
    Sep  7 06:30:28.579: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  7 06:30:29.582: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  7 06:30:29.582: INFO: ss-1  kind-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:29.582: INFO: ss-2  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:29.582: INFO: 
    Sep  7 06:30:29.582: INFO: StatefulSet ss has not reached scale 0, at 2
    Sep  7 06:30:30.585: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Sep  7 06:30:30.585: INFO: ss-1  kind-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:30.585: INFO: ss-2  kind-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:30:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-07 06:29:57 +0000 UTC  }]
    Sep  7 06:30:30.585: INFO: 
    Sep  7 06:30:30.585: INFO: StatefulSet ss has not reached scale 0, at 2
    Sep  7 06:30:31.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992117595s
    Sep  7 06:30:32.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.989993258s
    Sep  7 06:30:33.592: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.98763991s
    Sep  7 06:30:34.594: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985464672s
    Sep  7 06:30:35.597: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.982968975s
    Sep  7 06:30:36.599: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980232077s
    Sep  7 06:30:37.602: INFO: Verifying statefulset ss doesn't scale past 0 for another 977.592229ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2314 09/07/23 06:30:38.602
    Sep  7 06:30:38.605: INFO: Scaling statefulset ss to 0
    Sep  7 06:30:38.611: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 06:30:38.612: INFO: Deleting all statefulset in ns statefulset-2314
    Sep  7 06:30:38.614: INFO: Scaling statefulset ss to 0
    Sep  7 06:30:38.619: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:30:38.620: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:30:38.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2314" for this suite. 09/07/23 06:30:38.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:30:38.634
Sep  7 06:30:38.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename var-expansion 09/07/23 06:30:38.635
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:30:38.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:30:38.646
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/07/23 06:30:38.647
Sep  7 06:30:38.653: INFO: Waiting up to 2m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328" to be "running"
Sep  7 06:30:38.655: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56217ms
Sep  7 06:30:40.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005070005s
Sep  7 06:30:42.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00462185s
Sep  7 06:30:44.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003895365s
Sep  7 06:30:46.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004609519s
Sep  7 06:30:48.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004348604s
Sep  7 06:30:50.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005237549s
Sep  7 06:30:52.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004660203s
Sep  7 06:30:54.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005278597s
Sep  7 06:30:56.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004988821s
Sep  7 06:30:58.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005493655s
Sep  7 06:31:00.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005594589s
Sep  7 06:31:02.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004826412s
Sep  7 06:31:04.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005164986s
Sep  7 06:31:06.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004877059s
Sep  7 06:31:08.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004413762s
Sep  7 06:31:10.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005718585s
Sep  7 06:31:12.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004103458s
Sep  7 06:31:14.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005561791s
Sep  7 06:31:16.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005709823s
Sep  7 06:31:18.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005173866s
Sep  7 06:31:20.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005313518s
Sep  7 06:31:22.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00474581s
Sep  7 06:31:24.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004202623s
Sep  7 06:31:26.660: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006181325s
Sep  7 06:31:28.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005741277s
Sep  7 06:31:30.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005710658s
Sep  7 06:31:32.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00403262s
Sep  7 06:31:34.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005152161s
Sep  7 06:31:36.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004799653s
Sep  7 06:31:38.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.003920624s
Sep  7 06:31:40.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004783465s
Sep  7 06:31:42.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004233066s
Sep  7 06:31:44.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005661896s
Sep  7 06:31:46.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005635077s
Sep  7 06:31:48.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005401558s
Sep  7 06:31:50.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005329618s
Sep  7 06:31:52.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003782358s
Sep  7 06:31:54.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005168319s
Sep  7 06:31:56.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004900569s
Sep  7 06:31:58.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004228899s
Sep  7 06:32:00.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005451789s
Sep  7 06:32:02.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004857188s
Sep  7 06:32:04.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004915418s
Sep  7 06:32:06.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004047708s
Sep  7 06:32:08.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004431336s
Sep  7 06:32:10.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005670826s
Sep  7 06:32:12.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004356775s
Sep  7 06:32:14.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005786284s
Sep  7 06:32:16.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004246852s
Sep  7 06:32:18.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004950541s
Sep  7 06:32:20.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00538881s
Sep  7 06:32:22.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004630219s
Sep  7 06:32:24.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004691027s
Sep  7 06:32:26.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005636855s
Sep  7 06:32:28.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005504623s
Sep  7 06:32:30.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005732571s
Sep  7 06:32:32.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004081319s
Sep  7 06:32:34.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005108047s
Sep  7 06:32:36.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004617935s
Sep  7 06:32:38.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004945833s
Sep  7 06:32:38.660: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006498352s
STEP: updating the pod 09/07/23 06:32:38.66
Sep  7 06:32:39.171: INFO: Successfully updated pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc"
STEP: waiting for pod running 09/07/23 06:32:39.171
Sep  7 06:32:39.171: INFO: Waiting up to 2m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328" to be "running"
Sep  7 06:32:39.172: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486011ms
Sep  7 06:32:41.175: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003685248s
Sep  7 06:32:41.175: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" satisfied condition "running"
STEP: deleting the pod gracefully 09/07/23 06:32:41.175
Sep  7 06:32:41.175: INFO: Deleting pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328"
Sep  7 06:32:41.180: INFO: Wait up to 5m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-328" for this suite. 09/07/23 06:33:15.187
------------------------------
â€¢ [SLOW TEST] [156.556 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:30:38.634
    Sep  7 06:30:38.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename var-expansion 09/07/23 06:30:38.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:30:38.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:30:38.646
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/07/23 06:30:38.647
    Sep  7 06:30:38.653: INFO: Waiting up to 2m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328" to be "running"
    Sep  7 06:30:38.655: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56217ms
    Sep  7 06:30:40.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005070005s
    Sep  7 06:30:42.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00462185s
    Sep  7 06:30:44.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003895365s
    Sep  7 06:30:46.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004609519s
    Sep  7 06:30:48.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004348604s
    Sep  7 06:30:50.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005237549s
    Sep  7 06:30:52.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004660203s
    Sep  7 06:30:54.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005278597s
    Sep  7 06:30:56.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004988821s
    Sep  7 06:30:58.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005493655s
    Sep  7 06:31:00.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005594589s
    Sep  7 06:31:02.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004826412s
    Sep  7 06:31:04.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005164986s
    Sep  7 06:31:06.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004877059s
    Sep  7 06:31:08.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004413762s
    Sep  7 06:31:10.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005718585s
    Sep  7 06:31:12.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004103458s
    Sep  7 06:31:14.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005561791s
    Sep  7 06:31:16.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005709823s
    Sep  7 06:31:18.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005173866s
    Sep  7 06:31:20.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005313518s
    Sep  7 06:31:22.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 44.00474581s
    Sep  7 06:31:24.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004202623s
    Sep  7 06:31:26.660: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006181325s
    Sep  7 06:31:28.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005741277s
    Sep  7 06:31:30.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005710658s
    Sep  7 06:31:32.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00403262s
    Sep  7 06:31:34.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005152161s
    Sep  7 06:31:36.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004799653s
    Sep  7 06:31:38.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.003920624s
    Sep  7 06:31:40.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004783465s
    Sep  7 06:31:42.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004233066s
    Sep  7 06:31:44.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005661896s
    Sep  7 06:31:46.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005635077s
    Sep  7 06:31:48.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005401558s
    Sep  7 06:31:50.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005329618s
    Sep  7 06:31:52.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003782358s
    Sep  7 06:31:54.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.005168319s
    Sep  7 06:31:56.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004900569s
    Sep  7 06:31:58.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004228899s
    Sep  7 06:32:00.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005451789s
    Sep  7 06:32:02.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004857188s
    Sep  7 06:32:04.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004915418s
    Sep  7 06:32:06.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004047708s
    Sep  7 06:32:08.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004431336s
    Sep  7 06:32:10.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005670826s
    Sep  7 06:32:12.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004356775s
    Sep  7 06:32:14.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005786284s
    Sep  7 06:32:16.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004246852s
    Sep  7 06:32:18.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004950541s
    Sep  7 06:32:20.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00538881s
    Sep  7 06:32:22.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.004630219s
    Sep  7 06:32:24.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.004691027s
    Sep  7 06:32:26.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005636855s
    Sep  7 06:32:28.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005504623s
    Sep  7 06:32:30.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005732571s
    Sep  7 06:32:32.657: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004081319s
    Sep  7 06:32:34.659: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005108047s
    Sep  7 06:32:36.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004617935s
    Sep  7 06:32:38.658: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004945833s
    Sep  7 06:32:38.660: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006498352s
    STEP: updating the pod 09/07/23 06:32:38.66
    Sep  7 06:32:39.171: INFO: Successfully updated pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc"
    STEP: waiting for pod running 09/07/23 06:32:39.171
    Sep  7 06:32:39.171: INFO: Waiting up to 2m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328" to be "running"
    Sep  7 06:32:39.172: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486011ms
    Sep  7 06:32:41.175: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003685248s
    Sep  7 06:32:41.175: INFO: Pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" satisfied condition "running"
    STEP: deleting the pod gracefully 09/07/23 06:32:41.175
    Sep  7 06:32:41.175: INFO: Deleting pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" in namespace "var-expansion-328"
    Sep  7 06:32:41.180: INFO: Wait up to 5m0s for pod "var-expansion-2ff1463a-ca16-4039-bc65-ad4b7caacbfc" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:15.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-328" for this suite. 09/07/23 06:33:15.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:15.191
Sep  7 06:33:15.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename container-probe 09/07/23 06:33:15.191
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:15.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:15.2
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 in namespace container-probe-7411 09/07/23 06:33:15.202
Sep  7 06:33:15.206: INFO: Waiting up to 5m0s for pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642" in namespace "container-probe-7411" to be "not pending"
Sep  7 06:33:15.207: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642": Phase="Pending", Reason="", readiness=false. Elapsed: 1.21134ms
Sep  7 06:33:17.210: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642": Phase="Running", Reason="", readiness=true. Elapsed: 2.003980815s
Sep  7 06:33:17.210: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642" satisfied condition "not pending"
Sep  7 06:33:17.210: INFO: Started pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 in namespace container-probe-7411
STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:33:17.21
Sep  7 06:33:17.212: INFO: Initial restart count of pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 is 0
Sep  7 06:33:37.241: INFO: Restart count of pod container-probe-7411/liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 is now 1 (20.029778279s elapsed)
STEP: deleting the pod 09/07/23 06:33:37.241
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:37.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7411" for this suite. 09/07/23 06:33:37.253
------------------------------
â€¢ [SLOW TEST] [22.067 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:15.191
    Sep  7 06:33:15.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename container-probe 09/07/23 06:33:15.191
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:15.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:15.2
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 in namespace container-probe-7411 09/07/23 06:33:15.202
    Sep  7 06:33:15.206: INFO: Waiting up to 5m0s for pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642" in namespace "container-probe-7411" to be "not pending"
    Sep  7 06:33:15.207: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642": Phase="Pending", Reason="", readiness=false. Elapsed: 1.21134ms
    Sep  7 06:33:17.210: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642": Phase="Running", Reason="", readiness=true. Elapsed: 2.003980815s
    Sep  7 06:33:17.210: INFO: Pod "liveness-a481e5be-5b44-402f-9046-69f7cb4e2642" satisfied condition "not pending"
    Sep  7 06:33:17.210: INFO: Started pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 in namespace container-probe-7411
    STEP: checking the pod's current state and verifying that restartCount is present 09/07/23 06:33:17.21
    Sep  7 06:33:17.212: INFO: Initial restart count of pod liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 is 0
    Sep  7 06:33:37.241: INFO: Restart count of pod container-probe-7411/liveness-a481e5be-5b44-402f-9046-69f7cb4e2642 is now 1 (20.029778279s elapsed)
    STEP: deleting the pod 09/07/23 06:33:37.241
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:37.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7411" for this suite. 09/07/23 06:33:37.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:37.259
Sep  7 06:33:37.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename secrets 09/07/23 06:33:37.26
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:37.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:37.268
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-1a05ffc4-4822-41f6-93e0-7ca60006d323 09/07/23 06:33:37.27
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:37.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2463" for this suite. 09/07/23 06:33:37.273
------------------------------
â€¢ [0.018 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:37.259
    Sep  7 06:33:37.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename secrets 09/07/23 06:33:37.26
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:37.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:37.268
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-1a05ffc4-4822-41f6-93e0-7ca60006d323 09/07/23 06:33:37.27
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:37.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2463" for this suite. 09/07/23 06:33:37.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:37.278
Sep  7 06:33:37.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:33:37.278
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:37.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:37.286
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/07/23 06:33:37.288
Sep  7 06:33:37.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: rename a version 09/07/23 06:33:40.474
STEP: check the new version name is served 09/07/23 06:33:40.484
STEP: check the old version name is removed 09/07/23 06:33:41.787
STEP: check the other version is not changed 09/07/23 06:33:42.42
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4255" for this suite. 09/07/23 06:33:44.979
------------------------------
â€¢ [SLOW TEST] [7.705 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:37.278
    Sep  7 06:33:37.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:33:37.278
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:37.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:37.286
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/07/23 06:33:37.288
    Sep  7 06:33:37.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: rename a version 09/07/23 06:33:40.474
    STEP: check the new version name is served 09/07/23 06:33:40.484
    STEP: check the old version name is removed 09/07/23 06:33:41.787
    STEP: check the other version is not changed 09/07/23 06:33:42.42
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4255" for this suite. 09/07/23 06:33:44.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:44.983
Sep  7 06:33:44.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename daemonsets 09/07/23 06:33:44.984
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:44.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:44.992
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 09/07/23 06:33:45.001
STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 06:33:45.004
Sep  7 06:33:45.006: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:45.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 06:33:45.009: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 06:33:46.011: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:46.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 06:33:46.014: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 06:33:47.012: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:47.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 06:33:47.014: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/07/23 06:33:47.015
Sep  7 06:33:47.024: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:47.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 06:33:47.028: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 06:33:48.031: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:48.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  7 06:33:48.033: INFO: Node kind-worker is running 0 daemon pod, expected 1
Sep  7 06:33:49.031: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  7 06:33:49.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  7 06:33:49.034: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/07/23 06:33:49.034
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/07/23 06:33:49.037
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3861, will wait for the garbage collector to delete the pods 09/07/23 06:33:49.037
Sep  7 06:33:49.092: INFO: Deleting DaemonSet.extensions daemon-set took: 3.46684ms
Sep  7 06:33:49.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.09328ms
Sep  7 06:33:53.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  7 06:33:53.495: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  7 06:33:53.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30122"},"items":null}

Sep  7 06:33:53.498: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30122"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3861" for this suite. 09/07/23 06:33:53.504
------------------------------
â€¢ [SLOW TEST] [8.524 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:44.983
    Sep  7 06:33:44.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename daemonsets 09/07/23 06:33:44.984
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:44.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:44.992
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 09/07/23 06:33:45.001
    STEP: Check that daemon pods launch on every node of the cluster. 09/07/23 06:33:45.004
    Sep  7 06:33:45.006: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:45.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 06:33:45.009: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 06:33:46.011: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:46.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 06:33:46.014: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 06:33:47.012: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:47.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 06:33:47.014: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/07/23 06:33:47.015
    Sep  7 06:33:47.024: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:47.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 06:33:47.028: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 06:33:48.031: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:48.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  7 06:33:48.033: INFO: Node kind-worker is running 0 daemon pod, expected 1
    Sep  7 06:33:49.031: INFO: DaemonSet pods can't tolerate node kind-control-plane with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  7 06:33:49.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  7 06:33:49.034: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/07/23 06:33:49.034
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/07/23 06:33:49.037
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3861, will wait for the garbage collector to delete the pods 09/07/23 06:33:49.037
    Sep  7 06:33:49.092: INFO: Deleting DaemonSet.extensions daemon-set took: 3.46684ms
    Sep  7 06:33:49.193: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.09328ms
    Sep  7 06:33:53.495: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  7 06:33:53.495: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  7 06:33:53.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30122"},"items":null}

    Sep  7 06:33:53.498: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30122"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3861" for this suite. 09/07/23 06:33:53.504
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:53.508
Sep  7 06:33:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename dns 09/07/23 06:33:53.508
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:53.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:53.519
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/07/23 06:33:53.521
Sep  7 06:33:53.526: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9538  a12583fd-cac1-4b98-aaff-659b05bb17e4 30127 0 2023-09-07 06:33:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-07 06:33:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zftz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zftz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:33:53.526: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9538" to be "running and ready"
Sep  7 06:33:53.527: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.163181ms
Sep  7 06:33:53.527: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:33:55.530: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004566743s
Sep  7 06:33:55.530: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  7 06:33:55.530: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/07/23 06:33:55.53
Sep  7 06:33:55.531: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:33:55.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:33:55.531: INFO: ExecWithOptions: Clientset creation
Sep  7 06:33:55.531: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/07/23 06:33:55.659
Sep  7 06:33:55.659: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  7 06:33:55.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
Sep  7 06:33:55.659: INFO: ExecWithOptions: Clientset creation
Sep  7 06:33:55.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  7 06:33:55.803: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  7 06:33:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9538" for this suite. 09/07/23 06:33:55.815
------------------------------
â€¢ [2.311 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:53.508
    Sep  7 06:33:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename dns 09/07/23 06:33:53.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:53.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:53.519
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/07/23 06:33:53.521
    Sep  7 06:33:53.526: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9538  a12583fd-cac1-4b98-aaff-659b05bb17e4 30127 0 2023-09-07 06:33:53 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-07 06:33:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zftz4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zftz4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:33:53.526: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9538" to be "running and ready"
    Sep  7 06:33:53.527: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.163181ms
    Sep  7 06:33:53.527: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:33:55.530: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004566743s
    Sep  7 06:33:55.530: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  7 06:33:55.530: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/07/23 06:33:55.53
    Sep  7 06:33:55.531: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:33:55.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:33:55.531: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:33:55.531: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/07/23 06:33:55.659
    Sep  7 06:33:55.659: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  7 06:33:55.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    Sep  7 06:33:55.659: INFO: ExecWithOptions: Clientset creation
    Sep  7 06:33:55.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  7 06:33:55.803: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:33:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9538" for this suite. 09/07/23 06:33:55.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:33:55.819
Sep  7 06:33:55.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename deployment 09/07/23 06:33:55.82
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:55.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:55.829
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  7 06:33:55.831: INFO: Creating deployment "webserver-deployment"
Sep  7 06:33:55.833: INFO: Waiting for observed generation 1
Sep  7 06:33:57.837: INFO: Waiting for all required pods to come up
Sep  7 06:33:57.840: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/07/23 06:33:57.84
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w8zd7" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cl8k7" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pq6xl" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gd2rr" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5xg2h" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w2btz" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5slg4" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5vdgh" in namespace "deployment-8555" to be "running"
Sep  7 06:33:57.842: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90656ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12422ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0984ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29948ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374021ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274001ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182749ms
Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.442829ms
Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005897814s
Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4" satisfied condition "running"
Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006103324s
Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082394s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006288494s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006292643s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh": Phase="Running", Reason="", readiness=true. Elapsed: 2.006177553s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006359954s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006393574s
Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz" satisfied condition "running"
Sep  7 06:33:59.847: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  7 06:33:59.850: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  7 06:33:59.855: INFO: Updating deployment webserver-deployment
Sep  7 06:33:59.855: INFO: Waiting for observed generation 2
Sep  7 06:34:01.860: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  7 06:34:01.862: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  7 06:34:01.863: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  7 06:34:01.867: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  7 06:34:01.867: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  7 06:34:01.871: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  7 06:34:01.873: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  7 06:34:01.873: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  7 06:34:01.878: INFO: Updating deployment webserver-deployment
Sep  7 06:34:01.878: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  7 06:34:01.883: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  7 06:34:01.884: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  7 06:34:01.889: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8555  9f97a064-acb6-4812-9782-bc3a533b9305 30369 3 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfdb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 06:33:58 +0000 UTC,LastTransitionTime:2023-09-07 06:33:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-07 06:33:59 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  7 06:34:01.895: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8555  b8117449-539d-4670-87e9-b8cfad8a6d76 30373 3 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9f97a064-acb6-4812-9782-bc3a533b9305 0xc004936107 0xc004936108}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f97a064-acb6-4812-9782-bc3a533b9305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004936748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  7 06:34:01.895: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  7 06:34:01.895: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8555  cb629202-2886-446b-84e5-1b1e11ad6494 30370 3 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9f97a064-acb6-4812-9782-bc3a533b9305 0xc003cfdf57 0xc003cfdf58}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f97a064-acb6-4812-9782-bc3a533b9305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfdfe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5vdgh webserver-deployment-7f5969cbc7- deployment-8555  d59b8051-832e-4f09-8139-125579e75a50 30243 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc004937af7 0xc004937af8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gc7fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc7fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.82,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0f1cf84d30594f9bd265af8abe7f8be64dee4cb927eb8d91e6cd8970989bf9f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5xg2h webserver-deployment-7f5969cbc7- deployment-8555  4c2d5d24-9d21-4130-a512-42bc3ad5eecc 30246 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836090 0xc006836091}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qm229,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qm229,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.115,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ac1850f565a08d6a3bc7bd0238dcdb347adbe939d0207a26540967e4fcbcc7e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-b8k87" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b8k87 webserver-deployment-7f5969cbc7- deployment-8555  7a3a9cd5-e638-4912-a5bf-51cc76164dc3 30383 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068362b7 0xc0068362b8}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hl2vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hl2vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-bszj7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bszj7 webserver-deployment-7f5969cbc7- deployment-8555  21bd718e-9d9e-4eab-abb7-b542149dcda8 30232 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836490 0xc006836491}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwntb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwntb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.118,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a2994c656059bc772629463fba34c5b6de0943537449a3b0e4a1a983059dc0a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cl8k7 webserver-deployment-7f5969cbc7- deployment-8555  d2bb4e18-e2d5-4eb1-ab0a-b9f6472c48d6 30289 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836677 0xc006836678}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn5c8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn5c8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.84,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0fddedf118f71a8993ca20cc6040944d3fef56c8cc9115cf6a493e10805c73d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-ffxk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ffxk2 webserver-deployment-7f5969cbc7- deployment-8555  42bce00d-e7cf-4437-af6a-3bb8eca1b227 30388 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836890 0xc006836891}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d2w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d2w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gd2rr webserver-deployment-7f5969cbc7- deployment-8555  c4acd82b-0d2f-4c48-88dc-1c57871b454d 30252 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068369c7 0xc0068369c8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttrmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttrmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.83,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9aa4b8fc704ee1171a3847345c1266567527617d286c3ceb2574c886c754571c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-hk8mp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk8mp webserver-deployment-7f5969cbc7- deployment-8555  391135bb-75e3-4a28-ac88-dc0c94b32ef2 30387 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836bb0 0xc006836bb1}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9zc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9zc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-hstdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hstdt webserver-deployment-7f5969cbc7- deployment-8555  b52752cc-e7c4-47dd-aeef-353120f3c9b6 30390 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836ce7 0xc006836ce8}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8czk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8czk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-mdln6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mdln6 webserver-deployment-7f5969cbc7- deployment-8555  fc9f9956-7b7f-4ddd-9aba-dabc73044f95 30376 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836e37 0xc006836e38}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps8sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps8sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pq6xl webserver-deployment-7f5969cbc7- deployment-8555  e907506e-86e4-4364-a887-633c458cad92 30279 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836fa0 0xc006836fa1}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8n4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8n4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.116,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf9ececd01ab47dd30ab06246b384a3f4bef118ce3c4cdbee1bd26475622e28d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-vfhnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vfhnz webserver-deployment-7f5969cbc7- deployment-8555  e9bb292b-0f79-49b7-867f-3d512a761303 30384 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006837197 0xc006837198}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ws55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ws55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w2btz webserver-deployment-7f5969cbc7- deployment-8555  2ecdcba5-7646-4353-8ed1-4ec85cfa9802 30285 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068372d7 0xc0068372d8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tdkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tdkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.117,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://228c37e07c557fb21e5256f5304904c5aa0c8a14da304f1310edaa24e1ae40f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w8zd7 webserver-deployment-7f5969cbc7- deployment-8555  99a18304-a085-446a-8467-ba0e31538668 30282 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068374c7 0xc0068374c8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdsbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdsbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.81,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5314002158d7631dae5bc81587018d75422c9d4393759c69770d6f569c91c327,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-7f5969cbc7-x4h7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x4h7g webserver-deployment-7f5969cbc7- deployment-8555  1f15d640-3c33-42d8-bdb3-18fa5f36d886 30381 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006837710 0xc006837711}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x86f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x86f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-82z6m" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82z6m webserver-deployment-d9f79cb5- deployment-8555  73e6dc27-b5e5-4149-93c9-6b60d0b9a15b 30340 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc00683785f 0xc006837880}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkk24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkk24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-bb49q" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bb49q webserver-deployment-d9f79cb5- deployment-8555  b18950ec-94bb-4205-81e2-7318d309d6ec 30386 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837aaf 0xc006837ac0}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb9l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb9l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-bt8j6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bt8j6 webserver-deployment-d9f79cb5- deployment-8555  c11f5713-893a-48df-a86d-93adc21b68ee 30389 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837c1f 0xc006837c50}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2zrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2zrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-f6z6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f6z6l webserver-deployment-d9f79cb5- deployment-8555  d6b0a048-7183-444f-a45f-1109ec18a3fb 30356 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837da7 0xc006837da8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5jsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5jsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-rp5zq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rp5zq webserver-deployment-d9f79cb5- deployment-8555  3a036276-ad74-46d2-83cd-9d5a530cdef6 30339 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837fbf 0xc006837fd0}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mn9j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mn9j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-vgm6j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vgm6j webserver-deployment-d9f79cb5- deployment-8555  0fedc914-e19e-4870-aeac-63d386af02ec 30368 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc0069361df 0xc0069361f0}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6px4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6px4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-w4kxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w4kxd webserver-deployment-d9f79cb5- deployment-8555  0c818158-54f0-4891-af36-32aa513b6afc 30385 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc00693649f 0xc0069364b0}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzpws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzpws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-wlc2k" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wlc2k webserver-deployment-d9f79cb5- deployment-8555  ecbd096c-0fda-45ba-bca1-1f70d91964bf 30364 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006936607 0xc006936608}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8qt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8qt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.120,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:01.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8555" for this suite. 09/07/23 06:34:01.911
------------------------------
â€¢ [SLOW TEST] [6.104 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:33:55.819
    Sep  7 06:33:55.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename deployment 09/07/23 06:33:55.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:33:55.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:33:55.829
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  7 06:33:55.831: INFO: Creating deployment "webserver-deployment"
    Sep  7 06:33:55.833: INFO: Waiting for observed generation 1
    Sep  7 06:33:57.837: INFO: Waiting for all required pods to come up
    Sep  7 06:33:57.840: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/07/23 06:33:57.84
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w8zd7" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-cl8k7" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-pq6xl" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-gd2rr" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5xg2h" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w2btz" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5slg4" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.840: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5vdgh" in namespace "deployment-8555" to be "running"
    Sep  7 06:33:57.842: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90656ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.12422ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0984ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29948ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374021ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274001ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182749ms
    Sep  7 06:33:57.843: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.442829ms
    Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005897814s
    Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-5slg4" satisfied condition "running"
    Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006103324s
    Sep  7 06:33:59.846: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082394s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006288494s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl": Phase="Running", Reason="", readiness=true. Elapsed: 2.006292643s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh": Phase="Running", Reason="", readiness=true. Elapsed: 2.006177553s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.006359954s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006393574s
    Sep  7 06:33:59.847: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz" satisfied condition "running"
    Sep  7 06:33:59.847: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  7 06:33:59.850: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  7 06:33:59.855: INFO: Updating deployment webserver-deployment
    Sep  7 06:33:59.855: INFO: Waiting for observed generation 2
    Sep  7 06:34:01.860: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  7 06:34:01.862: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  7 06:34:01.863: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  7 06:34:01.867: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  7 06:34:01.867: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  7 06:34:01.871: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  7 06:34:01.873: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  7 06:34:01.873: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  7 06:34:01.878: INFO: Updating deployment webserver-deployment
    Sep  7 06:34:01.878: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  7 06:34:01.883: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  7 06:34:01.884: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  7 06:34:01.889: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8555  9f97a064-acb6-4812-9782-bc3a533b9305 30369 3 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfdb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-07 06:33:58 +0000 UTC,LastTransitionTime:2023-09-07 06:33:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-07 06:33:59 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  7 06:34:01.895: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8555  b8117449-539d-4670-87e9-b8cfad8a6d76 30373 3 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9f97a064-acb6-4812-9782-bc3a533b9305 0xc004936107 0xc004936108}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f97a064-acb6-4812-9782-bc3a533b9305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004936748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 06:34:01.895: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  7 06:34:01.895: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8555  cb629202-2886-446b-84e5-1b1e11ad6494 30370 3 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9f97a064-acb6-4812-9782-bc3a533b9305 0xc003cfdf57 0xc003cfdf58}] [] [{kube-controller-manager Update apps/v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f97a064-acb6-4812-9782-bc3a533b9305\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfdfe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-5vdgh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5vdgh webserver-deployment-7f5969cbc7- deployment-8555  d59b8051-832e-4f09-8139-125579e75a50 30243 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc004937af7 0xc004937af8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gc7fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gc7fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.82,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0f1cf84d30594f9bd265af8abe7f8be64dee4cb927eb8d91e6cd8970989bf9f8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-5xg2h" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5xg2h webserver-deployment-7f5969cbc7- deployment-8555  4c2d5d24-9d21-4130-a512-42bc3ad5eecc 30246 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836090 0xc006836091}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qm229,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qm229,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.115,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ac1850f565a08d6a3bc7bd0238dcdb347adbe939d0207a26540967e4fcbcc7e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-b8k87" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b8k87 webserver-deployment-7f5969cbc7- deployment-8555  7a3a9cd5-e638-4912-a5bf-51cc76164dc3 30383 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068362b7 0xc0068362b8}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hl2vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hl2vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-bszj7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bszj7 webserver-deployment-7f5969cbc7- deployment-8555  21bd718e-9d9e-4eab-abb7-b542149dcda8 30232 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836490 0xc006836491}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwntb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwntb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.118,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a2994c656059bc772629463fba34c5b6de0943537449a3b0e4a1a983059dc0a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-cl8k7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cl8k7 webserver-deployment-7f5969cbc7- deployment-8555  d2bb4e18-e2d5-4eb1-ab0a-b9f6472c48d6 30289 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836677 0xc006836678}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qn5c8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qn5c8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.84,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0fddedf118f71a8993ca20cc6040944d3fef56c8cc9115cf6a493e10805c73d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.903: INFO: Pod "webserver-deployment-7f5969cbc7-ffxk2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ffxk2 webserver-deployment-7f5969cbc7- deployment-8555  42bce00d-e7cf-4437-af6a-3bb8eca1b227 30388 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836890 0xc006836891}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5d2w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5d2w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-gd2rr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gd2rr webserver-deployment-7f5969cbc7- deployment-8555  c4acd82b-0d2f-4c48-88dc-1c57871b454d 30252 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068369c7 0xc0068369c8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttrmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttrmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.83,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9aa4b8fc704ee1171a3847345c1266567527617d286c3ceb2574c886c754571c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-hk8mp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hk8mp webserver-deployment-7f5969cbc7- deployment-8555  391135bb-75e3-4a28-ac88-dc0c94b32ef2 30387 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836bb0 0xc006836bb1}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9zc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9zc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-hstdt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hstdt webserver-deployment-7f5969cbc7- deployment-8555  b52752cc-e7c4-47dd-aeef-353120f3c9b6 30390 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836ce7 0xc006836ce8}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8czk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8czk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-mdln6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mdln6 webserver-deployment-7f5969cbc7- deployment-8555  fc9f9956-7b7f-4ddd-9aba-dabc73044f95 30376 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836e37 0xc006836e38}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps8sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps8sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-pq6xl" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pq6xl webserver-deployment-7f5969cbc7- deployment-8555  e907506e-86e4-4364-a887-633c458cad92 30279 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006836fa0 0xc006836fa1}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n8n4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n8n4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.116,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf9ececd01ab47dd30ab06246b384a3f4bef118ce3c4cdbee1bd26475622e28d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-vfhnz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vfhnz webserver-deployment-7f5969cbc7- deployment-8555  e9bb292b-0f79-49b7-867f-3d512a761303 30384 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006837197 0xc006837198}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ws55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ws55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.904: INFO: Pod "webserver-deployment-7f5969cbc7-w2btz" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w2btz webserver-deployment-7f5969cbc7- deployment-8555  2ecdcba5-7646-4353-8ed1-4ec85cfa9802 30285 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068372d7 0xc0068372d8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tdkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tdkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.117,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://228c37e07c557fb21e5256f5304904c5aa0c8a14da304f1310edaa24e1ae40f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-7f5969cbc7-w8zd7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-w8zd7 webserver-deployment-7f5969cbc7- deployment-8555  99a18304-a085-446a-8467-ba0e31538668 30282 0 2023-09-07 06:33:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc0068374c7 0xc0068374c8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdsbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdsbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:10.244.2.81,StartTime:2023-09-07 06:33:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-07 06:33:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5314002158d7631dae5bc81587018d75422c9d4393759c69770d6f569c91c327,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-7f5969cbc7-x4h7g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x4h7g webserver-deployment-7f5969cbc7- deployment-8555  1f15d640-3c33-42d8-bdb3-18fa5f36d886 30381 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 cb629202-2886-446b-84e5-1b1e11ad6494 0xc006837710 0xc006837711}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb629202-2886-446b-84e5-1b1e11ad6494\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x86f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x86f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-82z6m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-82z6m webserver-deployment-d9f79cb5- deployment-8555  73e6dc27-b5e5-4149-93c9-6b60d0b9a15b 30340 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc00683785f 0xc006837880}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkk24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkk24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-bb49q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bb49q webserver-deployment-d9f79cb5- deployment-8555  b18950ec-94bb-4205-81e2-7318d309d6ec 30386 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837aaf 0xc006837ac0}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb9l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb9l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:34:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-bt8j6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bt8j6 webserver-deployment-d9f79cb5- deployment-8555  c11f5713-893a-48df-a86d-93adc21b68ee 30389 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837c1f 0xc006837c50}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2zrt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2zrt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.905: INFO: Pod "webserver-deployment-d9f79cb5-f6z6l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f6z6l webserver-deployment-d9f79cb5- deployment-8555  d6b0a048-7183-444f-a45f-1109ec18a3fb 30356 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837da7 0xc006837da8}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5jsp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5jsp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-rp5zq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rp5zq webserver-deployment-d9f79cb5- deployment-8555  3a036276-ad74-46d2-83cd-9d5a530cdef6 30339 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006837fbf 0xc006837fd0}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mn9j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mn9j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-vgm6j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vgm6j webserver-deployment-d9f79cb5- deployment-8555  0fedc914-e19e-4870-aeac-63d386af02ec 30368 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc0069361df 0xc0069361f0}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g6px4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g6px4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.6,PodIP:,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-w4kxd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w4kxd webserver-deployment-d9f79cb5- deployment-8555  0c818158-54f0-4891-af36-32aa513b6afc 30385 0 2023-09-07 06:34:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc00693649f 0xc0069364b0}] [] [{kube-controller-manager Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qzpws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qzpws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  7 06:34:01.906: INFO: Pod "webserver-deployment-d9f79cb5-wlc2k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wlc2k webserver-deployment-d9f79cb5- deployment-8555  ecbd096c-0fda-45ba-bca1-1f70d91964bf 30364 0 2023-09-07 06:33:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 b8117449-539d-4670-87e9-b8cfad8a6d76 0xc006936607 0xc006936608}] [] [{kube-controller-manager Update v1 2023-09-07 06:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8117449-539d-4670-87e9-b8cfad8a6d76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-07 06:34:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8qt5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8qt5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-07 06:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.8.3,PodIP:10.244.1.120,StartTime:2023-09-07 06:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:01.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8555" for this suite. 09/07/23 06:34:01.911
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:01.923
Sep  7 06:34:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename certificates 09/07/23 06:34:01.924
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:01.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:01.935
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/07/23 06:34:02.353
STEP: getting /apis/certificates.k8s.io 09/07/23 06:34:02.355
STEP: getting /apis/certificates.k8s.io/v1 09/07/23 06:34:02.356
STEP: creating 09/07/23 06:34:02.357
STEP: getting 09/07/23 06:34:02.367
STEP: listing 09/07/23 06:34:02.369
STEP: watching 09/07/23 06:34:02.371
Sep  7 06:34:02.371: INFO: starting watch
STEP: patching 09/07/23 06:34:02.371
STEP: updating 09/07/23 06:34:02.376
Sep  7 06:34:02.379: INFO: waiting for watch events with expected annotations
Sep  7 06:34:02.379: INFO: saw patched and updated annotations
STEP: getting /approval 09/07/23 06:34:02.379
STEP: patching /approval 09/07/23 06:34:02.38
STEP: updating /approval 09/07/23 06:34:02.385
STEP: getting /status 09/07/23 06:34:02.388
STEP: patching /status 09/07/23 06:34:02.39
STEP: updating /status 09/07/23 06:34:02.394
STEP: deleting 09/07/23 06:34:02.399
STEP: deleting a collection 09/07/23 06:34:02.404
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:02.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7965" for this suite. 09/07/23 06:34:02.414
------------------------------
â€¢ [0.494 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:01.923
    Sep  7 06:34:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename certificates 09/07/23 06:34:01.924
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:01.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:01.935
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/07/23 06:34:02.353
    STEP: getting /apis/certificates.k8s.io 09/07/23 06:34:02.355
    STEP: getting /apis/certificates.k8s.io/v1 09/07/23 06:34:02.356
    STEP: creating 09/07/23 06:34:02.357
    STEP: getting 09/07/23 06:34:02.367
    STEP: listing 09/07/23 06:34:02.369
    STEP: watching 09/07/23 06:34:02.371
    Sep  7 06:34:02.371: INFO: starting watch
    STEP: patching 09/07/23 06:34:02.371
    STEP: updating 09/07/23 06:34:02.376
    Sep  7 06:34:02.379: INFO: waiting for watch events with expected annotations
    Sep  7 06:34:02.379: INFO: saw patched and updated annotations
    STEP: getting /approval 09/07/23 06:34:02.379
    STEP: patching /approval 09/07/23 06:34:02.38
    STEP: updating /approval 09/07/23 06:34:02.385
    STEP: getting /status 09/07/23 06:34:02.388
    STEP: patching /status 09/07/23 06:34:02.39
    STEP: updating /status 09/07/23 06:34:02.394
    STEP: deleting 09/07/23 06:34:02.399
    STEP: deleting a collection 09/07/23 06:34:02.404
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:02.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7965" for this suite. 09/07/23 06:34:02.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:02.418
Sep  7 06:34:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename proxy 09/07/23 06:34:02.419
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:02.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:02.428
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  7 06:34:02.429: INFO: Creating pod...
Sep  7 06:34:02.433: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9376" to be "running"
Sep  7 06:34:02.435: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39847ms
Sep  7 06:34:04.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003751973s
Sep  7 06:34:06.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003257826s
Sep  7 06:34:08.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003928059s
Sep  7 06:34:10.438: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004921862s
Sep  7 06:34:12.436: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 10.003142004s
Sep  7 06:34:12.436: INFO: Pod "agnhost" satisfied condition "running"
Sep  7 06:34:12.437: INFO: Creating service...
Sep  7 06:34:12.444: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/DELETE
Sep  7 06:34:12.447: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  7 06:34:12.448: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/GET
Sep  7 06:34:12.449: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  7 06:34:12.449: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/HEAD
Sep  7 06:34:12.451: INFO: http.Client request:HEAD | StatusCode:200
Sep  7 06:34:12.451: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  7 06:34:12.452: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  7 06:34:12.452: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/PATCH
Sep  7 06:34:12.454: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  7 06:34:12.454: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/POST
Sep  7 06:34:12.456: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  7 06:34:12.456: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/PUT
Sep  7 06:34:12.457: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  7 06:34:12.457: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/DELETE
Sep  7 06:34:12.459: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  7 06:34:12.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/GET
Sep  7 06:34:12.462: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  7 06:34:12.462: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/HEAD
Sep  7 06:34:12.464: INFO: http.Client request:HEAD | StatusCode:200
Sep  7 06:34:12.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/OPTIONS
Sep  7 06:34:12.467: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  7 06:34:12.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/PATCH
Sep  7 06:34:12.469: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  7 06:34:12.469: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/POST
Sep  7 06:34:12.472: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  7 06:34:12.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/PUT
Sep  7 06:34:12.474: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:12.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9376" for this suite. 09/07/23 06:34:12.476
------------------------------
â€¢ [SLOW TEST] [10.064 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:02.418
    Sep  7 06:34:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename proxy 09/07/23 06:34:02.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:02.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:02.428
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  7 06:34:02.429: INFO: Creating pod...
    Sep  7 06:34:02.433: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9376" to be "running"
    Sep  7 06:34:02.435: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39847ms
    Sep  7 06:34:04.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003751973s
    Sep  7 06:34:06.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003257826s
    Sep  7 06:34:08.437: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003928059s
    Sep  7 06:34:10.438: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004921862s
    Sep  7 06:34:12.436: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 10.003142004s
    Sep  7 06:34:12.436: INFO: Pod "agnhost" satisfied condition "running"
    Sep  7 06:34:12.437: INFO: Creating service...
    Sep  7 06:34:12.444: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/DELETE
    Sep  7 06:34:12.447: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  7 06:34:12.448: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/GET
    Sep  7 06:34:12.449: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  7 06:34:12.449: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/HEAD
    Sep  7 06:34:12.451: INFO: http.Client request:HEAD | StatusCode:200
    Sep  7 06:34:12.451: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  7 06:34:12.452: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  7 06:34:12.452: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/PATCH
    Sep  7 06:34:12.454: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  7 06:34:12.454: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/POST
    Sep  7 06:34:12.456: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  7 06:34:12.456: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/pods/agnhost/proxy/some/path/with/PUT
    Sep  7 06:34:12.457: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  7 06:34:12.457: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/DELETE
    Sep  7 06:34:12.459: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  7 06:34:12.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/GET
    Sep  7 06:34:12.462: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  7 06:34:12.462: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/HEAD
    Sep  7 06:34:12.464: INFO: http.Client request:HEAD | StatusCode:200
    Sep  7 06:34:12.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/OPTIONS
    Sep  7 06:34:12.467: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  7 06:34:12.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/PATCH
    Sep  7 06:34:12.469: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  7 06:34:12.469: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/POST
    Sep  7 06:34:12.472: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  7 06:34:12.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9376/services/test-service/proxy/some/path/with/PUT
    Sep  7 06:34:12.474: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:12.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9376" for this suite. 09/07/23 06:34:12.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:12.484
Sep  7 06:34:12.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-webhook 09/07/23 06:34:12.484
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:12.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:12.495
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/07/23 06:34:12.496
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/07/23 06:34:12.835
STEP: Deploying the custom resource conversion webhook pod 09/07/23 06:34:12.84
STEP: Wait for the deployment to be ready 09/07/23 06:34:12.849
Sep  7 06:34:12.855: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 06:34:14.861
STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:14.87
Sep  7 06:34:15.871: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  7 06:34:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Creating a v1 custom resource 09/07/23 06:34:18.421
STEP: Create a v2 custom resource 09/07/23 06:34:18.436
STEP: List CRs in v1 09/07/23 06:34:18.475
STEP: List CRs in v2 09/07/23 06:34:18.478
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:18.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1751" for this suite. 09/07/23 06:34:19.013
------------------------------
â€¢ [SLOW TEST] [6.532 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:12.484
    Sep  7 06:34:12.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-webhook 09/07/23 06:34:12.484
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:12.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:12.495
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/07/23 06:34:12.496
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/07/23 06:34:12.835
    STEP: Deploying the custom resource conversion webhook pod 09/07/23 06:34:12.84
    STEP: Wait for the deployment to be ready 09/07/23 06:34:12.849
    Sep  7 06:34:12.855: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 06:34:14.861
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:14.87
    Sep  7 06:34:15.871: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  7 06:34:15.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Creating a v1 custom resource 09/07/23 06:34:18.421
    STEP: Create a v2 custom resource 09/07/23 06:34:18.436
    STEP: List CRs in v1 09/07/23 06:34:18.475
    STEP: List CRs in v2 09/07/23 06:34:18.478
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:18.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1751" for this suite. 09/07/23 06:34:19.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:19.017
Sep  7 06:34:19.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:34:19.017
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:19.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:19.029
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  7 06:34:19.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 06:34:20.3
Sep  7 06:34:20.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 create -f -'
Sep  7 06:34:22.685: INFO: stderr: ""
Sep  7 06:34:22.685: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  7 06:34:22.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-8885-crds test-cr'
Sep  7 06:34:22.744: INFO: stderr: ""
Sep  7 06:34:22.744: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  7 06:34:22.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 apply -f -'
Sep  7 06:34:22.867: INFO: stderr: ""
Sep  7 06:34:22.867: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  7 06:34:22.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-8885-crds test-cr'
Sep  7 06:34:22.919: INFO: stderr: ""
Sep  7 06:34:22.919: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/07/23 06:34:22.919
Sep  7 06:34:22.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 explain e2e-test-crd-publish-openapi-8885-crds'
Sep  7 06:34:23.038: INFO: stderr: ""
Sep  7 06:34:23.038: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8885-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-430" for this suite. 09/07/23 06:34:24.331
------------------------------
â€¢ [SLOW TEST] [5.318 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:19.017
    Sep  7 06:34:19.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename crd-publish-openapi 09/07/23 06:34:19.017
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:19.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:19.029
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  7 06:34:19.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/07/23 06:34:20.3
    Sep  7 06:34:20.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 create -f -'
    Sep  7 06:34:22.685: INFO: stderr: ""
    Sep  7 06:34:22.685: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  7 06:34:22.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-8885-crds test-cr'
    Sep  7 06:34:22.744: INFO: stderr: ""
    Sep  7 06:34:22.744: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  7 06:34:22.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 apply -f -'
    Sep  7 06:34:22.867: INFO: stderr: ""
    Sep  7 06:34:22.867: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  7 06:34:22.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 --namespace=crd-publish-openapi-430 delete e2e-test-crd-publish-openapi-8885-crds test-cr'
    Sep  7 06:34:22.919: INFO: stderr: ""
    Sep  7 06:34:22.919: INFO: stdout: "e2e-test-crd-publish-openapi-8885-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/07/23 06:34:22.919
    Sep  7 06:34:22.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=crd-publish-openapi-430 explain e2e-test-crd-publish-openapi-8885-crds'
    Sep  7 06:34:23.038: INFO: stderr: ""
    Sep  7 06:34:23.038: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8885-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-430" for this suite. 09/07/23 06:34:24.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:24.335
Sep  7 06:34:24.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:34:24.336
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:24.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:24.349
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:34:24.358
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:34:24.69
STEP: Deploying the webhook pod 09/07/23 06:34:24.694
STEP: Wait for the deployment to be ready 09/07/23 06:34:24.701
Sep  7 06:34:24.704: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/07/23 06:34:26.71
STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:26.718
Sep  7 06:34:27.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  7 06:34:27.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/07/23 06:34:28.229
STEP: Creating a custom resource that should be denied by the webhook 09/07/23 06:34:28.24
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/07/23 06:34:30.262
STEP: Updating the custom resource with disallowed data should be denied 09/07/23 06:34:30.266
STEP: Deleting the custom resource should be denied 09/07/23 06:34:30.271
STEP: Remove the offending key and value from the custom resource data 09/07/23 06:34:30.274
STEP: Deleting the updated custom resource should be successful 09/07/23 06:34:30.278
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:30.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-239" for this suite. 09/07/23 06:34:30.826
STEP: Destroying namespace "webhook-239-markers" for this suite. 09/07/23 06:34:30.831
------------------------------
â€¢ [SLOW TEST] [6.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:24.335
    Sep  7 06:34:24.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:34:24.336
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:24.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:24.349
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:34:24.358
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:34:24.69
    STEP: Deploying the webhook pod 09/07/23 06:34:24.694
    STEP: Wait for the deployment to be ready 09/07/23 06:34:24.701
    Sep  7 06:34:24.704: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/07/23 06:34:26.71
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:26.718
    Sep  7 06:34:27.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  7 06:34:27.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/07/23 06:34:28.229
    STEP: Creating a custom resource that should be denied by the webhook 09/07/23 06:34:28.24
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/07/23 06:34:30.262
    STEP: Updating the custom resource with disallowed data should be denied 09/07/23 06:34:30.266
    STEP: Deleting the custom resource should be denied 09/07/23 06:34:30.271
    STEP: Remove the offending key and value from the custom resource data 09/07/23 06:34:30.274
    STEP: Deleting the updated custom resource should be successful 09/07/23 06:34:30.278
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:30.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-239" for this suite. 09/07/23 06:34:30.826
    STEP: Destroying namespace "webhook-239-markers" for this suite. 09/07/23 06:34:30.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:30.836
Sep  7 06:34:30.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:34:30.836
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:30.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:30.845
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:34:30.853
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:34:31.069
STEP: Deploying the webhook pod 09/07/23 06:34:31.073
STEP: Wait for the deployment to be ready 09/07/23 06:34:31.083
Sep  7 06:34:31.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/07/23 06:34:33.093
STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:33.101
Sep  7 06:34:34.102: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/07/23 06:34:34.105
STEP: create a pod that should be denied by the webhook 09/07/23 06:34:34.118
STEP: create a pod that causes the webhook to hang 09/07/23 06:34:34.125
STEP: create a configmap that should be denied by the webhook 09/07/23 06:34:44.13
STEP: create a configmap that should be admitted by the webhook 09/07/23 06:34:44.135
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/07/23 06:34:44.14
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/07/23 06:34:44.144
STEP: create a namespace that bypass the webhook 09/07/23 06:34:44.147
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/07/23 06:34:44.151
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:44.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8963" for this suite. 09/07/23 06:34:44.185
STEP: Destroying namespace "webhook-8963-markers" for this suite. 09/07/23 06:34:44.19
------------------------------
â€¢ [SLOW TEST] [13.359 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:30.836
    Sep  7 06:34:30.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:34:30.836
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:30.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:30.845
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:34:30.853
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:34:31.069
    STEP: Deploying the webhook pod 09/07/23 06:34:31.073
    STEP: Wait for the deployment to be ready 09/07/23 06:34:31.083
    Sep  7 06:34:31.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/07/23 06:34:33.093
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:34:33.101
    Sep  7 06:34:34.102: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/07/23 06:34:34.105
    STEP: create a pod that should be denied by the webhook 09/07/23 06:34:34.118
    STEP: create a pod that causes the webhook to hang 09/07/23 06:34:34.125
    STEP: create a configmap that should be denied by the webhook 09/07/23 06:34:44.13
    STEP: create a configmap that should be admitted by the webhook 09/07/23 06:34:44.135
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/07/23 06:34:44.14
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/07/23 06:34:44.144
    STEP: create a namespace that bypass the webhook 09/07/23 06:34:44.147
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/07/23 06:34:44.151
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:44.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8963" for this suite. 09/07/23 06:34:44.185
    STEP: Destroying namespace "webhook-8963-markers" for this suite. 09/07/23 06:34:44.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:44.195
Sep  7 06:34:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename emptydir 09/07/23 06:34:44.196
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:44.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:44.207
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/07/23 06:34:44.209
Sep  7 06:34:44.214: INFO: Waiting up to 5m0s for pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7" in namespace "emptydir-1134" to be "Succeeded or Failed"
Sep  7 06:34:44.216: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45897ms
Sep  7 06:34:46.219: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652501s
Sep  7 06:34:48.218: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003783902s
STEP: Saw pod success 09/07/23 06:34:48.218
Sep  7 06:34:48.218: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7" satisfied condition "Succeeded or Failed"
Sep  7 06:34:48.220: INFO: Trying to get logs from node kind-worker pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 container test-container: <nil>
STEP: delete the pod 09/07/23 06:34:48.229
Sep  7 06:34:48.237: INFO: Waiting for pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 to disappear
Sep  7 06:34:48.238: INFO: Pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  7 06:34:48.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1134" for this suite. 09/07/23 06:34:48.24
------------------------------
â€¢ [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:44.195
    Sep  7 06:34:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename emptydir 09/07/23 06:34:44.196
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:44.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:44.207
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/07/23 06:34:44.209
    Sep  7 06:34:44.214: INFO: Waiting up to 5m0s for pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7" in namespace "emptydir-1134" to be "Succeeded or Failed"
    Sep  7 06:34:44.216: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.45897ms
    Sep  7 06:34:46.219: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652501s
    Sep  7 06:34:48.218: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003783902s
    STEP: Saw pod success 09/07/23 06:34:48.218
    Sep  7 06:34:48.218: INFO: Pod "pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7" satisfied condition "Succeeded or Failed"
    Sep  7 06:34:48.220: INFO: Trying to get logs from node kind-worker pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 container test-container: <nil>
    STEP: delete the pod 09/07/23 06:34:48.229
    Sep  7 06:34:48.237: INFO: Waiting for pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 to disappear
    Sep  7 06:34:48.238: INFO: Pod pod-428e1390-5d8b-4a86-9f2f-7032bd2c85f7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:34:48.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1134" for this suite. 09/07/23 06:34:48.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:34:48.244
Sep  7 06:34:48.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename statefulset 09/07/23 06:34:48.244
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:48.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:48.253
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3274 09/07/23 06:34:48.255
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3274 09/07/23 06:34:48.258
Sep  7 06:34:48.263: INFO: Found 0 stateful pods, waiting for 1
Sep  7 06:34:58.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/07/23 06:34:58.269
STEP: updating a scale subresource 09/07/23 06:34:58.271
STEP: verifying the statefulset Spec.Replicas was modified 09/07/23 06:34:58.274
STEP: Patch a scale subresource 09/07/23 06:34:58.276
STEP: verifying the statefulset Spec.Replicas was modified 09/07/23 06:34:58.28
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  7 06:34:58.284: INFO: Deleting all statefulset in ns statefulset-3274
Sep  7 06:34:58.286: INFO: Scaling statefulset ss to 0
Sep  7 06:35:08.295: INFO: Waiting for statefulset status.replicas updated to 0
Sep  7 06:35:08.296: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:08.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3274" for this suite. 09/07/23 06:35:08.305
------------------------------
â€¢ [SLOW TEST] [20.065 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:34:48.244
    Sep  7 06:34:48.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename statefulset 09/07/23 06:34:48.244
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:34:48.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:34:48.253
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3274 09/07/23 06:34:48.255
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3274 09/07/23 06:34:48.258
    Sep  7 06:34:48.263: INFO: Found 0 stateful pods, waiting for 1
    Sep  7 06:34:58.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/07/23 06:34:58.269
    STEP: updating a scale subresource 09/07/23 06:34:58.271
    STEP: verifying the statefulset Spec.Replicas was modified 09/07/23 06:34:58.274
    STEP: Patch a scale subresource 09/07/23 06:34:58.276
    STEP: verifying the statefulset Spec.Replicas was modified 09/07/23 06:34:58.28
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  7 06:34:58.284: INFO: Deleting all statefulset in ns statefulset-3274
    Sep  7 06:34:58.286: INFO: Scaling statefulset ss to 0
    Sep  7 06:35:08.295: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  7 06:35:08.296: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:08.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3274" for this suite. 09/07/23 06:35:08.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:08.309
Sep  7 06:35:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename services 09/07/23 06:35:08.31
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:08.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:08.318
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5073 09/07/23 06:35:08.32
STEP: changing the ExternalName service to type=ClusterIP 09/07/23 06:35:08.322
STEP: creating replication controller externalname-service in namespace services-5073 09/07/23 06:35:08.332
I0907 06:35:08.335511      29 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5073, replica count: 2
I0907 06:35:11.385836      29 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  7 06:35:11.385: INFO: Creating new exec pod
Sep  7 06:35:11.389: INFO: Waiting up to 5m0s for pod "execpodvmp97" in namespace "services-5073" to be "running"
Sep  7 06:35:11.390: INFO: Pod "execpodvmp97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46899ms
Sep  7 06:35:13.392: INFO: Pod "execpodvmp97": Phase="Running", Reason="", readiness=true. Elapsed: 2.00315548s
Sep  7 06:35:13.392: INFO: Pod "execpodvmp97" satisfied condition "running"
Sep  7 06:35:14.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-5073 exec execpodvmp97 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  7 06:35:14.563: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  7 06:35:14.563: INFO: stdout: ""
Sep  7 06:35:14.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-5073 exec execpodvmp97 -- /bin/sh -x -c nc -v -z -w 2 10.96.141.1 80'
Sep  7 06:35:14.730: INFO: stderr: "+ nc -v -z -w 2 10.96.141.1 80\nConnection to 10.96.141.1 80 port [tcp/http] succeeded!\n"
Sep  7 06:35:14.730: INFO: stdout: ""
Sep  7 06:35:14.730: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5073" for this suite. 09/07/23 06:35:14.746
------------------------------
â€¢ [SLOW TEST] [6.440 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:08.309
    Sep  7 06:35:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename services 09/07/23 06:35:08.31
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:08.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:08.318
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5073 09/07/23 06:35:08.32
    STEP: changing the ExternalName service to type=ClusterIP 09/07/23 06:35:08.322
    STEP: creating replication controller externalname-service in namespace services-5073 09/07/23 06:35:08.332
    I0907 06:35:08.335511      29 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5073, replica count: 2
    I0907 06:35:11.385836      29 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  7 06:35:11.385: INFO: Creating new exec pod
    Sep  7 06:35:11.389: INFO: Waiting up to 5m0s for pod "execpodvmp97" in namespace "services-5073" to be "running"
    Sep  7 06:35:11.390: INFO: Pod "execpodvmp97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46899ms
    Sep  7 06:35:13.392: INFO: Pod "execpodvmp97": Phase="Running", Reason="", readiness=true. Elapsed: 2.00315548s
    Sep  7 06:35:13.392: INFO: Pod "execpodvmp97" satisfied condition "running"
    Sep  7 06:35:14.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-5073 exec execpodvmp97 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  7 06:35:14.563: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  7 06:35:14.563: INFO: stdout: ""
    Sep  7 06:35:14.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2623828026 --namespace=services-5073 exec execpodvmp97 -- /bin/sh -x -c nc -v -z -w 2 10.96.141.1 80'
    Sep  7 06:35:14.730: INFO: stderr: "+ nc -v -z -w 2 10.96.141.1 80\nConnection to 10.96.141.1 80 port [tcp/http] succeeded!\n"
    Sep  7 06:35:14.730: INFO: stdout: ""
    Sep  7 06:35:14.730: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5073" for this suite. 09/07/23 06:35:14.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:14.75
Sep  7 06:35:14.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename downward-api 09/07/23 06:35:14.751
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:14.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:14.761
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:35:14.763
Sep  7 06:35:14.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd" in namespace "downward-api-4684" to be "Succeeded or Failed"
Sep  7 06:35:14.771: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50791ms
Sep  7 06:35:16.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00483031s
Sep  7 06:35:18.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00475942s
STEP: Saw pod success 09/07/23 06:35:18.774
Sep  7 06:35:18.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd" satisfied condition "Succeeded or Failed"
Sep  7 06:35:18.776: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd container client-container: <nil>
STEP: delete the pod 09/07/23 06:35:18.785
Sep  7 06:35:18.793: INFO: Waiting for pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd to disappear
Sep  7 06:35:18.795: INFO: Pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:18.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4684" for this suite. 09/07/23 06:35:18.797
------------------------------
â€¢ [4.050 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:14.75
    Sep  7 06:35:14.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename downward-api 09/07/23 06:35:14.751
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:14.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:14.761
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:35:14.763
    Sep  7 06:35:14.769: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd" in namespace "downward-api-4684" to be "Succeeded or Failed"
    Sep  7 06:35:14.771: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50791ms
    Sep  7 06:35:16.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00483031s
    Sep  7 06:35:18.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00475942s
    STEP: Saw pod success 09/07/23 06:35:18.774
    Sep  7 06:35:18.774: INFO: Pod "downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd" satisfied condition "Succeeded or Failed"
    Sep  7 06:35:18.776: INFO: Trying to get logs from node kind-worker2 pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd container client-container: <nil>
    STEP: delete the pod 09/07/23 06:35:18.785
    Sep  7 06:35:18.793: INFO: Waiting for pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd to disappear
    Sep  7 06:35:18.795: INFO: Pod downwardapi-volume-b21d2e6f-c036-4039-ab23-f4377c2072bd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:18.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4684" for this suite. 09/07/23 06:35:18.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:18.813
Sep  7 06:35:18.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:35:18.814
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:18.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:18.826
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/07/23 06:35:18.828
Sep  7 06:35:18.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2" in namespace "projected-5071" to be "Succeeded or Failed"
Sep  7 06:35:18.834: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2765ms
Sep  7 06:35:20.837: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00424236s
Sep  7 06:35:22.836: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003892719s
STEP: Saw pod success 09/07/23 06:35:22.836
Sep  7 06:35:22.836: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2" satisfied condition "Succeeded or Failed"
Sep  7 06:35:22.838: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 container client-container: <nil>
STEP: delete the pod 09/07/23 06:35:22.842
Sep  7 06:35:22.850: INFO: Waiting for pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 to disappear
Sep  7 06:35:22.852: INFO: Pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5071" for this suite. 09/07/23 06:35:22.854
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:18.813
    Sep  7 06:35:18.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:35:18.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:18.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:18.826
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/07/23 06:35:18.828
    Sep  7 06:35:18.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2" in namespace "projected-5071" to be "Succeeded or Failed"
    Sep  7 06:35:18.834: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.2765ms
    Sep  7 06:35:20.837: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Running", Reason="", readiness=false. Elapsed: 2.00424236s
    Sep  7 06:35:22.836: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003892719s
    STEP: Saw pod success 09/07/23 06:35:22.836
    Sep  7 06:35:22.836: INFO: Pod "downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2" satisfied condition "Succeeded or Failed"
    Sep  7 06:35:22.838: INFO: Trying to get logs from node kind-worker pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 container client-container: <nil>
    STEP: delete the pod 09/07/23 06:35:22.842
    Sep  7 06:35:22.850: INFO: Waiting for pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 to disappear
    Sep  7 06:35:22.852: INFO: Pod downwardapi-volume-0f4e5cc4-e0ed-4299-89ec-d9883ed617c2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5071" for this suite. 09/07/23 06:35:22.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:22.858
Sep  7 06:35:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename resourcequota 09/07/23 06:35:22.859
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:22.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:22.868
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/07/23 06:35:22.869
STEP: Creating a ResourceQuota 09/07/23 06:35:27.872
STEP: Ensuring resource quota status is calculated 09/07/23 06:35:27.875
STEP: Creating a ReplicationController 09/07/23 06:35:29.878
STEP: Ensuring resource quota status captures replication controller creation 09/07/23 06:35:29.897
STEP: Deleting a ReplicationController 09/07/23 06:35:31.9
STEP: Ensuring resource quota status released usage 09/07/23 06:35:31.903
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:33.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7834" for this suite. 09/07/23 06:35:33.909
------------------------------
â€¢ [SLOW TEST] [11.055 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:22.858
    Sep  7 06:35:22.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename resourcequota 09/07/23 06:35:22.859
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:22.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:22.868
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/07/23 06:35:22.869
    STEP: Creating a ResourceQuota 09/07/23 06:35:27.872
    STEP: Ensuring resource quota status is calculated 09/07/23 06:35:27.875
    STEP: Creating a ReplicationController 09/07/23 06:35:29.878
    STEP: Ensuring resource quota status captures replication controller creation 09/07/23 06:35:29.897
    STEP: Deleting a ReplicationController 09/07/23 06:35:31.9
    STEP: Ensuring resource quota status released usage 09/07/23 06:35:31.903
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:33.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7834" for this suite. 09/07/23 06:35:33.909
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:33.913
Sep  7 06:35:33.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename webhook 09/07/23 06:35:33.914
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:33.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:33.926
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/07/23 06:35:33.934
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:35:34.189
STEP: Deploying the webhook pod 09/07/23 06:35:34.194
STEP: Wait for the deployment to be ready 09/07/23 06:35:34.204
Sep  7 06:35:34.207: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 09/07/23 06:35:36.213
STEP: Verifying the service has paired with the endpoint 09/07/23 06:35:36.22
Sep  7 06:35:37.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/07/23 06:35:37.223
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/07/23 06:35:37.224
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/07/23 06:35:37.224
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/07/23 06:35:37.224
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/07/23 06:35:37.225
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/07/23 06:35:37.225
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/07/23 06:35:37.225
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2734" for this suite. 09/07/23 06:35:37.25
STEP: Destroying namespace "webhook-2734-markers" for this suite. 09/07/23 06:35:37.253
------------------------------
â€¢ [3.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:33.913
    Sep  7 06:35:33.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename webhook 09/07/23 06:35:33.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:33.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:33.926
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/07/23 06:35:33.934
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/07/23 06:35:34.189
    STEP: Deploying the webhook pod 09/07/23 06:35:34.194
    STEP: Wait for the deployment to be ready 09/07/23 06:35:34.204
    Sep  7 06:35:34.207: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 09/07/23 06:35:36.213
    STEP: Verifying the service has paired with the endpoint 09/07/23 06:35:36.22
    Sep  7 06:35:37.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/07/23 06:35:37.223
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/07/23 06:35:37.224
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/07/23 06:35:37.224
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/07/23 06:35:37.224
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/07/23 06:35:37.225
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/07/23 06:35:37.225
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/07/23 06:35:37.225
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:37.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2734" for this suite. 09/07/23 06:35:37.25
    STEP: Destroying namespace "webhook-2734-markers" for this suite. 09/07/23 06:35:37.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:37.258
Sep  7 06:35:37.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename replication-controller 09/07/23 06:35:37.259
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:37.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:37.27
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/07/23 06:35:37.274
STEP: waiting for RC to be added 09/07/23 06:35:37.277
STEP: waiting for available Replicas 09/07/23 06:35:37.277
STEP: patching ReplicationController 09/07/23 06:35:38.641
STEP: waiting for RC to be modified 09/07/23 06:35:38.647
STEP: patching ReplicationController status 09/07/23 06:35:38.647
STEP: waiting for RC to be modified 09/07/23 06:35:38.651
STEP: waiting for available Replicas 09/07/23 06:35:38.652
STEP: fetching ReplicationController status 09/07/23 06:35:38.662
STEP: patching ReplicationController scale 09/07/23 06:35:38.664
STEP: waiting for RC to be modified 09/07/23 06:35:38.671
STEP: waiting for ReplicationController's scale to be the max amount 09/07/23 06:35:38.672
STEP: fetching ReplicationController; ensuring that it's patched 09/07/23 06:35:39.686
STEP: updating ReplicationController status 09/07/23 06:35:39.688
STEP: waiting for RC to be modified 09/07/23 06:35:39.691
STEP: listing all ReplicationControllers 09/07/23 06:35:39.692
STEP: checking that ReplicationController has expected values 09/07/23 06:35:39.693
STEP: deleting ReplicationControllers by collection 09/07/23 06:35:39.693
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/07/23 06:35:39.698
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:39.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5455" for this suite. 09/07/23 06:35:39.731
------------------------------
â€¢ [2.477 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:37.258
    Sep  7 06:35:37.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename replication-controller 09/07/23 06:35:37.259
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:37.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:37.27
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/07/23 06:35:37.274
    STEP: waiting for RC to be added 09/07/23 06:35:37.277
    STEP: waiting for available Replicas 09/07/23 06:35:37.277
    STEP: patching ReplicationController 09/07/23 06:35:38.641
    STEP: waiting for RC to be modified 09/07/23 06:35:38.647
    STEP: patching ReplicationController status 09/07/23 06:35:38.647
    STEP: waiting for RC to be modified 09/07/23 06:35:38.651
    STEP: waiting for available Replicas 09/07/23 06:35:38.652
    STEP: fetching ReplicationController status 09/07/23 06:35:38.662
    STEP: patching ReplicationController scale 09/07/23 06:35:38.664
    STEP: waiting for RC to be modified 09/07/23 06:35:38.671
    STEP: waiting for ReplicationController's scale to be the max amount 09/07/23 06:35:38.672
    STEP: fetching ReplicationController; ensuring that it's patched 09/07/23 06:35:39.686
    STEP: updating ReplicationController status 09/07/23 06:35:39.688
    STEP: waiting for RC to be modified 09/07/23 06:35:39.691
    STEP: listing all ReplicationControllers 09/07/23 06:35:39.692
    STEP: checking that ReplicationController has expected values 09/07/23 06:35:39.693
    STEP: deleting ReplicationControllers by collection 09/07/23 06:35:39.693
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/07/23 06:35:39.698
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:39.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5455" for this suite. 09/07/23 06:35:39.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:39.736
Sep  7 06:35:39.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename projected 09/07/23 06:35:39.736
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:39.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:39.744
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-b0e6d632-cf90-49db-af44-c346b9b5336c 09/07/23 06:35:39.746
STEP: Creating a pod to test consume configMaps 09/07/23 06:35:39.75
Sep  7 06:35:39.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f" in namespace "projected-127" to be "Succeeded or Failed"
Sep  7 06:35:39.755: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28769ms
Sep  7 06:35:41.758: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003980599s
Sep  7 06:35:43.759: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005333228s
STEP: Saw pod success 09/07/23 06:35:43.76
Sep  7 06:35:43.760: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f" satisfied condition "Succeeded or Failed"
Sep  7 06:35:43.761: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/07/23 06:35:43.765
Sep  7 06:35:43.775: INFO: Waiting for pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f to disappear
Sep  7 06:35:43.776: INFO: Pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:43.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-127" for this suite. 09/07/23 06:35:43.778
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:39.736
    Sep  7 06:35:39.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename projected 09/07/23 06:35:39.736
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:39.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:39.744
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-b0e6d632-cf90-49db-af44-c346b9b5336c 09/07/23 06:35:39.746
    STEP: Creating a pod to test consume configMaps 09/07/23 06:35:39.75
    Sep  7 06:35:39.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f" in namespace "projected-127" to be "Succeeded or Failed"
    Sep  7 06:35:39.755: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.28769ms
    Sep  7 06:35:41.758: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003980599s
    Sep  7 06:35:43.759: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005333228s
    STEP: Saw pod success 09/07/23 06:35:43.76
    Sep  7 06:35:43.760: INFO: Pod "pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f" satisfied condition "Succeeded or Failed"
    Sep  7 06:35:43.761: INFO: Trying to get logs from node kind-worker2 pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/07/23 06:35:43.765
    Sep  7 06:35:43.775: INFO: Waiting for pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f to disappear
    Sep  7 06:35:43.776: INFO: Pod pod-projected-configmaps-a611ce2d-e6bc-4edd-bf4e-b99c2770cf1f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:43.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-127" for this suite. 09/07/23 06:35:43.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/07/23 06:35:43.783
Sep  7 06:35:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: Building a namespace api object, basename pods 09/07/23 06:35:43.784
STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:43.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:43.792
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  7 06:35:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
STEP: creating the pod 09/07/23 06:35:43.794
STEP: submitting the pod to kubernetes 09/07/23 06:35:43.794
Sep  7 06:35:43.800: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83" in namespace "pods-4263" to be "running and ready"
Sep  7 06:35:43.801: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248821ms
Sep  7 06:35:43.801: INFO: The phase of Pod pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83 is Pending, waiting for it to be Running (with Ready = true)
Sep  7 06:35:45.804: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83": Phase="Running", Reason="", readiness=true. Elapsed: 2.003516069s
Sep  7 06:35:45.804: INFO: The phase of Pod pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83 is Running (Ready = true)
Sep  7 06:35:45.804: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  7 06:35:45.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4263" for this suite. 09/07/23 06:35:45.935
------------------------------
â€¢ [2.155 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/07/23 06:35:43.783
    Sep  7 06:35:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: Building a namespace api object, basename pods 09/07/23 06:35:43.784
    STEP: Waiting for a default service account to be provisioned in namespace 09/07/23 06:35:43.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/07/23 06:35:43.792
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  7 06:35:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2623828026
    STEP: creating the pod 09/07/23 06:35:43.794
    STEP: submitting the pod to kubernetes 09/07/23 06:35:43.794
    Sep  7 06:35:43.800: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83" in namespace "pods-4263" to be "running and ready"
    Sep  7 06:35:43.801: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248821ms
    Sep  7 06:35:43.801: INFO: The phase of Pod pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83 is Pending, waiting for it to be Running (with Ready = true)
    Sep  7 06:35:45.804: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83": Phase="Running", Reason="", readiness=true. Elapsed: 2.003516069s
    Sep  7 06:35:45.804: INFO: The phase of Pod pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83 is Running (Ready = true)
    Sep  7 06:35:45.804: INFO: Pod "pod-exec-websocket-40af683a-dde2-4437-a7be-22938d92ba83" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  7 06:35:45.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4263" for this suite. 09/07/23 06:35:45.935
  << End Captured GinkgoWriter Output
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  7 06:35:45.939: INFO: Running AfterSuite actions on node 1
Sep  7 06:35:45.939: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  7 06:35:45.939: INFO: Running AfterSuite actions on node 1
    Sep  7 06:35:45.939: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.053 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5428.959 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h30m29.288343699s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

